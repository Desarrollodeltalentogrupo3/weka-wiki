{
    "docs": [
        {
            "location": "/",
            "text": "Please note, the migration from the old wiki in this new format is still work in progress!\n\n\nNew to Weka?\n\n\nHave a look at the \nFrequently Asked Questions\n (= FAQ), the \nTroubleshooting\n article or search the \nmailing list archives\n.\nDon't forget to check out the documentation on the \nWeka homepage\n and the\n\nLearning Resources\n.\n\n\nYou have questions regarding Weka?\n\n\nYou can post questions to the \nWeka mailing list\n. Please keep\nin mind that you cannot expect an immediate answer to your question(s). The\nquestions are mainly answered by volunteers, Weka users just like you.\n\n\nYou are looking for packages?\n\n\nWith Weka 3.7.2 and later, you can easily install packages through Weka's\npackage manager interface, either official ones or unofficial ones.\nHave a look at the \nPackages\n article for more information on this topic.\n\n\nYou want to contribute to the wiki?\n\n\nThe wiki is based on \nMarkdown\n articles, which are turned into static HTML using \nMkDocs\n (see \nhere\n for details on writing articles). The content of the wiki is available as \nrepository on GitHub\n. Feel free to add/update and then do a \npull request\n.\n\n\nYou found a bug?\n\n\nPlease post the bug report to the \nWeka mailing list\n. The\nfollowing information will help tracking things down:\n\n\n\n\nversion of Weka (e.g., 3.9.2)\n\n\noperating system (e.g., Windows 10 or Ubuntu 16.04 64bit)\n\n\nJava version (e.g., 1.8.0_162 64bit)\n\n\n\n\nYou can also run the following command in the SimpleCLI and attach the generate output as text file to your post:\n\n\n    java weka.core.SystemInfo",
            "title": "Home"
        },
        {
            "location": "/#new-to-weka",
            "text": "Have a look at the  Frequently Asked Questions  (= FAQ), the  Troubleshooting  article or search the  mailing list archives .\nDon't forget to check out the documentation on the  Weka homepage  and the Learning Resources .",
            "title": "New to Weka?"
        },
        {
            "location": "/#you-have-questions-regarding-weka",
            "text": "You can post questions to the  Weka mailing list . Please keep\nin mind that you cannot expect an immediate answer to your question(s). The\nquestions are mainly answered by volunteers, Weka users just like you.",
            "title": "You have questions regarding Weka?"
        },
        {
            "location": "/#you-are-looking-for-packages",
            "text": "With Weka 3.7.2 and later, you can easily install packages through Weka's\npackage manager interface, either official ones or unofficial ones.\nHave a look at the  Packages  article for more information on this topic.",
            "title": "You are looking for packages?"
        },
        {
            "location": "/#you-want-to-contribute-to-the-wiki",
            "text": "The wiki is based on  Markdown  articles, which are turned into static HTML using  MkDocs  (see  here  for details on writing articles). The content of the wiki is available as  repository on GitHub . Feel free to add/update and then do a  pull request .",
            "title": "You want to contribute to the wiki?"
        },
        {
            "location": "/#you-found-a-bug",
            "text": "Please post the bug report to the  Weka mailing list . The\nfollowing information will help tracking things down:   version of Weka (e.g., 3.9.2)  operating system (e.g., Windows 10 or Ubuntu 16.04 64bit)  Java version (e.g., 1.8.0_162 64bit)   You can also run the following command in the SimpleCLI and attach the generate output as text file to your post:      java weka.core.SystemInfo",
            "title": "You found a bug?"
        },
        {
            "location": "/faq/",
            "text": "General\n\n\n\n\nWhat are the principal release branches of Weka?\n\n\nWhere can I get old versions of WEKA?\n\n\nHow do I get the latest bugfixes?\n\n\nCan I check my CLASSPATH from within WEKA?\n\n\nWhere is my home directory located?\n\n\nCan I check how much memory is available for WEKA?\n\n\nCan I use WEKA in commercial applications?\n\n\n\n\nBasic usage\n\n\n\n\nCan I use CSV files?\n\n\nHow do I perform CSV file conversion?\n\n\nHow do I divide a dataset into training and test set?\n\n\nHow do I generate compatible train and test sets that get processed with a filter?\n\n\nHow do I perform attribute selection?\n\n\nHow do I perform clustering?\n\n\nWhere do I find visualization of classifiers, etc.?\n\n\nHow do I perform text classification?\n\n\nHow can I perform multi-instance learning in WEKA?\n\n\nHow do I perform cost-sensitive classification?\n\n\nHow do I make predictions with a trained model?\n\n\nWhy am I missing certain nominal or string values from sparse instances?\n\n\nCan I use WEKA for time series analysis?\n\n\nDoes WEKA support multi-label classification?\n\n\nHow do I perform one-class classification?\n\n\nCan I make a screenshot of a plot or graph directly in WEKA?\n\n\nHow do I use the package manager?\n\n\nWhat do I do if the package manager does not start?\n\n\n\n\nAdvanced usage\n\n\n\n\nHow can I track instances in WEKA?\n\n\nHow do I use ID attributes?\n\n\nHow do I connect to a database?\n\n\nHow do I use WEKA from the command line?\n\n\nCan I tune the parameters of a classifier?\n\n\nHow do I generate Learning curves?\n\n\nWhere can I find information regarding ROC curves?\n\n\nI have unbalanced data - now what?\n\n\nCan I run an experiment using clusterers in the Experimenter?\n\n\nHow can I use transactional data in Weka?\n\n\nHow can I use Weka with Matlab or Octave?\n\n\n\n\nCustomizing Weka\n\n\n\n\nCan I change the colors (background, axes, etc.) of the plots in WEKA?\n\n\nHow do I add a new classifier, filter, kernel, etc\n\n\n\n\nUsing third-party tools\n\n\n\n\nHow do I use libsvm in WEKA?\n\n\nThe snowball stemmers don't work, what am I doing wrong?\n\n\n\n\nDeveloping with WEKA\n\n\n\n\nWhere can I get WEKA's source code?\n\n\nHow do I compile WEKA?\n\n\nWhat is Subversion and what do I need to do to access it?\n\n\nHow do I use WEKA's classes in my own code?\n\n\nHow do I write a new classifier or filter?\n\n\nCan I compile WEKA into native code?\n\n\nCan I use WEKA from C#?\n\n\nCan I use WEKA from Python?\n\n\nCan I use WEKA from Groovy?\n\n\nSerialization is nice, but what about generating actual Java code from WEKA classes?\n\n\nHow are packages structured for the package management system?\n\n\nPluggable evaluation metrics for classification/regression\n\n\nHow can I contribute to WEKA?\n\n\n\n\nWindows\n\n\n\n\nHow do I modify the CLASSPATH?\n\n\nHow do I modify the RunWeka.bat file?\n\n\nCan I process UTF-8 datasets or files?\n\n\nHow do I run the Windows Weka installer in silent mode?\n\n\n\n\nTroubleshooting\n\n\n\n\nI have Weka download problems - what's going wrong?\n\n\nMy ARFF file doesn't load - why?\n\n\nWhat does nominal value not declared in header, read Token[X], line Y mean?\n\n\nHow do I get rid of this OutOfMemoryException?\n\n\nHow do I deal with a StackOverflowError?\n\n\nWhy do I get the error message 'training and test set are not compatible'?\n\n\nCouldn't read from database: unknown data type\n\n\nTrying to add JDBC driver: ... - Error, not in CLASSPATH?\n\n\nI cannot process large datasets - any ideas?\n\n\nSee \nTroubleshooting\n article for more troubleshooting.",
            "title": "FAQ"
        },
        {
            "location": "/faq/#general",
            "text": "What are the principal release branches of Weka?  Where can I get old versions of WEKA?  How do I get the latest bugfixes?  Can I check my CLASSPATH from within WEKA?  Where is my home directory located?  Can I check how much memory is available for WEKA?  Can I use WEKA in commercial applications?",
            "title": "General"
        },
        {
            "location": "/faq/#basic-usage",
            "text": "Can I use CSV files?  How do I perform CSV file conversion?  How do I divide a dataset into training and test set?  How do I generate compatible train and test sets that get processed with a filter?  How do I perform attribute selection?  How do I perform clustering?  Where do I find visualization of classifiers, etc.?  How do I perform text classification?  How can I perform multi-instance learning in WEKA?  How do I perform cost-sensitive classification?  How do I make predictions with a trained model?  Why am I missing certain nominal or string values from sparse instances?  Can I use WEKA for time series analysis?  Does WEKA support multi-label classification?  How do I perform one-class classification?  Can I make a screenshot of a plot or graph directly in WEKA?  How do I use the package manager?  What do I do if the package manager does not start?",
            "title": "Basic usage"
        },
        {
            "location": "/faq/#advanced-usage",
            "text": "How can I track instances in WEKA?  How do I use ID attributes?  How do I connect to a database?  How do I use WEKA from the command line?  Can I tune the parameters of a classifier?  How do I generate Learning curves?  Where can I find information regarding ROC curves?  I have unbalanced data - now what?  Can I run an experiment using clusterers in the Experimenter?  How can I use transactional data in Weka?  How can I use Weka with Matlab or Octave?",
            "title": "Advanced usage"
        },
        {
            "location": "/faq/#customizing-weka",
            "text": "Can I change the colors (background, axes, etc.) of the plots in WEKA?  How do I add a new classifier, filter, kernel, etc",
            "title": "Customizing Weka"
        },
        {
            "location": "/faq/#using-third-party-tools",
            "text": "How do I use libsvm in WEKA?  The snowball stemmers don't work, what am I doing wrong?",
            "title": "Using third-party tools"
        },
        {
            "location": "/faq/#developing-with-weka",
            "text": "Where can I get WEKA's source code?  How do I compile WEKA?  What is Subversion and what do I need to do to access it?  How do I use WEKA's classes in my own code?  How do I write a new classifier or filter?  Can I compile WEKA into native code?  Can I use WEKA from C#?  Can I use WEKA from Python?  Can I use WEKA from Groovy?  Serialization is nice, but what about generating actual Java code from WEKA classes?  How are packages structured for the package management system?  Pluggable evaluation metrics for classification/regression  How can I contribute to WEKA?",
            "title": "Developing with WEKA"
        },
        {
            "location": "/faq/#windows",
            "text": "How do I modify the CLASSPATH?  How do I modify the RunWeka.bat file?  Can I process UTF-8 datasets or files?  How do I run the Windows Weka installer in silent mode?",
            "title": "Windows"
        },
        {
            "location": "/faq/#troubleshooting",
            "text": "I have Weka download problems - what's going wrong?  My ARFF file doesn't load - why?  What does nominal value not declared in header, read Token[X], line Y mean?  How do I get rid of this OutOfMemoryException?  How do I deal with a StackOverflowError?  Why do I get the error message 'training and test set are not compatible'?  Couldn't read from database: unknown data type  Trying to add JDBC driver: ... - Error, not in CLASSPATH?  I cannot process large datasets - any ideas?  See  Troubleshooting  article for more troubleshooting.",
            "title": "Troubleshooting"
        },
        {
            "location": "/not_so_faq/",
            "text": "Associators\n\n\n\n\nHow do I use the associator GeneralizedSequentialPatterns?\n\n\n\n\nClassifiers\n\n\n\n\nWhat do those numbers mean in a J48 tree?",
            "title": "Not so FAQ"
        },
        {
            "location": "/not_so_faq/#associators",
            "text": "How do I use the associator GeneralizedSequentialPatterns?",
            "title": "Associators"
        },
        {
            "location": "/not_so_faq/#classifiers",
            "text": "What do those numbers mean in a J48 tree?",
            "title": "Classifiers"
        },
        {
            "location": "/troubleshooting/",
            "text": "Click on one of the links for more information:\n\n\n\n\nWeka download problems\n\n\nOutOfMemoryException\n\n\nStackOverflowError\n\n\njust-in-time (JIT) compiler\n\n\nCSV file conversion\n\n\nARFF file doesn't load\n\n\nError message: nominal value not declared in header, read Token[X], line Y\n\n\nSpaces in labels of ARFF files\n\n\nSingle quotes in labels of ARFF files\n\n\nCLASSPATH problems\n\n\nInstance ID\n\n\nVisualization\n\n\nMemory consumption and Garbage collector\n\n\nGUIChooser starts but not Experimenter or Explorer\n\n\nKnowledgeFlow toolbars are empty\n\n\nOSX Mountain Lion - Weka x-y-z is damaged and can't be installed. You should eject the disk image\n\n\nUbuntu 18.04: WARNING: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n\n\n\n\nSee also the \nFrequently Asked Questions\n.",
            "title": "Troubleshooting"
        },
        {
            "location": "/learning_resources/",
            "text": "Videos\n\n\n\n\nYoutube channel of Data Mining with Weka MOOCs\n\n\n\n\nTutorials\n\n\n\n\nLearn Data Science Online\n\n\n\n\nMOOCs\n\n\n\n\nData Mining with Weka\n\n\nMore Data Mining with Weka\n\n\nAdvanced Data Mining with Weka",
            "title": "Learning resources"
        },
        {
            "location": "/learning_resources/#videos",
            "text": "Youtube channel of Data Mining with Weka MOOCs",
            "title": "Videos"
        },
        {
            "location": "/learning_resources/#tutorials",
            "text": "Learn Data Science Online",
            "title": "Tutorials"
        },
        {
            "location": "/learning_resources/#moocs",
            "text": "Data Mining with Weka  More Data Mining with Weka  Advanced Data Mining with Weka",
            "title": "MOOCs"
        },
        {
            "location": "/using_the_api/",
            "text": "Several articles describe certain aspects of using the Weka API:\n\n\n\n\nUse Weka in your Java code\n\n\nWeka Examples\n\n\nGenerating cross-validation folds\n\n\nCreating an ARFF file\n\n\nBinarize Attribute\n\n\nARFF files from Text Collections\n\n\nAdding attributes to a dataset\n\n\nSave Instances to an ARFF File\n\n\nGenerating ROC curve\n\n\nVisualizing ROC curve\n\n\nSerialization\n\n\n\n\nIt is possible to use Weka through \nJupyter notebooks\n \nas well, see the following article for more information:\n\n\n\n\nJupyter notebooks",
            "title": "Using the API"
        },
        {
            "location": "/extending_weka/",
            "text": "The following articles describe how you can extend Weka:\n\n\n\n\nWriting a new Filter\n\n\nWriting a new Classifier",
            "title": "Extending Weka"
        },
        {
            "location": "/arff/",
            "text": "Data format\n\n\nA description of the ARFF format can be found in the following articles:\n\n\n\n\nARFF (stable version)\n\n\nARFF (developer version)\n\n\n\n\nCreating an ARFF file\n\n\nHow to create an ARFF file on the fly, i.e., inside Java, you can find here:\n\n\n\n\nCreating an ARFF file\n\n\n\n\nSee also\n\n\n\n\nARFF Syntax Highlighting\n for various editors\n\n\n\n\nLinks\n\n\n\n\nARFF2DB.py\n - a Python script for importing an ARFF file into a database (similar functionality to the \nweka.core.converters.DatabaseSaver\n class)",
            "title": "ARFF Format"
        },
        {
            "location": "/arff/#data-format",
            "text": "A description of the ARFF format can be found in the following articles:   ARFF (stable version)  ARFF (developer version)",
            "title": "Data format"
        },
        {
            "location": "/arff/#creating-an-arff-file",
            "text": "How to create an ARFF file on the fly, i.e., inside Java, you can find here:   Creating an ARFF file",
            "title": "Creating an ARFF file"
        },
        {
            "location": "/arff/#see-also",
            "text": "ARFF Syntax Highlighting  for various editors",
            "title": "See also"
        },
        {
            "location": "/arff/#links",
            "text": "ARFF2DB.py  - a Python script for importing an ARFF file into a database (similar functionality to the  weka.core.converters.DatabaseSaver  class)",
            "title": "Links"
        },
        {
            "location": "/packages/",
            "text": "Weka 3.7.2 introduced support for packages, making it easy to extend Weka\nwithout having to recompile or patch the underlying Weka installation.\n\n\nHere are some pointers for using and developing packages:\n\n\n\n\nHow do I use the package manager?\n\n\nUnofficial packages\n\n\nHow are packages structured for the package management system?",
            "title": "Packages"
        },
        {
            "location": "/mailing_list/",
            "text": "The WEKA Mailing list can be found here:\n\n\n\n\nList\n for subscribing/unsubscribing to the list\n\n\nArchives\n (\nMirror 1\n, \nMirror 2\n) for searching previous posted messages\n\n\n\n\nBefore posting, please read the \nMailing List Etiquette\n.",
            "title": "Mailing list"
        },
        {
            "location": "/weka_experiment_database_utils.props/",
            "text": "File\n\n\nweka/experiment/DatabaseUtils.props\n\n\nDescription\n\n\nDefines the \nDatabases\n setup, i.e., JDBC driver information, JDBC URL, database type conversion, etc.\n\n\nVersion\n\n\n\n\n\n\n\n\n= 3.1.3\n\n\n\n\n\n\n\n\nFields\n\n\n\n\n\n\nGeneral\n\n\n\n\n\n\njdbcDriver\n\n\nthe comma-separated list of jdbc drivers to try loading\n\n\n\n\n\n\njdbcURL\n\n\nthe JDBC URL to the database\n\n\n\n\n\n\n\n\n\n\nTable creation\n\n\n\n\n\n\nCREATE_STRING\n\n\ndatabase specific datatype, e.g., \nTEXT\n\n\n\n\n\n\nCREATE_INT\n\n\ndatabase specific datatype, e.g., \nINT\n\n\n\n\n\n\nCREATE_DOUBLE\n\n\ndatabase specific datatype, e.g., \nDOUBLE\n\n\n\n\n\n\n\n\n\n\nDatabase flags\n\n\n\n\n\n\ncheckUpperCaseNames\n\n\nnecessary if database turns column names into upper case ones, e.g., HSQLDB\n\n\n\n\n\n\ncheckLowerCaseNames\n (> 3.5.3)\n\n\nnecessary if database turns column names into lower case ones, e.g., PostgreSQL\n\n\n\n\n\n\ncheckForTable\n (> 3.5.3)\n\n\nChecks whether the tables in the query are available in the meta-data of the JDBC Connection. Some tables, like \npg_tables\n, exist but are not available through the meta-data\n\n\n\n\n\n\nsetAutoCommit\n\n\nsetting for \njava.sql.Connection.setAutoCommit(boolean)\n\n\n\n\n\n\ncreateIndex\n\n\nwhether to create a primary key \nKey_IDX\n in the results table of an experiment\n\n\n\n\n\n\n\n\n\n\nSpecial flags for DatabaseLoader/Saver (package \nweka.core.converters\n)\n\n\n\n\n\n\nnominalToStringLimit\n (>= 3.4.1)\n\n\nbeyond this limit, nominal columns are loaded as STRING attributes and no longer as NOMINAL ones\n\n\n\n\n\n\nidColumn\n (>= 3.4.1)\n\n\nunique key in table that allows ordering for incremental loading\n\n\n\n\n\n\nKeywords\n (> 3.5.8, > 3.6.0)\n\n\nlists all the reserved keywords of the current database type\n\n\ndefault: \nAND,ASC,BY,DESC,FROM,GROUP,INSERT,ORDER,SELECT,UPDATE,WHERE\n\n\n\n\n\n\nKeywordsMaskChar\n (> 3.5.8, > 3.6.0)\n\n\nthe character to append to attribute names/table names that would be interpreted as keywords by the database, in order to avoid exceptions when executing SQL commands\ndefaut: \n_\n\n\n\n\n\n\n\n\n\n\nDatabase type mapping\n\n\nIn order to import the data from database correctly into Weka, one has to specify what JDBC datatype corresponds to what Java SQL retrieval method. Here's an overview of how the Java types are mapped to Weka's attribute types:\n\n\n\n\n\n\n\n\nJava type\n\n\nJava method\n\n\nIdentifier\n\n\nWeka attribute type\n\n\nVersion\n\n\n\n\n\n\n\n\n\n\nString\n\n\ngetString()\n\n\n0\n\n\nnominal\n\n\n\n\n\n\n\n\nboolean\n\n\ngetBoolean()\n\n\n1\n\n\nnominal\n\n\n\n\n\n\n\n\ndouble\n\n\ngetDouble()\n\n\n2\n\n\nnumeric\n\n\n\n\n\n\n\n\nbyte\n\n\ngetByte()\n\n\n3\n\n\nnumeric\n\n\n\n\n\n\n\n\nshort\n\n\ngetByte()\n\n\n4\n\n\nnumeric\n\n\n\n\n\n\n\n\nint\n\n\ngetInteger()\n\n\n5\n\n\nnumeric\n\n\n\n\n\n\n\n\nlong\n\n\ngetLong()\n\n\n6\n\n\nnumeric\n\n\n\n\n\n\n\n\nfloat\n\n\ngetFloat()\n\n\n7\n\n\nnumeric\n\n\n\n\n\n\n\n\ndate\n\n\ngetDate()\n\n\n8\n\n\ndate\n\n\n\n\n\n\n\n\ntext\n\n\ngetString()\n\n\n9\n\n\nstring\n\n\n>3.5.5\n\n\n\n\n\n\ntime\n\n\ngetTime()\n\n\n10\n\n\nstring\n\n\n>3.5.8\n\n\n\n\n\n\n\n\nIn the props file one lists now the type names that the database returns and what Java type it represents (via the identifier), e.g.:\n\n\n CHAR=0\n VARCHAR=0\n\n\n\n\nCHAR\n and \nVARCHAR\n are both String types, hence they are interpreted as \nString\n (identifier \n0\n)\n\n\nNote:\n in case database types have blanks, one needs to replace those blanks with an underscore, e.g., \nDOUBLE PRECISION\n must be listed like this:\n\n\n DOUBLE_PRECISION=2\n\n\n\n\nSee also\n\n\n\n\nDatabases\n\n\nProperties file",
            "title": " WEKA experiment DatabaseUtils.props"
        },
        {
            "location": "/weka_experiment_database_utils.props/#file",
            "text": "weka/experiment/DatabaseUtils.props",
            "title": "File"
        },
        {
            "location": "/weka_experiment_database_utils.props/#description",
            "text": "Defines the  Databases  setup, i.e., JDBC driver information, JDBC URL, database type conversion, etc.",
            "title": "Description"
        },
        {
            "location": "/weka_experiment_database_utils.props/#version",
            "text": "= 3.1.3",
            "title": "Version"
        },
        {
            "location": "/weka_experiment_database_utils.props/#fields",
            "text": "General    jdbcDriver  the comma-separated list of jdbc drivers to try loading    jdbcURL  the JDBC URL to the database      Table creation    CREATE_STRING  database specific datatype, e.g.,  TEXT    CREATE_INT  database specific datatype, e.g.,  INT    CREATE_DOUBLE  database specific datatype, e.g.,  DOUBLE      Database flags    checkUpperCaseNames  necessary if database turns column names into upper case ones, e.g., HSQLDB    checkLowerCaseNames  (> 3.5.3)  necessary if database turns column names into lower case ones, e.g., PostgreSQL    checkForTable  (> 3.5.3)  Checks whether the tables in the query are available in the meta-data of the JDBC Connection. Some tables, like  pg_tables , exist but are not available through the meta-data    setAutoCommit  setting for  java.sql.Connection.setAutoCommit(boolean)    createIndex  whether to create a primary key  Key_IDX  in the results table of an experiment      Special flags for DatabaseLoader/Saver (package  weka.core.converters )    nominalToStringLimit  (>= 3.4.1)  beyond this limit, nominal columns are loaded as STRING attributes and no longer as NOMINAL ones    idColumn  (>= 3.4.1)  unique key in table that allows ordering for incremental loading    Keywords  (> 3.5.8, > 3.6.0)  lists all the reserved keywords of the current database type  default:  AND,ASC,BY,DESC,FROM,GROUP,INSERT,ORDER,SELECT,UPDATE,WHERE    KeywordsMaskChar  (> 3.5.8, > 3.6.0)  the character to append to attribute names/table names that would be interpreted as keywords by the database, in order to avoid exceptions when executing SQL commands\ndefaut:  _",
            "title": "Fields"
        },
        {
            "location": "/weka_experiment_database_utils.props/#database-type-mapping",
            "text": "In order to import the data from database correctly into Weka, one has to specify what JDBC datatype corresponds to what Java SQL retrieval method. Here's an overview of how the Java types are mapped to Weka's attribute types:     Java type  Java method  Identifier  Weka attribute type  Version      String  getString()  0  nominal     boolean  getBoolean()  1  nominal     double  getDouble()  2  numeric     byte  getByte()  3  numeric     short  getByte()  4  numeric     int  getInteger()  5  numeric     long  getLong()  6  numeric     float  getFloat()  7  numeric     date  getDate()  8  date     text  getString()  9  string  >3.5.5    time  getTime()  10  string  >3.5.8     In the props file one lists now the type names that the database returns and what Java type it represents (via the identifier), e.g.:   CHAR=0\n VARCHAR=0  CHAR  and  VARCHAR  are both String types, hence they are interpreted as  String  (identifier  0 )  Note:  in case database types have blanks, one needs to replace those blanks with an underscore, e.g.,  DOUBLE PRECISION  must be listed like this:   DOUBLE_PRECISION=2",
            "title": "Database type mapping"
        },
        {
            "location": "/weka_experiment_database_utils.props/#see-also",
            "text": "Databases  Properties file",
            "title": "See also"
        },
        {
            "location": "/subversion/",
            "text": "General\n\n\nThe Weka \nSubversion\n repository is accessible and browseable via the following URL:\n\n\n\n\nhttps://svn.cms.waikato.ac.nz/svn/weka/\n\n\n\n\nA Subversion repository has usually the following layout:\n\n\n root\n  |\n  +- trunk\n  |\n  +- tags\n  |\n  +- branches\n\n\n\n\nWhere \ntrunk\n contains the \nmain trunk\n of the development, \ntags\n snapshots in time of the repository (e.g., when a new version got released) and \nbranches\n development branches that forked off the main trunk at some stage (e.g., legacy versions that still get bugfixed).\n\n\nSource code\n\n\nThe latest version of the Weka source code can be obtained with this URL:\n\n\nhttps://svn.cms.waikato.ac.nz/svn/weka/trunk/weka\n\n\nIf you want to obtain the source code of the book version, use this URL:\n\n\nhttps://svn.cms.waikato.ac.nz/svn/weka/branches/stable-3-8/\n\n\nSpecific version\n\n\nWhenever a release of Weka is generated, the repository gets \ntagged\n:\n\n\ndev-X-Y-Z\n\n\nthe tag for a release of the developer version, e.g., \ndev-3-9-2\n for Weka 3.9.2\n\n\nhttps://svn.cms.waikato.ac.nz/svn/weka/tags/dev-3-9-2\n\n\nstable-X-Y-Z\n\n\nthe tag for a release of a stable version. The book version is one of those stable versions, e.g., \nstable-3-8-2\n for Weka 3.8.2.\n\n\nhttps://svn.cms.waikato.ac.nz/svn/weka/tags/stable-3-8-2\n\n\nJUnit\n\n\nWeka's JUnit tests are no longer a separate module (as it was the case before the migration to Subversion). They are now located in the \nsrc/test\n directory of the Weka source code tree.\n\n\nCommandline\n\n\nModern Linux distributions already come with Subversion either pre-installed or easily installed via the package manager of the distribution. If that shouldn't be case, or if you are using Windows, you have to download the appropriate client from [http://subversion.tigris.org/ Subversion's homepage].\n\n\nA checkout of the current developer version of Weka looks like this:\n\n\nsvn co https://svn.cms.waikato.ac.nz/svn/weka/trunk/weka\n\n\n\n\nYou can also obtain the source code for a specific date. The \n-r\n option of the \nsvn\n command-line client can also take dates (format: YYYYMMDD) instead of only revision numbers. In order to distinguish the dates from revision numbers, you have to enclose the date within curly brackets:\n\n\nsvn co -r {20180616} https://svn.cms.waikato.ac.nz/svn/weka/trunk/weka\n\n\n\n\nLinks\n\n\n\n\nSubversion on WikiPedia\n\n\nSubversion homepage\n\n\nJUnit homepage",
            "title": " Subversion"
        },
        {
            "location": "/subversion/#general",
            "text": "The Weka  Subversion  repository is accessible and browseable via the following URL:   https://svn.cms.waikato.ac.nz/svn/weka/   A Subversion repository has usually the following layout:   root\n  |\n  +- trunk\n  |\n  +- tags\n  |\n  +- branches  Where  trunk  contains the  main trunk  of the development,  tags  snapshots in time of the repository (e.g., when a new version got released) and  branches  development branches that forked off the main trunk at some stage (e.g., legacy versions that still get bugfixed).",
            "title": "General"
        },
        {
            "location": "/subversion/#source-code",
            "text": "The latest version of the Weka source code can be obtained with this URL:  https://svn.cms.waikato.ac.nz/svn/weka/trunk/weka  If you want to obtain the source code of the book version, use this URL:  https://svn.cms.waikato.ac.nz/svn/weka/branches/stable-3-8/",
            "title": "Source code"
        },
        {
            "location": "/subversion/#specific-version",
            "text": "Whenever a release of Weka is generated, the repository gets  tagged :  dev-X-Y-Z  the tag for a release of the developer version, e.g.,  dev-3-9-2  for Weka 3.9.2  https://svn.cms.waikato.ac.nz/svn/weka/tags/dev-3-9-2  stable-X-Y-Z  the tag for a release of a stable version. The book version is one of those stable versions, e.g.,  stable-3-8-2  for Weka 3.8.2.  https://svn.cms.waikato.ac.nz/svn/weka/tags/stable-3-8-2",
            "title": "Specific version"
        },
        {
            "location": "/subversion/#junit",
            "text": "Weka's JUnit tests are no longer a separate module (as it was the case before the migration to Subversion). They are now located in the  src/test  directory of the Weka source code tree.",
            "title": "JUnit"
        },
        {
            "location": "/subversion/#commandline",
            "text": "Modern Linux distributions already come with Subversion either pre-installed or easily installed via the package manager of the distribution. If that shouldn't be case, or if you are using Windows, you have to download the appropriate client from [http://subversion.tigris.org/ Subversion's homepage].  A checkout of the current developer version of Weka looks like this:  svn co https://svn.cms.waikato.ac.nz/svn/weka/trunk/weka  You can also obtain the source code for a specific date. The  -r  option of the  svn  command-line client can also take dates (format: YYYYMMDD) instead of only revision numbers. In order to distinguish the dates from revision numbers, you have to enclose the date within curly brackets:  svn co -r {20180616} https://svn.cms.waikato.ac.nz/svn/weka/trunk/weka",
            "title": "Commandline"
        },
        {
            "location": "/subversion/#links",
            "text": "Subversion on WikiPedia  Subversion homepage  JUnit homepage",
            "title": "Links"
        },
        {
            "location": "/get_latest_bugfixes/",
            "text": "Weka is actively developed, that means that bugs are fixed and new functionality is added (only to the developer version) all the time. Every now and then (about every 6-12 months), when there was a sufficiently large number of improvements or fixes, a release is made and uploaded to \nSourceforget.net\n.\n\n\nIf you don't want to wait that long, you have two options:\n\n\n\n\n\n\nGet the latest source code from \nSubversion\n and compile it yourself. See the following articles for more information\n\n\n\n\nobtaining the source code from Subversion\n, either book or developer version\n\n\ncompiling the source code\n\n\n\n\n\n\n\n\nDownload a \nsnapshot\n from the download section of the \nWeka homepage\n. Snapshots for book and developer version are generated automatically every night, based on the current Subversion code. The \nZIP files\n have the same content as a release, i.e., compiled classes (= weka.jar), source code (= weka-src.jar), Javadoc and other documentation.\n\n\n\n\n\n\nNote:\n compare the timestamp of the \nWeka Mailing List\n post that reports a bugfix with the one of the snapshot to make sure the bugfix is already included in the snapshot.",
            "title": " Get latest Bugfixes"
        },
        {
            "location": "/compiling_weka/",
            "text": "There are several ways of compiling the Weka source code:\n\n\n\n\nwith \nant\n\n\n\n\ntakes care of compiling all the necessary classes and easily generates jar archives\n\n\n\n\nwith \nmaven\n\n\n\n\nsimilar to ant\n\n\n\n\nwith an IDE, like IntelliJ IDEA, Eclipse or NetBeans\n\n\n\n\ncan be very helpful for debugging tricky bugs",
            "title": " Compiling Weka"
        },
        {
            "location": "/ant/",
            "text": "What is ANT? This is how the ANT \nhomepage\n defines its tool:\n\n\nApache Ant is a Java-based build tool. In theory, it is kind of like Make, but without Make's wrinkles.\n\n\nBasics\n\n\n\n\nthe ANT build file is based on \nXML\n\n\nthe usual name for the build file is \nbuild.xml\n\n\ninvocation - the usual build file needs not be specified explicitly, if it's in the current directory; if not target is specified, the default one is used\n\n\n\n\nant [-f <build-file>] [<target>]\n\n\n\n\ndisplaying all the available targets of a build file\n\n\n\n\nant [-f <build-file>] -projecthelp\n\n\nWeka and ANT\n\n\n\n\na build file for Weka is available from \nsubversion\n (it has been included in the \nweka-src.jar\n since version 3.4.8 and 3.5.3)\n\n\nit is located in the \nweka\n directory\n\n\n\n\nsome targets of interest\n\n\n\n\n\n\nclean\n - Removes the build, dist and reports directories; also any class files in the source tree\n\n\n\n\ncompile\n - Compile weka and deposit class files in \n${path_modifier}/build/classes\n\n\ndocs\n - Make javadocs into {${path_modifier}/doc}}\n\n\nexejar\n - Create an executable jar file in \n${path_modifier}/dist\n\n\n\n\nLinks\n\n\n\n\nANT homepage\n\n\nXML",
            "title": " Ant"
        },
        {
            "location": "/ant/#basics",
            "text": "the ANT build file is based on  XML  the usual name for the build file is  build.xml  invocation - the usual build file needs not be specified explicitly, if it's in the current directory; if not target is specified, the default one is used   ant [-f <build-file>] [<target>]   displaying all the available targets of a build file   ant [-f <build-file>] -projecthelp",
            "title": "Basics"
        },
        {
            "location": "/ant/#weka-and-ant",
            "text": "a build file for Weka is available from  subversion  (it has been included in the  weka-src.jar  since version 3.4.8 and 3.5.3)  it is located in the  weka  directory   some targets of interest    clean  - Removes the build, dist and reports directories; also any class files in the source tree   compile  - Compile weka and deposit class files in  ${path_modifier}/build/classes  docs  - Make javadocs into {${path_modifier}/doc}}  exejar  - Create an executable jar file in  ${path_modifier}/dist",
            "title": "Weka and ANT"
        },
        {
            "location": "/ant/#links",
            "text": "ANT homepage  XML",
            "title": "Links"
        },
        {
            "location": "/maven/",
            "text": "Maven\n is another build tool. But unlike \nAnt\n, it is a more high-level tool. Though its configuration file, \npom.xml\n is written in XML as well, Maven uses a different approach to the build process. In Ant, you tell it where to find Java classes for compilation, what libraries to compile against, where to put the compiled ones and then how to combine them into a jar. With Maven, you only specify dependent libraries, a compile and a jar plugin and maybe tweak the options a bit. For this to work, Maven enforces a strict \ndirectory structure\n (though you can tweak that, if you need to).\n\n\nSo why another build tool?\n\n\nWhereas Ant scripts quite often create a \nfat jar\n, i.e., a jar that contains not only the project's code, but also the contain of libraries the code was compiled against. Handy if you only want to have a single jar. However, this is a nightmare, if you need to update a single library, but all you have is a single, enormous jar. Maven handles \ndependencies automatically\n, relying on libraries (they call them artifacts) to be publicly available, e.g., on \nMaven Central\n. It allows you to use newer versions of libraries than defined by the dependent libraries (e.g., critical bug fixes), without having to modify any jars manually. Though Maven can also generate \nfar jar\n files, it is not considered good practice, as it defeats Maven's automatic version resolution.\n\n\nIn order to make Weka, and most of its packages, available to a wider audience (e.g., other software developers), we also publish on Maven Central.\n\n\nCompiling\n\n\nFor compiling Weka, you would issue a command like this (in the same directory as \npom.xml\n):\n\n\nmvn clean install\n\n\n\n\nIf you don't want the tests to run, use this:\n\n\nmvn clean install -DskipTests=true",
            "title": " Maven"
        },
        {
            "location": "/maven/#so-why-another-build-tool",
            "text": "Whereas Ant scripts quite often create a  fat jar , i.e., a jar that contains not only the project's code, but also the contain of libraries the code was compiled against. Handy if you only want to have a single jar. However, this is a nightmare, if you need to update a single library, but all you have is a single, enormous jar. Maven handles  dependencies automatically , relying on libraries (they call them artifacts) to be publicly available, e.g., on  Maven Central . It allows you to use newer versions of libraries than defined by the dependent libraries (e.g., critical bug fixes), without having to modify any jars manually. Though Maven can also generate  far jar  files, it is not considered good practice, as it defeats Maven's automatic version resolution.  In order to make Weka, and most of its packages, available to a wider audience (e.g., other software developers), we also publish on Maven Central.",
            "title": "So why another build tool?"
        },
        {
            "location": "/maven/#compiling",
            "text": "For compiling Weka, you would issue a command like this (in the same directory as  pom.xml ):  mvn clean install  If you don't want the tests to run, use this:  mvn clean install -DskipTests=true",
            "title": "Compiling"
        },
        {
            "location": "/snapshots/",
            "text": "See \nHow to get the latest bugfixes?",
            "title": " Snapsnots"
        },
        {
            "location": "/arff_stable/",
            "text": "An ARFF (Attribute-Relation File Format) file is an ASCII text file that describes a list of instances sharing a set of attributes. \n\n\nOverview\n\n\nARFF files have two distinct sections. The first section is the \nHeader\n information, which is followed the \nData\n information.\n\n\nThe \nHeader\n of the ARFF file contains the name of the relation, a list of the attributes (the columns in the data), and their types. An example header on the standard IRIS dataset looks like this:\n\n\n   % 1. Title: Iris Plants Database\n   % \n   % 2. Sources:\n   %      (a) Creator: R.A. Fisher\n   %      (b) Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n   %      (c) Date: July, 1988\n   % \n   @RELATION iris\n\n   @ATTRIBUTE sepallength  NUMERIC\n   @ATTRIBUTE sepalwidth   NUMERIC\n   @ATTRIBUTE petallength  NUMERIC\n   @ATTRIBUTE petalwidth   NUMERIC\n   @ATTRIBUTE class        {Iris-setosa,Iris-versicolor,Iris-virginica}\n\n\n\n\nThe \nData\n of the ARFF file looks like the following:\n\n\n   @DATA\n   5.1,3.5,1.4,0.2,Iris-setosa\n   4.9,3.0,1.4,0.2,Iris-setosa\n   4.7,3.2,1.3,0.2,Iris-setosa\n   4.6,3.1,1.5,0.2,Iris-setosa\n   5.0,3.6,1.4,0.2,Iris-setosa\n   5.4,3.9,1.7,0.4,Iris-setosa\n   4.6,3.4,1.4,0.3,Iris-setosa\n   5.0,3.4,1.5,0.2,Iris-setosa\n   4.4,2.9,1.4,0.2,Iris-setosa\n   4.9,3.1,1.5,0.1,Iris-setosa\n\n\n\n\nLines that begin with a \n%\n are comments. The \n@RELATION\n, \n@ATTRIBUTE\n and \n@DATA\n declarations are case insensitive.\n\n\nExamples\n\n\nSeveral well-known machine learning datasets are distributed with Weka in the \n$WEKAHOME/data\n directory as ARFF files.\n\n\nThe ARFF Header Section\n\n\nThe ARFF Header section of the file contains the relation declaration and attribute declarations.\n\n\nThe @relation Declaration\n\n\nThe relation name is defined as the first line in the ARFF file. The format is:\n\n\n    @relation [relation-name]\n\n\n\n\nwhere \n[relation-name]\n is a string. The string must be quoted if the name includes spaces. Furthermore, relation names or attribute names (see below) cannot begin with\n\n\n\n\na character below \\u0021\n\n\n'{', '}', ',', or '%' \n\n\n\n\nMoreover, it can only begin with a single or double quote if there is a corresponding quote at the end of the name.\n\n\nThe @attribute Declarations\n\n\nAttribute declarations take the form of an ordered sequence of \n@attribute\n statements. Each attribute in the data set has its own \n@attribute\n statement which uniquely defines the name of that attribute and its data type. The order the attributes are declared indicates the column position in the data section of the file. For example, if an attribute is the third one declared then Weka expects that all that attributes values will be found in the third comma delimited column.\n\n\nThe format for the \n@attribute\n statement is:\n\n\n    @attribute [attribute-name] [datatype]\n\n\n\n\nwhere the \n[attribute-name]\n must adhere to the constraints specified in the above section on the @relation declaration.\n\n\nThe [datatype] can be any of the four types supported by Weka:\n\n\n\n\nnumeric\n\n\ninteger\n is treated as \nnumeric\n\n\nreal\n is treated as \nnumeric\n\n\n[nominal-specification]\n\n\nstring\n\n\ndate\n [date-format]\n\n\nrelational\n for multi-instance data (for future use)\n\n\n\n\nwhere \n[nominal-specification]\n and \n[date-format]\n are defined below. The keywords \nnumeric\n, \nreal\n, \ninteger\n, \nstring\n and \ndate\n are case insensitive.\n\n\nNumeric attributes\n\n\nNumeric attributes can be real or integer numbers.\n\n\nNominal attributes\n\n\nNominal values are defined by providing an [nominal-specification] listing the possible values: {[nominal-name1], [nominal-name2], [nominal-name3], ...}\n\n\nFor example, the class value of the Iris dataset can be defined as follows:\n\n\n    @ATTRIBUTE class        {Iris-setosa,Iris-versicolor,Iris-virginica}\n\n\n\n\nValues that contain spaces must be quoted.\n\n\nString attributes\n\n\nString attributes allow us to create attributes containing arbitrary textual values. This is very useful in text-mining applications, as we can create datasets with string attributes, then write Weka Filters to manipulate strings (like \nStringToWordVectorFilter\n). String attributes are declared as follows:\n\n\n    @ATTRIBUTE LCC    string\n\n\n\n\nDate attributes\n\n\nDate attribute declarations take the form:\n\n\n    @attribute [name] date [[date-format]]\n\n\n\n\nwhere [name] is the name for the attribute and \n[date-format]\n is an optional string specifying how date values should be parsed and printed (this is the same format used by \nSimpleDateFormat\n). The default format string accepts the ISO-8601 combined date and time format: \nyyyy-MM-dd'T'HH:mm:ss\n. Check out the Javadoc of the \njava.text.SimpleDateFormat\n class for supported character patterns.\n\n\nDates must be specified in the data section as the corresponding string representations of the date/time (see example below).\n\n\nRelational attributes\n\n\nRelational attribute declarations take the form:\n\n\n    @attribute [name] relational\n      [further attribute definitions]\n    @end [name]\n\n\n\n\nFor the multi-instance dataset \nMUSK1\n the definition would look like this (\n\"...\"\n denotes an omission):\n\n\n @attribute molecule_name {MUSK-jf78,...,NON-MUSK-199}\n @attribute bag relational\n   @attribute f1 numeric\n   ...\n   @attribute f166 numeric\n @end bag\n @attribute class {0,1}\n ...\n\n\n\n\nThe ARFF Data Section\n\n\nThe ARFF Data section of the file contains the data declaration line and the actual instance lines.\n\n\nThe @data Declaration\n\n\nThe \n@data\n declaration is a single line denoting the start of the data segment in the file. The format is:\n\n\n    @data\n\n\n\n\nThe instance data\n\n\nEach instance is represented on a single line, with carriage returns denoting the end of the instance.  A percent sign (%) introduces a comment, which continues to the end of the line.\n\n\nAttribute values for each instance can be delimited by commas or tabs. A comma/tab may be followed by zero or more spaces. Attribute values must appear in the order in which they were declared in the header section (i.e., the data corresponding to the nth \n@attribute\n declaration is always the nth field of the attribute).\n\n\nA missing value is represented by a single question mark, as in:\n\n\n    @data\n    4.4,?,1.5,?,Iris-setosa\n\n\n\n\nValues of string and nominal attributes are case sensitive, and any that contain space or the comment-delimiter character % must be quoted.  (The code suggests that double-quotes are acceptable and that a backslash will escape individual characters.)  An example follows:\n\n\n    @relation LCCvsLCSH\n\n    @attribute LCC string\n    @attribute LCSH string\n\n    @data\n    AG5,   'Encyclopedias and dictionaries.;Twentieth century.'\n    AS262, 'Science -- Soviet Union -- History.'\n    AE5,   'Encyclopedias and dictionaries.'\n    AS281, 'Astronomy, Assyro-Babylonian.;Moon -- Phases.'\n    AS281, 'Astronomy, Assyro-Babylonian.;Moon -- Tables.'\n\n\n\n\nDates must be specified in the data section using the string representation specified in the attribute declaration. For example:\n\n\n    @RELATION Timestamps\n\n    @ATTRIBUTE timestamp DATE \"yyyy-MM-dd HH:mm:ss\"\n\n    @DATA \n    \"2001-04-03 12:12:12\"\n    \"2001-05-03 12:59:55\"\n\n\n\n\nRelational data must be enclosed within double quotes \n\"\n. For example an instance of the \nMUSK1\n dataset (\n\"...\"\n denotes an omission):\n\n\n MUSK-188,\"42,...,30\",1\n\n\n\n\nSparse ARFF files\n\n\nSparse ARFF files are very similar to ARFF files, but data with value 0 are not be explicitly represented.\n\n\nSparse ARFF files have the same header (i.e \n@relation\n and \n@attribute\n tags) but the data section is different. Instead of representing each value in order, like this:\n\n\n    @data\n    0, X, 0, Y, \"class A\"\n    0, 0, W, 0, \"class B\"\n\n\n\n\nthe non-zero attributes are explicitly identified by attribute number and their value stated, like this:\n\n\n    @data\n    {1 X, 3 Y, 4 \"class A\"}\n    {2 W, 4 \"class B\"}\n\n\n\n\nEach instance is surrounded by curly braces, and the format for each entry is: [index] [space] [value] where index is the attribute index (starting from 0).\n\n\nNote that the omitted values in a sparse instance are \n0\n, they are not \"missing\" values! If a value is unknown, you must explicitly represent it with a question mark (?).\n\n\nWarning:\n There is a known problem saving SparseInstance objects from datasets that have string attributes. In Weka, string and nominal data values are stored as numbers; these numbers act as indexes into an array of possible attribute values (this is very efficient). However, the first string value is assigned index 0: this means that, internally, this value is stored as a 0. When a SparseInstance is written, string instances with internal value 0 are not output, so their string value is lost (and when the arff file is read again, the default value 0 is the index of a different string value, so the attribute value appears to change). To get around this problem, add a dummy string value at index 0 that is never used whenever you declare string attributes that are likely to be used in SparseInstance objects and saved as Sparse ARFF files.\n\n\nInstance weights in ARFF files\n\n\nA weight can be associated with an instance in a standard ARFF file by appending it to the end of the line for that instance and enclosing the value in curly braces. E.g:\n\n\n    @data\n    0, X, 0, Y, \"class A\", {5}\n\n\n\n\nFor a sparse instance, this example would look like:\n\n\n    @data\n    {1 X, 3 Y, 4 \"class A\"}, {5}\n\n\n\n\nNote that any instance without a weight value specified is assumed to have a weight of 1 for backwards compatibility.\n\n\nSee also\n\n\n\n\n[[Add weights to dataset]]\n\n\nARFF Syntax Highlighting\n for various editors\n\n\n\n\nLinks\n\n\n\n\nISO-8601\n\n\nJavadoc of \njava.text.SimpleDateFormat\n (lists the supported character patterns)\n\n\nANTLR syntax by Staal A. Vinterbo \narff.g",
            "title": " ARFF (stable version)"
        },
        {
            "location": "/arff_stable/#overview",
            "text": "ARFF files have two distinct sections. The first section is the  Header  information, which is followed the  Data  information.  The  Header  of the ARFF file contains the name of the relation, a list of the attributes (the columns in the data), and their types. An example header on the standard IRIS dataset looks like this:     % 1. Title: Iris Plants Database\n   % \n   % 2. Sources:\n   %      (a) Creator: R.A. Fisher\n   %      (b) Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n   %      (c) Date: July, 1988\n   % \n   @RELATION iris\n\n   @ATTRIBUTE sepallength  NUMERIC\n   @ATTRIBUTE sepalwidth   NUMERIC\n   @ATTRIBUTE petallength  NUMERIC\n   @ATTRIBUTE petalwidth   NUMERIC\n   @ATTRIBUTE class        {Iris-setosa,Iris-versicolor,Iris-virginica}  The  Data  of the ARFF file looks like the following:     @DATA\n   5.1,3.5,1.4,0.2,Iris-setosa\n   4.9,3.0,1.4,0.2,Iris-setosa\n   4.7,3.2,1.3,0.2,Iris-setosa\n   4.6,3.1,1.5,0.2,Iris-setosa\n   5.0,3.6,1.4,0.2,Iris-setosa\n   5.4,3.9,1.7,0.4,Iris-setosa\n   4.6,3.4,1.4,0.3,Iris-setosa\n   5.0,3.4,1.5,0.2,Iris-setosa\n   4.4,2.9,1.4,0.2,Iris-setosa\n   4.9,3.1,1.5,0.1,Iris-setosa  Lines that begin with a  %  are comments. The  @RELATION ,  @ATTRIBUTE  and  @DATA  declarations are case insensitive.",
            "title": "Overview"
        },
        {
            "location": "/arff_stable/#examples",
            "text": "Several well-known machine learning datasets are distributed with Weka in the  $WEKAHOME/data  directory as ARFF files.",
            "title": "Examples"
        },
        {
            "location": "/arff_stable/#the-arff-header-section",
            "text": "The ARFF Header section of the file contains the relation declaration and attribute declarations.",
            "title": "The ARFF Header Section"
        },
        {
            "location": "/arff_stable/#the-relation-declaration",
            "text": "The relation name is defined as the first line in the ARFF file. The format is:      @relation [relation-name]  where  [relation-name]  is a string. The string must be quoted if the name includes spaces. Furthermore, relation names or attribute names (see below) cannot begin with   a character below \\u0021  '{', '}', ',', or '%'    Moreover, it can only begin with a single or double quote if there is a corresponding quote at the end of the name.",
            "title": "The @relation Declaration"
        },
        {
            "location": "/arff_stable/#the-attribute-declarations",
            "text": "Attribute declarations take the form of an ordered sequence of  @attribute  statements. Each attribute in the data set has its own  @attribute  statement which uniquely defines the name of that attribute and its data type. The order the attributes are declared indicates the column position in the data section of the file. For example, if an attribute is the third one declared then Weka expects that all that attributes values will be found in the third comma delimited column.  The format for the  @attribute  statement is:      @attribute [attribute-name] [datatype]  where the  [attribute-name]  must adhere to the constraints specified in the above section on the @relation declaration.  The [datatype] can be any of the four types supported by Weka:   numeric  integer  is treated as  numeric  real  is treated as  numeric  [nominal-specification]  string  date  [date-format]  relational  for multi-instance data (for future use)   where  [nominal-specification]  and  [date-format]  are defined below. The keywords  numeric ,  real ,  integer ,  string  and  date  are case insensitive.",
            "title": "The @attribute Declarations"
        },
        {
            "location": "/arff_stable/#numeric-attributes",
            "text": "Numeric attributes can be real or integer numbers.",
            "title": "Numeric attributes"
        },
        {
            "location": "/arff_stable/#nominal-attributes",
            "text": "Nominal values are defined by providing an [nominal-specification] listing the possible values: {[nominal-name1], [nominal-name2], [nominal-name3], ...}  For example, the class value of the Iris dataset can be defined as follows:      @ATTRIBUTE class        {Iris-setosa,Iris-versicolor,Iris-virginica}  Values that contain spaces must be quoted.",
            "title": "Nominal attributes"
        },
        {
            "location": "/arff_stable/#string-attributes",
            "text": "String attributes allow us to create attributes containing arbitrary textual values. This is very useful in text-mining applications, as we can create datasets with string attributes, then write Weka Filters to manipulate strings (like  StringToWordVectorFilter ). String attributes are declared as follows:      @ATTRIBUTE LCC    string",
            "title": "String attributes"
        },
        {
            "location": "/arff_stable/#date-attributes",
            "text": "Date attribute declarations take the form:      @attribute [name] date [[date-format]]  where [name] is the name for the attribute and  [date-format]  is an optional string specifying how date values should be parsed and printed (this is the same format used by  SimpleDateFormat ). The default format string accepts the ISO-8601 combined date and time format:  yyyy-MM-dd'T'HH:mm:ss . Check out the Javadoc of the  java.text.SimpleDateFormat  class for supported character patterns.  Dates must be specified in the data section as the corresponding string representations of the date/time (see example below).",
            "title": "Date attributes"
        },
        {
            "location": "/arff_stable/#relational-attributes",
            "text": "Relational attribute declarations take the form:      @attribute [name] relational\n      [further attribute definitions]\n    @end [name]  For the multi-instance dataset  MUSK1  the definition would look like this ( \"...\"  denotes an omission):   @attribute molecule_name {MUSK-jf78,...,NON-MUSK-199}\n @attribute bag relational\n   @attribute f1 numeric\n   ...\n   @attribute f166 numeric\n @end bag\n @attribute class {0,1}\n ...",
            "title": "Relational attributes"
        },
        {
            "location": "/arff_stable/#the-arff-data-section",
            "text": "The ARFF Data section of the file contains the data declaration line and the actual instance lines.",
            "title": "The ARFF Data Section"
        },
        {
            "location": "/arff_stable/#the-data-declaration",
            "text": "The  @data  declaration is a single line denoting the start of the data segment in the file. The format is:      @data",
            "title": "The @data Declaration"
        },
        {
            "location": "/arff_stable/#the-instance-data",
            "text": "Each instance is represented on a single line, with carriage returns denoting the end of the instance.  A percent sign (%) introduces a comment, which continues to the end of the line.  Attribute values for each instance can be delimited by commas or tabs. A comma/tab may be followed by zero or more spaces. Attribute values must appear in the order in which they were declared in the header section (i.e., the data corresponding to the nth  @attribute  declaration is always the nth field of the attribute).  A missing value is represented by a single question mark, as in:      @data\n    4.4,?,1.5,?,Iris-setosa  Values of string and nominal attributes are case sensitive, and any that contain space or the comment-delimiter character % must be quoted.  (The code suggests that double-quotes are acceptable and that a backslash will escape individual characters.)  An example follows:      @relation LCCvsLCSH\n\n    @attribute LCC string\n    @attribute LCSH string\n\n    @data\n    AG5,   'Encyclopedias and dictionaries.;Twentieth century.'\n    AS262, 'Science -- Soviet Union -- History.'\n    AE5,   'Encyclopedias and dictionaries.'\n    AS281, 'Astronomy, Assyro-Babylonian.;Moon -- Phases.'\n    AS281, 'Astronomy, Assyro-Babylonian.;Moon -- Tables.'  Dates must be specified in the data section using the string representation specified in the attribute declaration. For example:      @RELATION Timestamps\n\n    @ATTRIBUTE timestamp DATE \"yyyy-MM-dd HH:mm:ss\"\n\n    @DATA \n    \"2001-04-03 12:12:12\"\n    \"2001-05-03 12:59:55\"  Relational data must be enclosed within double quotes  \" . For example an instance of the  MUSK1  dataset ( \"...\"  denotes an omission):   MUSK-188,\"42,...,30\",1",
            "title": "The instance data"
        },
        {
            "location": "/arff_stable/#sparse-arff-files",
            "text": "Sparse ARFF files are very similar to ARFF files, but data with value 0 are not be explicitly represented.  Sparse ARFF files have the same header (i.e  @relation  and  @attribute  tags) but the data section is different. Instead of representing each value in order, like this:      @data\n    0, X, 0, Y, \"class A\"\n    0, 0, W, 0, \"class B\"  the non-zero attributes are explicitly identified by attribute number and their value stated, like this:      @data\n    {1 X, 3 Y, 4 \"class A\"}\n    {2 W, 4 \"class B\"}  Each instance is surrounded by curly braces, and the format for each entry is: [index] [space] [value] where index is the attribute index (starting from 0).  Note that the omitted values in a sparse instance are  0 , they are not \"missing\" values! If a value is unknown, you must explicitly represent it with a question mark (?).  Warning:  There is a known problem saving SparseInstance objects from datasets that have string attributes. In Weka, string and nominal data values are stored as numbers; these numbers act as indexes into an array of possible attribute values (this is very efficient). However, the first string value is assigned index 0: this means that, internally, this value is stored as a 0. When a SparseInstance is written, string instances with internal value 0 are not output, so their string value is lost (and when the arff file is read again, the default value 0 is the index of a different string value, so the attribute value appears to change). To get around this problem, add a dummy string value at index 0 that is never used whenever you declare string attributes that are likely to be used in SparseInstance objects and saved as Sparse ARFF files.",
            "title": "Sparse ARFF files"
        },
        {
            "location": "/arff_stable/#instance-weights-in-arff-files",
            "text": "A weight can be associated with an instance in a standard ARFF file by appending it to the end of the line for that instance and enclosing the value in curly braces. E.g:      @data\n    0, X, 0, Y, \"class A\", {5}  For a sparse instance, this example would look like:      @data\n    {1 X, 3 Y, 4 \"class A\"}, {5}  Note that any instance without a weight value specified is assumed to have a weight of 1 for backwards compatibility.",
            "title": "Instance weights in ARFF files"
        },
        {
            "location": "/arff_stable/#see-also",
            "text": "[[Add weights to dataset]]  ARFF Syntax Highlighting  for various editors",
            "title": "See also"
        },
        {
            "location": "/arff_stable/#links",
            "text": "ISO-8601  Javadoc of  java.text.SimpleDateFormat  (lists the supported character patterns)  ANTLR syntax by Staal A. Vinterbo  arff.g",
            "title": "Links"
        },
        {
            "location": "/arff_developer/",
            "text": "An ARFF (Attribute-Relation File Format) file is an ASCII text file that describes a list of instances sharing a set of attributes. \n\n\nOverview\n\n\nARFF files have two distinct sections. The first section is the \nHeader\n information, which is followed the \nData\n information.\n\n\nThe \nHeader\n of the ARFF file contains the name of the relation, a list of the attributes (the columns in the data), and their types. An example header on the standard IRIS dataset looks like this:\n\n\n   % 1. Title: Iris Plants Database\n   % \n   % 2. Sources:\n   %      (a) Creator: R.A. Fisher\n   %      (b) Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n   %      (c) Date: July, 1988\n   % \n   @RELATION iris\n\n   @ATTRIBUTE sepallength  NUMERIC\n   @ATTRIBUTE sepalwidth   NUMERIC\n   @ATTRIBUTE petallength  NUMERIC\n   @ATTRIBUTE petalwidth   NUMERIC\n   @ATTRIBUTE class        {Iris-setosa,Iris-versicolor,Iris-virginica}\n\n\n\n\nThe \nData\n of the ARFF file looks like the following:\n\n\n   @DATA\n   5.1,3.5,1.4,0.2,Iris-setosa\n   4.9,3.0,1.4,0.2,Iris-setosa\n   4.7,3.2,1.3,0.2,Iris-setosa\n   4.6,3.1,1.5,0.2,Iris-setosa\n   5.0,3.6,1.4,0.2,Iris-setosa\n   5.4,3.9,1.7,0.4,Iris-setosa\n   4.6,3.4,1.4,0.3,Iris-setosa\n   5.0,3.4,1.5,0.2,Iris-setosa\n   4.4,2.9,1.4,0.2,Iris-setosa\n   4.9,3.1,1.5,0.1,Iris-setosa\n\n\n\n\nLines that begin with a \n%\n are comments. The \n@RELATION\n, \n@ATTRIBUTE\n and \n@DATA\n declarations are case insensitive.\n\n\nExamples\n\n\nSeveral well-known machine learning datasets are distributed with Weka in the \n$WEKAHOME/data\n directory as ARFF files.\n\n\nThe ARFF Header Section\n\n\nThe ARFF Header section of the file contains the relation declaration and attribute declarations.\n\n\nThe @relation Declaration\n\n\nThe relation name is defined as the first line in the ARFF file. The format is:\n\n\n    @relation [relation-name]\n\n\n\n\nwhere \n[relation-name]\n is a string. The string must be quoted if the name includes spaces. Furthermore, relation names or attribute names (see below) cannot begin with\n\n\n\n\na character below \\u0021\n\n\n'{', '}', ',', or '%' \n\n\n\n\nMoreover, it can only begin with a single or double quote if there is a corresponding quote at the end of the name.\n\n\n== The @attribute Declarations ==\nAttribute declarations take the form of an ordered sequence of \n@attribute\n statements. Each attribute in the data set has its own \n@attribute\n statement which uniquely defines the name of that attribute and its data type. The order the attributes are declared indicates the column position in the data section of the file. For example, if an attribute is the third one declared then Weka expects that all that attributes values will be found in the third comma delimited column.\n\n\nThe format for the \n@attribute\n statement is:\n\n\n    @attribute [attribute-name] [datatype]\n\n\n\n\nwhere the \n[attribute-name]\n must adhere to the constraints specified in the above section on the @relation declaration.\n\n\nThe [datatype] can be any of the four types supported by Weka:\n\n\n\n\nnumeric\n\n\ninteger\n is treated as \nnumeric\n\n\nreal\n is treated as \nnumeric\n\n\n[nominal-specification]\n\n\nstring\n\n\ndate\n [date-format]\n\n\nrelational\n for multi-instance data (for future use)\n\n\n\n\nwhere \n[nominal-specification]\n and \n[date-format]\n are defined below. The keywords \nnumeric\n, \nreal\n, \ninteger\n, \nstring\n and \ndate\n are case insensitive.\n\n\nNumeric attributes\n\n\nNumeric attributes can be real or integer numbers.\n\n\nNominal attributes\n\n\nNominal values are defined by providing an [nominal-specification] listing the possible values: {[nominal-name1], [nominal-name2], [nominal-name3], ...}\n\n\nFor example, the class value of the Iris dataset can be defined as follows:\n\n\n    @ATTRIBUTE class        {Iris-setosa,Iris-versicolor,Iris-virginica}\n\n\n\n\nValues that contain spaces must be quoted.\n\n\nString attributes\n\n\nString attributes allow us to create attributes containing arbitrary textual values. This is very useful in text-mining applications, as we can create datasets with string attributes, then write Weka Filters to manipulate strings (like \nStringToWordVectorFilter\n). String attributes are declared as follows:\n\n\n    @ATTRIBUTE LCC    string\n\n\n\n\nDate attributes\n\n\nDate attribute declarations take the form:\n\n\n    @attribute [name] date [[date-format]]\n\n\n\n\nwhere [name] is the name for the attribute and \n[date-format]\n is an optional string specifying how date values should be parsed and printed (this is the same format used by \nSimpleDateFormat\n). The default format string accepts the ISO-8601 combined date and time format: \nyyyy-MM-dd'T'HH:mm:ss\n. Check out the Javadoc of the \njava.text.SimpleDateFormat\n class for supported character patterns.\n\n\nDates must be specified in the data section as the corresponding string representations of the date/time (see example below).\n\n\nRelational attributes\n\n\nRelational attribute declarations take the form:\n\n\n    @attribute [name] relational\n      [further attribute definitions]\n    @end [name]\n\n\n\n\nFor the multi-instance dataset \nMUSK1\n the definition would look like this (\n\"...\"\n denotes an omission):\n\n\n @attribute molecule_name {MUSK-jf78,...,NON-MUSK-199}\n @attribute bag relational\n   @attribute f1 numeric\n   ...\n   @attribute f166 numeric\n @end bag\n @attribute class {0,1}\n ...\n\n\n\n\nThe ARFF Data Section\n\n\nThe ARFF Data section of the file contains the data declaration line and the actual instance lines.\n\n\nThe @data Declaration\n\n\nThe \n@data\n declaration is a single line denoting the start of the data segment in the file. The format is:\n\n\n    @data\n\n\n\n\nThe instance data\n\n\nEach instance is represented on a single line, with carriage returns denoting the end of the instance.  A percent sign (%) introduces a comment, which continues to the end of the line.\n\n\nAttribute values for each instance can be delimited by commas or tabs. A comma/tab may be followed by zero or more spaces. Attribute values must appear in the order in which they were declared in the header section (i.e., the data corresponding to the nth \n@attribute\n declaration is always the nth field of the attribute).\n\n\nA missing value is represented by a single question mark, as in:\n\n\n    @data\n    4.4,?,1.5,?,Iris-setosa\n\n\n\n\nValues of string and nominal attributes are case sensitive, and any that contain space or the comment-delimiter character % must be quoted.  (The code suggests that double-quotes are acceptable and that a backslash will escape individual characters.)  An example follows:\n\n\n    @relation LCCvsLCSH\n\n    @attribute LCC string\n    @attribute LCSH string\n\n    @data\n    AG5,   'Encyclopedias and dictionaries.;Twentieth century.'\n    AS262, 'Science -- Soviet Union -- History.'\n    AE5,   'Encyclopedias and dictionaries.'\n    AS281, 'Astronomy, Assyro-Babylonian.;Moon -- Phases.'\n    AS281, 'Astronomy, Assyro-Babylonian.;Moon -- Tables.'\n\n\n\n\nDates must be specified in the data section using the string representation specified in the attribute declaration. For example:\n\n\n    @RELATION Timestamps\n\n    @ATTRIBUTE timestamp DATE \"yyyy-MM-dd HH:mm:ss\"\n\n    @DATA \n    \"2001-04-03 12:12:12\"\n    \"2001-05-03 12:59:55\"\n\n\n\n\nRelational data must be enclosed within double quotes \n\"\n. For example an instance of the \nMUSK1\n dataset (\n\"...\"\n denotes an omission):\n\n\nMUSK-188,\"42,...,30\",1\n\n\n\n\nSparse ARFF files\n\n\nSparse ARFF files are very similar to ARFF files, but data with value 0 are not be explicitly represented.\n\n\nSparse ARFF files have the same header (i.e \n@relation\n and \n@attribute\n tags) but the data section is different. Instead of representing each value in order, like this:\n\n\n    @data\n    0, X, 0, Y, \"class A\"\n    0, 0, W, 0, \"class B\"\n\n\n\n\nthe non-zero attributes are explicitly identified by attribute number and their value stated, like this:\n\n\n    @data\n    {1 X, 3 Y, 4 \"class A\"}\n    {2 W, 4 \"class B\"}\n\n\n\n\nEach instance is surrounded by curly braces, and the format for each entry is: [index] [space] [value] where index is the attribute index (starting from 0).\n\n\nNote that the omitted values in a sparse instance are \n0\n, they are not \"missing\" values! If a value is unknown, you must explicitly represent it with a question mark (?).\n\n\nWarning:\n There is a known problem saving SparseInstance objects from datasets that have string attributes. In Weka, string and nominal data values are stored as numbers; these numbers act as indexes into an array of possible attribute values (this is very efficient). However, the first string value is assigned index 0: this means that, internally, this value is stored as a 0. When a SparseInstance is written, string instances with internal value 0 are not output, so their string value is lost (and when the arff file is read again, the default value 0 is the index of a different string value, so the attribute value appears to change). To get around this problem, add a dummy string value at index 0 that is never used whenever you declare string attributes that are likely to be used in SparseInstance objects and saved as Sparse ARFF files.\n\n\nInstance weights in ARFF files\n\n\nA weight can be associated with an instance in a standard ARFF file by appending it to the end of the line for that instance and enclosing the value in curly braces. E.g:\n\n\n    @data\n    0, X, 0, Y, \"class A\", {5}\n\n\n\n\nFor a sparse instance, this example would look like:\n\n\n    @data\n    {1 X, 3 Y, 4 \"class A\"}, {5}\n\n\n\n\nNote that any instance without a weight value specified is assumed to have a weight of 1 for backwards compatibility.\n\n\nSee also\n\n\n\n\n[[Add weights to dataset]]\n\n\nARFF Syntax Highlighting\n for various editors\n\n\n\n\nLinks\n\n\n\n\nISO 8601\n\n\nJavadoc of \njava.text.SimpleDateFormat\n (lists the supported character patterns)\n\n\nANTLR syntax by Staal A. Vinterbo \narff.g",
            "title": " ARFF (developer version)"
        },
        {
            "location": "/arff_developer/#overview",
            "text": "ARFF files have two distinct sections. The first section is the  Header  information, which is followed the  Data  information.  The  Header  of the ARFF file contains the name of the relation, a list of the attributes (the columns in the data), and their types. An example header on the standard IRIS dataset looks like this:     % 1. Title: Iris Plants Database\n   % \n   % 2. Sources:\n   %      (a) Creator: R.A. Fisher\n   %      (b) Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n   %      (c) Date: July, 1988\n   % \n   @RELATION iris\n\n   @ATTRIBUTE sepallength  NUMERIC\n   @ATTRIBUTE sepalwidth   NUMERIC\n   @ATTRIBUTE petallength  NUMERIC\n   @ATTRIBUTE petalwidth   NUMERIC\n   @ATTRIBUTE class        {Iris-setosa,Iris-versicolor,Iris-virginica}  The  Data  of the ARFF file looks like the following:     @DATA\n   5.1,3.5,1.4,0.2,Iris-setosa\n   4.9,3.0,1.4,0.2,Iris-setosa\n   4.7,3.2,1.3,0.2,Iris-setosa\n   4.6,3.1,1.5,0.2,Iris-setosa\n   5.0,3.6,1.4,0.2,Iris-setosa\n   5.4,3.9,1.7,0.4,Iris-setosa\n   4.6,3.4,1.4,0.3,Iris-setosa\n   5.0,3.4,1.5,0.2,Iris-setosa\n   4.4,2.9,1.4,0.2,Iris-setosa\n   4.9,3.1,1.5,0.1,Iris-setosa  Lines that begin with a  %  are comments. The  @RELATION ,  @ATTRIBUTE  and  @DATA  declarations are case insensitive.",
            "title": "Overview"
        },
        {
            "location": "/arff_developer/#examples",
            "text": "Several well-known machine learning datasets are distributed with Weka in the  $WEKAHOME/data  directory as ARFF files.",
            "title": "Examples"
        },
        {
            "location": "/arff_developer/#the-arff-header-section",
            "text": "The ARFF Header section of the file contains the relation declaration and attribute declarations.",
            "title": "The ARFF Header Section"
        },
        {
            "location": "/arff_developer/#the-relation-declaration",
            "text": "The relation name is defined as the first line in the ARFF file. The format is:      @relation [relation-name]  where  [relation-name]  is a string. The string must be quoted if the name includes spaces. Furthermore, relation names or attribute names (see below) cannot begin with   a character below \\u0021  '{', '}', ',', or '%'    Moreover, it can only begin with a single or double quote if there is a corresponding quote at the end of the name.  == The @attribute Declarations ==\nAttribute declarations take the form of an ordered sequence of  @attribute  statements. Each attribute in the data set has its own  @attribute  statement which uniquely defines the name of that attribute and its data type. The order the attributes are declared indicates the column position in the data section of the file. For example, if an attribute is the third one declared then Weka expects that all that attributes values will be found in the third comma delimited column.  The format for the  @attribute  statement is:      @attribute [attribute-name] [datatype]  where the  [attribute-name]  must adhere to the constraints specified in the above section on the @relation declaration.  The [datatype] can be any of the four types supported by Weka:   numeric  integer  is treated as  numeric  real  is treated as  numeric  [nominal-specification]  string  date  [date-format]  relational  for multi-instance data (for future use)   where  [nominal-specification]  and  [date-format]  are defined below. The keywords  numeric ,  real ,  integer ,  string  and  date  are case insensitive.",
            "title": "The @relation Declaration"
        },
        {
            "location": "/arff_developer/#numeric-attributes",
            "text": "Numeric attributes can be real or integer numbers.",
            "title": "Numeric attributes"
        },
        {
            "location": "/arff_developer/#nominal-attributes",
            "text": "Nominal values are defined by providing an [nominal-specification] listing the possible values: {[nominal-name1], [nominal-name2], [nominal-name3], ...}  For example, the class value of the Iris dataset can be defined as follows:      @ATTRIBUTE class        {Iris-setosa,Iris-versicolor,Iris-virginica}  Values that contain spaces must be quoted.",
            "title": "Nominal attributes"
        },
        {
            "location": "/arff_developer/#string-attributes",
            "text": "String attributes allow us to create attributes containing arbitrary textual values. This is very useful in text-mining applications, as we can create datasets with string attributes, then write Weka Filters to manipulate strings (like  StringToWordVectorFilter ). String attributes are declared as follows:      @ATTRIBUTE LCC    string",
            "title": "String attributes"
        },
        {
            "location": "/arff_developer/#date-attributes",
            "text": "Date attribute declarations take the form:      @attribute [name] date [[date-format]]  where [name] is the name for the attribute and  [date-format]  is an optional string specifying how date values should be parsed and printed (this is the same format used by  SimpleDateFormat ). The default format string accepts the ISO-8601 combined date and time format:  yyyy-MM-dd'T'HH:mm:ss . Check out the Javadoc of the  java.text.SimpleDateFormat  class for supported character patterns.  Dates must be specified in the data section as the corresponding string representations of the date/time (see example below).",
            "title": "Date attributes"
        },
        {
            "location": "/arff_developer/#relational-attributes",
            "text": "Relational attribute declarations take the form:      @attribute [name] relational\n      [further attribute definitions]\n    @end [name]  For the multi-instance dataset  MUSK1  the definition would look like this ( \"...\"  denotes an omission):   @attribute molecule_name {MUSK-jf78,...,NON-MUSK-199}\n @attribute bag relational\n   @attribute f1 numeric\n   ...\n   @attribute f166 numeric\n @end bag\n @attribute class {0,1}\n ...",
            "title": "Relational attributes"
        },
        {
            "location": "/arff_developer/#the-arff-data-section",
            "text": "The ARFF Data section of the file contains the data declaration line and the actual instance lines.",
            "title": "The ARFF Data Section"
        },
        {
            "location": "/arff_developer/#the-data-declaration",
            "text": "The  @data  declaration is a single line denoting the start of the data segment in the file. The format is:      @data",
            "title": "The @data Declaration"
        },
        {
            "location": "/arff_developer/#the-instance-data",
            "text": "Each instance is represented on a single line, with carriage returns denoting the end of the instance.  A percent sign (%) introduces a comment, which continues to the end of the line.  Attribute values for each instance can be delimited by commas or tabs. A comma/tab may be followed by zero or more spaces. Attribute values must appear in the order in which they were declared in the header section (i.e., the data corresponding to the nth  @attribute  declaration is always the nth field of the attribute).  A missing value is represented by a single question mark, as in:      @data\n    4.4,?,1.5,?,Iris-setosa  Values of string and nominal attributes are case sensitive, and any that contain space or the comment-delimiter character % must be quoted.  (The code suggests that double-quotes are acceptable and that a backslash will escape individual characters.)  An example follows:      @relation LCCvsLCSH\n\n    @attribute LCC string\n    @attribute LCSH string\n\n    @data\n    AG5,   'Encyclopedias and dictionaries.;Twentieth century.'\n    AS262, 'Science -- Soviet Union -- History.'\n    AE5,   'Encyclopedias and dictionaries.'\n    AS281, 'Astronomy, Assyro-Babylonian.;Moon -- Phases.'\n    AS281, 'Astronomy, Assyro-Babylonian.;Moon -- Tables.'  Dates must be specified in the data section using the string representation specified in the attribute declaration. For example:      @RELATION Timestamps\n\n    @ATTRIBUTE timestamp DATE \"yyyy-MM-dd HH:mm:ss\"\n\n    @DATA \n    \"2001-04-03 12:12:12\"\n    \"2001-05-03 12:59:55\"  Relational data must be enclosed within double quotes  \" . For example an instance of the  MUSK1  dataset ( \"...\"  denotes an omission):  MUSK-188,\"42,...,30\",1",
            "title": "The instance data"
        },
        {
            "location": "/arff_developer/#sparse-arff-files",
            "text": "Sparse ARFF files are very similar to ARFF files, but data with value 0 are not be explicitly represented.  Sparse ARFF files have the same header (i.e  @relation  and  @attribute  tags) but the data section is different. Instead of representing each value in order, like this:      @data\n    0, X, 0, Y, \"class A\"\n    0, 0, W, 0, \"class B\"  the non-zero attributes are explicitly identified by attribute number and their value stated, like this:      @data\n    {1 X, 3 Y, 4 \"class A\"}\n    {2 W, 4 \"class B\"}  Each instance is surrounded by curly braces, and the format for each entry is: [index] [space] [value] where index is the attribute index (starting from 0).  Note that the omitted values in a sparse instance are  0 , they are not \"missing\" values! If a value is unknown, you must explicitly represent it with a question mark (?).  Warning:  There is a known problem saving SparseInstance objects from datasets that have string attributes. In Weka, string and nominal data values are stored as numbers; these numbers act as indexes into an array of possible attribute values (this is very efficient). However, the first string value is assigned index 0: this means that, internally, this value is stored as a 0. When a SparseInstance is written, string instances with internal value 0 are not output, so their string value is lost (and when the arff file is read again, the default value 0 is the index of a different string value, so the attribute value appears to change). To get around this problem, add a dummy string value at index 0 that is never used whenever you declare string attributes that are likely to be used in SparseInstance objects and saved as Sparse ARFF files.",
            "title": "Sparse ARFF files"
        },
        {
            "location": "/arff_developer/#instance-weights-in-arff-files",
            "text": "A weight can be associated with an instance in a standard ARFF file by appending it to the end of the line for that instance and enclosing the value in curly braces. E.g:      @data\n    0, X, 0, Y, \"class A\", {5}  For a sparse instance, this example would look like:      @data\n    {1 X, 3 Y, 4 \"class A\"}, {5}  Note that any instance without a weight value specified is assumed to have a weight of 1 for backwards compatibility.",
            "title": "Instance weights in ARFF files"
        },
        {
            "location": "/arff_developer/#see-also",
            "text": "[[Add weights to dataset]]  ARFF Syntax Highlighting  for various editors",
            "title": "See also"
        },
        {
            "location": "/arff_developer/#links",
            "text": "ISO 8601  Javadoc of  java.text.SimpleDateFormat  (lists the supported character patterns)  ANTLR syntax by Staal A. Vinterbo  arff.g",
            "title": "Links"
        },
        {
            "location": "/arff_syntax/",
            "text": "Here you can find syntax highlightings for various editors:\n\n\nEmacs\n\n\nAdd the code from the \narff.emacs\n file into your startup file.\n\n\nNotepad++\n\n\n\n\nCopy the contents of tag of the \narff.notepadplus\n file into your \n%APPDATA%\\Notepad++\\userDefineLang.xml\n file.\n\n\n(Ensure that you maintain the XML structure). If \nuserDefineLang.xml\n does not exist, simply rename the \narff.notepadplus\n file to \nuserDefineLang.xml\n\n\n\n\nTextPad\n\n\n\n\nCopy the file \narff.syn\n into your \n<TEXTPAD-DIR>/system\n directory.\n\n\nThen run the wizard for adding a new document class (Configure -> New Document Class...).\n\n\n\n\nUltraedit\n\n\n\n\nJust copy/paste the content of the file \narff.ultraedit\n in your \n<ULTRAEDIT-DIR>/WORDFILE.TXT\n file.\n\n\nAdjust the \n/Lnn\n language number that it fits into the numbering of your current settings.\n\n\n\n\nvim/gvim\n\n\n\n\nSave the file \narff.vim\n in your \n$HOME/.vim/syntax\n directory.\n\n\nYou can enable the syntax with \n:set syntax=arff\n.\n\n\n\n\nLinks\n\n\n\n\nEmacs homepage\n\n\nNotepad++ homepage\n\n\nTextPad homepage\n\n\nUltraedit homepage\n\n\nvim homepage",
            "title": " ARFF Syntax Highlighting"
        },
        {
            "location": "/arff_syntax/#emacs",
            "text": "Add the code from the  arff.emacs  file into your startup file.",
            "title": "Emacs"
        },
        {
            "location": "/arff_syntax/#notepad",
            "text": "Copy the contents of tag of the  arff.notepadplus  file into your  %APPDATA%\\Notepad++\\userDefineLang.xml  file.  (Ensure that you maintain the XML structure). If  userDefineLang.xml  does not exist, simply rename the  arff.notepadplus  file to  userDefineLang.xml",
            "title": "Notepad++"
        },
        {
            "location": "/arff_syntax/#textpad",
            "text": "Copy the file  arff.syn  into your  <TEXTPAD-DIR>/system  directory.  Then run the wizard for adding a new document class (Configure -> New Document Class...).",
            "title": "TextPad"
        },
        {
            "location": "/arff_syntax/#ultraedit",
            "text": "Just copy/paste the content of the file  arff.ultraedit  in your  <ULTRAEDIT-DIR>/WORDFILE.TXT  file.  Adjust the  /Lnn  language number that it fits into the numbering of your current settings.",
            "title": "Ultraedit"
        },
        {
            "location": "/arff_syntax/#vimgvim",
            "text": "Save the file  arff.vim  in your  $HOME/.vim/syntax  directory.  You can enable the syntax with  :set syntax=arff .",
            "title": "vim/gvim"
        },
        {
            "location": "/arff_syntax/#links",
            "text": "Emacs homepage  Notepad++ homepage  TextPad homepage  Ultraedit homepage  vim homepage",
            "title": "Links"
        },
        {
            "location": "/classpath/",
            "text": "The \nCLASSPATH\n environment variable tells Java where to look for classes. Since Java does the search in a ''first-come-first-serve'' kind of manner, you'll have to take care where and what to put in your CLASSPATH. I, personally, never use the environment variable, since I'm working often on a project in different versions in parallel. The CLASSPATH would just mess up things, if you're not careful (or just forget to remove an entry). [[ANT]] offers a nice way for building (and separating source code and class files) Java projects.\nBut still, if you're only working on totally separate projects, it might be easiest for you to use the environment variable.\n\n\nSetting the CLASSPATH\n\n\nIn the following we add the \nmysql-connector-java-5.1.6-bin.jar\n to our \nCLASSPATH\n variable (this works for any other jar archive) to make it possible to access MySQL \nDatabases\n via JDBC.\n\n\nWindows\n\n\nWe assume that the \nmysql-connector-java-5.1.6-bin.jar\n archive is located in the following directory:\n\n\nC:\\Program Files\\Weka-3-8\n\n\n\n\nIn the \nControl Panel\n click on \nSystem\n (or right click on \nThis PC\n and select \nProperties\n) and then go to the \nAdvanced\n tab. There you will find a button called \nEnvironment Variables\n, click it.\n\n\nDepending on, whether you're the only person using this computer or it is a lab computer shared by many, you can either create a new system-wide (you are the only user) environment variable or a user dependent one (recommended for multi-user machines). Enter the following name for the variable\n\n\nCLASSPATH\n\n\n\n\nand add this value\n\n\nC:\\Program Files\\Weka-3-8\\mysql-connector-java-5.1.6-bin.jar\n\n\n\n\nIf you want to add additional jars, you'll have to separate them with the path separator, the semicolon \n;\n (no spaces!).\n\n\nUnix/Linux\n\n\nI assume, that the mysql jar is located in the following directory:\n\n\n/home/johndoe/jars/\n\n\n\n\nOpen a shell and execute the following command, depending on the shell you're using:\n\n\n\n\nbash\n\n\n\n\nexport CLASSPATH=$CLASSPATH:/home/johndoe/jars/mysql-connector-java-5.1.6-bin.jar\n\n\n\n\nc shell\n\n\n\n\nsetenv CLASSPATH $CLASSPATH:/home/johndoe/jars/mysql-connector-java-5.1.6-bin.jar\n\n\nUnix/Linux uses the colon \n:\n as path separator, in contrast to Windows, which uses the semicolon \n;\n.\n\n\nNote:\n the prefixing with \n$CLASSPATH\n adds the mysql jar at the end of the currently existing \nCLASSPATH\n.\n\n\nCygwin\n\n\nThe process is like with Unix/Linux systems, but since the host system is Win32 and therefore the Java installation also a Windows application, you'll have to use the semicolon \n;\n as separator for several jars.",
            "title": " CLASSPATH"
        },
        {
            "location": "/classpath/#setting-the-classpath",
            "text": "In the following we add the  mysql-connector-java-5.1.6-bin.jar  to our  CLASSPATH  variable (this works for any other jar archive) to make it possible to access MySQL  Databases  via JDBC.",
            "title": "Setting the CLASSPATH"
        },
        {
            "location": "/classpath/#windows",
            "text": "We assume that the  mysql-connector-java-5.1.6-bin.jar  archive is located in the following directory:  C:\\Program Files\\Weka-3-8  In the  Control Panel  click on  System  (or right click on  This PC  and select  Properties ) and then go to the  Advanced  tab. There you will find a button called  Environment Variables , click it.  Depending on, whether you're the only person using this computer or it is a lab computer shared by many, you can either create a new system-wide (you are the only user) environment variable or a user dependent one (recommended for multi-user machines). Enter the following name for the variable  CLASSPATH  and add this value  C:\\Program Files\\Weka-3-8\\mysql-connector-java-5.1.6-bin.jar  If you want to add additional jars, you'll have to separate them with the path separator, the semicolon  ;  (no spaces!).",
            "title": "Windows"
        },
        {
            "location": "/classpath/#unixlinux",
            "text": "I assume, that the mysql jar is located in the following directory:  /home/johndoe/jars/  Open a shell and execute the following command, depending on the shell you're using:   bash   export CLASSPATH=$CLASSPATH:/home/johndoe/jars/mysql-connector-java-5.1.6-bin.jar   c shell   setenv CLASSPATH $CLASSPATH:/home/johndoe/jars/mysql-connector-java-5.1.6-bin.jar  Unix/Linux uses the colon  :  as path separator, in contrast to Windows, which uses the semicolon  ; .  Note:  the prefixing with  $CLASSPATH  adds the mysql jar at the end of the currently existing  CLASSPATH .",
            "title": "Unix/Linux"
        },
        {
            "location": "/classpath/#cygwin",
            "text": "The process is like with Unix/Linux systems, but since the host system is Win32 and therefore the Java installation also a Windows application, you'll have to use the semicolon  ;  as separator for several jars.",
            "title": "Cygwin"
        },
        {
            "location": "/packages/manager/",
            "text": "Usually, the term \"package\" is used to refer to Java's concept of organizing classes. From version 3.7.2, Weka has the concept of a package as a bundle of additional functionality, separate from that supplied in the main weka.jar file. A package consists of various jar files, documentation, meta data, and possibly source code. Many learning algorithms and tools that were present in earlier versions of Weka have become separate packages from version 3.7.2. This simplifies the core Weka system and allows users to install just what they need or are interested in. It also provides a simple mechanism for people to use when contributing to Weka. There are a number of packages available for Weka that add learning schemes or extend the functionality of the core system in some fashion. Many are provided by the Weka team and others are from third parties.\n\n\nWeka includes a facility for the management of packages and a mechanism to load them dynamically at runtime. There is both a command-line and GUI package manager. If the package manager does not start when you try to run it, take a look at \nthis\n page.\n\n\nCommand line package management\n\n\nAssuming that the weka.jar file is in the classpath, the package manager can be accessed by typing:\n\n\n java weka.core.WekaPackageManager\n\n\n\n\nSupplying no options will print the usage information:\n\n\nUsage: weka.core.WekaPackageManager [option]\nOptions:\n  -list-packages <all | installed | available>\n  -package-info <repository | installed | archive> packageName\n  -install-package <packageName | packageZip | URL> [version]\n  -uninstall-package <packageName>\n  -refresh-cache\n\n\n\n\n\n\nWeka 3.7.8 and snapshot builds of the developer version of Weka after September 24 2012 now offer a completely \"offline\" mode that involves no attempts to connect to the internet. This mode can be used to install package zip files that the user already has on the file system, and to browse already installed packages. This mode can be accessed from the command line package manager by specifying the \"-offline\" option. Alternatively, the property weka.packageManager.offline=true can be provided to the Java virtual machine on the command line or in a properties file (see the section on properties below).\n\n\n\n\nInformation (meta data) about packages is stored on a web server hosted on Sourceforge. The first time the package manager is run, for a new installation of Weka, there will be a short delay while the system downloads and stores a cache of the meta data from the server. Maintaining a cache speeds up the process of browsing the package information. From time to time you should update the local cache of package meta data in order to get the latest information on packages from the server. This can be achieved by supplying the \n-refresh-cache\n option.\n\n\nThe \n-list-packages\n option will, as the name suggests, print information (version numbers and short descriptions) about various packages. The option must be followed by one of three keywords:\n\n\n\n\nall\n will print information on all packages that the system knows about\n\n\ninstalled\n will print information on all packages that are installed locally\n\n\navailable\n will print information on all packages that are not installed\n\n\n\n\nThe following shows an example of listing all packages installed locally:\n\n\njava weka.core.WekaPackageManager -list-packages installed\n\nInstalled    Repository    Package\n=========    ==========    =======\n1.0.0        1.0.0         DTNB: Class for building and using a decision table/naive bayes hybrid classifier.\n1.0.0        1.0.0         massiveOnlineAnalysis: MOA (Massive On-line Analysis).\n1.0.0        1.0.0         multiInstanceFilters: A collection of filters for manipulating multi-instance data.\n1.0.0        1.0.0         naiveBayesTree: Class for generating a decision tree with naive Bayes classifiers at the leaves.\n1.0.0        1.0.0         scatterPlot3D: A visualization component for displaying a 3D scatter plot of the data using Java 3D.\n\n\n\n\nThe \n-package-info\n command lists information about a package given its name. The command is followed by one of three keywords and then the name of a package:\n\n\n\n\nrepository\n will print info from the repository for the named package\n\n\ninstalled\n will print info on the installed version of the named package\n\n\narchive\n will print info for a package stored in a zip archive. In this case, the \u201carchive\u201d keyword must be followed by the path to an package zip archive file rather than just the name of a package\n\n\n\n\nThe following shows an example of listing information for the \u201cisotonicRegression\u201d package from the server:\n\n\njava weka.core.WekaPackageManager -package-info repository isotonicRegression\nDescription:Learns an isotonic regression model. Picks the attribute that results in the lowest squared error. Missing values are not allowed. Can only deal with numeric attributes. Considers the monotonically increasing case as well as the monotonically decreasing case.\nVersion:1.0.0\nPackageURL:http://prdownloads.sourceforge.net/weka/isotonicRegression1.0.0.zip?download\nAuthor:Eibe Frank\nPackageName:isotonicRegression\nTitle:Learns an isotonic regression model.\nDate:2009-09-10\nURL:http://weka.sourceforge.net/doc.dev/weka/classifiers/IsotonicRegression.html\nCategory:Regression\nDepends:weka (>=3.7.1)\nLicense:GPL 2.0\nMaintainer:Weka team <wekalist@list.scms.waikato.ac.nz>\n\n\n\n\nThe \n-install-package\n command allows a package to be installed from one of three locations:\n\n\n\n\nspecifying a name of a package will install the package using the information in the package description meta data stored on the server. If no version number is given, then the latest available version of the package is installed.\n\n\nproviding a path to a zip file will attempt to unpack and install the archive as a Weka package\n\n\nproviding a URL (beginning with \nhttp://\n) to a package zip file on the web will download and attempt to install the zip file as a Weka package\n\n\n\n\nThe \nuninstall-package\n command will uninstall the named package. Of course, the named package has to be installed for this command to have any effect!\n\n\nRunning installed learning algorithms\n\n\nRunning learning algorithms that come with the main weka distribution (i.e. are contained in the weka.jar file) was covered earlier in the [[Primer]]. But what about algorithms from packages that you\u2019ve installed using the package manager? We don\u2019t want to have to add a ton of jar files to our classpath every time we wan\u2019t to run a particular algorithm. Fortunately, we don\u2019t have to. Weka has a mechanism to load installed packages dynamically at run time. This means that newly installed packages are available in Weka's GUIs immediately.\n\n\nWhat about running algorithms from packages on the command line I hear you ask? We can run a named algorithm by using the Run command:\n\n\n java weka.Run\n\n\n\n\nIf no arguments are supplied, then Run outputs the following usage information:\n\n\n Usage:\n     weka.Run [-no-scan] [-no-load] <scheme name [scheme options]>\n\n\n\n\nThe Run command supports sub-string matching, so you can run a classifier (such as J48) like so:\n\n\n java weka.Run J48\n\n\n\n\nWhen there are multiple matches on a supplied scheme name you will be presented with a list. For example:\n\n\n java weka.Run NaiveBayes\n\n Select a scheme to run, or <return> to exit:\n     1) weka.classifiers.bayes.ComplementNaiveBayes\n     2) weka.classifiers.bayes.NaiveBayes\n     3) weka.classifiers.bayes.NaiveBayesMultinomial\n     4) weka.classifiers.bayes.NaiveBayesMultinomialUpdateable\n     5) weka.classifiers.bayes.NaiveBayesSimple\n     6) weka.classifiers.bayes.NaiveBayesUpdateable\n\n Enter a number >\n\n\n\n\nYou can turn off the scanning of packages and sub-string matching by providing the \n-no-scan\n option. This is useful when using the \nRun\n command in a script. In this case, you need to specify the fully qualified name of the algorithm to use. E.g.\n\n\n java weka.Run -no-scan weka.classifiers.bayes.NaiveBayes\n\n\n\n\nTo reduce startup time you can also turn off the dynamic loading of installed packages by specifying the \n-no-load\n option. In this case, you will need to explicitly include any packaged algorithms in your classpath if you plan to use them. E.g.\n\n\njava -classpath ./weka.jar:$HOME/wekafiles/packages/DTNB/DTNB.jar rweka.Run -no-load -no-scan weka.classifiers.rules.DTNB\n\n\n\n\nGUI package manager\n\n\nAs well as a command line client, there is also a graphical interface to Weka\u2019s package management system. This is available from the \nTools\n menu in the \nGUIChooser\n. All the functionality available in the command line client to the package management system is available in the GUI version, along with the ability to install and uninstall multiple packages in one hit.\n\n\n\n\nThe package manager\u2019s window is split horizontally into two parts: at the top is a list of packages and at the bottom is a mini browser that can be used to display information on the currently selected package.\n\n\nThe package list shows the name of a package, its category, the currently installed version (if installed), the latest version available via the repository and whether the package has been loaded or not. This list may be sorted by either package name or category by clicking on the appropriate column header. A second click on the same header reverses the sort order. Three radio buttons in the upper left of the window can be used to filter what is displayed in the list. All packages (default), all available packages (i.e. those not yet installed) or only installed packages can be displayed.\n\n\nIf multiple versions of a package are available, they can be accessed by clicking on an entry in the \u201cRepository version\u201d column:\n\n\n\n\nInstalling and removing packages\n\n\nAt the very top of the window are three buttons. On the left-hand side is a button that can be used to refresh the cached copy of the package repository meta data. The first time that the package manager (GUI or command line) is used there will be a short delay as the initial cache is established. \n\n\nNOTE: Weka (3.7.2) will not automatically check for new information at the central repository, so it is a good idea to refresh the local cache regularly. From Weka 3.7.3 the package manager will notify you if there are new packages available at the central repository\n.\n\n\nThe two buttons at the top right are used to install and remove packages repspectively. Multiple packages may be installed/removed by using a shift-left-click combination to select a range and/or by using a command-left-click combination to add to the selection. Underneath the install and uninstall but- tons is a checkbox that can be enabled to ignore any dependencies required by selected packages and any conflicts that may occur. Installing packages while this checkbox is selected will '''not''' install required dependencies.\n\n\nSome packages may have additional information on how to complete the installation or special instructions that gets displayed when the package is installed:\n\n\n\n\nUsually it is not necessary to restart Weka after packages have been installed\u2014the changes should be available immediately. An exception is when upgrading a package that is already installed. If in doubt, restart Weka.\n\n\nUnofficial packages\n\n\nThe package list shows those packages that have their meta data stored in Weka\u2019s central meta data repository. These packages are \u201cofficial\u201d Weka packages and the Weka team as verified that they appear to provide what is advertised (and do not contain malicious code or malware).\n\n\nIt is also possible to install an \nunofficial\n package that has not gone through the process of become official. Unofficial packages might be provided, for exam- ple, by researchers who want to make experimental algorithms quickly available to the community for feedback. Unofficial packages can be installed by clicking the \u201cFile/url\u201d button on the top-right of the package manager window. This will bring up an \u201cUnnoficial package install\u201d dialog where the user can browse their file system for a package zip file or directly enter an URL to the package zip file. Note that no dependency checking is done for unofficial packages.\n\n\nUsing a HTTP proxy\n\n\nBoth the GUI and command line package managers can operate via a http proxy. To do so, start Weka from the command line and supply property values for the proxy host and port:\n\n\n java -Dhttp.proxyHost=some.proxy.somewhere.net -Dhttp.proxyPort=port weka.gui.GUIChooser\n\n\n\n\nIf your proxy requires authentication, then Weka will present a GUI dialog where you can enter your username and password. If you are running on a headless environment, then two more (non-standard) properties can be supplied:\n\n\n -Dhttp.proxyUser=some_user_name -Dhttp.proxyPassword=some_password\n\n\n\n\nUsing an alternative central package meta data repository\n\n\nBy default, both the command-line and GUI package managers use the central package meta data repository hosted on Sourceforge. In the unlikely event that this site is unavailable for one reason or another, it is possible to point the package management system at an alternative repository. This mechanism allows a temporary backup of the official repostory to be accessed, local mirrors to be established and alternative repositories to be set up for use etc.\n\n\nAn alternative repository can be specified by setting a Java property:\n\n\nweka.core.wekaPackageRepositoryURL=http://some.mirror.somewhere\n\n\n\n\nThis can either be set when starting Weka from the command line with the \n-D\n flag, or it can be placed into a file called \u201cPackageRepository.props\u201d in \n$WEKA_HOME/props\n. The default value of \nWEKA_HOME\n is \nuser.home/wekafiles\n, where \nuser.home\n is the user\u2019s home directory. More information on how and where Weka stores configuration information is given in the [[How are packages structured for the package management system?|how are package structured for the package management system?]] article.\n\n\nPackage manager property file\n\n\nAs mentioned in the previous section, an alternative package meta data repository can be specified by placing an entry in the PackageRepository.props file in \n$WEKA_HOME/props\n. From Weka 3.7.8 (and snapshot builds after 24 September 2012), the package manager also looks for properties placed in \n$WEKA_HOME/props/PackageManager.props\n. The current set of properties that can be set are:\n\n\nweka.core.wekaPackageRepositoryURL=http://some.mirror.somewhere\nweka.packageManager.offline=[true | false]\nweka.packageManager.loadPackages=[true | false]\nweka.pluginManager.disable=com.funky.FunkyExplorerPluginTab\n\n\n\n\nThe default for offline mode (if unspecified) is \nfalse\n and for loadPackages is \ntrue\n. The weka.pluginManager.disable property can be used to specify a comma-separated list of fully qualified class names to \"disable\" in the GUI. This can be used to make problematic components unavailable in the GUI without having to prevent the entire package that contains them from being loaded. E.g. \"funkyPackage\" might provide several classifiers and a special Explorer plugin tab for visualization. Suppose, for example, that the plugin Explorer tab has issues with certain data sets and causes annoying exceptions to be generated (or perhaps in the worst cases crashes the Explorer!). In this case we might want to use the classifiers provided by the package and just disable the Explorer plugin. Listing the fully qualified name of the Explorer plugin as a member of the comma-separated list associated with the weka.pluginManager.disable property will achieve this.",
            "title": " Package manager"
        },
        {
            "location": "/packages/manager/#command-line-package-management",
            "text": "Assuming that the weka.jar file is in the classpath, the package manager can be accessed by typing:   java weka.core.WekaPackageManager  Supplying no options will print the usage information:  Usage: weka.core.WekaPackageManager [option]\nOptions:\n  -list-packages <all | installed | available>\n  -package-info <repository | installed | archive> packageName\n  -install-package <packageName | packageZip | URL> [version]\n  -uninstall-package <packageName>\n  -refresh-cache   Weka 3.7.8 and snapshot builds of the developer version of Weka after September 24 2012 now offer a completely \"offline\" mode that involves no attempts to connect to the internet. This mode can be used to install package zip files that the user already has on the file system, and to browse already installed packages. This mode can be accessed from the command line package manager by specifying the \"-offline\" option. Alternatively, the property weka.packageManager.offline=true can be provided to the Java virtual machine on the command line or in a properties file (see the section on properties below).   Information (meta data) about packages is stored on a web server hosted on Sourceforge. The first time the package manager is run, for a new installation of Weka, there will be a short delay while the system downloads and stores a cache of the meta data from the server. Maintaining a cache speeds up the process of browsing the package information. From time to time you should update the local cache of package meta data in order to get the latest information on packages from the server. This can be achieved by supplying the  -refresh-cache  option.  The  -list-packages  option will, as the name suggests, print information (version numbers and short descriptions) about various packages. The option must be followed by one of three keywords:   all  will print information on all packages that the system knows about  installed  will print information on all packages that are installed locally  available  will print information on all packages that are not installed   The following shows an example of listing all packages installed locally:  java weka.core.WekaPackageManager -list-packages installed\n\nInstalled    Repository    Package\n=========    ==========    =======\n1.0.0        1.0.0         DTNB: Class for building and using a decision table/naive bayes hybrid classifier.\n1.0.0        1.0.0         massiveOnlineAnalysis: MOA (Massive On-line Analysis).\n1.0.0        1.0.0         multiInstanceFilters: A collection of filters for manipulating multi-instance data.\n1.0.0        1.0.0         naiveBayesTree: Class for generating a decision tree with naive Bayes classifiers at the leaves.\n1.0.0        1.0.0         scatterPlot3D: A visualization component for displaying a 3D scatter plot of the data using Java 3D.  The  -package-info  command lists information about a package given its name. The command is followed by one of three keywords and then the name of a package:   repository  will print info from the repository for the named package  installed  will print info on the installed version of the named package  archive  will print info for a package stored in a zip archive. In this case, the \u201carchive\u201d keyword must be followed by the path to an package zip archive file rather than just the name of a package   The following shows an example of listing information for the \u201cisotonicRegression\u201d package from the server:  java weka.core.WekaPackageManager -package-info repository isotonicRegression\nDescription:Learns an isotonic regression model. Picks the attribute that results in the lowest squared error. Missing values are not allowed. Can only deal with numeric attributes. Considers the monotonically increasing case as well as the monotonically decreasing case.\nVersion:1.0.0\nPackageURL:http://prdownloads.sourceforge.net/weka/isotonicRegression1.0.0.zip?download\nAuthor:Eibe Frank\nPackageName:isotonicRegression\nTitle:Learns an isotonic regression model.\nDate:2009-09-10\nURL:http://weka.sourceforge.net/doc.dev/weka/classifiers/IsotonicRegression.html\nCategory:Regression\nDepends:weka (>=3.7.1)\nLicense:GPL 2.0\nMaintainer:Weka team <wekalist@list.scms.waikato.ac.nz>  The  -install-package  command allows a package to be installed from one of three locations:   specifying a name of a package will install the package using the information in the package description meta data stored on the server. If no version number is given, then the latest available version of the package is installed.  providing a path to a zip file will attempt to unpack and install the archive as a Weka package  providing a URL (beginning with  http:// ) to a package zip file on the web will download and attempt to install the zip file as a Weka package   The  uninstall-package  command will uninstall the named package. Of course, the named package has to be installed for this command to have any effect!",
            "title": "Command line package management"
        },
        {
            "location": "/packages/manager/#running-installed-learning-algorithms",
            "text": "Running learning algorithms that come with the main weka distribution (i.e. are contained in the weka.jar file) was covered earlier in the [[Primer]]. But what about algorithms from packages that you\u2019ve installed using the package manager? We don\u2019t want to have to add a ton of jar files to our classpath every time we wan\u2019t to run a particular algorithm. Fortunately, we don\u2019t have to. Weka has a mechanism to load installed packages dynamically at run time. This means that newly installed packages are available in Weka's GUIs immediately.  What about running algorithms from packages on the command line I hear you ask? We can run a named algorithm by using the Run command:   java weka.Run  If no arguments are supplied, then Run outputs the following usage information:   Usage:\n     weka.Run [-no-scan] [-no-load] <scheme name [scheme options]>  The Run command supports sub-string matching, so you can run a classifier (such as J48) like so:   java weka.Run J48  When there are multiple matches on a supplied scheme name you will be presented with a list. For example:   java weka.Run NaiveBayes\n\n Select a scheme to run, or <return> to exit:\n     1) weka.classifiers.bayes.ComplementNaiveBayes\n     2) weka.classifiers.bayes.NaiveBayes\n     3) weka.classifiers.bayes.NaiveBayesMultinomial\n     4) weka.classifiers.bayes.NaiveBayesMultinomialUpdateable\n     5) weka.classifiers.bayes.NaiveBayesSimple\n     6) weka.classifiers.bayes.NaiveBayesUpdateable\n\n Enter a number >  You can turn off the scanning of packages and sub-string matching by providing the  -no-scan  option. This is useful when using the  Run  command in a script. In this case, you need to specify the fully qualified name of the algorithm to use. E.g.   java weka.Run -no-scan weka.classifiers.bayes.NaiveBayes  To reduce startup time you can also turn off the dynamic loading of installed packages by specifying the  -no-load  option. In this case, you will need to explicitly include any packaged algorithms in your classpath if you plan to use them. E.g.  java -classpath ./weka.jar:$HOME/wekafiles/packages/DTNB/DTNB.jar rweka.Run -no-load -no-scan weka.classifiers.rules.DTNB",
            "title": "Running installed learning algorithms"
        },
        {
            "location": "/packages/manager/#gui-package-manager",
            "text": "As well as a command line client, there is also a graphical interface to Weka\u2019s package management system. This is available from the  Tools  menu in the  GUIChooser . All the functionality available in the command line client to the package management system is available in the GUI version, along with the ability to install and uninstall multiple packages in one hit.   The package manager\u2019s window is split horizontally into two parts: at the top is a list of packages and at the bottom is a mini browser that can be used to display information on the currently selected package.  The package list shows the name of a package, its category, the currently installed version (if installed), the latest version available via the repository and whether the package has been loaded or not. This list may be sorted by either package name or category by clicking on the appropriate column header. A second click on the same header reverses the sort order. Three radio buttons in the upper left of the window can be used to filter what is displayed in the list. All packages (default), all available packages (i.e. those not yet installed) or only installed packages can be displayed.  If multiple versions of a package are available, they can be accessed by clicking on an entry in the \u201cRepository version\u201d column:",
            "title": "GUI package manager"
        },
        {
            "location": "/packages/manager/#installing-and-removing-packages",
            "text": "At the very top of the window are three buttons. On the left-hand side is a button that can be used to refresh the cached copy of the package repository meta data. The first time that the package manager (GUI or command line) is used there will be a short delay as the initial cache is established.   NOTE: Weka (3.7.2) will not automatically check for new information at the central repository, so it is a good idea to refresh the local cache regularly. From Weka 3.7.3 the package manager will notify you if there are new packages available at the central repository .  The two buttons at the top right are used to install and remove packages repspectively. Multiple packages may be installed/removed by using a shift-left-click combination to select a range and/or by using a command-left-click combination to add to the selection. Underneath the install and uninstall but- tons is a checkbox that can be enabled to ignore any dependencies required by selected packages and any conflicts that may occur. Installing packages while this checkbox is selected will '''not''' install required dependencies.  Some packages may have additional information on how to complete the installation or special instructions that gets displayed when the package is installed:   Usually it is not necessary to restart Weka after packages have been installed\u2014the changes should be available immediately. An exception is when upgrading a package that is already installed. If in doubt, restart Weka.",
            "title": "Installing and removing packages"
        },
        {
            "location": "/packages/manager/#unofficial-packages",
            "text": "The package list shows those packages that have their meta data stored in Weka\u2019s central meta data repository. These packages are \u201cofficial\u201d Weka packages and the Weka team as verified that they appear to provide what is advertised (and do not contain malicious code or malware).  It is also possible to install an  unofficial  package that has not gone through the process of become official. Unofficial packages might be provided, for exam- ple, by researchers who want to make experimental algorithms quickly available to the community for feedback. Unofficial packages can be installed by clicking the \u201cFile/url\u201d button on the top-right of the package manager window. This will bring up an \u201cUnnoficial package install\u201d dialog where the user can browse their file system for a package zip file or directly enter an URL to the package zip file. Note that no dependency checking is done for unofficial packages.",
            "title": "Unofficial packages"
        },
        {
            "location": "/packages/manager/#using-a-http-proxy",
            "text": "Both the GUI and command line package managers can operate via a http proxy. To do so, start Weka from the command line and supply property values for the proxy host and port:   java -Dhttp.proxyHost=some.proxy.somewhere.net -Dhttp.proxyPort=port weka.gui.GUIChooser  If your proxy requires authentication, then Weka will present a GUI dialog where you can enter your username and password. If you are running on a headless environment, then two more (non-standard) properties can be supplied:   -Dhttp.proxyUser=some_user_name -Dhttp.proxyPassword=some_password",
            "title": "Using a HTTP proxy"
        },
        {
            "location": "/packages/manager/#using-an-alternative-central-package-meta-data-repository",
            "text": "By default, both the command-line and GUI package managers use the central package meta data repository hosted on Sourceforge. In the unlikely event that this site is unavailable for one reason or another, it is possible to point the package management system at an alternative repository. This mechanism allows a temporary backup of the official repostory to be accessed, local mirrors to be established and alternative repositories to be set up for use etc.  An alternative repository can be specified by setting a Java property:  weka.core.wekaPackageRepositoryURL=http://some.mirror.somewhere  This can either be set when starting Weka from the command line with the  -D  flag, or it can be placed into a file called \u201cPackageRepository.props\u201d in  $WEKA_HOME/props . The default value of  WEKA_HOME  is  user.home/wekafiles , where  user.home  is the user\u2019s home directory. More information on how and where Weka stores configuration information is given in the [[How are packages structured for the package management system?|how are package structured for the package management system?]] article.",
            "title": "Using an alternative central package meta data repository"
        },
        {
            "location": "/packages/manager/#package-manager-property-file",
            "text": "As mentioned in the previous section, an alternative package meta data repository can be specified by placing an entry in the PackageRepository.props file in  $WEKA_HOME/props . From Weka 3.7.8 (and snapshot builds after 24 September 2012), the package manager also looks for properties placed in  $WEKA_HOME/props/PackageManager.props . The current set of properties that can be set are:  weka.core.wekaPackageRepositoryURL=http://some.mirror.somewhere\nweka.packageManager.offline=[true | false]\nweka.packageManager.loadPackages=[true | false]\nweka.pluginManager.disable=com.funky.FunkyExplorerPluginTab  The default for offline mode (if unspecified) is  false  and for loadPackages is  true . The weka.pluginManager.disable property can be used to specify a comma-separated list of fully qualified class names to \"disable\" in the GUI. This can be used to make problematic components unavailable in the GUI without having to prevent the entire package that contains them from being loaded. E.g. \"funkyPackage\" might provide several classifiers and a special Explorer plugin tab for visualization. Suppose, for example, that the plugin Explorer tab has issues with certain data sets and causes annoying exceptions to be generated (or perhaps in the worst cases crashes the Explorer!). In this case we might want to use the classifiers provided by the package and just disable the Explorer plugin. Listing the fully qualified name of the Explorer plugin as a member of the comma-separated list associated with the weka.pluginManager.disable property will achieve this.",
            "title": "Package manager property file"
        },
        {
            "location": "/packages/unofficial/",
            "text": "There are a number of packages for WEKA 3.8 on the internet that are not listed in the \"official\" WEKA package repository. These packages can nevertheless be easily installed via the package manager in WEKA 3.8 (available via the Tools menu in WEKA's GUIChooser) by providing the URL for the package .zip file.\n\n\nBelow is an (incomplete list) of packages that are available.\n\n\nPreprocessing\n\n\n\n\ndataset-weights\n -- filters for setting attribute and instance weights using various methods.\n\n\nmissing-values-imputation\n -- various methods for imputing missing values using a filter.\n\n\nmxexpression\n -- filter for updating a target attribute using a mathematical expression.\n\n\n\n\nClassification\n\n\n\n\nJava neural network package\n -- Java (convolutional or fully-connected) neural network implementation with plugin for \nWeka\n. Uses dropout and rectified linear units. Implementation is multithreaded and uses \nMTJ\n matrix library with native libs for performance.\n\n\nHMMWeka\n -- This library makes Hidden Markov Model machine learning available in Weka.\n\n\nCollective classification\n -- Algorithms around semi-supervised learning and collective classification.\n\n\nBagging ensemble selection\n -- Bagging Ensemble Selection - a new ensemble learning strategy.\n\n\nDataSqueezer\n -- Efficient rule builder that generates a set of production rules from labeled input data. It can handle missing data and has log-linear asymptotic complexity with the number of training examples.\n\n\nmiDS\n -- mi-DS is a multiple-Instance learning supervised algorithm based on the DataSqueezer algorithm.\n\n\nLibD3C\n -- Ensemble classifiers with a clustering and dynamic selection strategy.\n\n\nICRM\n -- An Interpretable Classification Rule Mining Algorithm.\n\n\ntclass\n -- TClass is a supervised learner for multivariate time series, originally developed by \nWaleed Kadous\n.\n\n\nwekaclassalgos\n -- collection of artificial neural network (ANN) algorithms and artificial immune system (AIS) algorithms, originally developed by \nJason Brownlee\n.\n\n\nmxexpression\n -- classifier for making predictions using a mathematical expression.\n\n\n\n\nClustering\n\n\n\n\nAPCluster\n -- Affinity propagation algorithm for clustering, used especially in bioinformatics and computer vision.\n\n\nFast Optics\n -- Fast Implementation of OPTICS algorithm using random projections for Euclidean distances.\n\n\n\n\nSimilarity functions\n\n\n\n\nwekabiosimilarity\n -- implements several measures to compare binary feature vectors; and, additionally, extrapolates those measures to work with multi-value, string and numerical feature vectors.\n\n\n\n\nDiscretization\n\n\n\n\nur-CAIM\n -- Improved CAIM Discretization for Unbalanced and Balanced Data.\n\n\nCAIM\n -- Class-Attribute Interdependence Maximization algorithm: discretizes a continuous feature into a number of intervals. This is done by using class information, without requiring the user to provide this number.\n\n\n\n\nFeature selection\n\n\n\n\nRSARSubsetEval\n -- Rough set feature selection.\n\n\n\n\nFrequent pattern mining\n\n\n\n\nXApriori\n --Available case analysis modification of Apriori frequent pattern mining algorithm.\n\n\n\n\nStemming\n\n\n\n\nSnowball stemmers\n -- Contains the actual snowball stemmer algorithms to make the Snowball stemmer wrapper in Weka work.\n\n\nPTStemmer\n -- Wrapper for \nPedro Oliveira's stemmer library\n for Portuguese.\n\n\n\n\nText mining\n\n\n\n\nnlp\n -- Contains components for natural language processing, eg part-of-speech tagging filter and Penn Tree Bank tokenizer. Makes use of the \nStanford Parser\n (parser models need to be downloaded separately).\n\n\n\n\nVisualization\n\n\n\n\ngraphviz-treevisualize\n -- Generating nice graphs in the Explorer from trees (eg J48) using the GraphViz executables.\n\n\nconfusionmatrix\n -- Various visualizations of confusion matrices in the Explorer.\n\n\nserialized-model-viewer\n -- Adds a standalone tab to the Explorer that allows the user to load a serialized model and view its content as text (simply uses the objects' \ntoString()\n method).\n\n\n\n\nParameter optimization\n\n\n\n\nmultisearch\n -- Meta-classifier similar to GridSearch, but for optimizing arbitrary number of parameters.\n\n\n\n\nOthers\n\n\n\n\nscreencast4j\n -- Allows you to record sound, webcam and screen feeds, storing them in separate files to be combined into a screencast using a \nvideo editor\n. This screencast you can then share on YouTube, for instance.\n\n\ncommand-to-code\n -- Turns command-lines (eg of classifiers or filters) into various Java code snippets.\n\n\njshell-scripting\n -- Allows scripting in Java, using \njshell",
            "title": " Unofficial packages"
        },
        {
            "location": "/packages/unofficial/#preprocessing",
            "text": "dataset-weights  -- filters for setting attribute and instance weights using various methods.  missing-values-imputation  -- various methods for imputing missing values using a filter.  mxexpression  -- filter for updating a target attribute using a mathematical expression.",
            "title": "Preprocessing"
        },
        {
            "location": "/packages/unofficial/#classification",
            "text": "Java neural network package  -- Java (convolutional or fully-connected) neural network implementation with plugin for  Weka . Uses dropout and rectified linear units. Implementation is multithreaded and uses  MTJ  matrix library with native libs for performance.  HMMWeka  -- This library makes Hidden Markov Model machine learning available in Weka.  Collective classification  -- Algorithms around semi-supervised learning and collective classification.  Bagging ensemble selection  -- Bagging Ensemble Selection - a new ensemble learning strategy.  DataSqueezer  -- Efficient rule builder that generates a set of production rules from labeled input data. It can handle missing data and has log-linear asymptotic complexity with the number of training examples.  miDS  -- mi-DS is a multiple-Instance learning supervised algorithm based on the DataSqueezer algorithm.  LibD3C  -- Ensemble classifiers with a clustering and dynamic selection strategy.  ICRM  -- An Interpretable Classification Rule Mining Algorithm.  tclass  -- TClass is a supervised learner for multivariate time series, originally developed by  Waleed Kadous .  wekaclassalgos  -- collection of artificial neural network (ANN) algorithms and artificial immune system (AIS) algorithms, originally developed by  Jason Brownlee .  mxexpression  -- classifier for making predictions using a mathematical expression.",
            "title": "Classification"
        },
        {
            "location": "/packages/unofficial/#clustering",
            "text": "APCluster  -- Affinity propagation algorithm for clustering, used especially in bioinformatics and computer vision.  Fast Optics  -- Fast Implementation of OPTICS algorithm using random projections for Euclidean distances.",
            "title": "Clustering"
        },
        {
            "location": "/packages/unofficial/#similarity-functions",
            "text": "wekabiosimilarity  -- implements several measures to compare binary feature vectors; and, additionally, extrapolates those measures to work with multi-value, string and numerical feature vectors.",
            "title": "Similarity functions"
        },
        {
            "location": "/packages/unofficial/#discretization",
            "text": "ur-CAIM  -- Improved CAIM Discretization for Unbalanced and Balanced Data.  CAIM  -- Class-Attribute Interdependence Maximization algorithm: discretizes a continuous feature into a number of intervals. This is done by using class information, without requiring the user to provide this number.",
            "title": "Discretization"
        },
        {
            "location": "/packages/unofficial/#feature-selection",
            "text": "RSARSubsetEval  -- Rough set feature selection.",
            "title": "Feature selection"
        },
        {
            "location": "/packages/unofficial/#frequent-pattern-mining",
            "text": "XApriori  --Available case analysis modification of Apriori frequent pattern mining algorithm.",
            "title": "Frequent pattern mining"
        },
        {
            "location": "/packages/unofficial/#stemming",
            "text": "Snowball stemmers  -- Contains the actual snowball stemmer algorithms to make the Snowball stemmer wrapper in Weka work.  PTStemmer  -- Wrapper for  Pedro Oliveira's stemmer library  for Portuguese.",
            "title": "Stemming"
        },
        {
            "location": "/packages/unofficial/#text-mining",
            "text": "nlp  -- Contains components for natural language processing, eg part-of-speech tagging filter and Penn Tree Bank tokenizer. Makes use of the  Stanford Parser  (parser models need to be downloaded separately).",
            "title": "Text mining"
        },
        {
            "location": "/packages/unofficial/#visualization",
            "text": "graphviz-treevisualize  -- Generating nice graphs in the Explorer from trees (eg J48) using the GraphViz executables.  confusionmatrix  -- Various visualizations of confusion matrices in the Explorer.  serialized-model-viewer  -- Adds a standalone tab to the Explorer that allows the user to load a serialized model and view its content as text (simply uses the objects'  toString()  method).",
            "title": "Visualization"
        },
        {
            "location": "/packages/unofficial/#parameter-optimization",
            "text": "multisearch  -- Meta-classifier similar to GridSearch, but for optimizing arbitrary number of parameters.",
            "title": "Parameter optimization"
        },
        {
            "location": "/packages/unofficial/#others",
            "text": "screencast4j  -- Allows you to record sound, webcam and screen feeds, storing them in separate files to be combined into a screencast using a  video editor . This screencast you can then share on YouTube, for instance.  command-to-code  -- Turns command-lines (eg of classifiers or filters) into various Java code snippets.  jshell-scripting  -- Allows scripting in Java, using  jshell",
            "title": "Others"
        },
        {
            "location": "/packages/structure/",
            "text": "Articles such as \nHow do I use WEKA's classes in my own code?\n and \nHow do I write a new classifier or filter?\n describe how to extend Weka to add your own learning algorithms and so forth. This article describes how such enhancements can be assembled into a \npackage\n that can be accessed via Weka\u2019s \npackage management\n system. Bundling your enhancements in a package makes it easy to share with other Weka users.\n\n\nIn this article we refer to a \npackage\n as an archive containing various resources such as compiled code, source code, javadocs, package description files (meta data), third-party libraries and configuration property files. Not all of the preceding may be in a given package, and there may be other resources included as well. This concept of a \npackage\n is quite different to that of a Java packages, which simply define how classes are arranged hierarchically.\n\n\nWhere does WEKA store packages and other configuration stuff?\n\n\nBy default, Weka stores packages and other information in \n$WEKA_HOME\n. The default location for \nWEKA_HOME\n is \nuser.home/wekafiles\n, where \nuser.home\n is the user\u2019s home directory. You can change the default location for \nWEKA_HOME\n by setting this either as an evironment variable for your platform, or by specifying it as a Java property when starting Weka. E.g.:\n\n\nexport WEKA_HOME=/home/somewhere/weka_bits_and_bobs\n\n\n\n\nwill set the directory that Weka uses to \n/home/somewhere/weka_bits_and_bobs\n under the LINUX operating system.\n\n\nThe same thing can be accomplished when starting Weka by specifying a Java property on the command line, E.g.:\n\n\njava -DWEKA_HOME=/home/somewhere/weka_bits_and_bobs -jar weka.jar\n\n\n\n\nInside \n$WEKA_HOME\n you will find the main weka log file (weka.log) and a number of directories:\n\n\n\n\npackages\n holds installed packages. Each package is contained its own subdirectory.\n\n\nprops\n holds various Java property files used by Weka. This directory replaces the user\u2019s home directory (used in earlier releases of Weka) as one of the locations checked by Weka for properties files (such as \nDatabaseUtils.props\n). Weka first checks, in order, the current directory (i.e. the directory that Weka is launched from), then \n$WEKA_HOME/props\n and finally the \nweka.jar\n file for property files.\n\n\nrepCache\n holds the cached copy of the meta data from the central package repository. If the contents of this directory get corrupted it can be safely deleted and Weka will recreate it on the next restart.\n\n\nsystemDialogs\n holds marker files that are created when you check \nDon\u2019t show this again\n in various system popup dialogs. Removing this directory or its contents will cause Weka to display those prompts anew.\n\n\n\n\nAnatomy of a package\n\n\nA Weka package is a zip archive that must unpack to the current directory. For example, the \nDTNB\n package contains the decision table naive Bayes hybrid classifier and is delivered in a file called \nDTNB.zip\n. When unpacked this zip file creates the following directory structure:\n\n\n   <current directory>\n     +-DTNB.jar\n     +-Description.props\n     +-build_package.xml\n     +-src\n     |   +-main\n     |   |   +-java\n     |   |       +-weka\n     |   |           +-classifiers\n     |   |               +-rules\n     |   |                   +-DTNB.java\n     |   +-test\n     |       +-java\n     |           +-weka\n     |               +-classifiers\n     |                   +-rules\n     |                       +-DTNBTest.java\n     +-lib\n     +-doc\n\n\n\n\nWhen installing, the package manager will use the value of the \"PackageName\" field in the Description.props file (see below) to create a directory in \n$WEKA_HOME/packages\n to hold the package contents. The contents of the \ndoc\n directory have not been shown in the diagram above, but this directory contains javadoc for the \nDTNB\n class. A package must have a \nDescription.props\n file and contain at least one jar file with compiled Java classes. The package manager will attempt to load all jar files that it finds in the root directory and the \nlib\n directory. Other files are optional, but if the package is open-source then it is nice to include the source code and an ant build file that can be used to compile the code. Template versions of the \nDescription.props\n file and\n\nbuild_package.xml\n file are available from the Weka site and here.\n\n\n==The description file==\nA valid package must contain a \nDescription.props\n file that provides meta data on the package. Identical files are stored at the central package repository and the local cache maintained by the package manager. The package manager uses these files to compare what is installed to what is available and resolve dependencies.\n\n\nThe \nDescription.props\n contains basic information on the package in the following format:\n\n\n# Template Description file for a Weka package\n\n# Package name (required)\nPackageName=funkyPackage\n\n# Version (required)\nVersion=1.0.0\n\n#Date (year-month-day)\nDate=2010-01-01\n\n# Title (required)\nTitle=My cool algorithm\n\n# Category (recommended)\nCategory=Classification\n\n# Author (required)\nAuthor=Joe Dev <joe@somewhere.net>,Dev2 <dev2@somewhereelse.net>\n\n# Maintainer (required)\nMaintainer=Joe Dev <joe@somewhere.net>\n\n# License (required)\nLicense=GPL 2.0|Mozilla\n\n# Description (required)\nDescription=This package contains the famous Funky Classifer that performs \\\n truely funky prediction.\n\n# Changes and/or bug fixes in this package (optional)\nChanges=Fixed a serious bug that affected overall coolness of the Funky Classifier\n\n# Package URL for obtaining the package archive (required)\nPackageURL=http://somewhere.net/weka/funkyPackage.zip\n\n# URL for further information\nURL=http://somewhere.net/funkyResearchInfo.html\n\n# Enhances various other packages?\nEnhances=packageName1,packageName2,...\n\n# Related to other packages?\nRelated=packageName1,packageName2,...\n\n# Dependencies (required; format: packageName (equality/inequality version_number)\nDepends=weka (>=3.7.1), packageName1 (=x.y.z), packageName2 (>u.v.w|<=x.y.z),...\n\n\n\n\nThe \nPackageName\n and \nVersion\n give the name of the package and version number respectively. The name can consist of letters, numbers, and the dot character. It should not start with a dot and should not contain any spaces. The version number is a sequence of three non-negative integers separated by single \n.\n or \n-\n characters.\n\n\nThe \nTitle\n field should give a one sentence description of the package. The \nDescription\n field can give a longer description of the package spaning multiple sentences. It may include technical references and can use HTML markup.\n\n\nThe \nCategory\n field is strongly recommended as this information is displayed on both the repository web site and in the GUI package manager client. In the latter, the user can sort the packages on the basis of the category field. It is recommended that an existing category be assigned if possible. Some examples include (Classification, Text classification, Ensemble learning, Regression, Clustering, Associations, Preprocessing, Visualization, Explorer, Experimenter, KnowledgeFlow).\n\n\nThe \nAuthor\n field describes who wrote the package and may include multiple names (separated by commas). Email addresses may be given in angle brackets after each name. The field is intended for human readers and no email addresses are automatically extracted.\n\n\nThe \nMaintainer\n field lists who maintains the package and should include a single email address, enclosed in angle brackets, for sending bug reports to.\n\n\nThe \nLicense\n field lists the license(s) that apply to the package. This field may contain the short specification of a license (such as LGPL, GPL 2.0 etc.) or the string \nfile LICENSE\n, where \nLICENSE\n exists as a file in the top-level directory of the package. The string \nUnlimited\n may be supplied to indicate that there are no restrictions on distribution or use aside from those imposed by relevant laws.\n\n\nThe \nPackageURL\n field lists valid URL that points to the package zip file. This URL is used by the package manager to download and install the package.\n\n\nThe optional \nDepends\n field gives a comma separated list of packages which this package depends on. The name of a package is optionally followed by a version number constraint enclosed in parenthesis. Valid operators for version number constraints include =, <, >, <=, >=. The keyword \nweka\n is reserved to refer to the base Weka system and can be used to indicate a dependency on\n  a particular version of Weka. For example:\n\n\nDepends=weka (>=3.7.2), DTNB (=1.0.0)\n\n\n\n\nstates that this package requires Weka 3.7.2 or higher and version 1.0.0 of the package DTNB.\n\n\nDepends=weka (>3.7.1|<3.8.0)\n\n\n\n\nstates that this package requires a version of Weka between 3.7.1 and 3.8.0.\n\n\nDepends=DTNB (<1.5.0|>=2.0.1)\n\n\n\n\nstates that this package requires that a version of the DTNB package be installed that is either less than version 1.5.0 or greater than or equal to version 2.0.1.\n\n\nIf there is no version number constraint following a package name, the package manager assumes that the latest version of the dependent package is suitable.\n\n\nThe optional \nURL\n field gives a URL at which the user can find additional online information about the package or its constituent algorithms.\n\n\nThe optional \nEnhances\n field can be used to indicate which other packages this package is based on (i.e. if it extends methods/algorithms from another\n  package in some fashion).\n\n\nThe optional \nRelated\n field is similar to the \nEnhances\n field. It can be used to point the user to other packages that are related in some fashion to this one.\n\n\nThe optional \nChanges\n field should be used to indicate what changes/bug fixes are included in the current release of the package.\n\n\nThere are several other fields that can be used to provide information to assist the user with completing installation (if it can\u2019t be completely accomplished with the package zip file) or display error messages if necessary components are missing:\n\n\nMessageToDisplayOnInstallation=Funky package requires some extra\\n\\\n stuff to be installed after installing this package. You will\\n\\\n need to blah, blah, blah in order to blah, blah, blah...\n\nDoNotLoadIfFileNotPresent=lib/someLibrary.jar,otherStuff/important,...\n\nDoNotLoadIfFileNotPresentMessage=funkyPackage can't be loaded because some \\\n funky libraries are missing. Please download funkyLibrary.jar from \\\n http://www.funky.com and install in $WEKA_HOME/packages/funkyPackage/lib\n\nDoNotLoadIfClassNotPresent=com.some.class.from.some.Where,org.some.other.Class,...\n\nDoNotLoadIfClassNotPresentMessage=funkyPackage can't be loaded because \\\n com.funky.FunkyClass can't be instantiated. Have you downloaded and run \\\n the funky software installer for your platform?\n\n\n\n\nThe optional \nMessageToDisplayOnInstallation\n field allows you to specify special instructions to the user in order to help them complete the intallation manually. This message gets displayed on the console, written to the log and appears in a pop-up information dialog if using the GUI package manager. It should include \n\\n\n in order to avoid long lines when displayed in a GUI pop-up dialog.\n\n\nThe optional \nDoNotLoadIfFileNotPresent\n field can be used to prevent Weka from loading the package if the named ''files'' and/or ''directories'' are not present in the package\u2019s installation directory. An example is the massiveOnlineAnalysis package. This package is a connector only package and does not include the MOA library. Users of this package must download the moa.jar file separately and copy it to the package\u2019s \nlib\n directory manually. Multiple files and directories can be specified as a comma separated list. All paths are relative to the top-level directory of the package. \nIMPORTANT\n: use forward slashes as separator characters, as these are portable accross all platforms. The \nDoNotLoadIfFileNotPresentMessage\n field can be used to supply an optional message to display to the user if Weka detects that a file or directory is missing from the package. This message will be displayed on the console and in the log.\n\n\nThe optional \nDoNotLoadIfClassNotPresent\n field can be used to prevent Weka from loading the package if the named ''class(es)'' can\u2019t be instantiated. This is useful for packages that rely on stuff that has to be installed manually by the user. For example, Java3D is a separate download on all platforms except for OSX, and installs itself into the system JRE/JDK. The \nDoNotLoadIfClassNotPresentMessage\n field can be used to supply an optional message to display to the user if Weka detects that a class can\u2019t be instantiated. Again, this will be displayed on the console and in the log.\n\n\nNew in Weka 3.9.2 and 3.8.2 is the ability to constrain the OS and architecture that a package can be installed and loaded on. Two new fields are used for this:\n\n\n# Specify which OS's the package can operate with. Omitting this entry indicates no restrictions on OS. (optional)\nOSName=Windows,Mac,Linux\n\n# Specify which architecture the package can operate with. Omitting this entry indicates no restriction. (optional)\nOSArch=64\n\n\n\n\nEntries in the OSName field are compared against the value of the Java property \"os.name\" using a String.toLowerCase().contains() operation. Any single match indicates a pass. If an OSName field exists in the Description.props, and it matches, then the optional OSArch field is examined. If present, values in the OSArch list are compared against the value of the Java property \"os.arch\". The special OSArch entries \"32\" and \"64\" are tested using a String().contains() operation; all other entries are compared using String.equalsIgnoreCase().\n\n\nAdditional configuration files\n\n\nCertain types of packages may require additional configuration files to be present as part of the package. The last chapter covered various ways in which Weka can be extended without having to alter the core Weka code. These plugin mechanisms have been subsumed by the package management system, so some of the configuration property files they require must be present in the package\u2019s top-level directory if the package in question contains such a plugin. Examples include additional tabs for the Explorer, mappings to custom property editors for Weka\u2019s GenericObjectEditor and Knowledge Flow plugins. Here are some examples:\n\n\nThe scatterPlot3D package adds a new tab to the Explorer. In order to accomplish this a property has to be set in the Explorer.props file (which contains default values for the Explorer) in order to tell Weka to instantiate and display the new panel. The scatterPlot3D file includes an \nExplorer.props\n file in its top-level directory that has the following contents:\n\n\n# Explorer.props file. Adds the Explorer3DPanel to the Tabs key.\nTabs=weka.gui.explorer.Explorer3DPanel\nTabsPolicy=append\n\n\n\n\nThis property file is read by the package management system when the package is loaded and any key-value pairs are added to existing Explorer properties that have been loaded by the system at startup. If the key already exists in the Explorer properties, then the package has the option to either replace (i.e. overwrite) or append to the existing value. This can be specified with the \"TabsPolicy\" key. In this case, the value \nweka.gui.explorer.Explorer3DPanel\n is appended to any existing value associated with the \"Tabs\" key. \nExplorer3DPanel\n gets instantiated and added as a new tab when the Explorer starts.\n\n\nAnother example is the kfGroovy package. This package adds a plugin component to Weka\u2019s Knowledge Flow that allows a Knowledge Flow step to be implemented and compiled dynamically at runtime as a Groovy script. In order for the Knowledge Flow to make the new step appear in its \nPlugins\n toolbar, there needs to be a \nBeans.props\n file in the package\u2019s top level directory. In the case of kfGroovy, this property file has the following contents:\n\n\n# Specifies that this component goes into the Plugins toolbar\nweka.gui.beans.KnowledgeFlow.Plugins=org.pentaho.dm.kf.GroovyComponent\n\n\n\n\nThe new pluggable evaluation metrics for classification/regression (from Weka 3.7.8 and nightly developer snapshots from 15-11-2012) are managed by the PluginManager class. To tell PluginManager that your package provides a new evaluation metric you need to provide a \"PluginManager.props\" file in the package's top level directory. For example, a hypothetical bobsMetric package might declare a new \"Area under Bob curve\" metric like so:\n\n\n# Specify a new plugin Evaluation metric\nweka.classifiers.evaluation.AbstractEvaluationMetric=weka.classifiers.evaluation.BobsAUC\n\n\n\n\nContributing a package\n\n\nIf you have created a package for Weka then there are two options for making it available to the community. In both cases, hosting the package\u2019s zip archive is the responsibility of the contributer.\n\n\nThe first, and official, route is to contact the current Weka maintainer (normally also the admin of the WEKA homepage) and supply your package\u2019s \nDescription.props\n file. The Weka team will then test downloading and using your package to make sure that there are no obvious problems with what has been specified in the \nDescription.props\n file and that the software runs and does not contain any malware/malicious code. If all is well, then the package will become an \nofficial\n Weka package and the central package repository meta data will be updated with the package\u2019s \nDescription.props file\n. \n\n\nResponsibility for maintaining and supporting the package resides with the contributer\n.\n\n\nThe second, and unofficial, route is to simply make the package\u2019s zip archive available on the web somewhere and advertise it yourself. Although users will not be able to browse it\u2019s description in the official package repository, they will be able to download and install it directly from your URL by using the command line version of the package manager. This route could be attractive for people who have published a new algorithm and want to quiclky make a beta version available for others to try without having to go through the official route.\n\n\nCreating a mirror of the package meta data repository\n\n\nIn this section we discuss an easy approach to setting up and maintaining a mirror of the package meta data repository. Having a local mirror may provide faster access times than to that of the official repository on Sourceforge. Extending this approach to the creation of an alternative central repository (hosting packages not available at the official repository) should be straight forward.\n\n\nJust about everything necessary for creating a mirror exists in the local meta data cache created by Weka\u2019s package management system. This cache resides at \n$WEKA_HOME/repCache\n. The only thing missing (in Weka 3.7.2) for a complete mirror is the file \nimages.txt\n, that lists all the image files used in the html index files. This file contains the following two lines:\n\n\nTitle-Bird-Header.gif\npentaho_logo_rgb_sm.png\n\n\n\n\nimages.txt\n is downloaded automatically by the package management system in Weka 3.7.3 and higher.\n\n\nTo create a mirror:\n1. Copy the contents of \n$WEKA_HOME/repCache\n to a temporary directory. For the purposes of this example we\u2019ll call it \ntempRep\n\n2. Change directory into \ntempRep\n and run \njava weka.core.RepositoryIndexGenerator .\n. Don't forget the \".\" after the command (this tells \nRepoistoryIndexGenerator\n to operate on the current directory)\n3. Change directory to the parent of \ntempRep\n and synchronize its contents to wherever your web server is located (this is easy via rsync under Nix-like operating systems).\n\n\nRepositoryIndexGenerator\n automatically creates the main index.html file, all the package index.html files and html files correpsonding to all version prop files for each package. It will also create \npackageList.txt\n and \nnumPackages.txt\n files.\n\n\nIMPORTANT\n: Make sure that all the files in \ntempRep\n are world readable. It is easy to make packages available that are not part of the official Weka repository. Assuming you want to add a package called \nfunkyPackage\n (as specified by the \nPackageName\n field in the \nDescription.props\n file):\n\n\n\n\nCreate a directory called \nfunkyPackage\n in \ntempRep\n\n\nCopy the \nDescription.props\n file to \ntempRep/funkyPackage/Latest.props\n\n\nCopy the \nDescription.props file\n to \ntempRep/funkyPackage/<version number>.props\n, where \nversion number\n is the version number specified in the \nVersion\n field of \nDescription.props\n\n\nRun \nRepositoryIndexGenerator\n as described previously and sync \ntempRep\n to your web server\n\n\n\n\nAdding a new version of an existing package is very similar to what has already been described. All that is required is that the new \nDescription.props\n file corresponding to the new version is copied to \nLatest.props\n and to \n<version numer>.props\n in the package\u2019s folder. Running \nRepositoryIndexGenerator\n will ensure that all necessary html files are created and supporting text files are updated.\n\n\nAutomating the mirroring process would simply involve using your OS\u2019s scheduler to execute a script that:\n\n\n\n\nRuns \nweka.core.WekaPackageManager -refresh-cache\n\n\nrsyncs \n$WEKA_HOME/repCache\n to \ntempRep\n\n\nRuns \nweka.core.RepoistoryIndexGenerator\n\n\nrsyncs \ntempRep\n to your web server",
            "title": " How are packages structured for the package management system?"
        },
        {
            "location": "/packages/structure/#where-does-weka-store-packages-and-other-configuration-stuff",
            "text": "By default, Weka stores packages and other information in  $WEKA_HOME . The default location for  WEKA_HOME  is  user.home/wekafiles , where  user.home  is the user\u2019s home directory. You can change the default location for  WEKA_HOME  by setting this either as an evironment variable for your platform, or by specifying it as a Java property when starting Weka. E.g.:  export WEKA_HOME=/home/somewhere/weka_bits_and_bobs  will set the directory that Weka uses to  /home/somewhere/weka_bits_and_bobs  under the LINUX operating system.  The same thing can be accomplished when starting Weka by specifying a Java property on the command line, E.g.:  java -DWEKA_HOME=/home/somewhere/weka_bits_and_bobs -jar weka.jar  Inside  $WEKA_HOME  you will find the main weka log file (weka.log) and a number of directories:   packages  holds installed packages. Each package is contained its own subdirectory.  props  holds various Java property files used by Weka. This directory replaces the user\u2019s home directory (used in earlier releases of Weka) as one of the locations checked by Weka for properties files (such as  DatabaseUtils.props ). Weka first checks, in order, the current directory (i.e. the directory that Weka is launched from), then  $WEKA_HOME/props  and finally the  weka.jar  file for property files.  repCache  holds the cached copy of the meta data from the central package repository. If the contents of this directory get corrupted it can be safely deleted and Weka will recreate it on the next restart.  systemDialogs  holds marker files that are created when you check  Don\u2019t show this again  in various system popup dialogs. Removing this directory or its contents will cause Weka to display those prompts anew.",
            "title": "Where does WEKA store packages and other configuration stuff?"
        },
        {
            "location": "/packages/structure/#anatomy-of-a-package",
            "text": "A Weka package is a zip archive that must unpack to the current directory. For example, the  DTNB  package contains the decision table naive Bayes hybrid classifier and is delivered in a file called  DTNB.zip . When unpacked this zip file creates the following directory structure:     <current directory>\n     +-DTNB.jar\n     +-Description.props\n     +-build_package.xml\n     +-src\n     |   +-main\n     |   |   +-java\n     |   |       +-weka\n     |   |           +-classifiers\n     |   |               +-rules\n     |   |                   +-DTNB.java\n     |   +-test\n     |       +-java\n     |           +-weka\n     |               +-classifiers\n     |                   +-rules\n     |                       +-DTNBTest.java\n     +-lib\n     +-doc  When installing, the package manager will use the value of the \"PackageName\" field in the Description.props file (see below) to create a directory in  $WEKA_HOME/packages  to hold the package contents. The contents of the  doc  directory have not been shown in the diagram above, but this directory contains javadoc for the  DTNB  class. A package must have a  Description.props  file and contain at least one jar file with compiled Java classes. The package manager will attempt to load all jar files that it finds in the root directory and the  lib  directory. Other files are optional, but if the package is open-source then it is nice to include the source code and an ant build file that can be used to compile the code. Template versions of the  Description.props  file and build_package.xml  file are available from the Weka site and here.  ==The description file==\nA valid package must contain a  Description.props  file that provides meta data on the package. Identical files are stored at the central package repository and the local cache maintained by the package manager. The package manager uses these files to compare what is installed to what is available and resolve dependencies.  The  Description.props  contains basic information on the package in the following format:  # Template Description file for a Weka package\n\n# Package name (required)\nPackageName=funkyPackage\n\n# Version (required)\nVersion=1.0.0\n\n#Date (year-month-day)\nDate=2010-01-01\n\n# Title (required)\nTitle=My cool algorithm\n\n# Category (recommended)\nCategory=Classification\n\n# Author (required)\nAuthor=Joe Dev <joe@somewhere.net>,Dev2 <dev2@somewhereelse.net>\n\n# Maintainer (required)\nMaintainer=Joe Dev <joe@somewhere.net>\n\n# License (required)\nLicense=GPL 2.0|Mozilla\n\n# Description (required)\nDescription=This package contains the famous Funky Classifer that performs \\\n truely funky prediction.\n\n# Changes and/or bug fixes in this package (optional)\nChanges=Fixed a serious bug that affected overall coolness of the Funky Classifier\n\n# Package URL for obtaining the package archive (required)\nPackageURL=http://somewhere.net/weka/funkyPackage.zip\n\n# URL for further information\nURL=http://somewhere.net/funkyResearchInfo.html\n\n# Enhances various other packages?\nEnhances=packageName1,packageName2,...\n\n# Related to other packages?\nRelated=packageName1,packageName2,...\n\n# Dependencies (required; format: packageName (equality/inequality version_number)\nDepends=weka (>=3.7.1), packageName1 (=x.y.z), packageName2 (>u.v.w|<=x.y.z),...  The  PackageName  and  Version  give the name of the package and version number respectively. The name can consist of letters, numbers, and the dot character. It should not start with a dot and should not contain any spaces. The version number is a sequence of three non-negative integers separated by single  .  or  -  characters.  The  Title  field should give a one sentence description of the package. The  Description  field can give a longer description of the package spaning multiple sentences. It may include technical references and can use HTML markup.  The  Category  field is strongly recommended as this information is displayed on both the repository web site and in the GUI package manager client. In the latter, the user can sort the packages on the basis of the category field. It is recommended that an existing category be assigned if possible. Some examples include (Classification, Text classification, Ensemble learning, Regression, Clustering, Associations, Preprocessing, Visualization, Explorer, Experimenter, KnowledgeFlow).  The  Author  field describes who wrote the package and may include multiple names (separated by commas). Email addresses may be given in angle brackets after each name. The field is intended for human readers and no email addresses are automatically extracted.  The  Maintainer  field lists who maintains the package and should include a single email address, enclosed in angle brackets, for sending bug reports to.  The  License  field lists the license(s) that apply to the package. This field may contain the short specification of a license (such as LGPL, GPL 2.0 etc.) or the string  file LICENSE , where  LICENSE  exists as a file in the top-level directory of the package. The string  Unlimited  may be supplied to indicate that there are no restrictions on distribution or use aside from those imposed by relevant laws.  The  PackageURL  field lists valid URL that points to the package zip file. This URL is used by the package manager to download and install the package.  The optional  Depends  field gives a comma separated list of packages which this package depends on. The name of a package is optionally followed by a version number constraint enclosed in parenthesis. Valid operators for version number constraints include =, <, >, <=, >=. The keyword  weka  is reserved to refer to the base Weka system and can be used to indicate a dependency on   a particular version of Weka. For example:  Depends=weka (>=3.7.2), DTNB (=1.0.0)  states that this package requires Weka 3.7.2 or higher and version 1.0.0 of the package DTNB.  Depends=weka (>3.7.1|<3.8.0)  states that this package requires a version of Weka between 3.7.1 and 3.8.0.  Depends=DTNB (<1.5.0|>=2.0.1)  states that this package requires that a version of the DTNB package be installed that is either less than version 1.5.0 or greater than or equal to version 2.0.1.  If there is no version number constraint following a package name, the package manager assumes that the latest version of the dependent package is suitable.  The optional  URL  field gives a URL at which the user can find additional online information about the package or its constituent algorithms.  The optional  Enhances  field can be used to indicate which other packages this package is based on (i.e. if it extends methods/algorithms from another   package in some fashion).  The optional  Related  field is similar to the  Enhances  field. It can be used to point the user to other packages that are related in some fashion to this one.  The optional  Changes  field should be used to indicate what changes/bug fixes are included in the current release of the package.  There are several other fields that can be used to provide information to assist the user with completing installation (if it can\u2019t be completely accomplished with the package zip file) or display error messages if necessary components are missing:  MessageToDisplayOnInstallation=Funky package requires some extra\\n\\\n stuff to be installed after installing this package. You will\\n\\\n need to blah, blah, blah in order to blah, blah, blah...\n\nDoNotLoadIfFileNotPresent=lib/someLibrary.jar,otherStuff/important,...\n\nDoNotLoadIfFileNotPresentMessage=funkyPackage can't be loaded because some \\\n funky libraries are missing. Please download funkyLibrary.jar from \\\n http://www.funky.com and install in $WEKA_HOME/packages/funkyPackage/lib\n\nDoNotLoadIfClassNotPresent=com.some.class.from.some.Where,org.some.other.Class,...\n\nDoNotLoadIfClassNotPresentMessage=funkyPackage can't be loaded because \\\n com.funky.FunkyClass can't be instantiated. Have you downloaded and run \\\n the funky software installer for your platform?  The optional  MessageToDisplayOnInstallation  field allows you to specify special instructions to the user in order to help them complete the intallation manually. This message gets displayed on the console, written to the log and appears in a pop-up information dialog if using the GUI package manager. It should include  \\n  in order to avoid long lines when displayed in a GUI pop-up dialog.  The optional  DoNotLoadIfFileNotPresent  field can be used to prevent Weka from loading the package if the named ''files'' and/or ''directories'' are not present in the package\u2019s installation directory. An example is the massiveOnlineAnalysis package. This package is a connector only package and does not include the MOA library. Users of this package must download the moa.jar file separately and copy it to the package\u2019s  lib  directory manually. Multiple files and directories can be specified as a comma separated list. All paths are relative to the top-level directory of the package.  IMPORTANT : use forward slashes as separator characters, as these are portable accross all platforms. The  DoNotLoadIfFileNotPresentMessage  field can be used to supply an optional message to display to the user if Weka detects that a file or directory is missing from the package. This message will be displayed on the console and in the log.  The optional  DoNotLoadIfClassNotPresent  field can be used to prevent Weka from loading the package if the named ''class(es)'' can\u2019t be instantiated. This is useful for packages that rely on stuff that has to be installed manually by the user. For example, Java3D is a separate download on all platforms except for OSX, and installs itself into the system JRE/JDK. The  DoNotLoadIfClassNotPresentMessage  field can be used to supply an optional message to display to the user if Weka detects that a class can\u2019t be instantiated. Again, this will be displayed on the console and in the log.  New in Weka 3.9.2 and 3.8.2 is the ability to constrain the OS and architecture that a package can be installed and loaded on. Two new fields are used for this:  # Specify which OS's the package can operate with. Omitting this entry indicates no restrictions on OS. (optional)\nOSName=Windows,Mac,Linux\n\n# Specify which architecture the package can operate with. Omitting this entry indicates no restriction. (optional)\nOSArch=64  Entries in the OSName field are compared against the value of the Java property \"os.name\" using a String.toLowerCase().contains() operation. Any single match indicates a pass. If an OSName field exists in the Description.props, and it matches, then the optional OSArch field is examined. If present, values in the OSArch list are compared against the value of the Java property \"os.arch\". The special OSArch entries \"32\" and \"64\" are tested using a String().contains() operation; all other entries are compared using String.equalsIgnoreCase().",
            "title": "Anatomy of a package"
        },
        {
            "location": "/packages/structure/#additional-configuration-files",
            "text": "Certain types of packages may require additional configuration files to be present as part of the package. The last chapter covered various ways in which Weka can be extended without having to alter the core Weka code. These plugin mechanisms have been subsumed by the package management system, so some of the configuration property files they require must be present in the package\u2019s top-level directory if the package in question contains such a plugin. Examples include additional tabs for the Explorer, mappings to custom property editors for Weka\u2019s GenericObjectEditor and Knowledge Flow plugins. Here are some examples:  The scatterPlot3D package adds a new tab to the Explorer. In order to accomplish this a property has to be set in the Explorer.props file (which contains default values for the Explorer) in order to tell Weka to instantiate and display the new panel. The scatterPlot3D file includes an  Explorer.props  file in its top-level directory that has the following contents:  # Explorer.props file. Adds the Explorer3DPanel to the Tabs key.\nTabs=weka.gui.explorer.Explorer3DPanel\nTabsPolicy=append  This property file is read by the package management system when the package is loaded and any key-value pairs are added to existing Explorer properties that have been loaded by the system at startup. If the key already exists in the Explorer properties, then the package has the option to either replace (i.e. overwrite) or append to the existing value. This can be specified with the \"TabsPolicy\" key. In this case, the value  weka.gui.explorer.Explorer3DPanel  is appended to any existing value associated with the \"Tabs\" key.  Explorer3DPanel  gets instantiated and added as a new tab when the Explorer starts.  Another example is the kfGroovy package. This package adds a plugin component to Weka\u2019s Knowledge Flow that allows a Knowledge Flow step to be implemented and compiled dynamically at runtime as a Groovy script. In order for the Knowledge Flow to make the new step appear in its  Plugins  toolbar, there needs to be a  Beans.props  file in the package\u2019s top level directory. In the case of kfGroovy, this property file has the following contents:  # Specifies that this component goes into the Plugins toolbar\nweka.gui.beans.KnowledgeFlow.Plugins=org.pentaho.dm.kf.GroovyComponent  The new pluggable evaluation metrics for classification/regression (from Weka 3.7.8 and nightly developer snapshots from 15-11-2012) are managed by the PluginManager class. To tell PluginManager that your package provides a new evaluation metric you need to provide a \"PluginManager.props\" file in the package's top level directory. For example, a hypothetical bobsMetric package might declare a new \"Area under Bob curve\" metric like so:  # Specify a new plugin Evaluation metric\nweka.classifiers.evaluation.AbstractEvaluationMetric=weka.classifiers.evaluation.BobsAUC",
            "title": "Additional configuration files"
        },
        {
            "location": "/packages/structure/#contributing-a-package",
            "text": "If you have created a package for Weka then there are two options for making it available to the community. In both cases, hosting the package\u2019s zip archive is the responsibility of the contributer.  The first, and official, route is to contact the current Weka maintainer (normally also the admin of the WEKA homepage) and supply your package\u2019s  Description.props  file. The Weka team will then test downloading and using your package to make sure that there are no obvious problems with what has been specified in the  Description.props  file and that the software runs and does not contain any malware/malicious code. If all is well, then the package will become an  official  Weka package and the central package repository meta data will be updated with the package\u2019s  Description.props file .   Responsibility for maintaining and supporting the package resides with the contributer .  The second, and unofficial, route is to simply make the package\u2019s zip archive available on the web somewhere and advertise it yourself. Although users will not be able to browse it\u2019s description in the official package repository, they will be able to download and install it directly from your URL by using the command line version of the package manager. This route could be attractive for people who have published a new algorithm and want to quiclky make a beta version available for others to try without having to go through the official route.",
            "title": "Contributing a package"
        },
        {
            "location": "/packages/structure/#creating-a-mirror-of-the-package-meta-data-repository",
            "text": "In this section we discuss an easy approach to setting up and maintaining a mirror of the package meta data repository. Having a local mirror may provide faster access times than to that of the official repository on Sourceforge. Extending this approach to the creation of an alternative central repository (hosting packages not available at the official repository) should be straight forward.  Just about everything necessary for creating a mirror exists in the local meta data cache created by Weka\u2019s package management system. This cache resides at  $WEKA_HOME/repCache . The only thing missing (in Weka 3.7.2) for a complete mirror is the file  images.txt , that lists all the image files used in the html index files. This file contains the following two lines:  Title-Bird-Header.gif\npentaho_logo_rgb_sm.png  images.txt  is downloaded automatically by the package management system in Weka 3.7.3 and higher.  To create a mirror:\n1. Copy the contents of  $WEKA_HOME/repCache  to a temporary directory. For the purposes of this example we\u2019ll call it  tempRep \n2. Change directory into  tempRep  and run  java weka.core.RepositoryIndexGenerator . . Don't forget the \".\" after the command (this tells  RepoistoryIndexGenerator  to operate on the current directory)\n3. Change directory to the parent of  tempRep  and synchronize its contents to wherever your web server is located (this is easy via rsync under Nix-like operating systems).  RepositoryIndexGenerator  automatically creates the main index.html file, all the package index.html files and html files correpsonding to all version prop files for each package. It will also create  packageList.txt  and  numPackages.txt  files.  IMPORTANT : Make sure that all the files in  tempRep  are world readable. It is easy to make packages available that are not part of the official Weka repository. Assuming you want to add a package called  funkyPackage  (as specified by the  PackageName  field in the  Description.props  file):   Create a directory called  funkyPackage  in  tempRep  Copy the  Description.props  file to  tempRep/funkyPackage/Latest.props  Copy the  Description.props file  to  tempRep/funkyPackage/<version number>.props , where  version number  is the version number specified in the  Version  field of  Description.props  Run  RepositoryIndexGenerator  as described previously and sync  tempRep  to your web server   Adding a new version of an existing package is very similar to what has already been described. All that is required is that the new  Description.props  file corresponding to the new version is copied to  Latest.props  and to  <version numer>.props  in the package\u2019s folder. Running  RepositoryIndexGenerator  will ensure that all necessary html files are created and supporting text files are updated.  Automating the mirroring process would simply involve using your OS\u2019s scheduler to execute a script that:   Runs  weka.core.WekaPackageManager -refresh-cache  rsyncs  $WEKA_HOME/repCache  to  tempRep  Runs  weka.core.RepoistoryIndexGenerator  rsyncs  tempRep  to your web server",
            "title": "Creating a mirror of the package meta data repository"
        },
        {
            "location": "/not_so_faq/gsp/",
            "text": "Class\n\n\nweka.associators.GeneralizedSequentialPatterns\n\n\nPublication\n\n\nRamakrishnan Srikant, Rakesh Agrawal (1996). Mining Sequential Patterns: Generalizations and Performance Improvements.\n\n\nDownloads\n\n\n\n\nGeneralizedSequentialPattern_example.arff",
            "title": " Generalized Sequential Patterns"
        },
        {
            "location": "/not_so_faq/gsp/#class",
            "text": "weka.associators.GeneralizedSequentialPatterns",
            "title": "Class"
        },
        {
            "location": "/not_so_faq/gsp/#publication",
            "text": "Ramakrishnan Srikant, Rakesh Agrawal (1996). Mining Sequential Patterns: Generalizations and Performance Improvements.",
            "title": "Publication"
        },
        {
            "location": "/not_so_faq/gsp/#downloads",
            "text": "GeneralizedSequentialPattern_example.arff",
            "title": "Downloads"
        },
        {
            "location": "/not_so_faq/j48_numbers/",
            "text": "J48 pruned tree\nnode-caps = yes\n| deg-malig = 1: recurrence-events (1.01/0.4)\n| deg-malig = 2: no-recurrence-events (26.2/8.0)\n| deg-malig = 3: recurrence-events (30.4/7.4)\nnode-caps = no: no-recurrence-events (228.39/53.4)\n\n\n\n\nThe \nfirst\n number is the total number of instances (weight of instances) reaching the leaf. The \nsecond\n number is the number (weight) of those instances that are misclassified.\n\n\nIf your data has \nmissing\n attribute values then you will end up with \nfractional\n instances at the leafs. When splitting on an attribute where some of the training instances have missing values, J48 will divide a training instance with a missing value for the split attribute up into fractional parts proportional to the frequencies of the observed non-missing values. This is discussed in the Witten & Frank Data Mining book as well as Ross Quinlan's original publications on C4.5.",
            "title": " What do those numbers mean in a J48 tree?"
        },
        {
            "location": "/faqs/different_versions/",
            "text": "Refer to \nHistory\n for a tabular overview of all Weka releases.\n\n\nSeveral branches are associated with the 1st, 2nd, 3rd, and \n4th\n edition of the book \nData Mining: Practical Machine Learning Tools and Techniques\n by \nIan H. Witten\n and \nEibe Frank\n, joined by \nMark Hall\n for the 3rd edition and \nChris Pal\n for the 4th edition.\n\n\nOnce created, non-development branches receive bug fixes, but no new features (classifiers, filters, etc.).\n\n\n\n\n\n\n\n\nVersion name\n\n\nMost recent base number\n\n\nAssociated with book edition\n\n\n\n\n\n\n\n\n\n\nBook 1st ed. version\n\n\n3.0.x\n\n\n1st edition\n\n\n\n\n\n\nOld GUI version\n\n\n3.2.x\n\n\nnone\n\n\n\n\n\n\nBook 2nd ed. version\n\n\n3.4.x\n\n\n2nd edition\n\n\n\n\n\n\nBook 3rd ed. version\n\n\n3.6.x\n\n\n3rd edition\n\n\n\n\n\n\nBook 4th ed. version\n\n\n3.8.x\n\n\n4th edition\n\n\n\n\n\n\nDevelopment version\n\n\n3.9.x\n\n\nnone\n\n\n\n\n\n\n\n\nFor \ncontributions\n, you should always develop against the developer version.",
            "title": " What are the principal release branches of Weka?"
        },
        {
            "location": "/faqs/old_versions/",
            "text": "If you need a specific version of WEKA, e.g., due to some third-party tools, go \nWEKA's project page\n on \nSourceforge.net\n. In the \nFiles section\n you have access to all the releases ever made.",
            "title": " Where can I get old versions of WEKA?"
        },
        {
            "location": "/faqs/latest_bugfixes/",
            "text": "The article \nHow to get the latest bugfixes\n explains it in detail (it's basically either obtaining the source code from \nSubversion\n and compiling it yourself or getting a snapshot from the download section of the \nWEKA homepage\n).",
            "title": " How do I get the latest bugfixes?"
        },
        {
            "location": "/faqs/contribution/",
            "text": "Information on how to contribute to WEKA can be found in the \nContributing a package\n section of the \nHow are packages structured for the package management system?\n article. The conditions for new classifiers (schemes in general) are that, firstly, they have to be published in the proceedings of a renowned conference (e.g., ICML) or as an article of respected journal (e.g., Machine Learning) and, secondly, that they outperform other standard schemes (e.g., J48/C4.5).\n\n\nBut please bear in mind, that we don't have a lot of man power, i.e., being the WEKA maintainer is \nNOT\n a full-time position.",
            "title": " How can I contribute to WEKA?"
        },
        {
            "location": "/faqs/package_manager_doesnt_start/",
            "text": "The most likely reason for this is that your computer does not have direct access to the Internet and Java needs to be told to use a proxy server to access the web. The best way to achieve this is to configure an environment variable that provides the proxy details, e.g.,\n\n\n_JAVA_OPTIONS\n\n\n\n\nwhich is read by Oracle Java virtual machines. There is more information on this variable \nhere\n. Information on how to set environment variables in Windows is \nhere\n. For Mac users, there is a nice program to set environment variables available \nhere\n.\n\n\nSet the value of this variable to\n\n\n-Dhttp.proxyHost=some.proxy.somewhere.net -Dhttp.proxyPort=port\n\n\n\n\nwhere \nsome.proxy.somewhere.net\n needs to be replaced by the name of your proxy server and \nport\n needs to be replaced by the appropriate port number on the proxy server. Your IT department should be able to give you these details.\n\n\nThis should allow the package manager to connect to the website that hosts the package meta-information. However, if the package manager still cannot connect to the Internet, you can also force it to run in offline mode, by setting the above environment variable to\n\n\n-Dweka.packageManager.offline=true\n\n\n\n\nThen, you can download package .zip files manually via your web browser, by navigating to\n\n\nhttp://weka.sourceforge.net/packageMetaData/\n\n\nclicking on the link for the package you want to install, then clicking on \nLatest\n, and finally clicking on the URL given next to \nPackageURL\n.\n\n\nOnce you have downloaded the package .zip file, open the WEKA package manager, and click on the \nFile/URL\n button in the top-right corner of the package manager window (in the \nUnofficial\n panel). Then navigate to your package .zip file and install it.\n\n\nIf you are running Weka in \noffline\n mode, and the packages you are installing have some dependencies on one another, then there can still be some problems due to Weka not being able to verify the dependencies by checking against the central repository. This is usually a problem in the case where Weka has never been able to connect to the internet and thus has not downloaded and established a cache of the central package metadata repository. Fortunately there is a simple work-around to this, as long as you can access the internet via a web browser:\n\n\n\n\nUsing your web browser, download \nhttp://weka.sourceforge.net/packageMetaData/repo.zip\n\n\nIf it doesn't already exist, create the directory \n~/wekafiles/repCache\n\n\nCopy the downloaded \nrepo.zip\n into \n~/wekafiles/repCache\n and unzip it there\n\n\nStart Weka (use the \nweka.packageManager.offline=true\n property to speed up the startup process; see [http://weka.wikispaces.com/How+do+I+use+the+package+manager%3F#Package%20manager%20property%20file] for info)",
            "title": " Weka package manager does not start"
        },
        {
            "location": "/faqs/arff_does_not_load/",
            "text": "One way to figure out why \nARFF\n files are failing to load is to give them to the \nweka.core.Instances\n class. In the SimpleCLI or in the terminal, type the following:\n\n\n java weka.core.Instances filename.arff\n\n\n\n\nwhere you substitute \nfilename\n for the actual name of your file. This should return an error if there is a problem reading the file, or show some statistics if the file is ok. The error message you get should give some indication of what is wrong.\n\n\nnominal value not declared in header, read Token[X], line Y\n\n\nIf you get this error message than you seem to have declared a nominal attribute in the ARFF header section, but WEKA came across a value (\n\"X\"\n) in the data (in line \nY\n) for this particular attribute that wasn't listed as possible value.\n\n\nAll\n nominal values that appear in the data must be declared in the header.",
            "title": " ARFF file does not load"
        },
        {
            "location": "/faqs/arff_does_not_load/#nominal-value-not-declared-in-header-read-tokenx-line-y",
            "text": "If you get this error message than you seem to have declared a nominal attribute in the ARFF header section, but WEKA came across a value ( \"X\" ) in the data (in line  Y ) for this particular attribute that wasn't listed as possible value.  All  nominal values that appear in the data must be declared in the header.",
            "title": "nominal value not declared in header, read Token[X], line Y"
        },
        {
            "location": "/faqs/commercial_applications/",
            "text": "WEKA is licensed under the GNU General Public license (\nGPL 2.0 for Weka 3.6\n) and (\nGPL 3.0 for Weka > 3.7.5\n). Any derivative work obtained under this license must be licensed under the GPL if this derivative work is distributed to a third party.\n\n\nFor commercial projects that require the ability to distribute WEKA code as part of a program that cannot be distributed under the GPL, it may be possible to purchase an appropriate license from the copyright holders listed in the corresponding Java classes.\n\n\nThe copyright for most WEKA code is owned by the University of Waikato. For information on licenses for this code please contact \nWaikatoLink\n , the commercialization unit of the \nUniversity of Waikato\n, by sending email to weka-enquiries at waikatolink.co.nz.",
            "title": " Can I use WEKA in commercial applications?"
        },
        {
            "location": "/faqs/ubuntu_1804_blas_warning/",
            "text": "When running Ubuntu 18.04, you might see the following warning message(s) in the console:\n\n\nApr 03, 2019 5:40:10 PM com.github.fommil.netlib.BLAS <clinit>\nWARNING: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\nApr 03, 2019 5:40:10 PM com.github.fommil.netlib.BLAS <clinit>\nWARNING: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\nApr 03, 2019 5:40:10 PM com.github.fommil.netlib.LAPACK <clinit>\nWARNING: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\nApr 03, 2019 5:40:10 PM com.github.fommil.netlib.LAPACK <clinit>\nWARNING: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n\n\n\n\nYou can easily fix this by installing the missing dependencies with this command:\n\n\nsudo apt-get install libgfortran-6-dev",
            "title": " Ubuntu 18:08 BLAS Warning"
        },
        {
            "location": "/faqs/check_classpath_within_weka/",
            "text": "Yes, you can. Just start up the \nSimpleCLI\n and issue the following command:\n\n\njava weka.core.SystemInfo\n\n\n\n\nLook for the property \njava.class.path\n, which lists the \nCLASSPATH\n WEKA was started with.",
            "title": " Can I check my CLASSPATH from within WEKA?"
        },
        {
            "location": "/faqs/home_directory_location/",
            "text": "Where a user's home directory is located varies from platform to platform and among the users on a single computer. But the actual location of the home directory is available through special environment variables:\n\n\n\n\n\n\nUnix/Linux\n\n\n$HOME\n\n\n\n\n\n\nWindows\n\n\n%USERPROFILE%\n\n\n\n\n\n\nCygwin\n\n\n$USERPROFILE\n\n\n\n\n\n\nIn order to find out where these environment variables actually point to, do the following:\n\n\n\n\n\n\non \nUnix/Linux\n, open a terminal and type the following command\n\n\necho $HOME\n\n\n\n\n\n\non \nWindows\n, open a command-prompt and type the following command\n\n\necho %USERPROFILE%\n\n\n\n\n\n\non \nCygwin\n, open a bash and type the following command\n\n\necho $USERPROFILE",
            "title": " Where is my home directory located?"
        },
        {
            "location": "/faqs/check_memory_available/",
            "text": "You can easily check, how much memory WEKA can use (this depends on the \nmaximum heap size\n the [[Java Virtual Machine]] was started with).\n\n\n\n\n\n\nSimpelCLI\n\n\n\n\nstart the SimpleCLI\n\n\n\n\nrun the following command:\n\n\njava weka.core.SystemInfo\n\n\n\n\n\n\nthe property \nmemory.max\n lists the maximum amount of memory available to WEKA\n\n\n\n\n\n\n\n\n\n\nGUIChooser\n\n\n\n\nselect \nHelp -> SystemInfo\n\n\nthe property \nmemory.max\n lists the maximum amount of memory available to WEKA\n\n\n\n\n\n\n\n\nIn case you should run into an \nOutOfMemoryException\n, you will have to increase the \nmaximum heap size\n. How much you can allocate, depends heavily on the operating system and the underlying hardware, see the [[Java Virtual Machine]] article). Also, have a look at the \nOutOfMemoryException\n section further down.",
            "title": " Can I check how much memory is available for WEKA?"
        },
        {
            "location": "/faqs/OutOfMemoryException/",
            "text": "Most Java virtual machines only allocate a certain maximum amount of memory to run Java programs. Usually, this is much less than the amount of RAM in your computer. There is some information on default heap sizes in Oracle Java virtual machines for Java 8 \nhere\n. However, you can extend the memory available for the virtual machine by setting appropriate options. With Oracle's JDK, for example, you can go\n\n\njava -Xmx2g ...\n\n\n\n\nto set the maximum Java heap size to 2GB.\n\n\nA reliable way to set the maximum heap size for Oracle Java virtual machines (and overwrite any other settings that might be provided in startup scripts, etc.) is to use the _JAVA_OPTIONS environment variable to specify the \n-Xmx\n option. There is more information \nhere\n.",
            "title": " OutOfMemoryException"
        },
        {
            "location": "/faqs/use_csv_files/",
            "text": "Yes, you can. But be aware that there is a drawback in comparison to \nARFF\n files (WEKA's default file format):\n\n\nTrain and test set may not be compatible.\n Using CSV files as train and test set can be a frustrating exercise. Since CSV files don't contain any information about the attributes, WEKA needs to determine the labels for nominal attributes itself. Not only does the order of the appearance of these labels create different nominal attributes (\"1,2,3\" vs \"1,3,2\"), but it is also not guaranteed that all the labels that appeared in the train set also appear in the test set (\"1,2,3,4\" vs \"1,3,4\") and vice versa.",
            "title": " Can I use CSV files?"
        },
        {
            "location": "/faqs/csv_file_conversion/",
            "text": "Either load the CSV file in the Explorer or use the CSV converter on the commandline as follows:\n\n\n java weka.core.converters.CSVLoader filename.csv > filename.arff\n\n\n\n\nSee also the \nConverting CSV to ARFF\n article and FAQ \nCan I use CSV files?\n.",
            "title": " CSV File Conversion"
        },
        {
            "location": "/faqs/how_do_i_divide_a_dataset_into_training_and_test_set/",
            "text": "You can use the \nRemovePercentage\n filter (package \nweka.filters.unsupervised.instance\n).\n\n\nIn the Explorer just do the following:\n\n\n\n\ntraining set:\n\n\nLoad the full dataset\n\n\nselect the \nRemovePercentage\n filter in the preprocess panel\n\n\nset the correct percentage for the split\n\n\napply the filter\n\n\nsave the generated data as a new file\n\n\n\n\n\n\ntest set:\n\n\nLoad the full dataset (or just use undo to revert the changes to the dataset)\n\n\nselect the \nRemovePercentage\n filter if not yet selected\n\n\nset the \ninvertSelection\n property to true\n\n\napply the filter\n\n\nsave the generated data as new file",
            "title": " How do I divide a dataset into training and test sets"
        },
        {
            "location": "/faqs/how_do_i_generate_compatible_train_and_test_sets_that_get_processed_with_a_filter/",
            "text": "Running a filter twice, once with the train set as input and then the second time with the test set, will create almost certainly two incompatible files. Why is that? Every time you run a filter, it will get initialized based on the input data, and, of course, training and test set will differ, hence creating incompatible output. You can avoid this by using \nbatch filtering\n. See the article on \nBatch filtering\n for more details.",
            "title": " How do I generate compatible train and test sets that get processed with a filter? "
        },
        {
            "location": "/faqs/how_do_i_perform_attribute_selection/",
            "text": "WEKA offers different approaches for performing attribute selection:\n\n\n\n\ndirectly with the attribute selection classes,\n\n\nwith a meta-classifier, and \n\n\nwith a filter.\n\n\n\n\nCheck out the \nPerforming attribute selection\n article for more details and examples.",
            "title": " How do I perform attribute selection? "
        },
        {
            "location": "/faqs/how_do_i_perform_clustering/",
            "text": "WEKA offers clustering capabilities not only as standalone schemes, but also as filters and classifiers. Check out the article about \nUsing cluster algorithms\n more detailed information.",
            "title": " How do I perform clustering?"
        },
        {
            "location": "/faqs/how_do_i_perform_text_classification/",
            "text": "The article \nText categorization with WEKA\n explains a few basics of how to deal with text documents, like importing and pre-processing.",
            "title": " How do I perform text classification?"
        },
        {
            "location": "/faqs/how_can_i_perform_multi_instance_learning_in_weka/",
            "text": "The article \nMulti-instance classification\n explains what classifiers can perform multi-instance classification and what format the data has to be in for these multi-instance classifiers.",
            "title": " How can I perform multi-instance learning in WEKA?"
        },
        {
            "location": "/faqs/how_do_i_perform_cost_sensitive_classification/",
            "text": "Cost-sensitive classification can be achieved using a \nCost-Sensitive Classifier\n. Related articles to the cost-sensitive topic include:\n\n\n\n\nCost Matrix\n\n\nMetacost\n\n\n\n\nSearching for the term \ncost-sensitive\n will also show articles related to the topic.",
            "title": " How do I perform cost-sensitive classification?"
        },
        {
            "location": "/faqs/how_do_i_make_predictions_with_a_trained_model/",
            "text": "Since WEKA allows models to be saved (as Java binary serialized objects), one can use those models again to perform predictions. Check out the article \nMaking Predictions\n for more details.",
            "title": " How do I make predictions with a trained model?"
        },
        {
            "location": "/faqs/why_am_i_missing_certain_nominal_or_string_values_from_sparse_instances/",
            "text": "Internally, WEKA stores all attribute values as double precision floating point numbers. In the case of nominal or string attributes these numbers are interpreted as indexes into the set of values for the attribute in question, with 0 corresponding to the first value, 1 the second and so forth. Because sparse data does not explicitly store zeros, any instances containing the first value (with index 0) of a nominal or string attribute does not show this value when printing out an ARFF file that is sparse format.",
            "title": " Why am I missing certain nominal or string values from sparse instances?"
        },
        {
            "location": "/faqs/can_i_use_weka_for_time_series_analysis/",
            "text": "Weka 3.7.3 has a new package that provides an environment for time series analysis. The article \nHow do I use the package manager?\n can be followed to install this package. Once installed, the package provides a plugin tab in the Explorer. Documentation on the time series environment can be found \nhere\n \n\n\nOlder versions of Weka have limited support for time series analysis and consists of only two filters, \nTimeSeriesDelta\n and \nTimeSeriesTranslate\n. There are modified (not supported by the University of Waikato) versions of WEKA out there, that offer additional functionality (\n1\n, \n2\n).",
            "title": " Can I use WEKA for time series analysis?"
        },
        {
            "location": "/faqs/does_weka_support_multi_label_classification/",
            "text": "No, WEKA only allows you to specify a single class attribute (which can be numeric or contain an arbitrary number of labels). There are other third-party frameworks available that can handle this type of data. One of them is \nMulan\n, which is built on top of WEKA.",
            "title": " Does WEKA support multi-label classification?"
        },
        {
            "location": "/faqs/how_do_i_perform_one_class_classification/",
            "text": "WEKA offers some rudimentary support for one-class classfication:\n\n\n\n\nvia the \nweka.classifiers.functions.LibSVM\n wrapper classifier (stable 3.6 and developer version). See the \nLibSVM\n article for more information.\n\n\nvia the \nweka.classifiers.meta.OneClassClassifier\n meta-classifier (developer version >3.7.0 or \nsnapshot\n later than 23/7/2009)",
            "title": " How do I perform one-class classification?"
        },
        {
            "location": "/faqs/can_i_make_a_screenshot_of_a_plot_or_graph_directly_in_weka/",
            "text": "Yes, you can. The currently supported formats are BMP, EPS, JPEG and PNG. The \nmagic button\n is \nAlt+Shift+Left-Click\n.\n\n\nFrom Weka 3.7.5 it is also possible to export various charts as PNG files non-interactively from a Knowledge Flow process. See \nExporting Charts from the Knowledge Flow\n.",
            "title": " Can I make a screenshot of a plot or graph directly in WEKA?"
        },
        {
            "location": "/packages/manager/",
            "text": "Usually, the term \"package\" is used to refer to Java's concept of organizing classes. From version 3.7.2, Weka has the concept of a package as a bundle of additional functionality, separate from that supplied in the main weka.jar file. A package consists of various jar files, documentation, meta data, and possibly source code. Many learning algorithms and tools that were present in earlier versions of Weka have become separate packages from version 3.7.2. This simplifies the core Weka system and allows users to install just what they need or are interested in. It also provides a simple mechanism for people to use when contributing to Weka. There are a number of packages available for Weka that add learning schemes or extend the functionality of the core system in some fashion. Many are provided by the Weka team and others are from third parties.\n\n\nWeka includes a facility for the management of packages and a mechanism to load them dynamically at runtime. There is both a command-line and GUI package manager. If the package manager does not start when you try to run it, take a look at \nthis\n page.\n\n\nCommand line package management\n\n\nAssuming that the weka.jar file is in the classpath, the package manager can be accessed by typing:\n\n\n java weka.core.WekaPackageManager\n\n\n\n\nSupplying no options will print the usage information:\n\n\nUsage: weka.core.WekaPackageManager [option]\nOptions:\n  -list-packages <all | installed | available>\n  -package-info <repository | installed | archive> packageName\n  -install-package <packageName | packageZip | URL> [version]\n  -uninstall-package <packageName>\n  -refresh-cache\n\n\n\n\n\n\nWeka 3.7.8 and snapshot builds of the developer version of Weka after September 24 2012 now offer a completely \"offline\" mode that involves no attempts to connect to the internet. This mode can be used to install package zip files that the user already has on the file system, and to browse already installed packages. This mode can be accessed from the command line package manager by specifying the \"-offline\" option. Alternatively, the property weka.packageManager.offline=true can be provided to the Java virtual machine on the command line or in a properties file (see the section on properties below).\n\n\n\n\nInformation (meta data) about packages is stored on a web server hosted on Sourceforge. The first time the package manager is run, for a new installation of Weka, there will be a short delay while the system downloads and stores a cache of the meta data from the server. Maintaining a cache speeds up the process of browsing the package information. From time to time you should update the local cache of package meta data in order to get the latest information on packages from the server. This can be achieved by supplying the \n-refresh-cache\n option.\n\n\nThe \n-list-packages\n option will, as the name suggests, print information (version numbers and short descriptions) about various packages. The option must be followed by one of three keywords:\n\n\n\n\nall\n will print information on all packages that the system knows about\n\n\ninstalled\n will print information on all packages that are installed locally\n\n\navailable\n will print information on all packages that are not installed\n\n\n\n\nThe following shows an example of listing all packages installed locally:\n\n\njava weka.core.WekaPackageManager -list-packages installed\n\nInstalled    Repository    Package\n=========    ==========    =======\n1.0.0        1.0.0         DTNB: Class for building and using a decision table/naive bayes hybrid classifier.\n1.0.0        1.0.0         massiveOnlineAnalysis: MOA (Massive On-line Analysis).\n1.0.0        1.0.0         multiInstanceFilters: A collection of filters for manipulating multi-instance data.\n1.0.0        1.0.0         naiveBayesTree: Class for generating a decision tree with naive Bayes classifiers at the leaves.\n1.0.0        1.0.0         scatterPlot3D: A visualization component for displaying a 3D scatter plot of the data using Java 3D.\n\n\n\n\nThe \n-package-info\n command lists information about a package given its name. The command is followed by one of three keywords and then the name of a package:\n\n\n\n\nrepository\n will print info from the repository for the named package\n\n\ninstalled\n will print info on the installed version of the named package\n\n\narchive\n will print info for a package stored in a zip archive. In this case, the \u201carchive\u201d keyword must be followed by the path to an package zip archive file rather than just the name of a package\n\n\n\n\nThe following shows an example of listing information for the \u201cisotonicRegression\u201d package from the server:\n\n\njava weka.core.WekaPackageManager -package-info repository isotonicRegression\nDescription:Learns an isotonic regression model. Picks the attribute that results in the lowest squared error. Missing values are not allowed. Can only deal with numeric attributes. Considers the monotonically increasing case as well as the monotonically decreasing case.\nVersion:1.0.0\nPackageURL:http://prdownloads.sourceforge.net/weka/isotonicRegression1.0.0.zip?download\nAuthor:Eibe Frank\nPackageName:isotonicRegression\nTitle:Learns an isotonic regression model.\nDate:2009-09-10\nURL:http://weka.sourceforge.net/doc.dev/weka/classifiers/IsotonicRegression.html\nCategory:Regression\nDepends:weka (>=3.7.1)\nLicense:GPL 2.0\nMaintainer:Weka team <wekalist@list.scms.waikato.ac.nz>\n\n\n\n\nThe \n-install-package\n command allows a package to be installed from one of three locations:\n\n\n\n\nspecifying a name of a package will install the package using the information in the package description meta data stored on the server. If no version number is given, then the latest available version of the package is installed.\n\n\nproviding a path to a zip file will attempt to unpack and install the archive as a Weka package\n\n\nproviding a URL (beginning with \nhttp://\n) to a package zip file on the web will download and attempt to install the zip file as a Weka package\n\n\n\n\nThe \nuninstall-package\n command will uninstall the named package. Of course, the named package has to be installed for this command to have any effect!\n\n\nRunning installed learning algorithms\n\n\nRunning learning algorithms that come with the main weka distribution (i.e. are contained in the weka.jar file) was covered earlier in the [[Primer]]. But what about algorithms from packages that you\u2019ve installed using the package manager? We don\u2019t want to have to add a ton of jar files to our classpath every time we wan\u2019t to run a particular algorithm. Fortunately, we don\u2019t have to. Weka has a mechanism to load installed packages dynamically at run time. This means that newly installed packages are available in Weka's GUIs immediately.\n\n\nWhat about running algorithms from packages on the command line I hear you ask? We can run a named algorithm by using the Run command:\n\n\n java weka.Run\n\n\n\n\nIf no arguments are supplied, then Run outputs the following usage information:\n\n\n Usage:\n     weka.Run [-no-scan] [-no-load] <scheme name [scheme options]>\n\n\n\n\nThe Run command supports sub-string matching, so you can run a classifier (such as J48) like so:\n\n\n java weka.Run J48\n\n\n\n\nWhen there are multiple matches on a supplied scheme name you will be presented with a list. For example:\n\n\n java weka.Run NaiveBayes\n\n Select a scheme to run, or <return> to exit:\n     1) weka.classifiers.bayes.ComplementNaiveBayes\n     2) weka.classifiers.bayes.NaiveBayes\n     3) weka.classifiers.bayes.NaiveBayesMultinomial\n     4) weka.classifiers.bayes.NaiveBayesMultinomialUpdateable\n     5) weka.classifiers.bayes.NaiveBayesSimple\n     6) weka.classifiers.bayes.NaiveBayesUpdateable\n\n Enter a number >\n\n\n\n\nYou can turn off the scanning of packages and sub-string matching by providing the \n-no-scan\n option. This is useful when using the \nRun\n command in a script. In this case, you need to specify the fully qualified name of the algorithm to use. E.g.\n\n\n java weka.Run -no-scan weka.classifiers.bayes.NaiveBayes\n\n\n\n\nTo reduce startup time you can also turn off the dynamic loading of installed packages by specifying the \n-no-load\n option. In this case, you will need to explicitly include any packaged algorithms in your classpath if you plan to use them. E.g.\n\n\njava -classpath ./weka.jar:$HOME/wekafiles/packages/DTNB/DTNB.jar rweka.Run -no-load -no-scan weka.classifiers.rules.DTNB\n\n\n\n\nGUI package manager\n\n\nAs well as a command line client, there is also a graphical interface to Weka\u2019s package management system. This is available from the \nTools\n menu in the \nGUIChooser\n. All the functionality available in the command line client to the package management system is available in the GUI version, along with the ability to install and uninstall multiple packages in one hit.\n\n\n\n\nThe package manager\u2019s window is split horizontally into two parts: at the top is a list of packages and at the bottom is a mini browser that can be used to display information on the currently selected package.\n\n\nThe package list shows the name of a package, its category, the currently installed version (if installed), the latest version available via the repository and whether the package has been loaded or not. This list may be sorted by either package name or category by clicking on the appropriate column header. A second click on the same header reverses the sort order. Three radio buttons in the upper left of the window can be used to filter what is displayed in the list. All packages (default), all available packages (i.e. those not yet installed) or only installed packages can be displayed.\n\n\nIf multiple versions of a package are available, they can be accessed by clicking on an entry in the \u201cRepository version\u201d column:\n\n\n\n\nInstalling and removing packages\n\n\nAt the very top of the window are three buttons. On the left-hand side is a button that can be used to refresh the cached copy of the package repository meta data. The first time that the package manager (GUI or command line) is used there will be a short delay as the initial cache is established. \n\n\nNOTE: Weka (3.7.2) will not automatically check for new information at the central repository, so it is a good idea to refresh the local cache regularly. From Weka 3.7.3 the package manager will notify you if there are new packages available at the central repository\n.\n\n\nThe two buttons at the top right are used to install and remove packages repspectively. Multiple packages may be installed/removed by using a shift-left-click combination to select a range and/or by using a command-left-click combination to add to the selection. Underneath the install and uninstall but- tons is a checkbox that can be enabled to ignore any dependencies required by selected packages and any conflicts that may occur. Installing packages while this checkbox is selected will '''not''' install required dependencies.\n\n\nSome packages may have additional information on how to complete the installation or special instructions that gets displayed when the package is installed:\n\n\n\n\nUsually it is not necessary to restart Weka after packages have been installed\u2014the changes should be available immediately. An exception is when upgrading a package that is already installed. If in doubt, restart Weka.\n\n\nUnofficial packages\n\n\nThe package list shows those packages that have their meta data stored in Weka\u2019s central meta data repository. These packages are \u201cofficial\u201d Weka packages and the Weka team as verified that they appear to provide what is advertised (and do not contain malicious code or malware).\n\n\nIt is also possible to install an \nunofficial\n package that has not gone through the process of become official. Unofficial packages might be provided, for exam- ple, by researchers who want to make experimental algorithms quickly available to the community for feedback. Unofficial packages can be installed by clicking the \u201cFile/url\u201d button on the top-right of the package manager window. This will bring up an \u201cUnnoficial package install\u201d dialog where the user can browse their file system for a package zip file or directly enter an URL to the package zip file. Note that no dependency checking is done for unofficial packages.\n\n\nUsing a HTTP proxy\n\n\nBoth the GUI and command line package managers can operate via a http proxy. To do so, start Weka from the command line and supply property values for the proxy host and port:\n\n\n java -Dhttp.proxyHost=some.proxy.somewhere.net -Dhttp.proxyPort=port weka.gui.GUIChooser\n\n\n\n\nIf your proxy requires authentication, then Weka will present a GUI dialog where you can enter your username and password. If you are running on a headless environment, then two more (non-standard) properties can be supplied:\n\n\n -Dhttp.proxyUser=some_user_name -Dhttp.proxyPassword=some_password\n\n\n\n\nUsing an alternative central package meta data repository\n\n\nBy default, both the command-line and GUI package managers use the central package meta data repository hosted on Sourceforge. In the unlikely event that this site is unavailable for one reason or another, it is possible to point the package management system at an alternative repository. This mechanism allows a temporary backup of the official repostory to be accessed, local mirrors to be established and alternative repositories to be set up for use etc.\n\n\nAn alternative repository can be specified by setting a Java property:\n\n\nweka.core.wekaPackageRepositoryURL=http://some.mirror.somewhere\n\n\n\n\nThis can either be set when starting Weka from the command line with the \n-D\n flag, or it can be placed into a file called \u201cPackageRepository.props\u201d in \n$WEKA_HOME/props\n. The default value of \nWEKA_HOME\n is \nuser.home/wekafiles\n, where \nuser.home\n is the user\u2019s home directory. More information on how and where Weka stores configuration information is given in the [[How are packages structured for the package management system?|how are package structured for the package management system?]] article.\n\n\nPackage manager property file\n\n\nAs mentioned in the previous section, an alternative package meta data repository can be specified by placing an entry in the PackageRepository.props file in \n$WEKA_HOME/props\n. From Weka 3.7.8 (and snapshot builds after 24 September 2012), the package manager also looks for properties placed in \n$WEKA_HOME/props/PackageManager.props\n. The current set of properties that can be set are:\n\n\nweka.core.wekaPackageRepositoryURL=http://some.mirror.somewhere\nweka.packageManager.offline=[true | false]\nweka.packageManager.loadPackages=[true | false]\nweka.pluginManager.disable=com.funky.FunkyExplorerPluginTab\n\n\n\n\nThe default for offline mode (if unspecified) is \nfalse\n and for loadPackages is \ntrue\n. The weka.pluginManager.disable property can be used to specify a comma-separated list of fully qualified class names to \"disable\" in the GUI. This can be used to make problematic components unavailable in the GUI without having to prevent the entire package that contains them from being loaded. E.g. \"funkyPackage\" might provide several classifiers and a special Explorer plugin tab for visualization. Suppose, for example, that the plugin Explorer tab has issues with certain data sets and causes annoying exceptions to be generated (or perhaps in the worst cases crashes the Explorer!). In this case we might want to use the classifiers provided by the package and just disable the Explorer plugin. Listing the fully qualified name of the Explorer plugin as a member of the comma-separated list associated with the weka.pluginManager.disable property will achieve this.",
            "title": " How do I use the package manager?"
        },
        {
            "location": "/packages/manager/#command-line-package-management",
            "text": "Assuming that the weka.jar file is in the classpath, the package manager can be accessed by typing:   java weka.core.WekaPackageManager  Supplying no options will print the usage information:  Usage: weka.core.WekaPackageManager [option]\nOptions:\n  -list-packages <all | installed | available>\n  -package-info <repository | installed | archive> packageName\n  -install-package <packageName | packageZip | URL> [version]\n  -uninstall-package <packageName>\n  -refresh-cache   Weka 3.7.8 and snapshot builds of the developer version of Weka after September 24 2012 now offer a completely \"offline\" mode that involves no attempts to connect to the internet. This mode can be used to install package zip files that the user already has on the file system, and to browse already installed packages. This mode can be accessed from the command line package manager by specifying the \"-offline\" option. Alternatively, the property weka.packageManager.offline=true can be provided to the Java virtual machine on the command line or in a properties file (see the section on properties below).   Information (meta data) about packages is stored on a web server hosted on Sourceforge. The first time the package manager is run, for a new installation of Weka, there will be a short delay while the system downloads and stores a cache of the meta data from the server. Maintaining a cache speeds up the process of browsing the package information. From time to time you should update the local cache of package meta data in order to get the latest information on packages from the server. This can be achieved by supplying the  -refresh-cache  option.  The  -list-packages  option will, as the name suggests, print information (version numbers and short descriptions) about various packages. The option must be followed by one of three keywords:   all  will print information on all packages that the system knows about  installed  will print information on all packages that are installed locally  available  will print information on all packages that are not installed   The following shows an example of listing all packages installed locally:  java weka.core.WekaPackageManager -list-packages installed\n\nInstalled    Repository    Package\n=========    ==========    =======\n1.0.0        1.0.0         DTNB: Class for building and using a decision table/naive bayes hybrid classifier.\n1.0.0        1.0.0         massiveOnlineAnalysis: MOA (Massive On-line Analysis).\n1.0.0        1.0.0         multiInstanceFilters: A collection of filters for manipulating multi-instance data.\n1.0.0        1.0.0         naiveBayesTree: Class for generating a decision tree with naive Bayes classifiers at the leaves.\n1.0.0        1.0.0         scatterPlot3D: A visualization component for displaying a 3D scatter plot of the data using Java 3D.  The  -package-info  command lists information about a package given its name. The command is followed by one of three keywords and then the name of a package:   repository  will print info from the repository for the named package  installed  will print info on the installed version of the named package  archive  will print info for a package stored in a zip archive. In this case, the \u201carchive\u201d keyword must be followed by the path to an package zip archive file rather than just the name of a package   The following shows an example of listing information for the \u201cisotonicRegression\u201d package from the server:  java weka.core.WekaPackageManager -package-info repository isotonicRegression\nDescription:Learns an isotonic regression model. Picks the attribute that results in the lowest squared error. Missing values are not allowed. Can only deal with numeric attributes. Considers the monotonically increasing case as well as the monotonically decreasing case.\nVersion:1.0.0\nPackageURL:http://prdownloads.sourceforge.net/weka/isotonicRegression1.0.0.zip?download\nAuthor:Eibe Frank\nPackageName:isotonicRegression\nTitle:Learns an isotonic regression model.\nDate:2009-09-10\nURL:http://weka.sourceforge.net/doc.dev/weka/classifiers/IsotonicRegression.html\nCategory:Regression\nDepends:weka (>=3.7.1)\nLicense:GPL 2.0\nMaintainer:Weka team <wekalist@list.scms.waikato.ac.nz>  The  -install-package  command allows a package to be installed from one of three locations:   specifying a name of a package will install the package using the information in the package description meta data stored on the server. If no version number is given, then the latest available version of the package is installed.  providing a path to a zip file will attempt to unpack and install the archive as a Weka package  providing a URL (beginning with  http:// ) to a package zip file on the web will download and attempt to install the zip file as a Weka package   The  uninstall-package  command will uninstall the named package. Of course, the named package has to be installed for this command to have any effect!",
            "title": "Command line package management"
        },
        {
            "location": "/packages/manager/#running-installed-learning-algorithms",
            "text": "Running learning algorithms that come with the main weka distribution (i.e. are contained in the weka.jar file) was covered earlier in the [[Primer]]. But what about algorithms from packages that you\u2019ve installed using the package manager? We don\u2019t want to have to add a ton of jar files to our classpath every time we wan\u2019t to run a particular algorithm. Fortunately, we don\u2019t have to. Weka has a mechanism to load installed packages dynamically at run time. This means that newly installed packages are available in Weka's GUIs immediately.  What about running algorithms from packages on the command line I hear you ask? We can run a named algorithm by using the Run command:   java weka.Run  If no arguments are supplied, then Run outputs the following usage information:   Usage:\n     weka.Run [-no-scan] [-no-load] <scheme name [scheme options]>  The Run command supports sub-string matching, so you can run a classifier (such as J48) like so:   java weka.Run J48  When there are multiple matches on a supplied scheme name you will be presented with a list. For example:   java weka.Run NaiveBayes\n\n Select a scheme to run, or <return> to exit:\n     1) weka.classifiers.bayes.ComplementNaiveBayes\n     2) weka.classifiers.bayes.NaiveBayes\n     3) weka.classifiers.bayes.NaiveBayesMultinomial\n     4) weka.classifiers.bayes.NaiveBayesMultinomialUpdateable\n     5) weka.classifiers.bayes.NaiveBayesSimple\n     6) weka.classifiers.bayes.NaiveBayesUpdateable\n\n Enter a number >  You can turn off the scanning of packages and sub-string matching by providing the  -no-scan  option. This is useful when using the  Run  command in a script. In this case, you need to specify the fully qualified name of the algorithm to use. E.g.   java weka.Run -no-scan weka.classifiers.bayes.NaiveBayes  To reduce startup time you can also turn off the dynamic loading of installed packages by specifying the  -no-load  option. In this case, you will need to explicitly include any packaged algorithms in your classpath if you plan to use them. E.g.  java -classpath ./weka.jar:$HOME/wekafiles/packages/DTNB/DTNB.jar rweka.Run -no-load -no-scan weka.classifiers.rules.DTNB",
            "title": "Running installed learning algorithms"
        },
        {
            "location": "/packages/manager/#gui-package-manager",
            "text": "As well as a command line client, there is also a graphical interface to Weka\u2019s package management system. This is available from the  Tools  menu in the  GUIChooser . All the functionality available in the command line client to the package management system is available in the GUI version, along with the ability to install and uninstall multiple packages in one hit.   The package manager\u2019s window is split horizontally into two parts: at the top is a list of packages and at the bottom is a mini browser that can be used to display information on the currently selected package.  The package list shows the name of a package, its category, the currently installed version (if installed), the latest version available via the repository and whether the package has been loaded or not. This list may be sorted by either package name or category by clicking on the appropriate column header. A second click on the same header reverses the sort order. Three radio buttons in the upper left of the window can be used to filter what is displayed in the list. All packages (default), all available packages (i.e. those not yet installed) or only installed packages can be displayed.  If multiple versions of a package are available, they can be accessed by clicking on an entry in the \u201cRepository version\u201d column:",
            "title": "GUI package manager"
        },
        {
            "location": "/packages/manager/#installing-and-removing-packages",
            "text": "At the very top of the window are three buttons. On the left-hand side is a button that can be used to refresh the cached copy of the package repository meta data. The first time that the package manager (GUI or command line) is used there will be a short delay as the initial cache is established.   NOTE: Weka (3.7.2) will not automatically check for new information at the central repository, so it is a good idea to refresh the local cache regularly. From Weka 3.7.3 the package manager will notify you if there are new packages available at the central repository .  The two buttons at the top right are used to install and remove packages repspectively. Multiple packages may be installed/removed by using a shift-left-click combination to select a range and/or by using a command-left-click combination to add to the selection. Underneath the install and uninstall but- tons is a checkbox that can be enabled to ignore any dependencies required by selected packages and any conflicts that may occur. Installing packages while this checkbox is selected will '''not''' install required dependencies.  Some packages may have additional information on how to complete the installation or special instructions that gets displayed when the package is installed:   Usually it is not necessary to restart Weka after packages have been installed\u2014the changes should be available immediately. An exception is when upgrading a package that is already installed. If in doubt, restart Weka.",
            "title": "Installing and removing packages"
        },
        {
            "location": "/packages/manager/#unofficial-packages",
            "text": "The package list shows those packages that have their meta data stored in Weka\u2019s central meta data repository. These packages are \u201cofficial\u201d Weka packages and the Weka team as verified that they appear to provide what is advertised (and do not contain malicious code or malware).  It is also possible to install an  unofficial  package that has not gone through the process of become official. Unofficial packages might be provided, for exam- ple, by researchers who want to make experimental algorithms quickly available to the community for feedback. Unofficial packages can be installed by clicking the \u201cFile/url\u201d button on the top-right of the package manager window. This will bring up an \u201cUnnoficial package install\u201d dialog where the user can browse their file system for a package zip file or directly enter an URL to the package zip file. Note that no dependency checking is done for unofficial packages.",
            "title": "Unofficial packages"
        },
        {
            "location": "/packages/manager/#using-a-http-proxy",
            "text": "Both the GUI and command line package managers can operate via a http proxy. To do so, start Weka from the command line and supply property values for the proxy host and port:   java -Dhttp.proxyHost=some.proxy.somewhere.net -Dhttp.proxyPort=port weka.gui.GUIChooser  If your proxy requires authentication, then Weka will present a GUI dialog where you can enter your username and password. If you are running on a headless environment, then two more (non-standard) properties can be supplied:   -Dhttp.proxyUser=some_user_name -Dhttp.proxyPassword=some_password",
            "title": "Using a HTTP proxy"
        },
        {
            "location": "/packages/manager/#using-an-alternative-central-package-meta-data-repository",
            "text": "By default, both the command-line and GUI package managers use the central package meta data repository hosted on Sourceforge. In the unlikely event that this site is unavailable for one reason or another, it is possible to point the package management system at an alternative repository. This mechanism allows a temporary backup of the official repostory to be accessed, local mirrors to be established and alternative repositories to be set up for use etc.  An alternative repository can be specified by setting a Java property:  weka.core.wekaPackageRepositoryURL=http://some.mirror.somewhere  This can either be set when starting Weka from the command line with the  -D  flag, or it can be placed into a file called \u201cPackageRepository.props\u201d in  $WEKA_HOME/props . The default value of  WEKA_HOME  is  user.home/wekafiles , where  user.home  is the user\u2019s home directory. More information on how and where Weka stores configuration information is given in the [[How are packages structured for the package management system?|how are package structured for the package management system?]] article.",
            "title": "Using an alternative central package meta data repository"
        },
        {
            "location": "/packages/manager/#package-manager-property-file",
            "text": "As mentioned in the previous section, an alternative package meta data repository can be specified by placing an entry in the PackageRepository.props file in  $WEKA_HOME/props . From Weka 3.7.8 (and snapshot builds after 24 September 2012), the package manager also looks for properties placed in  $WEKA_HOME/props/PackageManager.props . The current set of properties that can be set are:  weka.core.wekaPackageRepositoryURL=http://some.mirror.somewhere\nweka.packageManager.offline=[true | false]\nweka.packageManager.loadPackages=[true | false]\nweka.pluginManager.disable=com.funky.FunkyExplorerPluginTab  The default for offline mode (if unspecified) is  false  and for loadPackages is  true . The weka.pluginManager.disable property can be used to specify a comma-separated list of fully qualified class names to \"disable\" in the GUI. This can be used to make problematic components unavailable in the GUI without having to prevent the entire package that contains them from being loaded. E.g. \"funkyPackage\" might provide several classifiers and a special Explorer plugin tab for visualization. Suppose, for example, that the plugin Explorer tab has issues with certain data sets and causes annoying exceptions to be generated (or perhaps in the worst cases crashes the Explorer!). In this case we might want to use the classifiers provided by the package and just disable the Explorer plugin. Listing the fully qualified name of the Explorer plugin as a member of the comma-separated list associated with the weka.pluginManager.disable property will achieve this.",
            "title": "Package manager property file"
        },
        {
            "location": "/faqs/package_manager_doesnt_start/",
            "text": "The most likely reason for this is that your computer does not have direct access to the Internet and Java needs to be told to use a proxy server to access the web. The best way to achieve this is to configure an environment variable that provides the proxy details, e.g.,\n\n\n_JAVA_OPTIONS\n\n\n\n\nwhich is read by Oracle Java virtual machines. There is more information on this variable \nhere\n. Information on how to set environment variables in Windows is \nhere\n. For Mac users, there is a nice program to set environment variables available \nhere\n.\n\n\nSet the value of this variable to\n\n\n-Dhttp.proxyHost=some.proxy.somewhere.net -Dhttp.proxyPort=port\n\n\n\n\nwhere \nsome.proxy.somewhere.net\n needs to be replaced by the name of your proxy server and \nport\n needs to be replaced by the appropriate port number on the proxy server. Your IT department should be able to give you these details.\n\n\nThis should allow the package manager to connect to the website that hosts the package meta-information. However, if the package manager still cannot connect to the Internet, you can also force it to run in offline mode, by setting the above environment variable to\n\n\n-Dweka.packageManager.offline=true\n\n\n\n\nThen, you can download package .zip files manually via your web browser, by navigating to\n\n\nhttp://weka.sourceforge.net/packageMetaData/\n\n\nclicking on the link for the package you want to install, then clicking on \nLatest\n, and finally clicking on the URL given next to \nPackageURL\n.\n\n\nOnce you have downloaded the package .zip file, open the WEKA package manager, and click on the \nFile/URL\n button in the top-right corner of the package manager window (in the \nUnofficial\n panel). Then navigate to your package .zip file and install it.\n\n\nIf you are running Weka in \noffline\n mode, and the packages you are installing have some dependencies on one another, then there can still be some problems due to Weka not being able to verify the dependencies by checking against the central repository. This is usually a problem in the case where Weka has never been able to connect to the internet and thus has not downloaded and established a cache of the central package metadata repository. Fortunately there is a simple work-around to this, as long as you can access the internet via a web browser:\n\n\n\n\nUsing your web browser, download \nhttp://weka.sourceforge.net/packageMetaData/repo.zip\n\n\nIf it doesn't already exist, create the directory \n~/wekafiles/repCache\n\n\nCopy the downloaded \nrepo.zip\n into \n~/wekafiles/repCache\n and unzip it there\n\n\nStart Weka (use the \nweka.packageManager.offline=true\n property to speed up the startup process; see [http://weka.wikispaces.com/How+do+I+use+the+package+manager%3F#Package%20manager%20property%20file] for info)",
            "title": " What do I do if the package manager does not start?"
        },
        {
            "location": "/faqs/visualization/",
            "text": "Access to \nvisualization\n from the \nClassifier\n, \nCluster\n and \nAttribute Selection\n panel is available from a popup menu. Click the right mouse button over an entry in the Result list to bring up the menu. You will be presented with options for viewing or saving the text output and, depending on the scheme, further options for visualizing errors, clusters, trees etc.",
            "title": " Where do I find visualization of classifiers, etc.?"
        },
        {
            "location": "/faqs/how_can_i_track_instances_in_weka/",
            "text": "WEKA doesn't support internal IDs for instances, one has to use ID attributes. See \nHow do I use ID attributes?",
            "title": " How can I track instances in WEKA?"
        },
        {
            "location": "/faqs/how_do_i_use_id_attributes/",
            "text": "See the \nInstance ID\n article for more information on how to use attribute IDs in WEKA.",
            "title": " How do I use ID attributes?"
        },
        {
            "location": "/faqs/how_do_i_connect_to_a_database/",
            "text": "With a bit of effort you can easily access databases via \nJDBC\n. You need the following:\n\n\n\n\nJDBC driver for the database you want to access in your CLASSPATH.\n\n\nA customized \nDatabaseUtils.props\n file. The following example files are located in the \nweka/experiment\n directory of the \nweka.jar\n archive:\n\n\nHSQLDB - \nDatabaseUtils.props.hsql\n (>= 3.4.1/3.5.0)\n\n\nMS SQL Server 2000 - \nDatabaseUtils.props.mssqlserver\n (>= 3.4.9/3.5.4)\n\n\nMS SQL Server 2005 Express Edition - \nDatabaseUtils.props.mssqlserver2005\n (> 3.4.10/3.5.5)\n\n\nMySQL - \nDatabaseUtils.props.mysql\n (>= 3.4.9/3.5.4)\n\n\nODBC - \nDatabaseUtils.props.odbc\n (>= 3.4.9/3.5.4)\n\n\nOracle - \nDatabaseUtils.props.oracle\n (>= 3.4.9/3.5.4)\n\n\nPostgreSQL - \nDatabaseUtils.props.postgresql\n (>= 3.4.9/3.5.4)\n\n\nSqlite 3.x - \nDatabaseUtils.props.sqlite3\n (> 3.4.12, > 3.5.7)\n\n\n\n\n\n\n\n\nFor more details see the following articles:\n\n\n\n\nDatabases\n\n\nDatabaseUtils.props\n\n\nWindows databases\n (covers access via ODBC)\n\n\n\n\nThe following FAQs could be of interest as well:\n\n\n\n\nCouldn't read from database: unknown data type\n\n\nTrying to add JDBC driver: ... - Error, not in CLASSPATH?",
            "title": " How do I connect to a database?"
        },
        {
            "location": "/faqs/how_do_i_use_weka_from_the_command_line/",
            "text": "Reading the \nPrimer\n article will help you understand the usage of the command line, as well as the \nHow to run WEKA schemes from commandline\n article.",
            "title": " How do I use WEKA from the command line?"
        },
        {
            "location": "/faqs/can_i_tune_the_parameters_of_a_classifier/",
            "text": "Yes, you can do that with one of the following meta-classifiers:\n\n\n\n\nweka.classifiers.meta.CVParameterSelection\n\n\nweka.classifiers.meta.GridSearch\n (only developer version)\n\n\nweka.classifiers.meta.AutoWEKAClassifier\n (via external package)\n\n\nweka.classifiers.meta.MultiSearch\n (via \nexternal package\n)\n\n\n\n\nSee the Javadoc of the respective classifier or the \nOptimizing parameters\n article for more information.",
            "title": " Can I tune the parameters of a classifier?"
        },
        {
            "location": "/faqs/how_do_i_generate_learning_curves/",
            "text": "You can generate learning curves using the \nAdvanced\n mode of the Experimenter. See the article \nLearning curves\n for more details.",
            "title": " How do I generate Learning curves?"
        },
        {
            "location": "/faqs/where_can_i_find_information_regarding_roc_curves/",
            "text": "Just check out the articles tagged with \nROC\n, which cover the subject of ROC curves and AUC. These articles cover GUI handling as well as how to create ROC curves from code.",
            "title": " Where can I find information regarding ROC curves?"
        },
        {
            "location": "/faqs/i_have_unbalanced_data_now_what/",
            "text": "You can either perform \ncost-sensitive classification\n or resample your data to get a more balanced class distribution (see \nsupervised resample\n filter).",
            "title": " I have unbalanced data - now what?"
        },
        {
            "location": "/faqs/can_i_run_an_experiment_using_clusterers_in_the_experimenter/",
            "text": "Yes, see the article \nRunning an Experiment Using Clusterers\n.",
            "title": " Can I run an experiment using clusterers in the Experimenter?"
        },
        {
            "location": "/faqs/how_can_i_use_transactional_data_in_weka/",
            "text": "Transactional data is often stored in databases by having a table with the transaction ID as the primary key. Individual items or elements of a given transaction may be split up over multiple rows in the table (each with the same ID). Data in this format needs to be converted to one row per transaction before it can be used to learn classifiers, association rules, clusterers etc. in WEKA. From WEKA 3.7.2 there is a package called \ndenormalize\n that contains a filter that can perform this kind of \"flattening\" process. The filter requires \n1) that the data contains an ID field that uniquely identifies each separate transaction\n, and \n2) the data is already sorted in order of this ID field\n. Here is an example scenario taken from the WEKA mailing list:\n\n\nHi,\n   I have data spanning multiple rows for an instance, such as below (User 1 span across multiple rows, User 2 as well).  Is it possible to use WEKA to cluster this dataset?  If not, any suggestion on how I should organize the data so that I can use WEKA to cluster this data? \n User  ItemID  Sequence TimeSpent\n 1     1       1        5\n 1     2       2        1\n 1     5       3        8\n 1     6       4        12\n 1     8       5        2\n\n 2     1       1        7\n 2     2       2        3\n 2     3       3        3\n 2     4       4        2\n 2     5       5        7\n\n\n\n\nIn WEKA 3.7.2 there is a package called \ndenormalize\n that contains a filter for flattening transactional data. The first thing you'd have to do to your example above is to convert it into an ARFF file:\n\n\n@relation test\n@attribute User numeric\n@attribute ItemID numeric\n@attribute Sequence numeric\n@attribute TimeSpent numeric\n@data\n1, 1, 1, 5\n1, 2, 2, 1\n1, 5, 3, 8\n1, 6, 4, 12\n1, 8, 5, 2\n2, 1, 1, 7\n2, 2, 2, 3\n2, 3, 3, 3\n2, 4, 4, 2\n2, 5, 5, 7\n\n\n\n\nNext, you can run the \nNumericToNominal\n filter to convert the attributes that need to be coded as nominal (the User attribute is an ID and can stay either as numeric or nominal). Here I've converted all attributes except the ID to nominal:\n\n\njava weka.filters.unsupervised.attribute.NumericToNominal -R 2-last -i test.arff > test2.arff\n\n\nThis results in:\n\n\n@relation test-weka.filters.unsupervised.attribute.NumericToNominal-R2-last\n\n@attribute User numeric\n@attribute ItemID {1,2,3,4,5,6,8}\n@attribute Sequence {1,2,3,4,5}\n@attribute TimeSpent {1,2,3,5,7,8,12}\n\n@data\n\n1,1,1,5\n1,2,2,1\n1,5,3,8\n1,6,4,12\n1,8,5,2\n2,1,1,7\n2,2,2,3\n2,3,3,3\n2,4,4,2\n2,5,5,7\n\n\n\n\nNow, assuming that the denormalize package is installed, and (\nIMPORTANT\n) that the data is already sorted in order of the ID attribute (\"User\" in this case):\n\n\njava weka.Run Denormalize -G first -i test2.arff > final.arff\n\n\nThis results in:\n\n\n@attribute User numeric\n@attribute ItemID_1 {f,t}\n@attribute ItemID_2 {f,t}\n@attribute ItemID_3 {f,t}\n@attribute ItemID_4 {f,t}\n@attribute ItemID_5 {f,t}\n@attribute ItemID_6 {f,t}\n@attribute ItemID_8 {f,t}\n@attribute Sequence_1 {f,t}\n@attribute Sequence_2 {f,t}\n@attribute Sequence_3 {f,t}\n@attribute Sequence_4 {f,t}\n@attribute Sequence_5 {f,t}\n@attribute TimeSpent_1 {f,t}\n@attribute TimeSpent_2 {f,t}\n@attribute TimeSpent_3 {f,t}\n@attribute TimeSpent_5 {f,t}\n@attribute TimeSpent_7 {f,t}\n@attribute TimeSpent_8 {f,t}\n@attribute TimeSpent_12 {f,t}\n\n@data\n\n1,t,t,f,f,t,t,t,t,t,t,t,t,t,t,f,t,f,t,t\n2,t,t,t,t,t,f,f,t,t,t,t,t,f,t,t,f,t,f,f\n\n\n\n\nNote, that for clustering/association rules you'd want to first remove the User ID attribute.\n\n\nI've shown this as an example using the command-line interface. It can all be done from the Explorer as well of course. The Denormalize filter has options for aggregating any numeric attributes (not the ID) as well, so if you left (for example) the TimeSpent attribute as numeric, rather than converting it to nominal using \nNumericToNominal\n, then \nDenormalize\n can aggregate it (sum, average, max, min).",
            "title": " How can I use transactional data in Weka?"
        },
        {
            "location": "/faqs/how_can_i_use_weka_with_matlab_or_octave/",
            "text": "Matlab\n and \nOctave\n allow you to interface with Java applications, which allows you to use Weka from within these applications.\n\n\nSee the following presentation, section \nOctave\n (\nOctave\n is fairly compatible with Matlab), on how to use the Java integration: \n\n\n\n\nWEKA Ecosystem",
            "title": " How can I use Weka with Matlab or Octave?"
        },
        {
            "location": "/faqs/can_i_change_the_colors_background_axes_etc_of_the_plots_in_weka/",
            "text": "Sure, this information is stored in the Visualize.props properties file:\n\n\n\n\nweka.gui.visualize.Plot2D.axisColour\n defines the color of the axes\n\n\nweka.gui.visualize.Plot2D.backgroundColour\n sets the background color\n\n\n\n\nFor more information see the articles about \nproperties file\n (especially the section \nPrecedence\n will tell you where to place the \n.props\n file.) and \nVisualize.props\n itself.",
            "title": " Can I change the colors (background, axes, etc.) of the plots in WEKA?"
        },
        {
            "location": "/faqs/how_do_i_add_a_new_classifier_filter_kernel_etc/",
            "text": "As of WEKA 3.4.4, all the derived classes of superclasses that can be edited in the GenericObjectEditor, like subclasses of \nweka.classifiers.Classifier\n for instance, can be determined dynamically at runtime. Read \nhere\n for more information.\n\n\nNote:\n WEKA 3.5.8 and 3.6.0 turned the automatic discovery \noff\n by default. Starting with 3.6.1 and 3.7.0 it is turned \non\n again.",
            "title": " How do I add a new classifier, filter, kernel, etc?"
        },
        {
            "location": "/faqs/how_do_i_use_libsvm_in_weka/",
            "text": "If you run the classifier \nweka.classifiers.functions.LibSVM\n and get the \nlibsvm classes not in CLASSPATH!\n error message, you are missing the libsvm jar archive in your current classpath. The \nLibSVM\n classifier is only a wrapper and doesn't need the libsvm classes to compile (uses Reflection). Check out the \nLibSVM\n article for details about how to use this classifier.",
            "title": " How do I use libsvm in WEKA?"
        },
        {
            "location": "/faqs/the_snowball_stemmers_dont_work_what_am_i_doing_wrong/",
            "text": "When you're trying to use the Snowball stemmers in the StringToWordVector nothing happens and you get the message \nStemmer 'porter' unknown!\n in the console. If this happens, you don't have the snowball classes in your classpath. Check out the article about the \nStemmers\n for how to add the snowball stemmers to WEKA.",
            "title": " The snowball stemmers dont work, what am I doing wrong?"
        },
        {
            "location": "/faqs/where_can_i_get_wekas_source_code/",
            "text": "Every WEKA release comes with a jar archive (this is just a simple ZIP archive) that contains the complete sources. It is called \nweka-src.jar\n. Alternatively, you can get WEKA's source code also from \nSubversion\n.",
            "title": " Where can I get WEKAs source code?"
        },
        {
            "location": "/faqs/how_do_i_compile_weka/",
            "text": "You can compile the source code simply with any (Sun-compliant) java compiler, or use ant, or an IDE. Check out the article about \nCompiling WEKA\n, which contains links to further articles, covering topics about \nant\n and \nIDEs\n (e.g., \nEclipse\n or \nNetBeans\n).",
            "title": " How do I compile WEKA?"
        },
        {
            "location": "/faqs/what_is_subversion_and_what_do_i_need_to_do_to_access_it/",
            "text": "Subversion\n is the \nversion control system\n that we use nowadays for WEKA's source code. See the \nSubversion\n article from more information of how to access the repository and retrieve the source code from there.",
            "title": " What is Subversion and what do I need to do to access it?"
        },
        {
            "location": "/faqs/how_do_i_use_wekas_classes_in_my_own_code/",
            "text": "It's not that hard to use WEKA classes in your own code, the following articles give a good overview of how to do that:\n\n\n\n\nUse WEKA in your Java code\n\n\nProgrammatic Use\n\n\nIn general, the articles tagged as \"\nsource code\n\".\n\n\n\n\nFurther resources:\n\n\n\n\nCheck out the chapter \nUsing the API\n in the Weka manual (\nsnapshots\n later than 09/08/2009 and releases >3.6.1 and >3.7.0).\n\n\nThe \nWeka Examples\n collection is a an \nANT\n project that is available through \nsnapshots\n and releases later than 09/08/2009, containing a lot of example classes.\n\n\n\n\nNote:\n WEKA is open-source software under the \nGNU General Public License\n, which means that your code has to be licensed under the GPL as well.",
            "title": " How do I use WEKAs classes in my own code?"
        },
        {
            "location": "/faqs/how_do_i_write_a_new_classifier_or_filter/",
            "text": "Basically, a classifier needs to be derived from \nweka.classifiers.Classifier\n and a filter from \nweka.filters.Filter\n, but this is only part of the story. The following articles cover the development of new schemes in greater detail:\n\n\n\n\nWriting your own Classifier\n\n\nWriting your own Filter\n\n\n\n\nIf your scheme is outside the usual WEKA packages, you need to make WEKA aware of this package in order to be able to use it in the GUI as well. See \nHow do I add a new classifier, filter, kernel, etc?\n for more information about this.\n\n\nNote:\n This is also covered in chapter \nExtending WEKA\n of the WEKA manual in versions later than 3.6.1/3.7.0 or \nsnapshots\n of the stable-3.6/developer version later than 10/01/2010. Furthermore, this chapter also covers clusterers, attribute selection algorithms and associators.",
            "title": " How do I write a new classifier or filter?"
        },
        {
            "location": "/faqs/can_i_compile_weka_into_native_code/",
            "text": "Yes, you have the following options:\n\n\n\n\nExcelsior JET\n - a commercial tool for compiling Java into native code (Windows/Linux)\n\n\ngcj\n - a free, cross-platform tool for compiling Java into native code\n\n\n\n\nSee the article \nCompiling WEKA with gcj\n for more details.",
            "title": " Can I compile WEKA into native code?"
        },
        {
            "location": "/faqs/can_i_use_weka_from_c_sharp/",
            "text": "Yes, you can. Read the \nUse WEKA with the Microsoft .NET Framework\n article for more information. There is also a tutorial for IKVM \navailable\n.",
            "title": " Can I use WEKA from C#?"
        },
        {
            "location": "/faqs/can_i_use_weka_from_python/",
            "text": "Yes and no. If you're starting from scratch, you might want to consider \nJython\n, a rewrite of \nPython\n to seamlessly integrate with Java. The drawback is, that you can only use the libraries that Jython implements, not others like \nNumPy\n or \nSciPy\n. The article \nUsing WEKA from Jython\n explains how to use WEKA classes from Jython and how to implement a new classifier in Jython, with an example of ZeroR implemented in Jython.\n\n\nAn approach making use of the \njavax.script\n package (new in Java 6) is \nJepp\n, \nJava embedded Python\n. Jepp seems to have the same limitations as Jython, not being able to import Scipy or Numpy, but one can import pure Python libraries. The arcticle \nUsing WEKA via Jepp\n contains more information and examples.\n\n\nAnother solution, to access Java from within Python applications is \nJPype\n, but It's still not fully matured.\n\n\nFinally, you can use the \npython-weka-wrapper\n Python 2.7 library to access most of the non-GUI functionality of Weka (3.9.x):\n\n\n\n\npypi\n \n\n\ngithub\n \n\n\n\n\nFor Python3, use the \npython-weka-wrapper3\n Python library:\n\n\n\n\npypi\n \n\n\ngithub",
            "title": " Can I use WEKA from Python?"
        },
        {
            "location": "/faqs/can_i_use_weka_from_groovy/",
            "text": "Yes, you can. Read the \nUsing WEKA from Groovy\n article for more information. This article tells you how to setup the Groovy CLASSPATH, in order to make the WEKA classes available to Groovy, and also contains some sample code.",
            "title": " Can I use WEKA from Groovy?"
        },
        {
            "location": "/faqs/serialization_is_nice_but_what_about_generating_actual_java_code_from_weka_classes/",
            "text": "Some of WEKA's schemes support the generation of Java source code based on their internal state. See the \nGenerating source code from WEKA classes\n article for more details.",
            "title": " Serialization is nice, but what about generating actual Java code from WEKA classes?"
        },
        {
            "location": "/packages/",
            "text": "Weka 3.7.2 introduced support for packages, making it easy to extend Weka\nwithout having to recompile or patch the underlying Weka installation.\n\n\nHere are some pointers for using and developing packages:\n\n\n\n\nHow do I use the package manager?\n\n\nUnofficial packages\n\n\nHow are packages structured for the package management system?",
            "title": " How are packages structured for the package management system?"
        },
        {
            "location": "/faqs/pluggable_evaluation_metrics/",
            "text": "WEKA 3.7.8 (and nightly snapshots of the developer branch of WEKA from 15-11-2012) has a mechanism to allow new classification and regression evaluation metrics to be added as plugins. The new metrics will be output, along with WEKA's standard set of evaluation metrics, in the output generated on the command line, in the Explorer's Classify panel and by the Knowledge Flow's ClassifierPerformanceEvaluator component. Furthermore, new plugin metrics are also available for analysis in the Experimenter.\n\n\nPreviously, adding a new evaluation metric involved editing and recompiling the monolithic weka.classifiers.Evaluation class - a shudder-worthy undertaking at the best of times. With the new plugin mechanism it is easy to add a new metric and deploy it via the package management system. The \"Additional configuration files\" section of \nHow are packages structured for the package management system?\n details how to tell the PluginManager class about your new plugin evaluation metric.\n\n\nClasses and interfaces\n\n\nThe main base class for all new metrics is \nweka.classifiers.evaluation.AbstractEvaluationMetric\n. This class requires the following methods to be implemented by concrete sub classes:\n\n\n\n\nboolean appliesToNominalClass()\n - true if the stats computed by this metric apply to nominal class problems\n\n\nboolean appliesToNumericClass()\n - true if the stats computed by this metric apply to numeric class problems\n\n\nString getMetricName()\n - return the name of the metric\n\n\nString getMetricDescription()\n - return a short description of the metric\n\n\nList<String> getStatisticNames()\n - return a list of statistics that this metric computes (e.g. a \"correct\" metric might return both the number correctly classified and the percentage correct)\n\n\ndouble getStatistic(String statName)\n - get the computed value for the named statistic\n\n\n\n\nTo facilitate computing statistics, the main Evaluation object (who's class now lives in weka.classifiers.evaluation) will pass a reference to itself to all plugin metrics when it is first constructed. Therefore, a plugin metric has access to all the protected fields in the Evaluation class, and can use these when computing it's own statistic(s).\n\n\nBeyond extending \nAbstractEvaluationMetric\n, a plugin metric will also need to implement one of the following interfaces:\n\n\nweka.classifiers.evaluation.StandardEvaluationMetric\n\n\nInterface for a \"standard\" evaluation metric - i.e. one that would be part of the normal output in WEKA without having to turn on specific display options.\n\n\nIt defines the following methods\n\n\n\n\nString toSummaryString()\n - return a formatted string (suitable for displaying in the console or GUI output) that contains all the statistics that this metric computes\n\n\nvoid updateStatsForClassifier(double[] predictedDistribution, Instance instance)\n - updates the statistics about a classifiers performance for the current test instance. Gets called when the class is nominal. Implementers need only implement this method if it is not possible to compute their statistics from what is stored in the base Evaluation object.\n\n\nvoid updateStatsForPredictor(double predictedValue, Instance instance)\n -  updates the statistics about a predictors performance for the current test instance. Gets called when the class is numeric. Implementers need only implement this method if it is not possible to compute their statistics from what is stored in the base Evaluation object.\n\n\n\n\nweka.classifiers.evaluation.InformationTheoreticEvaluationMetric\n\n\nInterface for information theoretic evaluation metrics to implement. Allows the command line interface to display these metrics or not based on user-supplied options.\n\n\nIt defines the following methods\n\n\n\n\nString toSummaryString()\n - return a formatted string (suitable for displaying in the console or GUI output) that contains all the statistics that this metric computes\n\n\nvoid updateStatsForClassifier(double[] predictedDistribution, Instance instance)\n - updates the statistics about a classifiers performance for the current test instance. Gets called when the class is nominal. Implementers need only implement this method if it is not possible to compute their statistics from what is stored in the base Evaluation object.\n\n\nvoid updateStatsForPredictor(double predictedValue, Instance instance)\n -  updates the statistics about a predictors performance for the current test instance. Gets called when the class is numeric. Implementers need only implement this method if it is not possible to compute their statistics from what is stored in the base Evaluation object.\n\n\nvoid updateStatsForConditionalDensityEstimator(ConditionalDensityEstimator classifier, Instance classMissing, double classValue)\n -  updates stats for conditional density estimator based on current test instance. Gets called when the class is numeric and the classifier is a ConditionalDensityEstimators. Implementers need only implement this method if it is not possible to compute their statistics from what is stored in the base Evaluation object.\n\n\n\n\nweka.classifiers.evaluation.InformationRetrievalMetric\n\n\nAn interface for information retrieval evaluation metrics to implement. Allows the command line interface to display these metrics or not based on user-supplied options. These statistics will be displayed as new columns in the table of information retrieval statistics. As such, a toSummaryString() formatted representation is not required.\n\n\nIt defines the following methods\n\n\n\n\nvoid updateStatsForClassifier(double[] predictedDistribution, Instance instance)\n - updates the statistics about a classifiers performance for the current test instance. Gets called when the class is nominal. Implementers need only implement this method if it is not possible to compute their statistics from what is stored in the base Evaluation object.\n\n\ndouble getStatistic(String name, int classIndex)\n - get the value of the named statistic for the given class index. If the implementing class is extending AbstractEvaluationMetric then the implementation of getStatistic(String statName) should just call this method with a classIndex of 0.\n\n\ndouble getClassWeightedAverageStatistic(String statName)\n - get the weighted (by class) average for this statistic.\n\n\n\n\nweka.classifiers.evaluation.IntervalBasedEvaluationMetric\n\n\nPrimarily a marker interface for interval-based evaluation metrics to implement. Allows the command line interface to display these metrics or not based on user-supplied options.\n\n\nIt defines the following methods\n\n\n\n\nString toSummaryString()\n - return a formatted string (suitable for displaying in the console or GUI output) that contains all the statistics that this metric computes\n\n\nvoid updateStatsForIntervalEstimator(IntervalEstimator classifier, Instance instance, double classValue)\n - updates stats for interval estimator based on current test instance. Implementers need only implement this method if it is not possible to compute their statistics from what is stored in the base Evaluation object.",
            "title": " Pluggable evaluation metrics for classification/regression"
        },
        {
            "location": "/faqs/pluggable_evaluation_metrics/#classes-and-interfaces",
            "text": "The main base class for all new metrics is  weka.classifiers.evaluation.AbstractEvaluationMetric . This class requires the following methods to be implemented by concrete sub classes:   boolean appliesToNominalClass()  - true if the stats computed by this metric apply to nominal class problems  boolean appliesToNumericClass()  - true if the stats computed by this metric apply to numeric class problems  String getMetricName()  - return the name of the metric  String getMetricDescription()  - return a short description of the metric  List<String> getStatisticNames()  - return a list of statistics that this metric computes (e.g. a \"correct\" metric might return both the number correctly classified and the percentage correct)  double getStatistic(String statName)  - get the computed value for the named statistic   To facilitate computing statistics, the main Evaluation object (who's class now lives in weka.classifiers.evaluation) will pass a reference to itself to all plugin metrics when it is first constructed. Therefore, a plugin metric has access to all the protected fields in the Evaluation class, and can use these when computing it's own statistic(s).  Beyond extending  AbstractEvaluationMetric , a plugin metric will also need to implement one of the following interfaces:",
            "title": "Classes and interfaces"
        },
        {
            "location": "/faqs/pluggable_evaluation_metrics/#wekaclassifiersevaluationstandardevaluationmetric",
            "text": "Interface for a \"standard\" evaluation metric - i.e. one that would be part of the normal output in WEKA without having to turn on specific display options.  It defines the following methods   String toSummaryString()  - return a formatted string (suitable for displaying in the console or GUI output) that contains all the statistics that this metric computes  void updateStatsForClassifier(double[] predictedDistribution, Instance instance)  - updates the statistics about a classifiers performance for the current test instance. Gets called when the class is nominal. Implementers need only implement this method if it is not possible to compute their statistics from what is stored in the base Evaluation object.  void updateStatsForPredictor(double predictedValue, Instance instance)  -  updates the statistics about a predictors performance for the current test instance. Gets called when the class is numeric. Implementers need only implement this method if it is not possible to compute their statistics from what is stored in the base Evaluation object.",
            "title": "weka.classifiers.evaluation.StandardEvaluationMetric"
        },
        {
            "location": "/faqs/pluggable_evaluation_metrics/#wekaclassifiersevaluationinformationtheoreticevaluationmetric",
            "text": "Interface for information theoretic evaluation metrics to implement. Allows the command line interface to display these metrics or not based on user-supplied options.  It defines the following methods   String toSummaryString()  - return a formatted string (suitable for displaying in the console or GUI output) that contains all the statistics that this metric computes  void updateStatsForClassifier(double[] predictedDistribution, Instance instance)  - updates the statistics about a classifiers performance for the current test instance. Gets called when the class is nominal. Implementers need only implement this method if it is not possible to compute their statistics from what is stored in the base Evaluation object.  void updateStatsForPredictor(double predictedValue, Instance instance)  -  updates the statistics about a predictors performance for the current test instance. Gets called when the class is numeric. Implementers need only implement this method if it is not possible to compute their statistics from what is stored in the base Evaluation object.  void updateStatsForConditionalDensityEstimator(ConditionalDensityEstimator classifier, Instance classMissing, double classValue)  -  updates stats for conditional density estimator based on current test instance. Gets called when the class is numeric and the classifier is a ConditionalDensityEstimators. Implementers need only implement this method if it is not possible to compute their statistics from what is stored in the base Evaluation object.",
            "title": "weka.classifiers.evaluation.InformationTheoreticEvaluationMetric"
        },
        {
            "location": "/faqs/pluggable_evaluation_metrics/#wekaclassifiersevaluationinformationretrievalmetric",
            "text": "An interface for information retrieval evaluation metrics to implement. Allows the command line interface to display these metrics or not based on user-supplied options. These statistics will be displayed as new columns in the table of information retrieval statistics. As such, a toSummaryString() formatted representation is not required.  It defines the following methods   void updateStatsForClassifier(double[] predictedDistribution, Instance instance)  - updates the statistics about a classifiers performance for the current test instance. Gets called when the class is nominal. Implementers need only implement this method if it is not possible to compute their statistics from what is stored in the base Evaluation object.  double getStatistic(String name, int classIndex)  - get the value of the named statistic for the given class index. If the implementing class is extending AbstractEvaluationMetric then the implementation of getStatistic(String statName) should just call this method with a classIndex of 0.  double getClassWeightedAverageStatistic(String statName)  - get the weighted (by class) average for this statistic.",
            "title": "weka.classifiers.evaluation.InformationRetrievalMetric"
        },
        {
            "location": "/faqs/pluggable_evaluation_metrics/#wekaclassifiersevaluationintervalbasedevaluationmetric",
            "text": "Primarily a marker interface for interval-based evaluation metrics to implement. Allows the command line interface to display these metrics or not based on user-supplied options.  It defines the following methods   String toSummaryString()  - return a formatted string (suitable for displaying in the console or GUI output) that contains all the statistics that this metric computes  void updateStatsForIntervalEstimator(IntervalEstimator classifier, Instance instance, double classValue)  - updates stats for interval estimator based on current test instance. Implementers need only implement this method if it is not possible to compute their statistics from what is stored in the base Evaluation object.",
            "title": "weka.classifiers.evaluation.IntervalBasedEvaluationMetric"
        },
        {
            "location": "/faqs/contribution/",
            "text": "Information on how to contribute to WEKA can be found in the \nContributing a package\n section of the \nHow are packages structured for the package management system?\n article. The conditions for new classifiers (schemes in general) are that, firstly, they have to be published in the proceedings of a renowned conference (e.g., ICML) or as an article of respected journal (e.g., Machine Learning) and, secondly, that they outperform other standard schemes (e.g., J48/C4.5).\n\n\nBut please bear in mind, that we don't have a lot of man power, i.e., being the WEKA maintainer is \nNOT\n a full-time position.",
            "title": " How can I contribute to WEKA?"
        },
        {
            "location": "/faqs/how_do_i_modify_the_runwekabat_file/",
            "text": "Check out the \nInvocation\n section of the \nJava Virtual Machine\n article.",
            "title": " How do I modify the RunWeka.bat file?"
        },
        {
            "location": "/faqs/can_i_process_utf8_datasets_or_files/",
            "text": "Java can process UTF-8 files without any problems, it is just that Java uses a different encoding for displaying them under Windows (= \"Cp1252\"). If you change the file encoding to \"utf-8\" you'll be fine (discussed in \nthis\n WEKAlist post).\n\n\nIf you are running WEKA directly from the commandline, just add the following parameter to your commandline:\n\n\n\n\n-Dfile.encoding=utf-8\n\n\n\n\nIf you are starting WEKA from the Start menu, then edit the \nRunWEKA.ini\n file:\n\n\n\n\nIf a \nfileEncoding\n placeholder already exists, then just change the value from \"Cp1252\" to \"utf-8\" (without the quotes of course).\n\n\nIf there isn't a \nfileEncoding\n yet, just the \n-Dfile.encoding=utf-8\n parameter to all the \njava\n/\njavaw\n commands (see example \nRunWEKA.ini\n in \nthis\n post).",
            "title": " Can I process UTF-8 datasets or files?"
        },
        {
            "location": "/faqs/how_do_i_run_the_windows_weka_installer_in_silent_mode/",
            "text": "To run the Windows installer for WEKA without in \"silent\" mode (i.e. without any graphical prompts/dialogs):\n\n\n\n\nOpen a command prompt window\n\n\nNavigate to the directory where the installer executable resides\n\n\nType .\\weka-x-y-z.exe /S\n\n\n\n\nReplace x, y and z with the correct version numbers for your installer of course. Note that the /S is a capital S.",
            "title": " How do I run the Windows Weka installer in silent mode?"
        },
        {
            "location": "/faqs/weka_download_problems/",
            "text": "When you \ndownload WEKA\n, make sure that the resulting file size is the same as the WEKA webpage. Otherwise things won't work properly. Apparently some web browsers have trouble downloading WEKA.\nAlso note, that the WEKA homepage only links to the files that are hosted on \nsourceforge.net\n. This normally involves a redirect to a mirror from which you'll download the actual file.",
            "title": " I have Weka download problems - what is going wrong?"
        },
        {
            "location": "/faqs/arff_does_not_load/",
            "text": "One way to figure out why \nARFF\n files are failing to load is to give them to the \nweka.core.Instances\n class. In the SimpleCLI or in the terminal, type the following:\n\n\n java weka.core.Instances filename.arff\n\n\n\n\nwhere you substitute \nfilename\n for the actual name of your file. This should return an error if there is a problem reading the file, or show some statistics if the file is ok. The error message you get should give some indication of what is wrong.\n\n\nnominal value not declared in header, read Token[X], line Y\n\n\nIf you get this error message than you seem to have declared a nominal attribute in the ARFF header section, but WEKA came across a value (\n\"X\"\n) in the data (in line \nY\n) for this particular attribute that wasn't listed as possible value.\n\n\nAll\n nominal values that appear in the data must be declared in the header.",
            "title": " My ARFF file does not load - why?"
        },
        {
            "location": "/faqs/arff_does_not_load/#nominal-value-not-declared-in-header-read-tokenx-line-y",
            "text": "If you get this error message than you seem to have declared a nominal attribute in the ARFF header section, but WEKA came across a value ( \"X\" ) in the data (in line  Y ) for this particular attribute that wasn't listed as possible value.  All  nominal values that appear in the data must be declared in the header.",
            "title": "nominal value not declared in header, read Token[X], line Y"
        },
        {
            "location": "/faqs/arff_does_not_load/",
            "text": "One way to figure out why \nARFF\n files are failing to load is to give them to the \nweka.core.Instances\n class. In the SimpleCLI or in the terminal, type the following:\n\n\n java weka.core.Instances filename.arff\n\n\n\n\nwhere you substitute \nfilename\n for the actual name of your file. This should return an error if there is a problem reading the file, or show some statistics if the file is ok. The error message you get should give some indication of what is wrong.\n\n\nnominal value not declared in header, read Token[X], line Y\n\n\nIf you get this error message than you seem to have declared a nominal attribute in the ARFF header section, but WEKA came across a value (\n\"X\"\n) in the data (in line \nY\n) for this particular attribute that wasn't listed as possible value.\n\n\nAll\n nominal values that appear in the data must be declared in the header.",
            "title": " What does nominal value not declared in header, read Token[X], line Y mean?"
        },
        {
            "location": "/faqs/arff_does_not_load/#nominal-value-not-declared-in-header-read-tokenx-line-y",
            "text": "If you get this error message than you seem to have declared a nominal attribute in the ARFF header section, but WEKA came across a value ( \"X\" ) in the data (in line  Y ) for this particular attribute that wasn't listed as possible value.  All  nominal values that appear in the data must be declared in the header.",
            "title": "nominal value not declared in header, read Token[X], line Y"
        },
        {
            "location": "/faqs/OutOfMemoryException/",
            "text": "Most Java virtual machines only allocate a certain maximum amount of memory to run Java programs. Usually, this is much less than the amount of RAM in your computer. There is some information on default heap sizes in Oracle Java virtual machines for Java 8 \nhere\n. However, you can extend the memory available for the virtual machine by setting appropriate options. With Oracle's JDK, for example, you can go\n\n\njava -Xmx2g ...\n\n\n\n\nto set the maximum Java heap size to 2GB.\n\n\nA reliable way to set the maximum heap size for Oracle Java virtual machines (and overwrite any other settings that might be provided in startup scripts, etc.) is to use the _JAVA_OPTIONS environment variable to specify the \n-Xmx\n option. There is more information \nhere\n.",
            "title": " How do I get rid of this OutOfMemoryException?"
        },
        {
            "location": "/faqs/stack_overflow_error/",
            "text": "Try increasing the stack of your virtual machine. With Sun's JDK you can use this command to increase the stacksize:\n\n\n java -Xss512k ...\n\n\n\n\nto set the maximum Java stack size to 512KB. If still not sufficient, slowly increase it.\n\n\nFor Windows, see \nOutOfMemoryException\n for pointers on how to modify your setup.",
            "title": " How do I deal with a StackOverflowError?"
        },
        {
            "location": "/faqs/why_do_i_get_the_error_message_training_and_test_set_are_not_compatible/",
            "text": "One of WEKA's fundamental assumption is that the structure of the training and test sets are \nexactly\n the same. This does not only mean that you need the exact \nsame number\n of attributes, but also the exact \nsame type\n. In case of \nnominal\n attributes, you must ensure that the \nnumber\n of labels and the \norder\n of the labels are the same.\n\n\nThis may seem odd, as for \nmaking predictions\n with a trained classifier, you wouldn't need to include any class attribute information. This is true from a human perspective, but for speed reasons, WEKA doesn't perform any checks regarding the structure of dataset (no mapping of attribute names from training space to test space, also no mapping of labels). Internally, a single row in a dataset is represented as an array of doubles. In case of \nnumeric\n attributes, this doesn't pose a problem, but for other attribute types, like \nnominal\n ones, the doubles represent indexes in the list of available labels. A different order of the labels would result in different labels represented by the same index. Predictions cannot be trusted then.\n\n\nNow, if you want to quickly check where the problem is, a visual diff program is very helpful. There is a plethora of \napplications\n available. To name a few cross-platform open-source ones: \n\n\n\n\nkdiff3\n\n\nkompare\n\n\ndiffuse\n\n\n\n\nIf you used a filter for processing training and test set, then have a look at FAQ \nHow do I generate compatible train and test sets that get processed with a filter?",
            "title": " Why do I get the error message \"training and test set are not compatible\"?"
        },
        {
            "location": "/faqs/couldnt_read_from_database_unknown_data_type/",
            "text": "Since there is a plethora of different databases out there, each with their own data types, it is impossible to define all of them beforehand. WEKA therefore comes with setups for different databases that allow you to run experiments without any additional tuning. But if you want to read database from a different data source, then it can happen that you have to tell WEKA how to import these data types.\n\n\nHere is what to do:\n\n\n\n\nExtract the \nweka/experiment/DatabaseUtils.props\n file from either the \nweka.jar\n or \nweka-src.jar\n and place it in your home directory.\n\n\nFinally, check out the section \nMissing Datatypes\n in the \nDatabases\n article and add the missing data types accordingly.\n\n\n\n\nNotes:\n\n\n\n\njar\n files are just \nZIP files\n. Just use an archive manager that can handle ZIP files to open them. Windows users can use \n7-zip\n for instance.\n\n\nMore information on props files can be found in the \nProperties file\n article.\n\n\nIf you don't know where to find your \nhome directory\n, see FAQ \nWhere is my home directory located?\n.",
            "title": " Could not read from database: unknown data type"
        },
        {
            "location": "/faqs/trying_to_add_jdbc_driver_error_not_in_classpath/",
            "text": "WEKA's default setup for databases tries to locate some common \nJDBC\n driver classes (\"JDBC\" is the Java way of connecting to databases, like MySQL, HSQLDB, etc.) at startup time. By just adding these JDBC drivers to your \nCLASSPATH\n, WEKA will be automatically able to connect to these databases. If you are not trying to access a database, just forget about these messages. Otherwise, check out the \ndatabases\n article for more information (the database type that you are trying to connect to might not be listed by default).",
            "title": " Trying to add JDBC driver: ... - Error, not in CLASSPATH?"
        },
        {
            "location": "/faqs/i_cannot_process_large_datasets_any_ideas/",
            "text": "Since most schemes in WEKA need to have all of the data present in memory, large datasets can be a problem. The article \nClassifying large datasets\n tries to present some solutions. But make sure that you have already read how to deal with an \nOutOfMemoryException\n.",
            "title": " I cannot process large datasets - any ideas?"
        },
        {
            "location": "/command_redirection/",
            "text": "Console\n\n\nWith command redirection one can redirect \nstandard streams\n like \nstdin\n, \nstdout\n and \nstderr\n to user-specified locations. Quite often it is useful to redirect the output of a program to a text file.\n\n\n\n\n\n\nredirecting \nstdout\n to a file\n\n\nsomeProgram >/some/where/output.txt     (Linux/Unix Bash)\nsomeProgram >c:\\some\\where\\output.txt   (Windows command prompt)\n\n\n\n\n\n\nredirecting \nstderr\n to a file\n\n\nsomeProgram 2>/some/where/output.txt     (Linux/Unix Bash)\nsomeProgram 2>c:\\some\\where\\output.txt   (Windows command prompt)\n\n\n\n\n\n\nredirecting \nstdout\n and \nstderr\n to a file\n\n\nsomeProgram &>/some/where/output.txt         (Linux/Unix Bash)\nsomeProgram >c:\\some\\where\\output.txt 2>&1   (Windows command prompt)\n\n\n\n\n\n\nNote:\n under Weka quite often the output is printed to \nstderr\n, e.g., if one is using the \n-p 0\n option from the commandline to print the predicted values for a test file:\n\n\n java weka.classifiers.trees.J48 -t train.arff -T test.arff -p 0 2> j48.txt\n\n\n\n\nor if one already has a trained model:\n\n\n java weka.classifiers.trees.J48 -l j48.model -T test.arff -p 0 2> j48.txt\n\n\n\n\nSimpleCLI\n\n\nOne can perform a basic redirection also in the SimpleCLI, e.g.:\n\n\n java weka.classifiers.trees.J48 -t test.arff > j48.txt\n\n\n\n\nNote:\n the \n>\n must be preceded and followed by a \nspace\n, otherwise it is not recognized as redirection, but part of another parameter.\n\n\nLinks\n\n\n\n\n\n\nLinux\n\n\n\n\nCommand redirection under Bash\n\n\nI/O Redirection under Bash\n\n\nRedirection under Unix (WikiPedia)\n\n\n\n\n\n\n\n\nWindows\n\n\n\n\nCommand redirection under MS Windows\n\n\nCommand redirection under MS DOS",
            "title": " Command redirection"
        },
        {
            "location": "/command_redirection/#console",
            "text": "With command redirection one can redirect  standard streams  like  stdin ,  stdout  and  stderr  to user-specified locations. Quite often it is useful to redirect the output of a program to a text file.    redirecting  stdout  to a file  someProgram >/some/where/output.txt     (Linux/Unix Bash)\nsomeProgram >c:\\some\\where\\output.txt   (Windows command prompt)    redirecting  stderr  to a file  someProgram 2>/some/where/output.txt     (Linux/Unix Bash)\nsomeProgram 2>c:\\some\\where\\output.txt   (Windows command prompt)    redirecting  stdout  and  stderr  to a file  someProgram &>/some/where/output.txt         (Linux/Unix Bash)\nsomeProgram >c:\\some\\where\\output.txt 2>&1   (Windows command prompt)    Note:  under Weka quite often the output is printed to  stderr , e.g., if one is using the  -p 0  option from the commandline to print the predicted values for a test file:   java weka.classifiers.trees.J48 -t train.arff -T test.arff -p 0 2> j48.txt  or if one already has a trained model:   java weka.classifiers.trees.J48 -l j48.model -T test.arff -p 0 2> j48.txt",
            "title": "Console"
        },
        {
            "location": "/command_redirection/#simplecli",
            "text": "One can perform a basic redirection also in the SimpleCLI, e.g.:   java weka.classifiers.trees.J48 -t test.arff > j48.txt  Note:  the  >  must be preceded and followed by a  space , otherwise it is not recognized as redirection, but part of another parameter.",
            "title": "SimpleCLI"
        },
        {
            "location": "/command_redirection/#links",
            "text": "Linux   Command redirection under Bash  I/O Redirection under Bash  Redirection under Unix (WikiPedia)     Windows   Command redirection under MS Windows  Command redirection under MS DOS",
            "title": "Links"
        },
        {
            "location": "/auc/",
            "text": "AUC = the \nA\nrea \nU\nnder the ROC \nC\nurve. \n\n\nWeka uses the \nMann Whitney statistic\n to calculate the AUC via the \nweka.classifiers.evaluation.ThresholdCurve\n class.\n\n\nExplorer\n\n\nSee [[ROC curves]].\n\n\nKnowledgeFlow\n\n\nSee [[ROC curves]].\n\n\nCommandline\n\n\nClassifiers can output the AUC if the \n-i\n option is provided. The \n-i\n option provides detailed information per class. \n\n\nRunning the J48 classifier on the iris UCI [[Datasets|dataset]] with the following commandline:\n\n\n java [CLASSPATH|-classpath <your-classpath>] weka.classifiers.trees.J48 -t /some/where/iris.arff -i\n\n\n\n\nproduces this output:\n\n\n == Detailed Accuracy By Class ==\n\n TP Rate   FP Rate   Precision   Recall  F-Measure   ROC Area  Class\n   0.98      0          1         0.98      0.99       0.99     Iris-setosa\n   0.94      0.03       0.94      0.94      0.94       0.952    Iris-versicolor\n   0.96      0.03       0.941     0.96      0.95       0.961    Iris-virginica\n\n\n\n\nSee also\n\n\n\n\n[[ROC curves]]\n\n\nMann Whitney statistic\n on WikiPedia\n\n\n\n\nLinks\n\n\n\n\nUniversity of Nebraska Medical Center, Interpreting Diagnostic Tests\n\n\nweka.classifiers.evaluation.ThresholdCurve",
            "title": " Area under the curve"
        },
        {
            "location": "/auc/#explorer",
            "text": "See [[ROC curves]].",
            "title": "Explorer"
        },
        {
            "location": "/auc/#knowledgeflow",
            "text": "See [[ROC curves]].",
            "title": "KnowledgeFlow"
        },
        {
            "location": "/auc/#commandline",
            "text": "Classifiers can output the AUC if the  -i  option is provided. The  -i  option provides detailed information per class.   Running the J48 classifier on the iris UCI [[Datasets|dataset]] with the following commandline:   java [CLASSPATH|-classpath <your-classpath>] weka.classifiers.trees.J48 -t /some/where/iris.arff -i  produces this output:   == Detailed Accuracy By Class ==\n\n TP Rate   FP Rate   Precision   Recall  F-Measure   ROC Area  Class\n   0.98      0          1         0.98      0.99       0.99     Iris-setosa\n   0.94      0.03       0.94      0.94      0.94       0.952    Iris-versicolor\n   0.96      0.03       0.941     0.96      0.95       0.961    Iris-virginica",
            "title": "Commandline"
        },
        {
            "location": "/auc/#see-also",
            "text": "[[ROC curves]]  Mann Whitney statistic  on WikiPedia",
            "title": "See also"
        },
        {
            "location": "/auc/#links",
            "text": "University of Nebraska Medical Center, Interpreting Diagnostic Tests  weka.classifiers.evaluation.ThresholdCurve",
            "title": "Links"
        },
        {
            "location": "/batch_filtering/",
            "text": "Batch filtering\n is used if a second dataset, normally the test set, needs to be processed with the same statistics as the the first dataset, normally the training set.\n\n\nFor example, performing standardization with the \nStandardize\n filter on two datasets separately will most certainly create two differently standardized output files, since the mean and the standard deviation are based on the input data (and those will differ if the datasets are different). The same applies to the \nStringToWordVector\n: here the word dictionary will change, since word occurrences will differ in training and test set. The generated output will be two incompatible files.\n\n\nIn order to create compatible train and test set, batch filtering is necessary. Here, the first input/output pair (\n-i\n/\n-o\n) initializes the filter's statistics and the second input/output pair (\n-r\n/\n-s\n) gets processed according to those statistics. To enable batch filtering, one has to provide the additional parameter \n-b\n on the commandline.\n\n\nHere is an example Java call:\n\n\n java weka.filters.unsupervised.attribute.Standardize \\\n   -b \\\n   -i train.arff \\\n   -o train_std.arff \\\n   -r test.arff \\\n   -s test_std.arff\n\n\n\n\nNote:\n The commandline outlined above is for a Linux/Unix bash (the backslash tells the shell that the command isn't finished yet and continues on the next line). In case of Windows or the SimpleCLI, just remove those backslashes and put everything on one line.\n\n\nSee also\n\n\n\n\nSee section \nBatch filtering\n in the article \nUse Weka in your Java code\n, in case you need to perform batch filtering from within your own code",
            "title": " Batch filtering"
        },
        {
            "location": "/batch_filtering/#see-also",
            "text": "See section  Batch filtering  in the article  Use Weka in your Java code , in case you need to perform batch filtering from within your own code",
            "title": "See also"
        },
        {
            "location": "/roc_curves/",
            "text": "General\n\n\nWeka just varies the threshold on the class probability estimates in each case. What does that mean? In case of a classifier that does not return proper class probabilities (like SMO with the -M option, or IB1), you will end up with only two points in the curve. Using a classifier that returns proper distributions, like BayesNet, J48 or SMO with -M option for building logistic models, you will get nice curves.\n\n\nThe class used for calculating the ROC and also the \nAUC\n (= area under the curve) is \nweka.classifiers.evaluation.ThresholdCurve\n.\n\n\nCommandline\n\n\nYou can output the data for the ROC curves with the following options:\n\n\n -threshold-file <file>\n        The file to save the threshold data to.\n        The format is determined by the extensions, e.g., '.arff' for ARFF\n        format or '.csv' for CSV.\n -threshold-label <label>\n        The class label to determine the threshold data for\n        (default is the first label)\n\n\n\n\nHere's an example for using J48 on the UCI dataset \nanneal\n, generating the ROC curve file for label \nU\n from a 10-fold cross-validation:\n\n\n java weka.classifiers.trees.J48 -t /some/where/anneal.arff \\\n      -threshold-file anneal_roc_U.arff -threshold-label U\n\n\n\n\nExplorer\n\n\nGenerating\n\n\nThe Weka Explorer enables you to plot the ROC (\nReceiver operating characteristic\n) curve for a certain class label of dataset:\n\n\n\n\nrun a classifier on a dataset\n\n\nright-click in the result list on the result you want to display the curve for\n\n\nselect \nVisualize threshold curve\n and choose the class label you want the plot for\n\n\n\n\nNote:\n the \nAUC\n for this plot is also displayed, just above the actual plot.\n\n\nSaving\n\n\nYou can save the ROC curve in two ways:\n\n\n\n\nas an \nARFF\n file, containing the data points (can be displayed again)\n\n\nas an \nimage\n (using \nAlt+Shift+Left click\n to bring up a save dialog)\n\n\n\n\nLoading\n\n\nA previously saved ROC data file can be displayed in two ways:\n\n\n\n\n\n\nwithout the \nAUC\n - with the following command\n\n\njava [CLASSPATH|-classpath <your-classpath>] weka.gui.visualize.VisualizePanel <file>\n\n\n\n\n\n\nwith the \nAUC\n - needs \nthis\n source code\n\n\n\n\n\n\nKnowledgeFlow\n\n\nSee \nPlotting multiple ROC curves\n.\n\n\nSee also\n\n\n\n\nPlotting multiple ROC curves\n\n\nGenerating ROC curve\n\n\n\n\nLinks\n\n\n\n\nWikiPedia article on ROC curve\n\n\nweka.classifiers.evaluation.ThresholdCurve",
            "title": " ROC Curves"
        },
        {
            "location": "/roc_curves/#general",
            "text": "Weka just varies the threshold on the class probability estimates in each case. What does that mean? In case of a classifier that does not return proper class probabilities (like SMO with the -M option, or IB1), you will end up with only two points in the curve. Using a classifier that returns proper distributions, like BayesNet, J48 or SMO with -M option for building logistic models, you will get nice curves.  The class used for calculating the ROC and also the  AUC  (= area under the curve) is  weka.classifiers.evaluation.ThresholdCurve .",
            "title": "General"
        },
        {
            "location": "/roc_curves/#commandline",
            "text": "You can output the data for the ROC curves with the following options:   -threshold-file <file>\n        The file to save the threshold data to.\n        The format is determined by the extensions, e.g., '.arff' for ARFF\n        format or '.csv' for CSV.\n -threshold-label <label>\n        The class label to determine the threshold data for\n        (default is the first label)  Here's an example for using J48 on the UCI dataset  anneal , generating the ROC curve file for label  U  from a 10-fold cross-validation:   java weka.classifiers.trees.J48 -t /some/where/anneal.arff \\\n      -threshold-file anneal_roc_U.arff -threshold-label U",
            "title": "Commandline"
        },
        {
            "location": "/roc_curves/#explorer",
            "text": "",
            "title": "Explorer"
        },
        {
            "location": "/roc_curves/#generating",
            "text": "The Weka Explorer enables you to plot the ROC ( Receiver operating characteristic ) curve for a certain class label of dataset:   run a classifier on a dataset  right-click in the result list on the result you want to display the curve for  select  Visualize threshold curve  and choose the class label you want the plot for   Note:  the  AUC  for this plot is also displayed, just above the actual plot.",
            "title": "Generating"
        },
        {
            "location": "/roc_curves/#saving",
            "text": "You can save the ROC curve in two ways:   as an  ARFF  file, containing the data points (can be displayed again)  as an  image  (using  Alt+Shift+Left click  to bring up a save dialog)",
            "title": "Saving"
        },
        {
            "location": "/roc_curves/#loading",
            "text": "A previously saved ROC data file can be displayed in two ways:    without the  AUC  - with the following command  java [CLASSPATH|-classpath <your-classpath>] weka.gui.visualize.VisualizePanel <file>    with the  AUC  - needs  this  source code",
            "title": "Loading"
        },
        {
            "location": "/roc_curves/#knowledgeflow",
            "text": "See  Plotting multiple ROC curves .",
            "title": "KnowledgeFlow"
        },
        {
            "location": "/roc_curves/#see-also",
            "text": "Plotting multiple ROC curves  Generating ROC curve",
            "title": "See also"
        },
        {
            "location": "/roc_curves/#links",
            "text": "WikiPedia article on ROC curve  weka.classifiers.evaluation.ThresholdCurve",
            "title": "Links"
        },
        {
            "location": "/plotting_multiple_roc_curves/",
            "text": "KnowledgeFlow\n\n\nDescription\n\n\nComparing different classifiers on one dataset can also be done via \nROC curves\n, not just via Accuracy, Correlation coefficient etc. In the \nExplorer\n it is \nnot\n possible to do that for several classifiers, this is only possible in the \nKnowledgeFlow\n.\n\n\nThis is the basic setup (based on a Wekalist post):\n\n\n ArffLoader \n ---dataSet---> ClassAssigner \n ---dataSet---> ClassValuePicker              (the class label you want the plot for)\n ---dataSet---> CrossValidationFoldMaker \n ---trainingSet/testSet (i.e. BOTH connections)---> Classifier of your choice \n ---batchClassifier---> ClassifierPerformanceEvaluator \n ---thresholdData---> ModelPerformanceChart\n\n\n\n\n\n\n\n\nThis setup can be easily extended to host several classifiers, which illustrates the \nPlotting_multiple_roc.kfml\n example, containing \nJ48\n and \nRandomForest\n as classifiers.\n\n\nJava\n\n\nDescription\n\n\nThe \nVisualizeMultipleROC.java\n class lets you display several ROC curves in a single plot. The data it is using for display is from previously saved ROC curves. This example class is just a modified version of the \nVisualizeROC.java\n class, which displays only a single ROC curve (see \nVisualizing ROC curve\n article).\n\n\nSee also\n\n\n\n\nWikipedia article on ROC curve\n\n\nVisualizing ROC curve\n\n\nROC curves\n\n\n\n\nDownloads\n\n\n\n\nPlotting_multiple_roc.kfml\n - Example KnowledgeFlow layout file\n\n\nVisualizeMultipleROC.java\n (\nstable\n, \ndeveloper\n)",
            "title": " Plotting multiple ROC curves"
        },
        {
            "location": "/plotting_multiple_roc_curves/#knowledgeflow",
            "text": "",
            "title": "KnowledgeFlow"
        },
        {
            "location": "/plotting_multiple_roc_curves/#description",
            "text": "Comparing different classifiers on one dataset can also be done via  ROC curves , not just via Accuracy, Correlation coefficient etc. In the  Explorer  it is  not  possible to do that for several classifiers, this is only possible in the  KnowledgeFlow .  This is the basic setup (based on a Wekalist post):   ArffLoader \n ---dataSet---> ClassAssigner \n ---dataSet---> ClassValuePicker              (the class label you want the plot for)\n ---dataSet---> CrossValidationFoldMaker \n ---trainingSet/testSet (i.e. BOTH connections)---> Classifier of your choice \n ---batchClassifier---> ClassifierPerformanceEvaluator \n ---thresholdData---> ModelPerformanceChart    This setup can be easily extended to host several classifiers, which illustrates the  Plotting_multiple_roc.kfml  example, containing  J48  and  RandomForest  as classifiers.",
            "title": "Description"
        },
        {
            "location": "/plotting_multiple_roc_curves/#java",
            "text": "",
            "title": "Java"
        },
        {
            "location": "/plotting_multiple_roc_curves/#description_1",
            "text": "The  VisualizeMultipleROC.java  class lets you display several ROC curves in a single plot. The data it is using for display is from previously saved ROC curves. This example class is just a modified version of the  VisualizeROC.java  class, which displays only a single ROC curve (see  Visualizing ROC curve  article).",
            "title": "Description"
        },
        {
            "location": "/plotting_multiple_roc_curves/#see-also",
            "text": "Wikipedia article on ROC curve  Visualizing ROC curve  ROC curves",
            "title": "See also"
        },
        {
            "location": "/plotting_multiple_roc_curves/#downloads",
            "text": "Plotting_multiple_roc.kfml  - Example KnowledgeFlow layout file  VisualizeMultipleROC.java  ( stable ,  developer )",
            "title": "Downloads"
        },
        {
            "location": "/databases/",
            "text": "CLASSPATH\n\n\nSee the \nCLASSPATH\n article for how to set up your CLASSPATH environment variable, in order to make the JDBC driver available for Weka.\n\n\nConfiguration files\n\n\nThanks to JDBC it is easy to connect to Databases that provide a JDBC driver. Responsible for the setup is the following properties file, located in the \nweka.experiment\n package:\n\n\n DatabaseUtils.props\n\n\n\n\nYou can get this properties file from the \nweka.jar\n or \nweka-src.jar\n jar-archive, both part of a normal Weka release. If you open up one of those files, you'll find the properties file in the sub-folder \nweka/experiment\n.\n\n\nWeka comes with example files for a wide range of databases:\n\n\n\n\nDatabaseUtils.props.hsql\n - HSQLDB\n\n\nDatabaseUtils.props.msaccess\n - MS Access (see the \nWindows Databases\n article for more information)\n\n\nDatabaseUtils.props.mssqlserver\n - MS SQL Server 2000\n\n\nDatabaseUtils.props.mssqlserver2005\n - MS SQL Server 2005\n\n\nDatabaseUtils.props.mysql\n - MySQL\n\n\nDatabaseUtils.props.odbc\n - ODBC access via Sun's ODBC/JDBC bridge, e.g., for MS Sql Server (see the \nWindows Databases\n article for more information)\n\n\nDatabaseUtils.props.oracle\n - Oracle 10g\n\n\nDatabaseUtils.props.postgresql\n - PostgreSQL 7.4\n\n\nDatabaseUtils.props.sqlite3\n - sqlite 3.x\n\n\n\n\nThe easiest way is just to place the extracted properties file into your HOME directory. For more information on how property files are processed, check out [[Properties File|this]] article.\n\n\nNote:\n Weka \nonly\n looks for the \nDatabaseUtils.props\n file. If you take one of the example files listed above, you need to rename it first.\n\n\nSetup\n\n\nUnder normal circumstances you only have to edit the following two properties:\n\n\n\n\njdbcDriver\n\n\njdbcURL\n\n\n\n\nDriver\n\n\njdbcDriver\n is the classname of the JDBC driver, necessary to connect to your database, e.g.:\n\n\n\n\nHSQLDB - \norg.hsqldb.jdbcDriver\n\n\nMS SQL Server 2000 (Desktop Edition) - \ncom.microsoft.jdbc.sqlserver.SQLServerDriver\n\n\nMS SQL Server 2005 - \ncom.microsoft.sqlserver.jdbc.SQLServerDriver\n\n\nMySQL - \norg.gjt.mm.mysql.Driver\n (or \ncom.mysql.jdbc.Driver\n)\n\n\nODBC - part of Sun's JDKs/JREs, no external driver necessary - \nsun.jdbc.odbc.JdbcOdbcDriver\n\n\nOracle - \noracle.jdbc.driver.OracleDriver\n\n\nPostgreSQL - \norg.postgresql.Driver\n\n\nsqlite 3.x - \norg.sqlite.JDBC\n\n\n\n\nURL\n\n\njdbcURL\n specifies the JDBC URL pointing to your database (can be still changed in the Experimenter/Explorer), e.g. for the database \nMyDatabase\n on the server \nserver.my.domain\n:\n\n\n\n\nHSQLDB - \njdbc:hsqldb:hsql://server.my.domain/MyDatabase\n\n\n\n\nMS SQL Server 2000 (Desktop Edition) - \njdbc:microsoft:sqlserver://server.my.comain:1433\n\n\n\n\nNote: if you add \n;databasename=*db-name*\n you can connect to a different database than the default one, e.g., \nMyDatabase\n\n\n\n\n\n\n\n\nMS SQL Server 2005 - \njdbc:sqlserver://server.my.domain:1433\n\n\n\n\nMySQL - \njdbc:mysql://server.my.domain:3306/MyDatabase\n\n\nODBC - \njdbc:odbc:DSN_name (replace DSN_name with the DSN that you want to use)\n\n\n\n\nOracle (thin driver) - \njdbc:oracle:thin:@server.my.domain:1526:orcl\n\n\n\n\nNote: \n@machineName:port:SID\n\n\nfor the \nExpress Edition\n you can use: \njdbc:oracle:thin:@server.my.domain:1521:XE\n\n\n\n\n\n\n\n\nPostgreSQL - \njdbc:postgresql://server.my.domain:5432/MyDatabase\n\n\n\n\nYou can also specify user and password directly in the URL: \njdbc:postgresql://server.my.domain:5432/MyDatabase?user=<...>&password=<...>\n\n\nwhere you have to replace the \n<...>\n with the correct values\n\n\n\n\n\n\n\n\nsqlite 3.x - \njdbc:sqlite:/path/to/database.db (you can access only local files)\n\n\n\n\n\n\nMissing Datatypes\n\n\nSometimes (e.g. with MySQL) it can happen that a column type cannot be interpreted. In that case it is necessary to map the name of the column type to the Java type it should be interpreted as. E.g. the MySQL type \nTEXT\n is returned as \nBLOB\n from the JDBC driver and has to be mapped to \nString\n (\n0\n represents \nString\n - the mappings can be found in the comments of the properties file):\n\n\n BLOB=0\n\n\n\n\nThe article [[weka_experiment_DatabaseUtils.props|weka/experiment/DatabaseUtils.props]] contains more details on this topic.\n\n\nStored Procedures\n\n\nLet's say you're tired of typing the same query over and over again. A good way to shorten that, is to create a stored procedure.\n\n\nPostgreSQL 7.4.x\n\n\nThe following example creates a procedure called \nemplyoee_name\n that returns the names of all the employees in table \nemployee\n. Even though it doesn't make much sense to create a stored procedure for this query, nonetheless, it shows how to create and call stored procedures in PostgreSQL.\n\n\n\n\n\n\nCreate\n\n\nCREATE OR REPLACE FUNCTION public.employee_name()\n  RETURNS SETOF text AS 'select name from employee'\n  LANGUAGE 'sql' VOLATILE;\n\n\n\n\n\n\nSQL statement to call procedure\n\n\nSELECT * FROM employee_name()\n\n\n\n\n\n\nRetrieve data via \nInstanceQuery\n\n\njava weka.experiment.InstanceQuery -Q \"SELECT * FROM employee_name()\" -U <user> -P <password>\n\n\n\n\n\n\nTroubleshooting\n\n\n\n\nIn case you're experiencing problems connecting to your database, check out the \nmailing list\n. It is possible that somebody else encountered the same problem as you and you'll find a post containing the solution to your problem.\n\n\nSpecific [[MS SQL Server 2000 (Desktop Engine)#Troubleshooting|MS SQL Server 2000]] Troubleshooting\n\n\n\n\nMS SQL Server 2005: TCP/IP is not enabled for SQL Server, or the server or port number specified is incorrect.Verify that SQL Server is listening with TCP/IP on the specified server and port. This might be reported with an exception similar to: \"The login has failed. The TCP/IP connection to the host has failed.\" This indicates one of the following:\n\n\n\n\nSQL Server is installed but TCP/IP has not been installed as a network protocol for SQL Server by using the SQL Server Network Utility for SQL Server 2000, or the SQL Server Configuration Manager for SQL Server 2005\n\n\nTCP/IP is installed as a SQL Server protocol, but it is not listening on the port specified in the JDBC connection URL. The default port is 1433.\n\n\nThe port that is used by the server has not been opened in the firewall\n\n\n\n\n\n\n\n\nThe \nAdded driver: ...\n output on the commandline does not mean that the actual class was found, but only that Weka will \nattempt\n to load the class later on in order to establish a database connection.\n\n\n\n\n\n\nThe error message \nNo suitable driver\n can be caused by the following:\n\n\n\n\nThe JDBC driver you are attempting to load is not in the [[CLASSPATH]] (Note: using \n-jar\n in the java commandline \noverwrites\n the CLASSPATH environment variable!). Open the SimpleCLI, run the command \njava weka.core.SystemInfo\n and check whether the property \njava.class.path\n lists your database jar. If not correct your [[CLASSPATH]] or the Java call you start Weka with.\n\n\nThe JDBC driver class is misspelled in the \njdbcDriver\n property or you have multiple entries of \njdbcDriver\n ([[properties file]]s need \nunique\n keys!)\n\n\nThe \njdbcURL\n property has a spelling error and tries to use a non-existing protocol or you listed it multiple times, which doesn't work either (remember, [[properties file]]s need \nunique\n keys!)\n\n\n\n\n\n\n\n\nSee also\n\n\n\n\n[[weka_experiment_DatabaseUtils.props|weka/experiment/DatabaseUtils.props]]\n\n\n[[Properties File]]\n\n\nCLASSPATH\n\n\n\n\nLinks\n\n\n\n\n\n\nHSQLDB\n\n\n\n\nhomepage\n\n\n\n\n\n\n\n\nIBM Cloudscape\n\n\n\n\nhomepage\n\n\n\n\n\n\n\n\nMicrosoft SQL Server\n\n\n\n\nSQL Server 2000 (Desktop Engine)\n\n\nSQL Server 2000 JDBC Driver SP 3\n\n\nSQL Server 2005 JDBC Driver\n\n\n\n\n\n\n\n\nMySQL\n\n\n\n\nhomepage\n\n\nJDBC driver\n\n\n\n\n\n\n\n\nOracle \n\n\n\n\nhomepage\n\n\nJDBC driver\n\n\nJDBC FAQ\n\n\n\n\n\n\n\n\nPostgreSQL\n\n\n\n\nhomepage\n\n\nJDBC driver\n\n\n\n\n\n\n\n\nsqlite\n\n\n\n\nhomepage\n\n\nJDBC driver\n\n\n\n\n\n\n\n\nWeka Mailing list",
            "title": " Databases"
        },
        {
            "location": "/databases/#classpath",
            "text": "See the  CLASSPATH  article for how to set up your CLASSPATH environment variable, in order to make the JDBC driver available for Weka.",
            "title": "CLASSPATH"
        },
        {
            "location": "/databases/#configuration-files",
            "text": "Thanks to JDBC it is easy to connect to Databases that provide a JDBC driver. Responsible for the setup is the following properties file, located in the  weka.experiment  package:   DatabaseUtils.props  You can get this properties file from the  weka.jar  or  weka-src.jar  jar-archive, both part of a normal Weka release. If you open up one of those files, you'll find the properties file in the sub-folder  weka/experiment .  Weka comes with example files for a wide range of databases:   DatabaseUtils.props.hsql  - HSQLDB  DatabaseUtils.props.msaccess  - MS Access (see the  Windows Databases  article for more information)  DatabaseUtils.props.mssqlserver  - MS SQL Server 2000  DatabaseUtils.props.mssqlserver2005  - MS SQL Server 2005  DatabaseUtils.props.mysql  - MySQL  DatabaseUtils.props.odbc  - ODBC access via Sun's ODBC/JDBC bridge, e.g., for MS Sql Server (see the  Windows Databases  article for more information)  DatabaseUtils.props.oracle  - Oracle 10g  DatabaseUtils.props.postgresql  - PostgreSQL 7.4  DatabaseUtils.props.sqlite3  - sqlite 3.x   The easiest way is just to place the extracted properties file into your HOME directory. For more information on how property files are processed, check out [[Properties File|this]] article.  Note:  Weka  only  looks for the  DatabaseUtils.props  file. If you take one of the example files listed above, you need to rename it first.",
            "title": "Configuration files"
        },
        {
            "location": "/databases/#setup",
            "text": "Under normal circumstances you only have to edit the following two properties:   jdbcDriver  jdbcURL",
            "title": "Setup"
        },
        {
            "location": "/databases/#driver",
            "text": "jdbcDriver  is the classname of the JDBC driver, necessary to connect to your database, e.g.:   HSQLDB -  org.hsqldb.jdbcDriver  MS SQL Server 2000 (Desktop Edition) -  com.microsoft.jdbc.sqlserver.SQLServerDriver  MS SQL Server 2005 -  com.microsoft.sqlserver.jdbc.SQLServerDriver  MySQL -  org.gjt.mm.mysql.Driver  (or  com.mysql.jdbc.Driver )  ODBC - part of Sun's JDKs/JREs, no external driver necessary -  sun.jdbc.odbc.JdbcOdbcDriver  Oracle -  oracle.jdbc.driver.OracleDriver  PostgreSQL -  org.postgresql.Driver  sqlite 3.x -  org.sqlite.JDBC",
            "title": "Driver"
        },
        {
            "location": "/databases/#url",
            "text": "jdbcURL  specifies the JDBC URL pointing to your database (can be still changed in the Experimenter/Explorer), e.g. for the database  MyDatabase  on the server  server.my.domain :   HSQLDB -  jdbc:hsqldb:hsql://server.my.domain/MyDatabase   MS SQL Server 2000 (Desktop Edition) -  jdbc:microsoft:sqlserver://server.my.comain:1433   Note: if you add  ;databasename=*db-name*  you can connect to a different database than the default one, e.g.,  MyDatabase     MS SQL Server 2005 -  jdbc:sqlserver://server.my.domain:1433   MySQL -  jdbc:mysql://server.my.domain:3306/MyDatabase  ODBC -  jdbc:odbc:DSN_name (replace DSN_name with the DSN that you want to use)   Oracle (thin driver) -  jdbc:oracle:thin:@server.my.domain:1526:orcl   Note:  @machineName:port:SID  for the  Express Edition  you can use:  jdbc:oracle:thin:@server.my.domain:1521:XE     PostgreSQL -  jdbc:postgresql://server.my.domain:5432/MyDatabase   You can also specify user and password directly in the URL:  jdbc:postgresql://server.my.domain:5432/MyDatabase?user=<...>&password=<...>  where you have to replace the  <...>  with the correct values     sqlite 3.x -  jdbc:sqlite:/path/to/database.db (you can access only local files)",
            "title": "URL"
        },
        {
            "location": "/databases/#missing-datatypes",
            "text": "Sometimes (e.g. with MySQL) it can happen that a column type cannot be interpreted. In that case it is necessary to map the name of the column type to the Java type it should be interpreted as. E.g. the MySQL type  TEXT  is returned as  BLOB  from the JDBC driver and has to be mapped to  String  ( 0  represents  String  - the mappings can be found in the comments of the properties file):   BLOB=0  The article [[weka_experiment_DatabaseUtils.props|weka/experiment/DatabaseUtils.props]] contains more details on this topic.",
            "title": "Missing Datatypes"
        },
        {
            "location": "/databases/#stored-procedures",
            "text": "Let's say you're tired of typing the same query over and over again. A good way to shorten that, is to create a stored procedure.",
            "title": "Stored Procedures"
        },
        {
            "location": "/databases/#postgresql-74x",
            "text": "The following example creates a procedure called  emplyoee_name  that returns the names of all the employees in table  employee . Even though it doesn't make much sense to create a stored procedure for this query, nonetheless, it shows how to create and call stored procedures in PostgreSQL.    Create  CREATE OR REPLACE FUNCTION public.employee_name()\n  RETURNS SETOF text AS 'select name from employee'\n  LANGUAGE 'sql' VOLATILE;    SQL statement to call procedure  SELECT * FROM employee_name()    Retrieve data via  InstanceQuery  java weka.experiment.InstanceQuery -Q \"SELECT * FROM employee_name()\" -U <user> -P <password>",
            "title": "PostgreSQL 7.4.x"
        },
        {
            "location": "/databases/#troubleshooting",
            "text": "In case you're experiencing problems connecting to your database, check out the  mailing list . It is possible that somebody else encountered the same problem as you and you'll find a post containing the solution to your problem.  Specific [[MS SQL Server 2000 (Desktop Engine)#Troubleshooting|MS SQL Server 2000]] Troubleshooting   MS SQL Server 2005: TCP/IP is not enabled for SQL Server, or the server or port number specified is incorrect.Verify that SQL Server is listening with TCP/IP on the specified server and port. This might be reported with an exception similar to: \"The login has failed. The TCP/IP connection to the host has failed.\" This indicates one of the following:   SQL Server is installed but TCP/IP has not been installed as a network protocol for SQL Server by using the SQL Server Network Utility for SQL Server 2000, or the SQL Server Configuration Manager for SQL Server 2005  TCP/IP is installed as a SQL Server protocol, but it is not listening on the port specified in the JDBC connection URL. The default port is 1433.  The port that is used by the server has not been opened in the firewall     The  Added driver: ...  output on the commandline does not mean that the actual class was found, but only that Weka will  attempt  to load the class later on in order to establish a database connection.    The error message  No suitable driver  can be caused by the following:   The JDBC driver you are attempting to load is not in the [[CLASSPATH]] (Note: using  -jar  in the java commandline  overwrites  the CLASSPATH environment variable!). Open the SimpleCLI, run the command  java weka.core.SystemInfo  and check whether the property  java.class.path  lists your database jar. If not correct your [[CLASSPATH]] or the Java call you start Weka with.  The JDBC driver class is misspelled in the  jdbcDriver  property or you have multiple entries of  jdbcDriver  ([[properties file]]s need  unique  keys!)  The  jdbcURL  property has a spelling error and tries to use a non-existing protocol or you listed it multiple times, which doesn't work either (remember, [[properties file]]s need  unique  keys!)",
            "title": "Troubleshooting"
        },
        {
            "location": "/databases/#see-also",
            "text": "[[weka_experiment_DatabaseUtils.props|weka/experiment/DatabaseUtils.props]]  [[Properties File]]  CLASSPATH",
            "title": "See also"
        },
        {
            "location": "/databases/#links",
            "text": "HSQLDB   homepage     IBM Cloudscape   homepage     Microsoft SQL Server   SQL Server 2000 (Desktop Engine)  SQL Server 2000 JDBC Driver SP 3  SQL Server 2005 JDBC Driver     MySQL   homepage  JDBC driver     Oracle    homepage  JDBC driver  JDBC FAQ     PostgreSQL   homepage  JDBC driver     sqlite   homepage  JDBC driver     Weka Mailing list",
            "title": "Links"
        },
        {
            "location": "/converting_csv_to_arff/",
            "text": "For converting CSV (comma separated value) files into \nARFF\n files you need the following two \nconverters\n:\n\n\n\n\nCSVLoader\n for loading the CSV file into an \nInstances\n object\n\n\nArffSaver\n to save the \nInstances\n as an ARFF file\n\n\n\n\nIn the following you'll find some example code to show you how to use the \nconverters\n. The class takes 2 arguments:\n\n\n\n\nthe \ninput\n CSV file\n\n\nthe \noutput\n \nARFF\n file\n\n\n\n\nExample code:\n\n\nimport weka.core.Instances;\nimport weka.core.converters.ArffSaver;\nimport weka.core.converters.CSVLoader;\n\nimport java.io.File;\n\npublic class CSV2Arff {\n  /**\n   * takes 2 arguments:\n   * - CSV input file\n   * - ARFF output file\n   */\n  public static void main(String[] args) throws Exception {\n    if (args.length != 2) {\n      System.out.println(\"\\nUsage: CSV2Arff <input.csv> <output.arff>\\n\");\n      System.exit(1);\n    }\n\n    // load CSV\n    CSVLoader loader = new CSVLoader();\n    loader.setSource(new File(args[0]));\n    Instances data = loader.getDataSet();\n\n    // save ARFF\n    ArffSaver saver = new ArffSaver();\n    saver.setInstances(data);\n    saver.setFile(new File(args[1]));\n    saver.setDestination(new File(args[1]));\n    saver.writeBatch();\n  }\n}\n\n\n\n\nNote:\n with versions of Weka later than 3.5.3 the call of \nsaver.setDestination(new File(args[1]));\n is no longer necessary, it is automatically done in the \nsaver.setFile(new File(args[1]));\n method.\n\n\nSee also\n\n\nThe \nWeka Examples\n collection dedicates several example classes of loading from and saving to various file formats:\n\n\n\n\nbook\n\n\nstable-3-6\n\n\ndeveloper",
            "title": " Converting CSV to ARFF"
        },
        {
            "location": "/converting_csv_to_arff/#see-also",
            "text": "The  Weka Examples  collection dedicates several example classes of loading from and saving to various file formats:   book  stable-3-6  developer",
            "title": "See also"
        },
        {
            "location": "/use_weka_in_your_java_code/",
            "text": "The most common components you might want to use are\n\n\n\n\nInstances\n - your data\n\n\nFilter\n - for preprocessing the data\n\n\nClassifier/Clusterer\n - built on the processed data\n\n\nEvaluating\n - how good is the classifier/clusterer?\n\n\nAttribute selection\n - removing irrelevant attributes from your data\n\n\n\n\nThe following sections explain how to use them in your own code. A link to an \nexample class\n can be found at the end of this page, under the \nLinks\n section. The classifiers and filters always list their options in the Javadoc API (\nstable\n, \ndeveloper\n version) specification.\n\n\nA comprehensive source of information is the chapter \nUsing the API\n of the Weka manual.\n\n\nInstances\n\n\nDatasets\n\n\nThe \nDataSource\n class is not limited to ARFF files. It can also read CSV files and other formats (basically all file formats that Weka can import via its converters; it uses the file extension to determine the associated loader).\n\n\n import weka.core.converters.ConverterUtils.DataSource;\n ...\n DataSource source = new DataSource(\"/some/where/data.arff\");\n Instances data = source.getDataSet();\n // setting class attribute if the data format does not provide this information\n // For example, the XRFF format saves the class attribute information as well\n if (data.classIndex() == -1)\n   data.setClassIndex(data.numAttributes() - 1);\n\n\n\n\nDatabase\n\n\nReading from \nDatabases\n is slightly more complicated, but still very easy. First, you'll have to modify your \nDatabaseUtils.props\n file to reflect your database connection. Suppose you want to connect to a \nMySQL\n server that is running on the local machine on the default port \n3306\n. The MySQL JDBC driver is called \nConnector/J\n. (The driver class is \norg.gjt.mm.mysql.Driver\n.) The database where your target data resides is called \nsome_database\n. Since you're only reading, you can use the default user \nnobody\n without a password. Your props file must contain the following lines:\n\n\n jdbcDriver=org.gjt.mm.mysql.Driver\n jdbcURL=jdbc:mysql://localhost:3306/some_database\n\n\n\n\nSecondly, your Java code needs to look like this to load the data from the database:\n\n\n import weka.core.Instances;\n import weka.experiment.InstanceQuery;\n ...\n InstanceQuery query = new InstanceQuery();\n query.setUsername(\"nobody\");\n query.setPassword(\"\");\n query.setQuery(\"select * from whatsoever\");\n // You can declare that your data set is sparse\n // query.setSparseData(true);\n Instances data = query.retrieveInstances();\n\n\n\n\nNotes:\n\n\n Don't forget to add the JDBC driver to your \nCLASSPATH\n.\n\n For MS Access, you must use the JDBC-ODBC-bridge that is part of a JDK. The [[Windows databases]] article explains how to do this.\n* InstanceQuery automatically converts VARCHAR database columns to NOMINAL attributes, and long TEXT database columns to STRING attributes. So if you use InstanceQuery to do text mining against text that appears in a VARCHAR column, Weka will regard such text as nominal values. Thus it will fail to tokenize and mine that text. Use the \nNominalToString\n or \nStringToNominal\n filter (package \nweka.filters.unsupervised.attribute\n) to convert the attributes into the correct type.\n\n\nOption handling\n\n\nWeka schemes that implement the \nweka.core.OptionHandler\n interface, such as classifiers, clusterers, and filters, offer the following methods for setting and retrieving options:\n\n\n\n\nvoid setOptions(String[] options)\n\n\nString[] getOptions()\n\n\n\n\nThere are several ways of setting the options:\n\n\n\n\nManually creating a String array:\n\n\n\n\n   String[] options = new String[2];\n   options[0] = \"-R\";\n   options[1] = \"1\";\n\n\n\n\n\n\nUsing a single command-line string and using the \nsplitOptions\n method of the \nweka.core.Utils\n class to turn it into an array:\n\n\n\n\n    String[] options = weka.core.Utils.splitOptions(\"-R 1\");\n\n\n\n\n\n\nUsing the \nOptionsToCode.java\n class to automatically turn a command line into code. Especially handy if the command line contains nested classes that have their own options, such as kernels for SMO:\n\n\n\n\n  java OptionsToCode weka.classifiers.functions.SMO\n\n\n\n\nwill generate output like this:\n\n\n // create new instance of scheme\n weka.classifiers.functions.SMO scheme = new weka.classifiers.functions.SMO();\n // set options\n scheme.setOptions(weka.core.Utils.splitOptions(\"-C 1.0 -L 0.0010 -P 1.0E-12 -N 0 -V -1 -W 1 -K \\\"weka.classifiers.functions.supportVector.PolyKernel -C 250007 -E 1.0\\\"\"));\n\n\n\n\nAlso, the \nOptionTree.java\n tool allows you to view a nested options string, e.g., used at the command line, as a tree. This can help you spot nesting errors.\n\n\nFilter\n\n\nA filter has two different properties:\n\n\n\n\n\n\nsupervised\n or \nunsupervised\n\n  either takes the class attribute into account or not\n\n\n\n\n\n\nattribute\n- or \ninstance\n-based\n  e.g., removing a certain attribute or removing instances that meet a certain condition\n\n\n\n\n\n\nMost filters implement the \nOptionHandler\n interface, which means you can set the options via a String array, rather than setting them each manually via \nset\n-methods.\nFor example, if you want to remove the \nfirst\n attribute of a dataset, you need this filter\n\n\n weka.filters.unsupervised.attribute.Remove\n\n\n\n\nwith this option\n\n\n -R 1\n\n\n\n\nIf you have an \nInstances\n object, called \ndata\n, you can create and apply the filter like this:\n\n\n import weka.core.Instances;\n import weka.filters.Filter;\n import weka.filters.unsupervised.attribute.Remove;\n ...\n String[] options = new String[2];\n options[0] = \"-R\";                                    // \"range\"\n options[1] = \"1\";                                     // first attribute\n Remove remove = new Remove();                         // new instance of filter\n remove.setOptions(options);                           // set options\n remove.setInputFormat(data);                          // inform filter about dataset **AFTER** setting options\n Instances newData = Filter.useFilter(data, remove);   // apply filter\n\n\n\n\nFiltering on-the-fly\n\n\nThe \nFilteredClassifer\n meta-classifier is an easy way of filtering data on the fly. It removes the necessity of filtering the data before the classifier can be trained. Also, the data need not be passed through the trained filter again at prediction time. The following is an example of using this meta-classifier with the \nRemove\n filter and \nJ48\n for getting rid of a numeric ID attribute in the data:\n\n\n import weka.classifiers.meta.FilteredClassifier;\n import weka.classifiers.trees.J48;\n import weka.filters.unsupervised.attribute.Remove;\n ...\n Instances train = ...         // from somewhere\n Instances test = ...          // from somewhere\n // filter\n Remove rm = new Remove();\n rm.setAttributeIndices(\"1\");  // remove 1st attribute\n // classifier\n J48 j48 = new J48();\n j48.setUnpruned(true);        // using an unpruned J48\n // meta-classifier\n FilteredClassifier fc = new FilteredClassifier();\n fc.setFilter(rm);\n fc.setClassifier(j48);\n // train and make predictions\n fc.buildClassifier(train);\n for (int i = 0; i < test.numInstances(); i++) {\n   double pred = fc.classifyInstance(test.instance(i));\n   System.out.print(\"ID: \" + test.instance(i).value(0));\n   System.out.print(\", actual: \" + test.classAttribute().value((int) test.instance(i).classValue()));\n   System.out.println(\", predicted: \" + test.classAttribute().value((int) pred));\n }\n\n\n\n\nOther handy meta-schemes in Weka:\n\n\n\n\nweka.clusterers.FilteredClusterer\n\n\nweka.assocations.FilteredAssociator\n\n\n\n\nBatch filtering\n\n\nOn the command line, you can enable a second input/output pair (via \n-r\n and \n-s\n) with the \n-b\n option, in order to process the second file with the same filter setup as the first one. Necessary, if you're using attribute selection or standardization - otherwise you end up with incompatible datasets. This is done fairly easy, since one initializes the filter only once with the \nsetInputFormat(Instances)\n method, namely with the training set, and then applies the filter subsequently to the training set \nand\n the test set. The following example shows how to apply the \nStandardize\n filter to a train and a test set.\n\n\n Instances train = ...   // from somewhere\n Instances test = ...    // from somewhere\n Standardize filter = new Standardize();\n filter.setInputFormat(train);  // initializing the filter once with training set\n Instances newTrain = Filter.useFilter(train, filter);  // configures the Filter based on train instances and returns filtered instances\n Instances newTest = Filter.useFilter(test, filter);    // create new test set\n\n\n\n\nCalling conventions\n\n\nThe \nsetInputFormat(Instances)\n method \nalways\n has to be the last call before the filter is applied, e.g., with \nFilter.useFilter(Instances,Filter)\n. \nWhy?\n First, it is the convention for using filters and, secondly, lots of filters generate the header of the output format in the \nsetInputFormat(Instances)\n method with the currently set options (setting otpions \nafter\n this call doesn't have any effect any more).\n\n\nClassification\n\n\nThe necessary classes can be found in this package:\n\n\n weka.classifiers\n\n\n\n\nBuilding a Classifier\n\n\nBatch\n\n\nA Weka classifier is rather simple to train on a given dataset. E.g., we can train an unpruned C4.5 tree algorithm on a given dataset \ndata\n. The training is done via the \nbuildClassifier(Instances)\n method.\n\n\n import weka.classifiers.trees.J48;\n ...\n String[] options = new String[1];\n options[0] = \"-U\";            // unpruned tree\n J48 tree = new J48();         // new instance of tree\n tree.setOptions(options);     // set the options\n tree.buildClassifier(data);   // build classifier\n\n\n\n\nIncremental\n\n\nClassifiers implementing the \nweka.classifiers.UpdateableClassifier\n interface can be trained incrementally. This conserves memory, since the data doesn't have to be loaded into memory all at once. See the Javadoc of this interface to see what classifiers are implementing it.\n\n\nThe actual process of training an incremental classifier is fairly simple:\n\n\n\n\nCall \nbuildClassifier(Instances)\n with the structure of the dataset (may or may not contain any actual data rows).\n\n\nSubsequently call the \nupdateClassifier(Instance)\n method to feed the classifier new \nweka.core.Instance\n objects, one by one.\n\n\n\n\nHere is an example using data from a \nweka.core.converters.ArffLoader\n to train \nweka.classifiers.bayes.NaiveBayesUpdateable\n:\n\n\n // load data\n ArffLoader loader = new ArffLoader();\n loader.setFile(new File(\"/some/where/data.arff\"));\n Instances structure = loader.getStructure();\n structure.setClassIndex(structure.numAttributes() - 1);\n\n // train NaiveBayes\n NaiveBayesUpdateable nb = new NaiveBayesUpdateable();\n nb.buildClassifier(structure);\n Instance current;\n while ((current = loader.getNextInstance(structure)) != null)\n   nb.updateClassifier(current);\n\n\n\n\nA working example is \nIncrementalClassifier.java\n.\n\n\nEvaluating\n\n\nCross-validation\n\n\nIf you only have a training set and no test you might want to evaluate the classifier by using 10 times 10-fold cross-validation. This can be easily done via the \nEvaluation\n class. Here we \nseed\n the random selection of our folds for the CV with \n1\n. Check out the \nEvaluation\n class for more information about the statistics it produces.\n\n\n import weka.classifiers.Evaluation;\n import java.util.Random;\n ...\n Evaluation eval = new Evaluation(newData);\n eval.crossValidateModel(tree, newData, 10, new Random(1));\n\n\n\n\nNote:\n The classifier (in our example \ntree\n) should not be trained when handed over to the \ncrossValidateModel\n method. \nWhy?\n If the classifier does not abide to the Weka convention that a classifier must be re-initialized every time the \nbuildClassifier\n method is called (in other words: subsequent calls to the \nbuildClassifier\n method always return the same results), you will get inconsistent and worthless results. The \ncrossValidateModel\n takes care of training and evaluating the classifier. (It creates a copy of the original classifier that you hand over to the \ncrossValidateModel\n for each run of the cross-validation.)\n\n\nTrain/test set\n\n\nIn case you have a dedicated test set, you can train the classifier and then evaluate it on this test set. In the following example, a J48 is instantiated, trained and then evaluated. Some statistics are printed to \nstdout\n:\n\n\n import weka.core.Instances;\n import weka.classifiers.Evaluation;\n import weka.classifiers.trees.J48;\n ...\n Instances train = ...   // from somewhere\n Instances test = ...    // from somewhere\n // train classifier\n Classifier cls = new J48();\n cls.buildClassifier(train);\n // evaluate classifier and print some statistics\n Evaluation eval = new Evaluation(train);\n eval.evaluateModel(cls, test);\n System.out.println(eval.toSummaryString(\"\\nResults\\n======\\n\", false));\n\n\n\n\nStatistics\n\n\nSome methods for retrieving the results from the evaluation:\n\n\n\n\n\n\nnominal class\n\n\n\n\ncorrect()\n - number of correctly classified instances (see also \nincorrect()\n)\n\n\npctCorrect()\n - percentage of correctly classified instances (see also \npctIncorrect()\n)\n\n\nkappa()\n - Kappa statistics\n\n\n\n\n\n\n\n\nnumeric class\n\n\n\n\ncorrelationCoefficient()\n - correlation coefficient\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\nmeanAbsoluteError()\n - the mean absolute error\n\n\nrootMeanSquaredError()\n - the root mean squared error\n\n\nunclassified()\n - number of unclassified instances\n\n\npctUnclassified()\n - percentage of unclassified instances\n\n\n\n\n\n\n\n\nIf you want to have the exact same behavior as from the command line, use this call:\n\n\n import weka.classifiers.trees.J48;\n import weka.classifiers.Evaluation;\n ...\n String[] options = new String[2];\n options[0] = \"-t\";\n options[1] = \"/some/where/somefile.arff\";\n System.out.println(Evaluation.evaluateModel(new J48(), options));\n\n\n\n\nROC curves/AUC\n\n\nYou can also generate ROC curves/AUC with the predictions Weka recorded during testing. You can access these predictions via the \npredictions()\n method of the \nEvaluation\n class. See the [[Generating ROC curve]] article for a full example of how to generate ROC curves.\n\n\nClassifying instances\n\n\nIn case you have an unlabeled dataset that you want to classify with your newly trained classifier, you can use the following code snippet. It loads the file \n/some/where/unlabeled.arff\n, uses the previously built classifier \ntree\n to label the instances, and saves the labeled data as \n/some/where/labeled.arff\n.\n\n\n import java.io.BufferedReader;\n import java.io.BufferedWriter;\n import java.io.FileReader;\n import java.io.FileWriter;\n import weka.core.Instances;\n ...\n // load unlabeled data\n Instances unlabeled = new Instances(\n                         new BufferedReader(\n                           new FileReader(\"/some/where/unlabeled.arff\")));\n\n // set class attribute\n unlabeled.setClassIndex(unlabeled.numAttributes() - 1);\n\n // create copy\n Instances labeled = new Instances(unlabeled);\n\n // label instances\n for (int i = 0; i < unlabeled.numInstances(); i++) {\n   double clsLabel = tree.classifyInstance(unlabeled.instance(i));\n   labeled.instance(i).setClassValue(clsLabel);\n }\n // save labeled data\n BufferedWriter writer = new BufferedWriter(\n                           new FileWriter(\"/some/where/labeled.arff\"));\n writer.write(labeled.toString());\n writer.newLine();\n writer.flush();\n writer.close();\n\n\n\n\nNote on nominal classes:\n\n\n\n\nIf you're interested in the distribution over all the classes, use the method \ndistributionForInstance(Instance)\n. This method returns a double array with the probability for each class.\n\n\nThe returned double value from \nclassifyInstance\n (or the index in the array returned by \ndistributionForInstance\n) is just the index for the string values in the attribute. That is, if you want the string representation for the class label returned above \nclsLabel\n, then you can print it like this:\n\n\n\n\nSystem.out.println(clsLabel + \" -> \" + unlabeled.classAttribute().value((int) clsLabel));\n\n\n\n\nClustering\n\n\nClustering is similar to classification. The necessary classes can be found in this package:\n\n\n weka.clusterers\n\n\n\n\nBuilding a Clusterer\n\n\nBatch\n\n\nA clusterer is built in much the same way as a classifier, but the \nbuildClusterer(Instances)\n method instead of \nbuildClassifier(Instances)\n. The following code snippet shows how to build an \nEM\n clusterer with a maximum of \n100\n iterations.\n\n\n import weka.clusterers.EM;\n ...\n String[] options = new String[2];\n options[0] = \"-I\";                 // max. iterations\n options[1] = \"100\";\n EM clusterer = new EM();   // new instance of clusterer\n clusterer.setOptions(options);     // set the options\n clusterer.buildClusterer(data);    // build the clusterer\n\n\n\n\nIncremental\n\n\nClusterers implementing the \nweka.clusterers.UpdateableClusterer\n interface can be trained incrementally. This conserves memory, since the data doesn't have to be loaded into memory all at once. See the Javadoc for this interface to see which clusterers implement it.\n\n\nThe actual process of training an incremental clusterer is fairly simple:\n\n\n\n\nCall \nbuildClusterer(Instances)\n with the structure of the dataset (may or may not contain any actual data rows).\n\n\nSubsequently call the \nupdateClusterer(Instance)\n method to feed the clusterer new \nweka.core.Instance\n objects, one by one.\n\n\nCall \nupdateFinished()\n after all Instance objects have been processed, for the clusterer to perform additional computations.\n\n\n\n\nHere is an example using data from a \nweka.core.converters.ArffLoader\n to train \nweka.clusterers.Cobweb\n:\n\n\n // load data\n ArffLoader loader = new ArffLoader();\n loader.setFile(new File(\"/some/where/data.arff\"));\n Instances structure = loader.getStructure();\n\n // train Cobweb\n Cobweb cw = new Cobweb();\n cw.buildClusterer(structure);\n Instance current;\n while ((current = loader.getNextInstance(structure)) != null)\n   cw.updateClusterer(current);\n cw.updateFinished();\n\n\n\n\nA working example is \nIncrementalClusterer.java\n.\n\n\nEvaluating\n\n\nFor evaluating a clusterer, you can use the \nClusterEvaluation\n class. In this example, the number of clusters found is written to output:\n\n\n import weka.clusterers.ClusterEvaluation;\n import weka.clusterers.Clusterer;\n ...\n ClusterEvaluation eval = new ClusterEvaluation();\n Clusterer clusterer = new EM();                                 // new clusterer instance, default options\n clusterer.buildClusterer(data);                                 // build clusterer\n eval.setClusterer(clusterer);                                   // the cluster to evaluate\n eval.evaluateClusterer(newData);                                // data to evaluate the clusterer on\n System.out.println(\"# of clusters: \" + eval.getNumClusters());  // output # of clusters\n\n\n\n\nOr, in the case of \nDensityBasedClusterer\n, you can cross-validate the clusterer (Note: with \nMakeDensityBasedClusterer\n you can turn any clusterer into a density-based one):\n\n\n import weka.clusterers.ClusterEvaluation;\n import weka.clusterers.DensityBasedClusterer;\n import weka.core.Instances;\n import java.util.Random;\n ...\n Instances data = ...                                     // from somewhere\n DensityBasedClusterer clusterer = new ...                // the clusterer to evaluate\n double logLikelyhood =\n    ClusterEvaluation.crossValidateModel(                 // cross-validate\n    clusterer, data, 10,                                  // with 10 folds\n    new Random(1));                                       // and random number generator with seed 1\n\n\n\n\nOr, if you want the same behavior/print-out from command line, use this call:\n\n\n import weka.clusterers.EM;\n import weka.clusterers.ClusterEvaluation;\n ...\n String[] options = new String[2];\n options[0] = \"-t\";\n options[1] = \"/some/where/somefile.arff\";\n System.out.println(ClusterEvaluation.evaluateClusterer(new EM(), options));\n\n\n\n\nClustering instances\n\n\nThe only difference with regard to classification is the method name. Instead of \nclassifyInstance(Instance)\n, it is now \nclusterInstance(Instance)\n. The method for obtaining the distribution is still the same, i.e., \ndistributionForInstance(Instance)\n.\n\n\nClasses to clusters evaluation\n\n\nIf your data contains a class attribute and you want to check how well the generated clusters fit the classes, you can perform a so-called \nclasses to clusters\n evaluation. The Weka Explorer offers this functionality, and it's quite easy to implement. These are the necessary steps (complete source code: \nClassesToClusters.java\n):\n\n\n\n\nload the data and set the class attribute\n\n\n\n\n Instances data = new Instances(new BufferedReader(new FileReader(\"/some/where/file.arff\")));\n data.setClassIndex(data.numAttributes() - 1);\n\n\n\n\n\n\ngenerate the \nclass-less\n data to train the clusterer with\n\n\n\n\n weka.filters.unsupervised.attribute.Remove filter = new weka.filters.unsupervised.attribute.Remove();\n filter.setAttributeIndices(\"\" + (data.classIndex() + 1));\n filter.setInputFormat(data);\n Instances dataClusterer = Filter.useFilter(data, filter);\n\n\n\n\n\n\ntrain the clusterer, e.g., \nEM\n\n\n\n\n EM clusterer = new EM();\n // set further options for EM, if necessary...\n clusterer.buildClusterer(dataClusterer);\n\n\n\n\n\n\nevaluate the clusterer with the data still containing the class attribute\n\n\n\n\n ClusterEvaluation eval = new ClusterEvaluation();\n eval.setClusterer(clusterer);\n eval.evaluateClusterer(data);\n\n\n\n\n\n\nprint the results of the evaluation to \nstdout\n\n\n\n\n System.out.println(eval.clusterResultsToString());\n\n\n\n\nAttribute selection\n\n\nThere is no real need to use the attribute selection classes directly in your own code, since there are already a meta-classifier and a filter available for applying attribute selection, but the low-level approach is still listed for the sake of completeness. The following examples all use \nCfsSubsetEval\n and \nGreedyStepwise\n (backwards). The code listed below is taken from the \nAttributeSelectionTest.java\n.\n\n\nMeta-Classifier\n\n\nThe following meta-classifier performs a preprocessing step of attribute selection before the data gets presented to the base classifier (in the example here, this is \nJ48\n).\n\n\n  Instances data = ...  // from somewhere\n  AttributeSelectedClassifier classifier = new AttributeSelectedClassifier();\n  CfsSubsetEval eval = new CfsSubsetEval();\n  GreedyStepwise search = new GreedyStepwise();\n  search.setSearchBackwards(true);\n  J48 base = new J48();\n  classifier.setClassifier(base);\n  classifier.setEvaluator(eval);\n  classifier.setSearch(search);\n  // 10-fold cross-validation\n  Evaluation evaluation = new Evaluation(data);\n  evaluation.crossValidateModel(classifier, data, 10, new Random(1));\n  System.out.println(evaluation.toSummaryString());\n\n\n\n\nFilter\n\n\nThe filter approach is straightforward: after setting up the filter, one just filters the data through the filter and obtains the reduced dataset.\n\n\n  Instances data = ...  // from somewhere\n  AttributeSelection filter = new AttributeSelection();  // package weka.filters.supervised.attribute!\n  CfsSubsetEval eval = new CfsSubsetEval();\n  GreedyStepwise search = new GreedyStepwise();\n  search.setSearchBackwards(true);\n  filter.setEvaluator(eval);\n  filter.setSearch(search);\n  filter.setInputFormat(data);\n  // generate new data\n  Instances newData = Filter.useFilter(data, filter);\n  System.out.println(newData);\n\n\n\n\nLow-level\n\n\nIf neither the meta-classifier nor filter approach is suitable for your purposes, you can use the attribute selection classes themselves.\n\n\n  Instances data = ...  // from somewhere\n  AttributeSelection attsel = new AttributeSelection();  // package weka.attributeSelection!\n  CfsSubsetEval eval = new CfsSubsetEval();\n  GreedyStepwise search = new GreedyStepwise();\n  search.setSearchBackwards(true);\n  attsel.setEvaluator(eval);\n  attsel.setSearch(search);\n  attsel.SelectAttributes(data);\n  // obtain the attribute indices that were selected\n  int[] indices = attsel.selectedAttributes();\n  System.out.println(Utils.arrayToString(indices));\n\n\n\n\nNote on randomization\n\n\nMost machine learning schemes, like classifiers and clusterers, are susceptible to the ordering of the data. Using a different seed for randomizing the data will most likely produce a different result. For example, the Explorer, or a classifier/clusterer run from the command line, uses only a seeded \njava.util.Random\n number generator, whereas the \nweka.core.Instances.getRandomNumberGenerator(int)\n (which the \nWekaDemo.java\n uses) also takes the data into account for seeding. Unless one runs 10-fold cross-validation 10 times and averages the results, one will most likely get different results.\n\n\nSee also\n\n\n\n\nWeka Examples\n - pointer to collection of example classes\n\n\nDatabases\n - for more information about using databases in Weka (includes ODBC, e.g., for MS Access)\n\n\n[[weka_experiment_DatabaseUtils.props|weka/experiment/DatabaseUtils.props]] - the database setup file\n\n\nGenerating cross-validation folds (Java approach)\n - in case you want to run 10-fold cross-validation manually\n\n\n[[Generating classifier evaluation output manually]] - if you want to generate some of the evaluation statistics output manually\n\n\nCreating Instances on-the-fly\n - explains how to generate a \nweka.core.Instances\n object from scratch\n\n\n[[Save Instances to an ARFF File]] - shows how to output a dataset\n\n\n[[Using the Experiment API]]\n\n\n\n\nExamples\n\n\nThe following are a few sample classes for using various parts of the Weka API:\n\n\n\n\n\n\nWekaDemo.java\n (\nstable\n, \ndeveloper\n) - little demo class that loads data from a file, runs it through a filter and trains/evaluates a classifier\n\n\n\n\n\n\nClusteringDemo.java\n (\nstable\n, \ndeveloper\n) - a basic example for using the clusterer API\n\n\n\n\n\n\nClassesToClusters.java\n (\nstable\n, \ndeveloper\n) - performs a \nclasses to clusters\n evaluation like in the Explorer\n\n\n\n\n\n\nAttributeSelectionTest.java\n (\nstable\n, \ndeveloper\n) - example code for using the attribute selection API\n\n\n\n\n\n\nM5PExample.java\n (\nstable\n, \ndeveloper\n) - example using M5P to obtain data from database, train model, serialize it to a file, and use this serialized model to make predictions again.\n\n\n\n\n\n\nOptionsToCode.java\n (\nstable\n, \ndeveloper\n) - turns a Weka command line for a scheme with options into Java code, correctly escaping quotes and backslashes.\n\n\n\n\n\n\nOptionTree.java\n (\nstable\n, \ndeveloper\n) - displays nested Weka options as tree.\n\n\n\n\n\n\nIncrementalClassifier.java\n (\nstable\n, \ndeveloper\n) - Example class for how to train an incremental classifier (in this case, \nweka.classifiers.bayes.NaiveBayesUpdateable\n).\n\n\n\n\n\n\nIncrementalClusterer.java\n (\nstable\n, \ndeveloper\n) - Example class for how to train an incremental clusterer (in this case, \nweka.clusterers.Cobweb\n).\n\n\n\n\n\n\nLinks\n\n\n\n\nWeka API\n\n\nStable version\n\n\nDeveloper version",
            "title": " Use Weka in your Java code"
        },
        {
            "location": "/use_weka_in_your_java_code/#instances",
            "text": "",
            "title": "Instances"
        },
        {
            "location": "/use_weka_in_your_java_code/#datasets",
            "text": "The  DataSource  class is not limited to ARFF files. It can also read CSV files and other formats (basically all file formats that Weka can import via its converters; it uses the file extension to determine the associated loader).   import weka.core.converters.ConverterUtils.DataSource;\n ...\n DataSource source = new DataSource(\"/some/where/data.arff\");\n Instances data = source.getDataSet();\n // setting class attribute if the data format does not provide this information\n // For example, the XRFF format saves the class attribute information as well\n if (data.classIndex() == -1)\n   data.setClassIndex(data.numAttributes() - 1);",
            "title": "Datasets"
        },
        {
            "location": "/use_weka_in_your_java_code/#database",
            "text": "Reading from  Databases  is slightly more complicated, but still very easy. First, you'll have to modify your  DatabaseUtils.props  file to reflect your database connection. Suppose you want to connect to a  MySQL  server that is running on the local machine on the default port  3306 . The MySQL JDBC driver is called  Connector/J . (The driver class is  org.gjt.mm.mysql.Driver .) The database where your target data resides is called  some_database . Since you're only reading, you can use the default user  nobody  without a password. Your props file must contain the following lines:   jdbcDriver=org.gjt.mm.mysql.Driver\n jdbcURL=jdbc:mysql://localhost:3306/some_database  Secondly, your Java code needs to look like this to load the data from the database:   import weka.core.Instances;\n import weka.experiment.InstanceQuery;\n ...\n InstanceQuery query = new InstanceQuery();\n query.setUsername(\"nobody\");\n query.setPassword(\"\");\n query.setQuery(\"select * from whatsoever\");\n // You can declare that your data set is sparse\n // query.setSparseData(true);\n Instances data = query.retrieveInstances();  Notes:   Don't forget to add the JDBC driver to your  CLASSPATH .  For MS Access, you must use the JDBC-ODBC-bridge that is part of a JDK. The [[Windows databases]] article explains how to do this.\n* InstanceQuery automatically converts VARCHAR database columns to NOMINAL attributes, and long TEXT database columns to STRING attributes. So if you use InstanceQuery to do text mining against text that appears in a VARCHAR column, Weka will regard such text as nominal values. Thus it will fail to tokenize and mine that text. Use the  NominalToString  or  StringToNominal  filter (package  weka.filters.unsupervised.attribute ) to convert the attributes into the correct type.",
            "title": "Database"
        },
        {
            "location": "/use_weka_in_your_java_code/#option-handling",
            "text": "Weka schemes that implement the  weka.core.OptionHandler  interface, such as classifiers, clusterers, and filters, offer the following methods for setting and retrieving options:   void setOptions(String[] options)  String[] getOptions()   There are several ways of setting the options:   Manually creating a String array:      String[] options = new String[2];\n   options[0] = \"-R\";\n   options[1] = \"1\";   Using a single command-line string and using the  splitOptions  method of the  weka.core.Utils  class to turn it into an array:       String[] options = weka.core.Utils.splitOptions(\"-R 1\");   Using the  OptionsToCode.java  class to automatically turn a command line into code. Especially handy if the command line contains nested classes that have their own options, such as kernels for SMO:     java OptionsToCode weka.classifiers.functions.SMO  will generate output like this:   // create new instance of scheme\n weka.classifiers.functions.SMO scheme = new weka.classifiers.functions.SMO();\n // set options\n scheme.setOptions(weka.core.Utils.splitOptions(\"-C 1.0 -L 0.0010 -P 1.0E-12 -N 0 -V -1 -W 1 -K \\\"weka.classifiers.functions.supportVector.PolyKernel -C 250007 -E 1.0\\\"\"));  Also, the  OptionTree.java  tool allows you to view a nested options string, e.g., used at the command line, as a tree. This can help you spot nesting errors.",
            "title": "Option handling"
        },
        {
            "location": "/use_weka_in_your_java_code/#filter",
            "text": "A filter has two different properties:    supervised  or  unsupervised \n  either takes the class attribute into account or not    attribute - or  instance -based\n  e.g., removing a certain attribute or removing instances that meet a certain condition    Most filters implement the  OptionHandler  interface, which means you can set the options via a String array, rather than setting them each manually via  set -methods.\nFor example, if you want to remove the  first  attribute of a dataset, you need this filter   weka.filters.unsupervised.attribute.Remove  with this option   -R 1  If you have an  Instances  object, called  data , you can create and apply the filter like this:   import weka.core.Instances;\n import weka.filters.Filter;\n import weka.filters.unsupervised.attribute.Remove;\n ...\n String[] options = new String[2];\n options[0] = \"-R\";                                    // \"range\"\n options[1] = \"1\";                                     // first attribute\n Remove remove = new Remove();                         // new instance of filter\n remove.setOptions(options);                           // set options\n remove.setInputFormat(data);                          // inform filter about dataset **AFTER** setting options\n Instances newData = Filter.useFilter(data, remove);   // apply filter",
            "title": "Filter"
        },
        {
            "location": "/use_weka_in_your_java_code/#filtering-on-the-fly",
            "text": "The  FilteredClassifer  meta-classifier is an easy way of filtering data on the fly. It removes the necessity of filtering the data before the classifier can be trained. Also, the data need not be passed through the trained filter again at prediction time. The following is an example of using this meta-classifier with the  Remove  filter and  J48  for getting rid of a numeric ID attribute in the data:   import weka.classifiers.meta.FilteredClassifier;\n import weka.classifiers.trees.J48;\n import weka.filters.unsupervised.attribute.Remove;\n ...\n Instances train = ...         // from somewhere\n Instances test = ...          // from somewhere\n // filter\n Remove rm = new Remove();\n rm.setAttributeIndices(\"1\");  // remove 1st attribute\n // classifier\n J48 j48 = new J48();\n j48.setUnpruned(true);        // using an unpruned J48\n // meta-classifier\n FilteredClassifier fc = new FilteredClassifier();\n fc.setFilter(rm);\n fc.setClassifier(j48);\n // train and make predictions\n fc.buildClassifier(train);\n for (int i = 0; i < test.numInstances(); i++) {\n   double pred = fc.classifyInstance(test.instance(i));\n   System.out.print(\"ID: \" + test.instance(i).value(0));\n   System.out.print(\", actual: \" + test.classAttribute().value((int) test.instance(i).classValue()));\n   System.out.println(\", predicted: \" + test.classAttribute().value((int) pred));\n }  Other handy meta-schemes in Weka:   weka.clusterers.FilteredClusterer  weka.assocations.FilteredAssociator",
            "title": "Filtering on-the-fly"
        },
        {
            "location": "/use_weka_in_your_java_code/#batch-filtering",
            "text": "On the command line, you can enable a second input/output pair (via  -r  and  -s ) with the  -b  option, in order to process the second file with the same filter setup as the first one. Necessary, if you're using attribute selection or standardization - otherwise you end up with incompatible datasets. This is done fairly easy, since one initializes the filter only once with the  setInputFormat(Instances)  method, namely with the training set, and then applies the filter subsequently to the training set  and  the test set. The following example shows how to apply the  Standardize  filter to a train and a test set.   Instances train = ...   // from somewhere\n Instances test = ...    // from somewhere\n Standardize filter = new Standardize();\n filter.setInputFormat(train);  // initializing the filter once with training set\n Instances newTrain = Filter.useFilter(train, filter);  // configures the Filter based on train instances and returns filtered instances\n Instances newTest = Filter.useFilter(test, filter);    // create new test set",
            "title": "Batch filtering"
        },
        {
            "location": "/use_weka_in_your_java_code/#calling-conventions",
            "text": "The  setInputFormat(Instances)  method  always  has to be the last call before the filter is applied, e.g., with  Filter.useFilter(Instances,Filter) .  Why?  First, it is the convention for using filters and, secondly, lots of filters generate the header of the output format in the  setInputFormat(Instances)  method with the currently set options (setting otpions  after  this call doesn't have any effect any more).",
            "title": "Calling conventions"
        },
        {
            "location": "/use_weka_in_your_java_code/#classification",
            "text": "The necessary classes can be found in this package:   weka.classifiers",
            "title": "Classification"
        },
        {
            "location": "/use_weka_in_your_java_code/#building-a-classifier",
            "text": "",
            "title": "Building a Classifier"
        },
        {
            "location": "/use_weka_in_your_java_code/#batch",
            "text": "A Weka classifier is rather simple to train on a given dataset. E.g., we can train an unpruned C4.5 tree algorithm on a given dataset  data . The training is done via the  buildClassifier(Instances)  method.   import weka.classifiers.trees.J48;\n ...\n String[] options = new String[1];\n options[0] = \"-U\";            // unpruned tree\n J48 tree = new J48();         // new instance of tree\n tree.setOptions(options);     // set the options\n tree.buildClassifier(data);   // build classifier",
            "title": "Batch"
        },
        {
            "location": "/use_weka_in_your_java_code/#incremental",
            "text": "Classifiers implementing the  weka.classifiers.UpdateableClassifier  interface can be trained incrementally. This conserves memory, since the data doesn't have to be loaded into memory all at once. See the Javadoc of this interface to see what classifiers are implementing it.  The actual process of training an incremental classifier is fairly simple:   Call  buildClassifier(Instances)  with the structure of the dataset (may or may not contain any actual data rows).  Subsequently call the  updateClassifier(Instance)  method to feed the classifier new  weka.core.Instance  objects, one by one.   Here is an example using data from a  weka.core.converters.ArffLoader  to train  weka.classifiers.bayes.NaiveBayesUpdateable :   // load data\n ArffLoader loader = new ArffLoader();\n loader.setFile(new File(\"/some/where/data.arff\"));\n Instances structure = loader.getStructure();\n structure.setClassIndex(structure.numAttributes() - 1);\n\n // train NaiveBayes\n NaiveBayesUpdateable nb = new NaiveBayesUpdateable();\n nb.buildClassifier(structure);\n Instance current;\n while ((current = loader.getNextInstance(structure)) != null)\n   nb.updateClassifier(current);  A working example is  IncrementalClassifier.java .",
            "title": "Incremental"
        },
        {
            "location": "/use_weka_in_your_java_code/#evaluating",
            "text": "",
            "title": "Evaluating"
        },
        {
            "location": "/use_weka_in_your_java_code/#cross-validation",
            "text": "If you only have a training set and no test you might want to evaluate the classifier by using 10 times 10-fold cross-validation. This can be easily done via the  Evaluation  class. Here we  seed  the random selection of our folds for the CV with  1 . Check out the  Evaluation  class for more information about the statistics it produces.   import weka.classifiers.Evaluation;\n import java.util.Random;\n ...\n Evaluation eval = new Evaluation(newData);\n eval.crossValidateModel(tree, newData, 10, new Random(1));  Note:  The classifier (in our example  tree ) should not be trained when handed over to the  crossValidateModel  method.  Why?  If the classifier does not abide to the Weka convention that a classifier must be re-initialized every time the  buildClassifier  method is called (in other words: subsequent calls to the  buildClassifier  method always return the same results), you will get inconsistent and worthless results. The  crossValidateModel  takes care of training and evaluating the classifier. (It creates a copy of the original classifier that you hand over to the  crossValidateModel  for each run of the cross-validation.)",
            "title": "Cross-validation"
        },
        {
            "location": "/use_weka_in_your_java_code/#traintest-set",
            "text": "In case you have a dedicated test set, you can train the classifier and then evaluate it on this test set. In the following example, a J48 is instantiated, trained and then evaluated. Some statistics are printed to  stdout :   import weka.core.Instances;\n import weka.classifiers.Evaluation;\n import weka.classifiers.trees.J48;\n ...\n Instances train = ...   // from somewhere\n Instances test = ...    // from somewhere\n // train classifier\n Classifier cls = new J48();\n cls.buildClassifier(train);\n // evaluate classifier and print some statistics\n Evaluation eval = new Evaluation(train);\n eval.evaluateModel(cls, test);\n System.out.println(eval.toSummaryString(\"\\nResults\\n======\\n\", false));",
            "title": "Train/test set"
        },
        {
            "location": "/use_weka_in_your_java_code/#statistics",
            "text": "Some methods for retrieving the results from the evaluation:    nominal class   correct()  - number of correctly classified instances (see also  incorrect() )  pctCorrect()  - percentage of correctly classified instances (see also  pctIncorrect() )  kappa()  - Kappa statistics     numeric class   correlationCoefficient()  - correlation coefficient     general   meanAbsoluteError()  - the mean absolute error  rootMeanSquaredError()  - the root mean squared error  unclassified()  - number of unclassified instances  pctUnclassified()  - percentage of unclassified instances     If you want to have the exact same behavior as from the command line, use this call:   import weka.classifiers.trees.J48;\n import weka.classifiers.Evaluation;\n ...\n String[] options = new String[2];\n options[0] = \"-t\";\n options[1] = \"/some/where/somefile.arff\";\n System.out.println(Evaluation.evaluateModel(new J48(), options));",
            "title": "Statistics"
        },
        {
            "location": "/use_weka_in_your_java_code/#roc-curvesauc",
            "text": "You can also generate ROC curves/AUC with the predictions Weka recorded during testing. You can access these predictions via the  predictions()  method of the  Evaluation  class. See the [[Generating ROC curve]] article for a full example of how to generate ROC curves.",
            "title": "ROC curves/AUC"
        },
        {
            "location": "/use_weka_in_your_java_code/#classifying-instances",
            "text": "In case you have an unlabeled dataset that you want to classify with your newly trained classifier, you can use the following code snippet. It loads the file  /some/where/unlabeled.arff , uses the previously built classifier  tree  to label the instances, and saves the labeled data as  /some/where/labeled.arff .   import java.io.BufferedReader;\n import java.io.BufferedWriter;\n import java.io.FileReader;\n import java.io.FileWriter;\n import weka.core.Instances;\n ...\n // load unlabeled data\n Instances unlabeled = new Instances(\n                         new BufferedReader(\n                           new FileReader(\"/some/where/unlabeled.arff\")));\n\n // set class attribute\n unlabeled.setClassIndex(unlabeled.numAttributes() - 1);\n\n // create copy\n Instances labeled = new Instances(unlabeled);\n\n // label instances\n for (int i = 0; i < unlabeled.numInstances(); i++) {\n   double clsLabel = tree.classifyInstance(unlabeled.instance(i));\n   labeled.instance(i).setClassValue(clsLabel);\n }\n // save labeled data\n BufferedWriter writer = new BufferedWriter(\n                           new FileWriter(\"/some/where/labeled.arff\"));\n writer.write(labeled.toString());\n writer.newLine();\n writer.flush();\n writer.close();  Note on nominal classes:   If you're interested in the distribution over all the classes, use the method  distributionForInstance(Instance) . This method returns a double array with the probability for each class.  The returned double value from  classifyInstance  (or the index in the array returned by  distributionForInstance ) is just the index for the string values in the attribute. That is, if you want the string representation for the class label returned above  clsLabel , then you can print it like this:   System.out.println(clsLabel + \" -> \" + unlabeled.classAttribute().value((int) clsLabel));",
            "title": "Classifying instances"
        },
        {
            "location": "/use_weka_in_your_java_code/#clustering",
            "text": "Clustering is similar to classification. The necessary classes can be found in this package:   weka.clusterers",
            "title": "Clustering"
        },
        {
            "location": "/use_weka_in_your_java_code/#building-a-clusterer",
            "text": "",
            "title": "Building a Clusterer"
        },
        {
            "location": "/use_weka_in_your_java_code/#batch_1",
            "text": "A clusterer is built in much the same way as a classifier, but the  buildClusterer(Instances)  method instead of  buildClassifier(Instances) . The following code snippet shows how to build an  EM  clusterer with a maximum of  100  iterations.   import weka.clusterers.EM;\n ...\n String[] options = new String[2];\n options[0] = \"-I\";                 // max. iterations\n options[1] = \"100\";\n EM clusterer = new EM();   // new instance of clusterer\n clusterer.setOptions(options);     // set the options\n clusterer.buildClusterer(data);    // build the clusterer",
            "title": "Batch"
        },
        {
            "location": "/use_weka_in_your_java_code/#incremental_1",
            "text": "Clusterers implementing the  weka.clusterers.UpdateableClusterer  interface can be trained incrementally. This conserves memory, since the data doesn't have to be loaded into memory all at once. See the Javadoc for this interface to see which clusterers implement it.  The actual process of training an incremental clusterer is fairly simple:   Call  buildClusterer(Instances)  with the structure of the dataset (may or may not contain any actual data rows).  Subsequently call the  updateClusterer(Instance)  method to feed the clusterer new  weka.core.Instance  objects, one by one.  Call  updateFinished()  after all Instance objects have been processed, for the clusterer to perform additional computations.   Here is an example using data from a  weka.core.converters.ArffLoader  to train  weka.clusterers.Cobweb :   // load data\n ArffLoader loader = new ArffLoader();\n loader.setFile(new File(\"/some/where/data.arff\"));\n Instances structure = loader.getStructure();\n\n // train Cobweb\n Cobweb cw = new Cobweb();\n cw.buildClusterer(structure);\n Instance current;\n while ((current = loader.getNextInstance(structure)) != null)\n   cw.updateClusterer(current);\n cw.updateFinished();  A working example is  IncrementalClusterer.java .",
            "title": "Incremental"
        },
        {
            "location": "/use_weka_in_your_java_code/#evaluating_1",
            "text": "For evaluating a clusterer, you can use the  ClusterEvaluation  class. In this example, the number of clusters found is written to output:   import weka.clusterers.ClusterEvaluation;\n import weka.clusterers.Clusterer;\n ...\n ClusterEvaluation eval = new ClusterEvaluation();\n Clusterer clusterer = new EM();                                 // new clusterer instance, default options\n clusterer.buildClusterer(data);                                 // build clusterer\n eval.setClusterer(clusterer);                                   // the cluster to evaluate\n eval.evaluateClusterer(newData);                                // data to evaluate the clusterer on\n System.out.println(\"# of clusters: \" + eval.getNumClusters());  // output # of clusters  Or, in the case of  DensityBasedClusterer , you can cross-validate the clusterer (Note: with  MakeDensityBasedClusterer  you can turn any clusterer into a density-based one):   import weka.clusterers.ClusterEvaluation;\n import weka.clusterers.DensityBasedClusterer;\n import weka.core.Instances;\n import java.util.Random;\n ...\n Instances data = ...                                     // from somewhere\n DensityBasedClusterer clusterer = new ...                // the clusterer to evaluate\n double logLikelyhood =\n    ClusterEvaluation.crossValidateModel(                 // cross-validate\n    clusterer, data, 10,                                  // with 10 folds\n    new Random(1));                                       // and random number generator with seed 1  Or, if you want the same behavior/print-out from command line, use this call:   import weka.clusterers.EM;\n import weka.clusterers.ClusterEvaluation;\n ...\n String[] options = new String[2];\n options[0] = \"-t\";\n options[1] = \"/some/where/somefile.arff\";\n System.out.println(ClusterEvaluation.evaluateClusterer(new EM(), options));",
            "title": "Evaluating"
        },
        {
            "location": "/use_weka_in_your_java_code/#clustering-instances",
            "text": "The only difference with regard to classification is the method name. Instead of  classifyInstance(Instance) , it is now  clusterInstance(Instance) . The method for obtaining the distribution is still the same, i.e.,  distributionForInstance(Instance) .",
            "title": "Clustering instances"
        },
        {
            "location": "/use_weka_in_your_java_code/#classes-to-clusters-evaluation",
            "text": "If your data contains a class attribute and you want to check how well the generated clusters fit the classes, you can perform a so-called  classes to clusters  evaluation. The Weka Explorer offers this functionality, and it's quite easy to implement. These are the necessary steps (complete source code:  ClassesToClusters.java ):   load the data and set the class attribute    Instances data = new Instances(new BufferedReader(new FileReader(\"/some/where/file.arff\")));\n data.setClassIndex(data.numAttributes() - 1);   generate the  class-less  data to train the clusterer with    weka.filters.unsupervised.attribute.Remove filter = new weka.filters.unsupervised.attribute.Remove();\n filter.setAttributeIndices(\"\" + (data.classIndex() + 1));\n filter.setInputFormat(data);\n Instances dataClusterer = Filter.useFilter(data, filter);   train the clusterer, e.g.,  EM    EM clusterer = new EM();\n // set further options for EM, if necessary...\n clusterer.buildClusterer(dataClusterer);   evaluate the clusterer with the data still containing the class attribute    ClusterEvaluation eval = new ClusterEvaluation();\n eval.setClusterer(clusterer);\n eval.evaluateClusterer(data);   print the results of the evaluation to  stdout    System.out.println(eval.clusterResultsToString());",
            "title": "Classes to clusters evaluation"
        },
        {
            "location": "/use_weka_in_your_java_code/#attribute-selection",
            "text": "There is no real need to use the attribute selection classes directly in your own code, since there are already a meta-classifier and a filter available for applying attribute selection, but the low-level approach is still listed for the sake of completeness. The following examples all use  CfsSubsetEval  and  GreedyStepwise  (backwards). The code listed below is taken from the  AttributeSelectionTest.java .",
            "title": "Attribute selection"
        },
        {
            "location": "/use_weka_in_your_java_code/#meta-classifier",
            "text": "The following meta-classifier performs a preprocessing step of attribute selection before the data gets presented to the base classifier (in the example here, this is  J48 ).    Instances data = ...  // from somewhere\n  AttributeSelectedClassifier classifier = new AttributeSelectedClassifier();\n  CfsSubsetEval eval = new CfsSubsetEval();\n  GreedyStepwise search = new GreedyStepwise();\n  search.setSearchBackwards(true);\n  J48 base = new J48();\n  classifier.setClassifier(base);\n  classifier.setEvaluator(eval);\n  classifier.setSearch(search);\n  // 10-fold cross-validation\n  Evaluation evaluation = new Evaluation(data);\n  evaluation.crossValidateModel(classifier, data, 10, new Random(1));\n  System.out.println(evaluation.toSummaryString());",
            "title": "Meta-Classifier"
        },
        {
            "location": "/use_weka_in_your_java_code/#filter_1",
            "text": "The filter approach is straightforward: after setting up the filter, one just filters the data through the filter and obtains the reduced dataset.    Instances data = ...  // from somewhere\n  AttributeSelection filter = new AttributeSelection();  // package weka.filters.supervised.attribute!\n  CfsSubsetEval eval = new CfsSubsetEval();\n  GreedyStepwise search = new GreedyStepwise();\n  search.setSearchBackwards(true);\n  filter.setEvaluator(eval);\n  filter.setSearch(search);\n  filter.setInputFormat(data);\n  // generate new data\n  Instances newData = Filter.useFilter(data, filter);\n  System.out.println(newData);",
            "title": "Filter"
        },
        {
            "location": "/use_weka_in_your_java_code/#low-level",
            "text": "If neither the meta-classifier nor filter approach is suitable for your purposes, you can use the attribute selection classes themselves.    Instances data = ...  // from somewhere\n  AttributeSelection attsel = new AttributeSelection();  // package weka.attributeSelection!\n  CfsSubsetEval eval = new CfsSubsetEval();\n  GreedyStepwise search = new GreedyStepwise();\n  search.setSearchBackwards(true);\n  attsel.setEvaluator(eval);\n  attsel.setSearch(search);\n  attsel.SelectAttributes(data);\n  // obtain the attribute indices that were selected\n  int[] indices = attsel.selectedAttributes();\n  System.out.println(Utils.arrayToString(indices));",
            "title": "Low-level"
        },
        {
            "location": "/use_weka_in_your_java_code/#note-on-randomization",
            "text": "Most machine learning schemes, like classifiers and clusterers, are susceptible to the ordering of the data. Using a different seed for randomizing the data will most likely produce a different result. For example, the Explorer, or a classifier/clusterer run from the command line, uses only a seeded  java.util.Random  number generator, whereas the  weka.core.Instances.getRandomNumberGenerator(int)  (which the  WekaDemo.java  uses) also takes the data into account for seeding. Unless one runs 10-fold cross-validation 10 times and averages the results, one will most likely get different results.",
            "title": "Note on randomization"
        },
        {
            "location": "/use_weka_in_your_java_code/#see-also",
            "text": "Weka Examples  - pointer to collection of example classes  Databases  - for more information about using databases in Weka (includes ODBC, e.g., for MS Access)  [[weka_experiment_DatabaseUtils.props|weka/experiment/DatabaseUtils.props]] - the database setup file  Generating cross-validation folds (Java approach)  - in case you want to run 10-fold cross-validation manually  [[Generating classifier evaluation output manually]] - if you want to generate some of the evaluation statistics output manually  Creating Instances on-the-fly  - explains how to generate a  weka.core.Instances  object from scratch  [[Save Instances to an ARFF File]] - shows how to output a dataset  [[Using the Experiment API]]",
            "title": "See also"
        },
        {
            "location": "/use_weka_in_your_java_code/#examples",
            "text": "The following are a few sample classes for using various parts of the Weka API:    WekaDemo.java  ( stable ,  developer ) - little demo class that loads data from a file, runs it through a filter and trains/evaluates a classifier    ClusteringDemo.java  ( stable ,  developer ) - a basic example for using the clusterer API    ClassesToClusters.java  ( stable ,  developer ) - performs a  classes to clusters  evaluation like in the Explorer    AttributeSelectionTest.java  ( stable ,  developer ) - example code for using the attribute selection API    M5PExample.java  ( stable ,  developer ) - example using M5P to obtain data from database, train model, serialize it to a file, and use this serialized model to make predictions again.    OptionsToCode.java  ( stable ,  developer ) - turns a Weka command line for a scheme with options into Java code, correctly escaping quotes and backslashes.    OptionTree.java  ( stable ,  developer ) - displays nested Weka options as tree.    IncrementalClassifier.java  ( stable ,  developer ) - Example class for how to train an incremental classifier (in this case,  weka.classifiers.bayes.NaiveBayesUpdateable ).    IncrementalClusterer.java  ( stable ,  developer ) - Example class for how to train an incremental clusterer (in this case,  weka.clusterers.Cobweb ).",
            "title": "Examples"
        },
        {
            "location": "/use_weka_in_your_java_code/#links",
            "text": "Weka API  Stable version  Developer version",
            "title": "Links"
        },
        {
            "location": "/weka_examples/",
            "text": "The \nWeka Examples\n collection is a comprehensive collection of examples for the different versions of Weka in the form of an \nANT\n project. You can access these examples as follows:\n\n\nSnapshots\n\n\nThrough \nsnapshots\n or releases (they contain a separate ZIP file with the \nANT\n project)\n\n\nSubversion\n\n\nThrough \nsubversion\n\n\n\n\n\n\nstable-3.8 version (3.8.x): \n\n\nhttps://svn.cms.waikato.ac.nz/svn/weka/branches/stable-3-8/wekaexamples/\n\n\n\n\n\n\ndeveloper version (3.9.x): \n\n\nhttps://svn.cms.waikato.ac.nz/svn/weka/trunk/wekaexamples/",
            "title": " Weka Examples"
        },
        {
            "location": "/weka_examples/#snapshots",
            "text": "Through  snapshots  or releases (they contain a separate ZIP file with the  ANT  project)",
            "title": "Snapshots"
        },
        {
            "location": "/weka_examples/#subversion",
            "text": "Through  subversion    stable-3.8 version (3.8.x):   https://svn.cms.waikato.ac.nz/svn/weka/branches/stable-3-8/wekaexamples/    developer version (3.9.x):   https://svn.cms.waikato.ac.nz/svn/weka/trunk/wekaexamples/",
            "title": "Subversion"
        },
        {
            "location": "/generating_cv_folds/",
            "text": "You have two choices of generating cross-validation folds:\n\n\n\n\nFilter approach\n - uses a \nbash\n script to generate the train/test pairs beforehand\n\n\nJava approach\n - to be used from within your own Java code, creates train/test pairs on the fly",
            "title": " Generating cross-validation folds"
        },
        {
            "location": "/generating_cv_folds_filter/",
            "text": "The filter \nRemoveFolds\n (package \nweka.filters.unsupervised.instance\n) can be used to generate the train/test splits used in cross-validation (for stratified folds, use \nweka.filters.supervised.instance.StratifiedRemoveFolds\n). The filter has to be used twice for each train/test split, first to generate the train set and then to obtain the test set.\n\n\nSince this is rather cumbersome by hand, one can also put this into a \nbash\n script:\n\n\n #!/bin/bash\n #\n # expects the weka.jar as first parameter and the datasets to work on as \n # second parameter.\n #\n # FracPete, 2007-04-10\n\n if [ ! $# -eq 2 ]\n then\n   echo\n   echo \"usage: folds.sh <weka.jar> <dataset>\"\n   echo\n   exit 1\n fi\n\n JAR=$1\n DATASET=$2\n FOLDS=10\n FILTER=weka.filters.unsupervised.instance.RemoveFolds\n SEED=1\n\n for ((i = 1; i <= $FOLDS; i++))\n do\n   echo \"Generating pair $i/$FOLDS...\"\n\n   OUTFILE=`echo $DATASET | sed s/\"\\.arff\"//g`\n   # train set\n   java -cp $JAR $FILTER -V -N $FOLDS -F $i -S $SEED -i $DATASET -o \"$OUTFILE-train-$i-of-$FOLDS.arff\"\n   # test set\n   java -cp $JAR $FILTER    -N $FOLDS -F $i -S $SEED -i $DATASET -o \"$OUTFILE-test-$i-of-$FOLDS.arff\"\n done\n\n\n\n\nThe script expects two parameters:\n\n\n\n\nthe \nweka.jar\n (or the path to the Weka classes)\n\n\nthe dataset to generate the train/test pairs from \n\n\n\n\nExample:\n\n\n ./folds.sh /some/where/weka.jar /some/where/else/dataset.arff\n\n\n\n\nThis example will create the train/test splits for a 10-fold cross-validation at the same location as the original dataset, i.e., in the directory \n/some/where/else/\n.\n\n\nDownloads\n\n\n\n\nfolds.sh",
            "title": " Generating cross-validation folds (Filter approach)"
        },
        {
            "location": "/generating_cv_folds_filter/#downloads",
            "text": "folds.sh",
            "title": "Downloads"
        },
        {
            "location": "/generating_cv_folds_java/",
            "text": "This article describes how to generate train/test splits for \ncross-validation\n using the Weka API directly. \n\n\nThe following variables are given:\n\n\n Instances data =  ...;   // contains the full dataset we wann create train/test sets from\n int seed = ...;          // the seed for randomizing the data\n int folds = ...;         // the number of folds to generate, >=2\n\n\n\n\nRandomize the data\n\n\nFirst, randomize your data:\n\n\n Random rand = new Random(seed);   // create seeded number generator\n randData = new Instances(data);   // create copy of original data\n randData.randomize(rand);         // randomize data with number generator\n\n\n\n\nIn case your data has a nominal class and you wanna perform stratified cross-validation:\n\n\n randData.stratify(folds);\n\n\n\n\nGenerate the folds\n\n\nSingle run\n\n\nNext thing that we have to do is creating the train and the test set:\n\n\n for (int n = 0; n < folds; n++) {\n   Instances train = randData.trainCV(folds, n, rand);\n   Instances test = randData.testCV(folds, n);\n\n   // further processing, classification, etc.\n   ...\n }\n\n\n\n\nNote:\n\n\n\n\nthe above code is used by the \nweka.filters.supervised.instance.StratifiedRemoveFolds\n filter\n\n\nthe \nweka.classifiers.Evaluation\n class and the Explorer/Experimenter would use this method for obtaining the train set:\n[[code format=\"java\"]]\n Instances train = randData.trainCV(folds, n, rand);\n[[code]]\n\n\n\n\nMultiple runs\n\n\nThe example above only performs one run of a cross-validation. In case you want to run 10 runs of 10-fold cross-validation, use the following loop:\n\n\n Instances data = ...;  // our dataset again, obtained from somewhere\n int runs = 10;\n for (int i = 0; i < runs; i++) {\n   seed = i+1;  // every run gets a new, but defined seed value\n\n   // see: randomize the data\n   ...\n\n   // see: generate the folds\n   ...\n }\n\n\n\n\nSee also\n\n\n\n\nUse Weka in your Java code\n - for general use of the Weka API\n\n\n\n\nDownloads\n\n\n\n\nCrossValidationSingleRun.java\n (\nstable\n, \ndeveloper\n) - simulates a single run of 10-fold cross-validation\n\n\nCrossValidationSingleRunVariant.java\n (\nstable\n, \ndeveloper\n) - simulates a single run of 10-fold cross-validation, but outputs the confusion matrices for each single train/test pair as well.\n\n\nCrossValidationMultipleRuns.java\n (\nstable\n, \ndeveloper\n) - simulates 10 runs of 10-fold cross-validation\n\n\nCrossValidationAddPrediction.java\n (\nstable\n, \ndeveloper\n) - simulates a single run of 10-fold cross-validation, but also adds the classification/distribution/error flag to the test data (uses the \nAddClassification\n filter)",
            "title": " Generating cross-validation folds (Java approach)"
        },
        {
            "location": "/generating_cv_folds_java/#randomize-the-data",
            "text": "First, randomize your data:   Random rand = new Random(seed);   // create seeded number generator\n randData = new Instances(data);   // create copy of original data\n randData.randomize(rand);         // randomize data with number generator  In case your data has a nominal class and you wanna perform stratified cross-validation:   randData.stratify(folds);",
            "title": "Randomize the data"
        },
        {
            "location": "/generating_cv_folds_java/#generate-the-folds",
            "text": "",
            "title": "Generate the folds"
        },
        {
            "location": "/generating_cv_folds_java/#single-run",
            "text": "Next thing that we have to do is creating the train and the test set:   for (int n = 0; n < folds; n++) {\n   Instances train = randData.trainCV(folds, n, rand);\n   Instances test = randData.testCV(folds, n);\n\n   // further processing, classification, etc.\n   ...\n }  Note:   the above code is used by the  weka.filters.supervised.instance.StratifiedRemoveFolds  filter  the  weka.classifiers.Evaluation  class and the Explorer/Experimenter would use this method for obtaining the train set:\n[[code format=\"java\"]]\n Instances train = randData.trainCV(folds, n, rand);\n[[code]]",
            "title": "Single run"
        },
        {
            "location": "/generating_cv_folds_java/#multiple-runs",
            "text": "The example above only performs one run of a cross-validation. In case you want to run 10 runs of 10-fold cross-validation, use the following loop:   Instances data = ...;  // our dataset again, obtained from somewhere\n int runs = 10;\n for (int i = 0; i < runs; i++) {\n   seed = i+1;  // every run gets a new, but defined seed value\n\n   // see: randomize the data\n   ...\n\n   // see: generate the folds\n   ...\n }",
            "title": "Multiple runs"
        },
        {
            "location": "/generating_cv_folds_java/#see-also",
            "text": "Use Weka in your Java code  - for general use of the Weka API",
            "title": "See also"
        },
        {
            "location": "/generating_cv_folds_java/#downloads",
            "text": "CrossValidationSingleRun.java  ( stable ,  developer ) - simulates a single run of 10-fold cross-validation  CrossValidationSingleRunVariant.java  ( stable ,  developer ) - simulates a single run of 10-fold cross-validation, but outputs the confusion matrices for each single train/test pair as well.  CrossValidationMultipleRuns.java  ( stable ,  developer ) - simulates 10 runs of 10-fold cross-validation  CrossValidationAddPrediction.java  ( stable ,  developer ) - simulates a single run of 10-fold cross-validation, but also adds the classification/distribution/error flag to the test data (uses the  AddClassification  filter)",
            "title": "Downloads"
        },
        {
            "location": "/generating_cv_folds/",
            "text": "You have two choices of generating cross-validation folds:\n\n\n\n\nFilter approach\n - uses a \nbash\n script to generate the train/test pairs beforehand\n\n\nJava approach\n - to be used from within your own Java code, creates train/test pairs on the fly",
            "title": " Generating cross-validation folds"
        },
        {
            "location": "/creating_arff_file/",
            "text": "The following code generates an \nInstances\n object and outputs it to stdout as \nARFF\n file.\n\n\nIt generates the following types of attributes:\n\n\n\n\nnumeric\n\n\nnominal\n\n\nstring\n\n\ndate\n\n\nrelational\n\n\n\n\nExample class \nAttTest\n:\n\n\n import weka.core.Attribute;\n import weka.core.FastVector;\n import weka.core.Instance;\n import weka.core.Instances;\n\n /**\n  * Generates a little ARFF file with different attribute types.\n  *\n  * @author FracPete\n  */\n public class AttTest {\n   public static void main(String[] args) throws Exception {\n     FastVector      atts;\n     FastVector      attsRel;\n     FastVector      attVals;\n     FastVector      attValsRel;\n     Instances       data;\n     Instances       dataRel;\n     double[]        vals;\n     double[]        valsRel;\n     int             i;\n\n     // 1. set up attributes\n     atts = new FastVector();\n     // - numeric\n     atts.addElement(new Attribute(\"att1\"));\n     // - nominal\n     attVals = new FastVector();\n     for (i = 0; i < 5; i++)\n       attVals.addElement(\"val\" + (i+1));\n     atts.addElement(new Attribute(\"att2\", attVals));\n     // - string\n     atts.addElement(new Attribute(\"att3\", (FastVector) null));\n     // - date\n     atts.addElement(new Attribute(\"att4\", \"yyyy-MM-dd\"));\n     // - relational\n     attsRel = new FastVector();\n     // -- numeric\n     attsRel.addElement(new Attribute(\"att5.1\"));\n     // -- nominal\n     attValsRel = new FastVector();\n     for (i = 0; i < 5; i++)\n       attValsRel.addElement(\"val5.\" + (i+1));\n     attsRel.addElement(new Attribute(\"att5.2\", attValsRel));\n     dataRel = new Instances(\"att5\", attsRel, 0);\n     atts.addElement(new Attribute(\"att5\", dataRel, 0));\n\n     // 2. create Instances object\n     data = new Instances(\"MyRelation\", atts, 0);\n\n     // 3. fill with data\n     // first instance\n     vals = new double[data.numAttributes()];\n     // - numeric\n     vals[0] = Math.PI;\n     // - nominal\n     vals[1] = attVals.indexOf(\"val3\");\n     // - string\n     vals[2] = data.attribute(2).addStringValue(\"This is a string!\");\n     // - date\n     vals[3] = data.attribute(3).parseDate(\"2001-11-09\");\n     // - relational\n     dataRel = new Instances(data.attribute(4).relation(), 0);\n     // -- first instance\n     valsRel = new double[2];\n     valsRel[0] = Math.PI + 1;\n     valsRel[1] = attValsRel.indexOf(\"val5.3\");\n     dataRel.add(new Instance(1.0, valsRel));\n     // -- second instance\n     valsRel = new double[2];\n     valsRel[0] = Math.PI + 2;\n     valsRel[1] = attValsRel.indexOf(\"val5.2\");\n     dataRel.add(new Instance(1.0, valsRel));\n     vals[4] = data.attribute(4).addRelation(dataRel);\n     // add\n     data.add(new Instance(1.0, vals));\n\n     // second instance\n     vals = new double[data.numAttributes()];  // important: needs NEW array!\n     // - numeric\n     vals[0] = Math.E;\n     // - nominal\n     vals[1] = attVals.indexOf(\"val1\");\n     // - string\n     vals[2] = data.attribute(2).addStringValue(\"And another one!\");\n     // - date\n     vals[3] = data.attribute(3).parseDate(\"2000-12-01\");\n     // - relational\n     dataRel = new Instances(data.attribute(4).relation(), 0);\n     // -- first instance\n     valsRel = new double[2];\n     valsRel[0] = Math.E + 1;\n     valsRel[1] = attValsRel.indexOf(\"val5.4\");\n     dataRel.add(new Instance(1.0, valsRel));\n     // -- second instance\n     valsRel = new double[2];\n     valsRel[0] = Math.E + 2;\n     valsRel[1] = attValsRel.indexOf(\"val5.1\");\n     dataRel.add(new Instance(1.0, valsRel));\n     vals[4] = data.attribute(4).addRelation(dataRel);\n     // add\n     data.add(new Instance(1.0, vals));\n\n     // 4. output data\n     System.out.println(data);\n   }\n }\n\n\n\n\nMissing values\n\n\nBy default, a new double array will be initialized with 0s. In case you want to be a value missing at a certain position, you have to explicitly set the \nmissing value\n via the \nmissingValue()\n method of the \nweka.core.Instance\n class. In Weka > 3.7.1 Instance is an interface, so \nmissingValue()\n moved into \nweka.core.Utils\n. In case you already have an existing \nweka.core.Instance\n object, then you use its \nsetMissing(int)\n method, which sets a missing value at the given position. Here are examples, which set the \nthird\n attribute to missing:\n\n\n\n\n\n\ndouble array:\n\n\ndouble[] vals = ...  // from somewhere, e.g., from AttTest.java example\nvals[2] = Instance.missingValue(); // or ... = Utils.missingValue() for Weka > 3.7.1\n\n\n\n\n\n\nweka.core.Instance\n object:\n\n\ndouble[] vals = ... // from somewhere, e.g., from AttTest.java example\nInstance inst = new Instance(1.0, vals);\ninst.setMissing(2);\n\n\n\n\n\n\nDownloads\n\n\n\n\nAttTest.java\n (\nstable\n, \ndeveloper\n) - the above class\n\n\n\n\nSee also\n\n\n\n\n[[Save Instances to an ARFF File]] - if you want to save the data to a file instead of printing it to stdout\n\n\n[[Adding attributes to a dataset]] - shows how to add attributes to an existing dataset\n\n\nARFF\n format",
            "title": " Creating an ARFF file"
        },
        {
            "location": "/creating_arff_file/#missing-values",
            "text": "By default, a new double array will be initialized with 0s. In case you want to be a value missing at a certain position, you have to explicitly set the  missing value  via the  missingValue()  method of the  weka.core.Instance  class. In Weka > 3.7.1 Instance is an interface, so  missingValue()  moved into  weka.core.Utils . In case you already have an existing  weka.core.Instance  object, then you use its  setMissing(int)  method, which sets a missing value at the given position. Here are examples, which set the  third  attribute to missing:    double array:  double[] vals = ...  // from somewhere, e.g., from AttTest.java example\nvals[2] = Instance.missingValue(); // or ... = Utils.missingValue() for Weka > 3.7.1    weka.core.Instance  object:  double[] vals = ... // from somewhere, e.g., from AttTest.java example\nInstance inst = new Instance(1.0, vals);\ninst.setMissing(2);",
            "title": "Missing values"
        },
        {
            "location": "/creating_arff_file/#downloads",
            "text": "AttTest.java  ( stable ,  developer ) - the above class",
            "title": "Downloads"
        },
        {
            "location": "/creating_arff_file/#see-also",
            "text": "[[Save Instances to an ARFF File]] - if you want to save the data to a file instead of printing it to stdout  [[Adding attributes to a dataset]] - shows how to add attributes to an existing dataset  ARFF  format",
            "title": "See also"
        },
        {
            "location": "/creating_instances/",
            "text": "see \nCreating an ARFF file",
            "title": " Creating Instances on-the-fly"
        },
        {
            "location": "/binarize_attribute/",
            "text": "Sometimes one wants to binarize a nominal attribute of a certain dataset by grouping all values except the one of interest together as a negation of this value. E.g., in the {{weather}} data the outlook attribute, where \nsunny\n is of interest and the other values, \nrainy\n and \novercast\n, are grouped together as \nnot-sunny\n.\n\n\nOriginal dataset:\n\n\n @relation weather\n\n @attribute outlook {sunny, overcast, rainy}\n @attribute temperature real\n @attribute humidity real\n @attribute windy {TRUE, FALSE}\n @attribute play {yes, no}\n\n @data\n sunny,85,85,FALSE,no\n sunny,80,90,TRUE,no\n overcast,83,86,FALSE,yes\n rainy,70,96,FALSE,yes\n rainy,68,80,FALSE,yes\n rainy,65,70,TRUE,no\n overcast,64,65,TRUE,yes\n sunny,72,95,FALSE,no\n sunny,69,70,FALSE,yes\n rainy,75,80,FALSE,yes\n sunny,75,70,TRUE,yes\n overcast,72,90,TRUE,yes\n overcast,81,75,FALSE,yes\n rainy,71,91,TRUE,no\n\n\n\n\nDesired output:\n\n\n @relation weather-sunny-and-not_sunny\n\n @attribute outlook {sunny,not_sunny}\n @attribute temperature numeric\n @attribute humidity numeric\n @attribute windy {TRUE,FALSE}\n @attribute play {yes,no}\n\n @data\n sunny,85,85,FALSE,no\n sunny,80,90,TRUE,no\n not_sunny,83,86,FALSE,yes\n not_sunny,70,96,FALSE,yes\n not_sunny,68,80,FALSE,yes\n not_sunny,65,70,TRUE,no\n not_sunny,64,65,TRUE,yes\n sunny,72,95,FALSE,no\n sunny,69,70,FALSE,yes\n not_sunny,75,80,FALSE,yes\n sunny,75,70,TRUE,yes\n not_sunny,72,90,TRUE,yes\n not_sunny,81,75,FALSE,yes\n not_sunny,71,91,TRUE,no\n\n\n\n\nThe Weka filter \nNominalToBinary\n cannot be used directly, since it generates a new attribute for each value of the nominal attribute. As a postprocessing step one could delete all the attributes that are of no interest, but this is quite cumbersome.\n\n\nThe \nBinarize.java\n class on the other hand generates directly several \nARFF\n out of a given one in the desired format.\n\n\nDownload\n\n\n\n\nBinarize.java\n (\nstable\n, \ndeveloper",
            "title": " Binarize Attribute"
        },
        {
            "location": "/binarize_attribute/#download",
            "text": "Binarize.java  ( stable ,  developer",
            "title": "Download"
        },
        {
            "location": "/arff_from_text_collections/",
            "text": "The following utility generates an \nARFF\n file from text documents in a given directory (download link is at the end of this article).\n\n\nThe stable/developer version of Weka offer this tool as the \nweka.core.converters.TextDirectoryLoader\n converter. This can be used as:\n\n\njava -cp <path to weka.jar> weka.core.converters.TextDirectoryLoader -dir .\n\n\n\n\nFor help just type:\n\n\njava -cp <path to weka.jar> weka.core.converters.TextDirectoryLoader\n\n\n\n\n/*\n *    TextDirectoryToArff.java\n *    Copyright (C) 2002 Richard Kirkby\n *\n *    This program is free software; you can redistribute it and/or modify\n *    it under the terms of the GNU General Public License as published by\n *    the Free Software Foundation; either version 2 of the License, or\n *    (at your option) any later version.\n *\n *    This program is distributed in the hope that it will be useful,\n *    but WITHOUT ANY WARRANTY; without even the implied warranty of\n *    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n *    GNU General Public License for more details.\n *\n *    You should have received a copy of the GNU General Public License\n *    along with this program; if not, write to the Free Software\n *    Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.\n */\n\nimport java.io.*;\nimport weka.core.*;\n\n/**\n * Builds an arff dataset from the documents in a given directory.\n * Assumes that the file names for the documents end with \".txt\".\n *\n * Usage:<p/>\n *\n * TextDirectoryToArff <directory path> <p/>\n *\n * @author Richard Kirkby (rkirkby at cs.waikato.ac.nz)\n * @version 1.0\n */\npublic class TextDirectoryToArff {\n\n  public Instances createDataset(String directoryPath) throws Exception {\n\n    FastVector atts = new FastVector(2);\n    atts.addElement(new Attribute(\"filename\", (FastVector) null));\n    atts.addElement(new Attribute(\"contents\", (FastVector) null));\n    Instances data = new Instances(\"text_files_in_\" + directoryPath, atts, 0);\n\n    File dir = new File(directoryPath);\n    String[] files = dir.list();\n    for (int i = 0; i < files.length; i++) {\n      if (files[i].endsWith(\".txt\")) {\n    try {\n      double[] newInst = new double[2];\n      newInst[0] = (double)data.attribute(0).addStringValue(files[i]);\n      File txt = new File(directoryPath + File.separator + files[i]);\n      InputStreamReader is;\n      is = new InputStreamReader(new FileInputStream(txt));\n      StringBuffer txtStr = new StringBuffer();\n      int c;\n      while ((c = is.read()) != -1) {\n        txtStr.append((char)c);\n      }\n      newInst[1] = (double)data.attribute(1).addStringValue(txtStr.toString());\n      data.add(new Instance(1.0, newInst));\n    } catch (Exception e) {\n      //System.err.println(\"failed to convert file: \" + directoryPath + File.separator + files[i]);\n    }\n      }\n    }\n    return data;\n  }\n\n  public static void main(String[] args) {\n\n    if (args.length == 1) {\n      TextDirectoryToArff tdta = new TextDirectoryToArff();\n      try {\n    Instances dataset = tdta.createDataset(args[0]);\n    System.out.println(dataset);\n      } catch (Exception e) {\n    System.err.println(e.getMessage());\n    e.printStackTrace();\n      }\n    } else {\n      System.out.println(\"Usage: java TextDirectoryToArff <directory name>\");\n    }\n  }\n}\n\n\n\n\nSee also\n\n\n\n\n[[Text categorization with Weka]]\n\n\n\n\nDownloads\n\n\n\n\nTextDirectoryToArff.java",
            "title": " ARFF files from Text Collections"
        },
        {
            "location": "/arff_from_text_collections/#see-also",
            "text": "[[Text categorization with Weka]]",
            "title": "See also"
        },
        {
            "location": "/arff_from_text_collections/#downloads",
            "text": "TextDirectoryToArff.java",
            "title": "Downloads"
        },
        {
            "location": "/adding_attributes_to_dataset/",
            "text": "The following example class adds a \nnominal\n and a \nnumeric\n attribute to the dataset identified by the filename given as first parameter. The second parameter defines whether the data is manipulated via the \nAdd\n filter (= \nfilter\n) or through the Weka API directly (= \njava\n).\n\n\nUsage:\n\n\n AddAttribute <file.arff> <filter|java>\n\n\n\n\nSource code:\n\n\n  import weka.core.*;\n  import weka.filters.Filter;\n  import weka.filters.unsupervised.attribute.Add;\n\n  import java.io.*;\n  import java.util.*;\n\n  /**\n   * Adds a nominal and a numeric attribute to the dataset provided as first\n   * parameter (and fills it with random values) and outputs the result to\n   * stdout. It's either done via the Add filter (first option \"filter\")\n   * or manual with Java (second option \"java\").\n   *\n   * Usage: AddAttribute &lt;file.arff&gt; &lt;filter|java&gt;\n   *\n   * @author FracPete (fracpete at waikato dot ac dot nz)\n   */\n  public class AddAttribute {\n    /**\n     * adds the attributes\n     *\n     * @param args    the commandline arguments\n     */\n    public static void main(String[] args) throws Exception {\n      if (args.length != 2) {\n        System.out.println(\"\\nUsage: AddAttribute <file.arff> <filter|java>\\n\");\n        System.exit(1);\n      }\n\n      // load dataset\n      Instances data = new Instances(new BufferedReader(new FileReader(args[0])));\n      Instances newData = null;\n\n      // filter or java?\n      if (args[1].equals(\"filter\")) {\n        Add filter;\n        newData = new Instances(data);\n        // 1. nominal attribute\n        filter = new Add();\n        filter.setAttributeIndex(\"last\");\n        filter.setNominalLabels(\"A,B,C,D\");\n        filter.setAttributeName(\"NewNominal\");\n        filter.setInputFormat(newData);\n        newData = Filter.useFilter(newData, filter);\n        // 2. numeric attribute\n        filter = new Add();\n        filter.setAttributeIndex(\"last\");\n        filter.setAttributeName(\"NewNumeric\");\n        filter.setInputFormat(newData);\n        newData = Filter.useFilter(newData, filter);\n      }\n      else if (args[1].equals(\"java\")) {\n        newData = new Instances(data);\n        // add new attributes\n        // 1. nominal\n        FastVector values = new FastVector(); /* FastVector is now deprecated. Users can use any java.util.List */\n        values.addElement(\"A\");               /* implementation now */\n        values.addElement(\"B\");\n        values.addElement(\"C\");\n        values.addElement(\"D\");\n        newData.insertAttributeAt(new Attribute(\"NewNominal\", values), newData.numAttributes());\n        // 2. numeric\n        newData.insertAttributeAt(new Attribute(\"NewNumeric\"), newData.numAttributes());\n      }\n      else {\n        System.out.println(\"\\nUsage: AddAttribute <file.arff> <filter|java>\\n\");\n        System.exit(2);\n      }\n\n      // random values\n      Random rand = new Random(1);\n      for (int i = 0; i < newData.numInstances(); i++) {\n        // 1. nominal\n        // index of labels A:0,B:1,C:2,D:3\n        newData.instance(i).setValue(newData.numAttributes() - 2, rand.nextInt(4));\n        // 2. numeric\n        newData.instance(i).setValue(newData.numAttributes() - 1, rand.nextDouble());\n      }\n\n      // output on stdout\n      System.out.println(newData);\n    }\n  }\n\n\n\n\nSee also\n\n\n\n\nCreating an ARFF file\n - explains the creation of all the different attribute types\n\n\nUse Weka in your Java code\n - for general usage of the Weka API\n\n\nSave Instances to an ARFF File\n - if you want to save the output to a file instead of printing them to stdout\n\n\n\n\nDownloads\n\n\n\n\nAddAttribute.java\n (\nstable\n, \ndeveloper\n)",
            "title": " Adding attributes to a dataset"
        },
        {
            "location": "/adding_attributes_to_dataset/#see-also",
            "text": "Creating an ARFF file  - explains the creation of all the different attribute types  Use Weka in your Java code  - for general usage of the Weka API  Save Instances to an ARFF File  - if you want to save the output to a file instead of printing them to stdout",
            "title": "See also"
        },
        {
            "location": "/adding_attributes_to_dataset/#downloads",
            "text": "AddAttribute.java  ( stable ,  developer )",
            "title": "Downloads"
        },
        {
            "location": "/save_instances_to_arff/",
            "text": "DataSink\n\n\nThe easiest way to save an \nweka.core.Instances\n object to a file is by using\nthe \nweka.core.converters.ConverterUtils.DataSink\n class.\n\n\n import weka.core.converters.ConverterUtils.DataSink;\n import weka.core.Instances;\n\n Instances dataset = ...\n String outputFilename = ...\n try {\n   DataSink.write(outputFilename, dataset);\n }\n catch (Exception e) {\n   System.err.println(\"Failed to save data to: \" + outputFilename);\n   e.printStackTrace();\n }\n\n\n\n\nConverter\n\n\nYou can use the \nArffSaver\n class (\nweka.core.converters.ArffSaver\n) for saving a \nweka.core.Instances\n object to a file.\n\n\nHere is the snippet :\n\n\n Instances dataSet = ...\n ArffSaver saver = new ArffSaver();\n saver.setInstances(dataSet);\n saver.setFile(new File(\"./data/test.arff\"));\n saver.writeBatch();\n\n\n\n\nNotes:\n \n\n\n\n\nusing the converter approach, one can easily swap the \nArffSaver\n with another saver, e.g., the \nCSVSaver\n to output the data in a different format.\n\n\nThe \nWeka Examples\n collection dedicates quite a few examples to the use of converters in the \nwekaexamples.core.converters\n package (\nstable\n, \ndeveloper\n)\n\n\n\n\nJava I/O\n\n\nYou can also save the \nweka.core.Instances\n object directly using Java I/O classes:\n\n\n import java.io.BufferedWriter;\n import java.io.FileWriter;\n ...\n Instances dataSet = ...\n BufferedWriter writer = new BufferedWriter(new FileWriter(\"./data/test.arff\"));\n writer.write(dataSet.toString());\n writer.flush();\n writer.close();\n\n\n\n\nNote:\n using the \ntoString()\n of the \nweka.core.Instances\n doesn't scale very well for large datasets, since the complete string has to fit into memory. It is best to use a converter, as described in the previous section, which uses an incremental approach for writing the dataset to disk.",
            "title": " Save Instances to an ARFF File"
        },
        {
            "location": "/save_instances_to_arff/#datasink",
            "text": "The easiest way to save an  weka.core.Instances  object to a file is by using\nthe  weka.core.converters.ConverterUtils.DataSink  class.   import weka.core.converters.ConverterUtils.DataSink;\n import weka.core.Instances;\n\n Instances dataset = ...\n String outputFilename = ...\n try {\n   DataSink.write(outputFilename, dataset);\n }\n catch (Exception e) {\n   System.err.println(\"Failed to save data to: \" + outputFilename);\n   e.printStackTrace();\n }",
            "title": "DataSink"
        },
        {
            "location": "/save_instances_to_arff/#converter",
            "text": "You can use the  ArffSaver  class ( weka.core.converters.ArffSaver ) for saving a  weka.core.Instances  object to a file.  Here is the snippet :   Instances dataSet = ...\n ArffSaver saver = new ArffSaver();\n saver.setInstances(dataSet);\n saver.setFile(new File(\"./data/test.arff\"));\n saver.writeBatch();  Notes:     using the converter approach, one can easily swap the  ArffSaver  with another saver, e.g., the  CSVSaver  to output the data in a different format.  The  Weka Examples  collection dedicates quite a few examples to the use of converters in the  wekaexamples.core.converters  package ( stable ,  developer )",
            "title": "Converter"
        },
        {
            "location": "/save_instances_to_arff/#java-io",
            "text": "You can also save the  weka.core.Instances  object directly using Java I/O classes:   import java.io.BufferedWriter;\n import java.io.FileWriter;\n ...\n Instances dataSet = ...\n BufferedWriter writer = new BufferedWriter(new FileWriter(\"./data/test.arff\"));\n writer.write(dataSet.toString());\n writer.flush();\n writer.close();  Note:  using the  toString()  of the  weka.core.Instances  doesn't scale very well for large datasets, since the complete string has to fit into memory. It is best to use a converter, as described in the previous section, which uses an incremental approach for writing the dataset to disk.",
            "title": "Java I/O"
        },
        {
            "location": "/generating_roc_curve/",
            "text": "The following little Java class trains a NaiveBayes classifier with a dataset provided by the user and displays the ROC curve for the first class label.\n\n\nSource code:\n\n\n import java.awt.*;\n import java.io.*;\n import java.util.*;\n import javax.swing.*;\n import weka.core.*;\n import weka.classifiers.*;\n import weka.classifiers.bayes.NaiveBayes;\n import weka.classifiers.evaluation.Evaluation;\n import weka.classifiers.evaluation.ThresholdCurve;\n import weka.gui.visualize.*;\n\n /**\n   * Generates and displays a ROC curve from a dataset. Uses a default\n   * NaiveBayes to generate the ROC data.\n   *\n   * @author FracPete\n   */\n public class GenerateROC {\n\n   /**\n    * takes one argument: dataset in ARFF format (expects class to\n    * be last attribute)\n    */\n   public static void main(String[] args) throws Exception {\n     // load data\n     Instances data = new Instances(\n                           new BufferedReader(\n                             new FileReader(args[0])));\n     data.setClassIndex(data.numAttributes() - 1);\n\n     // train classifier\n     Classifier cl = new NaiveBayes();\n     Evaluation eval = new Evaluation(data);\n     eval.crossValidateModel(cl, data, 10, new Random(1));\n\n     // generate curve\n     ThresholdCurve tc = new ThresholdCurve();\n     int classIndex = 0;\n     Instances result = tc.getCurve(eval.predictions(), classIndex);\n\n     // plot curve\n     ThresholdVisualizePanel vmc = new ThresholdVisualizePanel();\n     vmc.setROCString(\"(Area under ROC = \" +\n         Utils.doubleToString(tc.getROCArea(result), 4) + \")\");\n     vmc.setName(result.relationName());\n     PlotData2D tempd = new PlotData2D(result);\n     tempd.setPlotName(result.relationName());\n     tempd.addInstanceNumberAttribute();\n     // specify which points are connected\n     boolean[] cp = new boolean[result.numInstances()];\n     for (int n = 1; n < cp.length; n++)\n       cp[n] = true;\n     tempd.setConnectPoints(cp);\n     // add plot\n     vmc.addPlot(tempd);\n\n     // display curve\n     String plotName = vmc.getName();\n     final javax.swing.JFrame jf =\n       new javax.swing.JFrame(\"Weka Classifier Visualize: \"+plotName);\n     jf.setSize(500,400);\n     jf.getContentPane().setLayout(new BorderLayout());\n     jf.getContentPane().add(vmc, BorderLayout.CENTER);\n     jf.addWindowListener(new java.awt.event.WindowAdapter() {\n       public void windowClosing(java.awt.event.WindowEvent e) {\n       jf.dispose();\n       }\n     });\n     jf.setVisible(true);\n   }\n }\n\n\n\n\nSee also\n\n\n\n\nROC curves\n\n\nVisualizing ROC curve\n\n\nPlotting multiple ROC curves\n\n\n\n\nDownloads\n\n\n\n\nGenerateROC.java\n (\nstable\n, \ndeveloper\n)",
            "title": " Generating ROC curve"
        },
        {
            "location": "/generating_roc_curve/#see-also",
            "text": "ROC curves  Visualizing ROC curve  Plotting multiple ROC curves",
            "title": "See also"
        },
        {
            "location": "/generating_roc_curve/#downloads",
            "text": "GenerateROC.java  ( stable ,  developer )",
            "title": "Downloads"
        },
        {
            "location": "/visualizing_roc_curve/",
            "text": "The following class lets you display a previously saved [[ROC curves|ROC curve]], which also displays the \nAUC\n.\n\n\nIf you don't need the \nAUC\n, then you can also use this command to display the curve:\n\n\n java [CLASSPATH|-classpath <your-classpath>] weka.gui.visualize.VisualizePanel <file>\n\n\n\n\nSource code:\n\n\n  import java.awt.*;\n  import java.io.*;\n  import javax.swing.*;\n  import weka.core.*;\n  import weka.classifiers.evaluation.*;\n  import weka.gui.visualize.*;\n\n  /**\n   * Visualizes a previously saved ROC curve. Code taken from the \n   * <code>weka.gui.explorer.ClassifierPanel</code> - involved methods:\n   * <ul>\n   *    <li>visualize(String,int,int)</li>\n   *    </li>visualizeClassifierErrors(VisualizePanel)</li>\n   * </ul>\n   *\n   * @author FracPete\n   */\n  public class VisualizeROC {\n\n    /**\n     * takes one argument: previously saved ROC curve data (ARFF file)\n     */\n    public static void main(String[] args) throws Exception {\n      Instances result = new Instances(\n                            new BufferedReader(\n                              new FileReader(args[0])));\n      result.setClassIndex(result.numAttributes() - 1);\n      ThresholdCurve tc = new ThresholdCurve();\n      // method visualize\n      ThresholdVisualizePanel vmc = new ThresholdVisualizePanel();\n      vmc.setROCString(\"(Area under ROC = \" + \n          Utils.doubleToString(tc.getROCArea(result), 4) + \")\");\n      vmc.setName(result.relationName());\n      PlotData2D tempd = new PlotData2D(result);\n      tempd.setPlotName(result.relationName());\n      tempd.addInstanceNumberAttribute();\n      // specify which points are connected\n      boolean[] cp = new boolean[result.numInstances()];\n      for (int n = 1; n < cp.length; n++)\n        cp[n] = true;\n      tempd.setConnectPoints(cp);\n      // add plot\n      vmc.addPlot(tempd);\n      // method visualizeClassifierErrors\n      String plotName = vmc.getName(); \n      final javax.swing.JFrame jf = \n        new javax.swing.JFrame(\"Weka Classifier Visualize: \"+plotName);\n      jf.setSize(500,400);\n      jf.getContentPane().setLayout(new BorderLayout());\n\n      jf.getContentPane().add(vmc, BorderLayout.CENTER);\n      jf.addWindowListener(new java.awt.event.WindowAdapter() {\n        public void windowClosing(java.awt.event.WindowEvent e) {\n        jf.dispose();\n        }\n      });\n\n      jf.setVisible(true);\n    }\n  }\n\n\n\n\nSee also\n\n\n\n\nROC curves\n\n\nPlotting multiple ROC curves\n - also contains a Java example of plotting multiple ROC curves in a single plot\n\n\n\n\nDownloads\n\n\n\n\nVisualizeROC.java\n (\nstable\n, \ndeveloper\n)",
            "title": " Visualizing ROC curve"
        },
        {
            "location": "/visualizing_roc_curve/#see-also",
            "text": "ROC curves  Plotting multiple ROC curves  - also contains a Java example of plotting multiple ROC curves in a single plot",
            "title": "See also"
        },
        {
            "location": "/visualizing_roc_curve/#downloads",
            "text": "VisualizeROC.java  ( stable ,  developer )",
            "title": "Downloads"
        },
        {
            "location": "/serialization/",
            "text": "Serialization\n is the process of saving an object in a persistent form, e.g., on the harddisk as a bytestream. \nDeserialization\n is the process in the opposite direction, creating an object from a persistently saved data structure.\nIn \nJava\n, an object can be serialized if it imports the \njava.io.Serializable\n interface. \n\n\nMembers of an object that are not supposed to be serialized, need to be prefixed with the keyword \ntransient\n.\n\n\nIn the following you'll find some Java code snippets for serializing and deserializing a \nJ48\n classifier. Of course, serialization is not limited to classifiers. Most schemes in Weka, like clusterers and filters, are also serializable.\n\n\nSerializing\n\n\nHere we create a J48 classifier \ncls\n, train it with a dataset \n/some/where/data.arff\n, and save the built model to a file \n/some/where/j48.model\n.\n\n\n // create J48\n Classifier cls = new J48();\n\n // train\n Instances inst = new Instances(\n                    new BufferedReader(\n                      new FileReader(\"/some/where/data.arff\")));\n inst.setClassIndex(inst.numAttributes() - 1);\n cls.buildClassifier(inst);\n\n // serialize model\n ObjectOutputStream oos = new ObjectOutputStream(\n                            new FileOutputStream(\"/some/where/j48.model\"));\n oos.writeObject(cls);\n oos.flush();\n oos.close();\n\n\n\n\nIf you use the \nSerializationHelper\n class, then this shrinks to:\n\n\n // serialize model\n weka.core.SerializationHelper.write(\"/some/where/j48.model\", cls);\n\n\n\n\nDeserializing\n\n\nHere the previously saved model is deserialized as \ncls\n and again available for classification.\n\n\n // deserialize model\n ObjectInputStream ois = new ObjectInputStream(\n                           new FileInputStream(\"/some/where/j48.model\"));\n Classifier cls = (Classifier) ois.readObject();\n ois.close();\n\n\n\n\nOr, with the \nSerializationHelper\n class:\n\n\n // deserialize model\n Classifier cls = (Classifier) weka.core.SerializationHelper.read(\"/some/where/j48.model\");\n\n\n\n\nSerialization in Weka\n\n\nThe Explorer serializes the classifier and the training header together. This makes it easy to test whether a dataset is compatible with the dataset the classifier was trained with. The commandline option \n-d \n of the \ndeveloper version\n stores the training header as well.\n\n\nIn order to read serialized models that contain the header information as well, you can use the \nreadAll\n method of the \nweka.core.SerializationHelper\n. For serializing models with their datasets, use \nwriteAll\n.\n\n\nSee also\n\n\n\n\nUse Weka in your Java code\n\n\n\n\nLinks\n\n\n\n\nJava Serialization Specifications",
            "title": " Serialization"
        },
        {
            "location": "/serialization/#serializing",
            "text": "Here we create a J48 classifier  cls , train it with a dataset  /some/where/data.arff , and save the built model to a file  /some/where/j48.model .   // create J48\n Classifier cls = new J48();\n\n // train\n Instances inst = new Instances(\n                    new BufferedReader(\n                      new FileReader(\"/some/where/data.arff\")));\n inst.setClassIndex(inst.numAttributes() - 1);\n cls.buildClassifier(inst);\n\n // serialize model\n ObjectOutputStream oos = new ObjectOutputStream(\n                            new FileOutputStream(\"/some/where/j48.model\"));\n oos.writeObject(cls);\n oos.flush();\n oos.close();  If you use the  SerializationHelper  class, then this shrinks to:   // serialize model\n weka.core.SerializationHelper.write(\"/some/where/j48.model\", cls);",
            "title": "Serializing"
        },
        {
            "location": "/serialization/#deserializing",
            "text": "Here the previously saved model is deserialized as  cls  and again available for classification.   // deserialize model\n ObjectInputStream ois = new ObjectInputStream(\n                           new FileInputStream(\"/some/where/j48.model\"));\n Classifier cls = (Classifier) ois.readObject();\n ois.close();  Or, with the  SerializationHelper  class:   // deserialize model\n Classifier cls = (Classifier) weka.core.SerializationHelper.read(\"/some/where/j48.model\");",
            "title": "Deserializing"
        },
        {
            "location": "/serialization/#serialization-in-weka",
            "text": "The Explorer serializes the classifier and the training header together. This makes it easy to test whether a dataset is compatible with the dataset the classifier was trained with. The commandline option  -d   of the  developer version  stores the training header as well.  In order to read serialized models that contain the header information as well, you can use the  readAll  method of the  weka.core.SerializationHelper . For serializing models with their datasets, use  writeAll .",
            "title": "Serialization in Weka"
        },
        {
            "location": "/serialization/#see-also",
            "text": "Use Weka in your Java code",
            "title": "See also"
        },
        {
            "location": "/serialization/#links",
            "text": "Java Serialization Specifications",
            "title": "Links"
        },
        {
            "location": "/jupyter_notebooks/",
            "text": "Jupyter notebooks\n are extremely popular in the Python world,\nsimply because it is great to combine documentation and code in a visually appealing\nway. Great tool for teaching!\n\n\nThanks to the \nIJava kernel\n and the JDK 9+\n\nJShell\n feature, it is possible to run Java within Notebooks without compiling the\ncode now as well.\n\n\nInstallation on Linux\n\n\nThe following worked on Linux Mint 18.2:\n\n\n\n\n\n\ncreate a directory called \nweka-notebooks\n\n\nmkdir weka-notebooks\n\n\n\n\n\n\nchange into the directory and create a Python virtual environment:\n\n\ncd weka-notebooks\nvirtualenv -p /usr/bin/python3.5 venv\n\n\n\n\n\n\ninstall Jupyter notebooks and its dependencies:\n\n\nvenv/bin/pip install jupyter\n\n\n\n\n\n\nthen download the latest IJava \nrelease\n (at time of writing, this was \n1.20\n) into this directory\n\n\n\n\n\n\nunzip the IJava archive:\n\n\nunzip -q ijava*.zip\n\n\n\n\n\n\ninstall the Java kernel into the virtual environment, using the IJava installer:\n\n\nvenv/bin/python install.py --sys-prefix\n\n\n\n\n\n\nafter that, fire up Jupyter using:\n\n\nvenv/bin/jupyter-notebook\n\n\n\n\n\n\nnow you can create new (Java) notebooks!\n\n\n\n\n\n\nInstallation on Windows (using anaconda)\n\n\n\n\nopen a command prompt\n\n\n\n\ncreate a new environment using anaconda (e.g., for Python 3.5)\n\n\nconda create -n py35-ijava python=3.5\n\n\n\n\n\n\nactivate environment \n\n\nactivate py35-ijava\n\n\n\n\n\n\ninstall Jupyter\n\n\npip install jupyter\n\n\n\n\n\n\ndownload the latest IJava \nrelease\n (at time of writing, this was \n1.20\n)\n\n\n\n\nunzip the IJava release (e.g., with your File browser or 7-Zip)\n\n\n\n\nchange into the directory where you extracted the release, containing the \ninstall.py\n, e.g.:\n\n\ncd C:\\Users\\fracpete\\Downloads\\ijava-1.2.0\n\n\n\n\n\n\ninstall the kernel\n\n\npython install.py --sys-prefix\n\n\n\n\n\n\nstart Jupyter\n\n\njupyter-notebook\n\n\n\n\n\n\nnow you can create new (Java) notebooks!",
            "title": " Jupyter Notebooks"
        },
        {
            "location": "/jupyter_notebooks/#installation-on-linux",
            "text": "The following worked on Linux Mint 18.2:    create a directory called  weka-notebooks  mkdir weka-notebooks    change into the directory and create a Python virtual environment:  cd weka-notebooks\nvirtualenv -p /usr/bin/python3.5 venv    install Jupyter notebooks and its dependencies:  venv/bin/pip install jupyter    then download the latest IJava  release  (at time of writing, this was  1.20 ) into this directory    unzip the IJava archive:  unzip -q ijava*.zip    install the Java kernel into the virtual environment, using the IJava installer:  venv/bin/python install.py --sys-prefix    after that, fire up Jupyter using:  venv/bin/jupyter-notebook    now you can create new (Java) notebooks!",
            "title": "Installation on Linux"
        },
        {
            "location": "/jupyter_notebooks/#installation-on-windows-using-anaconda",
            "text": "open a command prompt   create a new environment using anaconda (e.g., for Python 3.5)  conda create -n py35-ijava python=3.5    activate environment   activate py35-ijava    install Jupyter  pip install jupyter    download the latest IJava  release  (at time of writing, this was  1.20 )   unzip the IJava release (e.g., with your File browser or 7-Zip)   change into the directory where you extracted the release, containing the  install.py , e.g.:  cd C:\\Users\\fracpete\\Downloads\\ijava-1.2.0    install the kernel  python install.py --sys-prefix    start Jupyter  jupyter-notebook    now you can create new (Java) notebooks!",
            "title": "Installation on Windows (using anaconda)"
        },
        {
            "location": "/writing_classifier/",
            "text": "In case you have a flash idea for a new classifier and want to write one for Weka, this HOWTO will help you developing it. \n\n\nThe Mindmap (\nBuild_classifier.pdf\n, produced with \nFreeMind\n) helps you decide from which base classifier to start, what methods are to be implemented and general guidelines.\n\n\nThe base classifiers are all located in the following package:\n\n\n weka.classifiers\n\n\n\n\nNote:\n This is also covered in chapter \nExtending WEKA\n of the WEKA manual.\n\n\nPackages\n\n\nA few comments about the different classifier sub-packages:\n\n\n\n\nbayes\n - contains bayesian classifiers, e.g. NaiveBayes\n\n\nevaluation\n - classes related to evaluation, e.g., cost matrix\n\n\nfunctions\n - e.g., Support Vector Machines, regression algorithms, neural nets\n\n\nlazy\n - no \noffline\n learning, that is done during runtime, e.g., k-NN\n\n\nmeta\n - Meta classifiers that use a \nbase\n classifier as input, e.g., boosting or bagging\n\n\nmi\n - classifiers that handle multi-instance data\n\n\nmisc\n - various classifiers that don't fit in any another category\n\n\nrules\n - rule-based classifiers, e.g. ZeroR\n\n\ntrees\n - tree classifiers, like decision trees\n\n\n\n\nCoding\n\n\nIn the following you'll find notes about certain implementation parts listed in the Mindmap, which need a bit more explanation.\n\n\nRandom number generators\n\n\nIn order to get repeatable experiments, one is not allowed to use \nunseeded\n random number generators like \nMath.random()\n. Instead, one has to instantiate a \njava.util.Random\n object in the \nbuildClassifier(Instances)\n method with a specific seed value. The seed value can be user supplied, of course, which all the \nRandomizable...\n abstract classifiers already implement.\n\n\nCapabilities\n\n\nIn old versions of Weka (up to version 3.5.2), all classifiers could handle basically every kind of data by default, unless they were throwing an Exception (in the \nbuildClassifier(Instances)\n method). Since this behavior makes it cumbersome to introduce new attribute types, for instance (\nall\n classifiers have to be modified, which can't handle the new attribute type!), the general \nCapabilities\n were introduced.\n\n\nBase-classifier\n\n\nNormal classifiers only state what kind of attributes and what kind of classes they can handle.\n\n\nThe \ngetCapabilities()\n method of \nweka.classifiers.trees.RandomTree\n, for instance, looks like this:\n\n\n  public Capabilities getCapabilities() {\n    Capabilities result = super.getCapabilities();   // returns the object from weka.classifiers.Classifier\n\n    // attributes\n    result.enable(Capability.NOMINAL_ATTRIBUTES);\n    result.enable(Capability.NUMERIC_ATTRIBUTES);\n    result.enable(Capability.DATE_ATTRIBUTES);\n    result.enable(Capability.MISSING_VALUES);\n\n    // class\n    result.enable(Capability.NOMINAL_CLASS);\n    result.enable(Capability.MISSING_CLASS_VALUES);\n\n    return result;\n  }\n\n\n\n\nSpecial cases:\n\n\n\n\n\n\nincremental classifiers\n - By default, at least 1 instance has to be in the dataset, which does not apply for incremental classifiers. They have to lower the limit to \n0\n:\n\n\nresult.setMinimumNumberInstances(0);\n\n\n\n\n\n\nmulti-instance classifiers\n - The structure for multi-instance classifiers is always fixed to \nbagID,bag-data,class\n. To restrict the data to multi-instance data, add the following:\n\n\nresult.enable(Capability.ONLY_MULTIINSTANCE);\n\n\nMulti-instance classifiers also implement the following interface, which returns the Capabilities for the bag-data, which is just a \nrelational\n attribute (the reason why \nRELATIONAL_ATTRIBUTES\n has to be enabled):\n\n\nweka.core.MultiInstanceCapabilitiesHandler\n\n\n\n\n\n\nclusterer\n - Since clusterer don't need a class attribute like classifiers, the following Capability has to be specified to enable datasets without a class attribute (which is already done in the superclass \nweka.clusterers.Clusterer\n):\n\n\nresult.enable(Capability.NO_CLASS);\n\n\n\n\n\n\nMeta-classifier\n\n\nMeta-classifiers, by default, just return the capabilities of their base classifiers - in case of descendants of the \nweka.classifier.MultipleClassifiersCombiner\n, an \nAND\n over all the Capabilities of the base classifiers is returned.\n\n\nDue to this behavior, the Capabilities depend (normally) only on the currently configured base classifier(s). To \nsoften\n filtering for certain behavior, meta-classifiers also define so-called \nDependencies\n on a per-Capability basis. These dependencies tell the filter that even though a certain capability is not supported right now, it is possible that it will be supported with a different base classifier. By default, all Capabilities are initialized as Dependencies. \n\n\nweka.classifiers.meta.LogitBoost\n, e.g., is restricted to nominal classes. For that reason it disables the Dependencies for the class:\n\n\n    result.disableAllClasses();               // disable all class types\n    result.disableAllClassDependencies();     // no dependencies!\n    result.enable(Capability.NOMINAL_CLASS);  // only nominal classes allowed\n\n\n\n\nRelevant classes\n\n\n\n\nweka.core.Capabilities\n\n\nweka.core.CapabilitiesHandler\n\n\nweka.core.MultiInstanceCapabilitiesHandler\n (for multi-instance classifiers)\n\n\n\n\nPaper reference(s)\n\n\nIn order to make it easy to generate a bibliography of all the algorithms in Weka, the [[paper references]] located so far in the Javadoc were extracted and placed in the code.\n\n\nClasses that are based on some technical paper should implement the \nTechnicalInformationHandler\n interface and return a customized \nTechnicalInformation\n instance. The format used is based on \nBibTeX\n and the \nTechnicalInformation\n class can either return a plain text string via the \ntoString()\n method or a real \nBibTeX\n entry via the \ntoBibTex()\n method. This two methods are then used to automatically update the Javadoc (see \nJavadoc\n further down) of a class.\n\n\nRelevant classes:\n\n\n\n\nweka.core.TechnicalInformation\n\n\nweka.core.TechnicalInformationHandler\n\n\n\n\nJavadoc\n\n\nOpen-source software is only as good as its documentation. Hence, correct and up-to-date documentation is vital. So far most of the Javadoc was maintained manually, which made it hard to maintain, e.g., as soon as new options were added the Javadoc had to be changed accordingly, too. And that normally in several places:\n\n\n\n\nClass description\n\n\nsetOptions(String[])\n method\n\n\n\n\nOver the time the documentation got out of sync, which made it frustrating determining what options were really relevant and active. Since a lot of the documentation is already available in the code itself, the next logical step was to automate the Javadoc generation as much as possible. In the following you will see how to structure your Javadoc to reduce maintainance. For this purpose special comment tags are used, where the content in between will be replaced automatically by the classes listed below in the \nRelevant classes\n section.\n\n\nThe indentation of the generated Javadoc depends on the indentation of the \n&lt;\n of the starting comment tag.\n\n\nThis general layout order should be used for all classes:\n\n\n\n\n\n\nclass description\n Javadoc\n\n\n\n\nglobalinfo\n\n\nbibtex - \nif available\n\n\ncommandline options\n\n\n\n\n\n\n\n\nsetOptions\n Javadoc\n\n\n\n\ncommandline options\n\n\n\n\n\n\n\n\nGeneral\n\n\nThe general description for all classes displayed in the GenericObjectEditor was already in place, with the following method:\n\n\n globalInfo()\n\n\n\n\nThe return value can be placed in the Javadoc, surrounded by the following comment tags:\n\n\n <!-- globalinfo-start -->\n will be automatically replaced\n <!-- globalinfo-end -->\n\n\n\n\nPaper reference(s)\n\n\nIf available, the paper reference should also be listed in the Javadoc. Since the \nglobalInfo()\n method should return a short version of the reference, it is sufficient to list the full \nBibTeX\n documentation:\n\n\n <!-- technical-bibtex-start -->\n will be automatically replaced\n <!-- technical-bibtex-end -->\n\n\n\n\nIn case it is necessary to list the short, plain text version, too, one can use the following tags:\n\n\n <!-- technical-plaintext-start -->\n will be automatically replaced\n <!-- technical-plaintext-end -->\n\n\n\n\nOptions\n\n\nTo place the commandline options, use the following comment tags:\n\n\n <!-- options-start -->\n will be automatically replaced\n <!-- options-end -->\n\n\n\n\nRelevant classes\n\n\n\n\nweka.core.AllJavadoc\n - executes all Javadoc-producing classes\n\n\nweka.core.GlobalInfoJavadoc\n - updates the globalInfo tags\n\n\nweka.core.OptionHandlerJavadoc\n - updates the option tags\n\n\nweka.core.TechnicalInformationHandlerJavadoc\n - updates the technical tags (plain text and \nBibTeX\n)\n\n\n\n\nIntegration\n\n\nAfter finishing the coding stage, it's time to integrate your classifier in the Weka framework, i.e., to make it available in the Explorer, Experimenter, etc.\n\n\nThe [[GenericObjectEditor]] article shows you how to tell Weka where to find your classifier and therefore displaying it in the \nGenericObjectEditor\n.\n\n\nRevisions\n\n\nClassifiers also implement the \nweka.core.RevisionHandler\n interface. This provides the functionality of obtaining the \nSubversion\n revision from within Java. Classifiers that are not part of the official Weka distribution will have to implement the method \ngetRevision()\n as follows, which will return a dummy revision of \n1.0\n:\n\n\n  /**\n   * Returns the revision string.\n   * \n   * @return        the revision\n   */\n  public String getRevision() {\n    return RevisionUtils.extract(\"$Revision: 1.0 $\");\n  }\n\n\n\n\nTesting\n\n\nWeka provides already a test framework to ensure the basic functionality of a classifier. It is essential for the classifier to pass these tests.\n\n\nCommandline test\n\n\nGeneral\n\n\nUse the CheckClassifier class to test your classifier from commandline:\n\n\n weka.classifiers.CheckClassifier -W classname [-- additional parameters]\n\n\n\n\nOnly the following tests may have \"no\" as result, the others must have a \"no (OK error message)\" or \"yes\":\n\n\n\n\noptions\n\n\nupdateable classifier\n\n\nweighted instances classifier\n\n\nmulti-instance classifier\n\n\n\n\nOption handling\n\n\nAdditionally, check the \noption handling\n of your classifier with the following tool from commandline:\n\n\n weka.core.CheckOptionHandler -W classname [-- additional parameters]\n\n\n\n\nAll tests need to return \nyes\n.\n\n\nGenericObjectEditor\n\n\nThe \nCheckGOE\n class checks whether all the properties available in the GUI have a tooltip accompanying them and whether the \nglobalInfo()\n method is declared:\n\n\n weka.core.CheckGOE -W classname [-- additional parameters]\n\n\n\n\nAll tests, once again, need to return \nyes\n.\n\n\nSource code\n\n\nClassifiers that implement the \nweka.classifiers.Sourcable\n interface can output Java code of their model. In order to check the generated code, one should not only compile the code, but also test it with the following test class:\n\n\n weka.classifiers.CheckSource\n\n\n\n\nThis class takes the original Weka classifier, the generated code and the dataset used for generating the source code as parameters. It builds the Weka classifier on the dataset and compares the predictions, the ones from the Weka classifier and the ones from the generated source code, whether they are the same.\n\n\nHere's an example call for \nweka.classifiers.trees.Id3\n and the generated class \nweka.classifiers.WekaWrapper\n (it wraps the actual generated code in a pseudo-classifier):\n\n\n java weka.classifiers.CheckSource \\\n    -W \"weka.classifiers.trees.Id3\" \\\n    -S weka.classifiers.WekaWrapper \\\n    -t data.arff \\\n    -c last\n\n\n\n\nIt needs to return \nTests OK!\n.\n\n\nUnit tests\n\n\nIn order to make sure that your classifier applies to the Weka criteria, you should add your classifier to the \njunit\n unit test framework, i.e., by creating a Test class derived from \nAbstractClassifierTest\n. This class uses the \nCheckClassifier\n, \nCheckOptionHandler\n and \nCheckGOE\n class to run a battery of tests.\n\n\nHow to check out the unit test framework, you can find \nhere\n.\n\n\nSee also\n\n\n\n\n[[GenericObjectEditor|GenericObjectEditor/GenericPropertiesCreator]]\n\n\n[[Paper References|HOWTO extract paper references]]\n\n\n\n\nLinks\n\n\n\n\nBuild_classifier.pdf\n - MindMap for implementing a new classifier\n\n\nWeka API (\nstable\n, \ndeveloper\n)\n\n\nFreemind\n\n\njunit",
            "title": " Writing your own Classifier"
        },
        {
            "location": "/writing_classifier/#packages",
            "text": "A few comments about the different classifier sub-packages:   bayes  - contains bayesian classifiers, e.g. NaiveBayes  evaluation  - classes related to evaluation, e.g., cost matrix  functions  - e.g., Support Vector Machines, regression algorithms, neural nets  lazy  - no  offline  learning, that is done during runtime, e.g., k-NN  meta  - Meta classifiers that use a  base  classifier as input, e.g., boosting or bagging  mi  - classifiers that handle multi-instance data  misc  - various classifiers that don't fit in any another category  rules  - rule-based classifiers, e.g. ZeroR  trees  - tree classifiers, like decision trees",
            "title": "Packages"
        },
        {
            "location": "/writing_classifier/#coding",
            "text": "In the following you'll find notes about certain implementation parts listed in the Mindmap, which need a bit more explanation.",
            "title": "Coding"
        },
        {
            "location": "/writing_classifier/#random-number-generators",
            "text": "In order to get repeatable experiments, one is not allowed to use  unseeded  random number generators like  Math.random() . Instead, one has to instantiate a  java.util.Random  object in the  buildClassifier(Instances)  method with a specific seed value. The seed value can be user supplied, of course, which all the  Randomizable...  abstract classifiers already implement.",
            "title": "Random number generators"
        },
        {
            "location": "/writing_classifier/#capabilities",
            "text": "In old versions of Weka (up to version 3.5.2), all classifiers could handle basically every kind of data by default, unless they were throwing an Exception (in the  buildClassifier(Instances)  method). Since this behavior makes it cumbersome to introduce new attribute types, for instance ( all  classifiers have to be modified, which can't handle the new attribute type!), the general  Capabilities  were introduced.",
            "title": "Capabilities"
        },
        {
            "location": "/writing_classifier/#base-classifier",
            "text": "Normal classifiers only state what kind of attributes and what kind of classes they can handle.  The  getCapabilities()  method of  weka.classifiers.trees.RandomTree , for instance, looks like this:    public Capabilities getCapabilities() {\n    Capabilities result = super.getCapabilities();   // returns the object from weka.classifiers.Classifier\n\n    // attributes\n    result.enable(Capability.NOMINAL_ATTRIBUTES);\n    result.enable(Capability.NUMERIC_ATTRIBUTES);\n    result.enable(Capability.DATE_ATTRIBUTES);\n    result.enable(Capability.MISSING_VALUES);\n\n    // class\n    result.enable(Capability.NOMINAL_CLASS);\n    result.enable(Capability.MISSING_CLASS_VALUES);\n\n    return result;\n  }  Special cases:    incremental classifiers  - By default, at least 1 instance has to be in the dataset, which does not apply for incremental classifiers. They have to lower the limit to  0 :  result.setMinimumNumberInstances(0);    multi-instance classifiers  - The structure for multi-instance classifiers is always fixed to  bagID,bag-data,class . To restrict the data to multi-instance data, add the following:  result.enable(Capability.ONLY_MULTIINSTANCE);  Multi-instance classifiers also implement the following interface, which returns the Capabilities for the bag-data, which is just a  relational  attribute (the reason why  RELATIONAL_ATTRIBUTES  has to be enabled):  weka.core.MultiInstanceCapabilitiesHandler    clusterer  - Since clusterer don't need a class attribute like classifiers, the following Capability has to be specified to enable datasets without a class attribute (which is already done in the superclass  weka.clusterers.Clusterer ):  result.enable(Capability.NO_CLASS);",
            "title": "Base-classifier"
        },
        {
            "location": "/writing_classifier/#meta-classifier",
            "text": "Meta-classifiers, by default, just return the capabilities of their base classifiers - in case of descendants of the  weka.classifier.MultipleClassifiersCombiner , an  AND  over all the Capabilities of the base classifiers is returned.  Due to this behavior, the Capabilities depend (normally) only on the currently configured base classifier(s). To  soften  filtering for certain behavior, meta-classifiers also define so-called  Dependencies  on a per-Capability basis. These dependencies tell the filter that even though a certain capability is not supported right now, it is possible that it will be supported with a different base classifier. By default, all Capabilities are initialized as Dependencies.   weka.classifiers.meta.LogitBoost , e.g., is restricted to nominal classes. For that reason it disables the Dependencies for the class:      result.disableAllClasses();               // disable all class types\n    result.disableAllClassDependencies();     // no dependencies!\n    result.enable(Capability.NOMINAL_CLASS);  // only nominal classes allowed",
            "title": "Meta-classifier"
        },
        {
            "location": "/writing_classifier/#relevant-classes",
            "text": "weka.core.Capabilities  weka.core.CapabilitiesHandler  weka.core.MultiInstanceCapabilitiesHandler  (for multi-instance classifiers)",
            "title": "Relevant classes"
        },
        {
            "location": "/writing_classifier/#paper-references",
            "text": "In order to make it easy to generate a bibliography of all the algorithms in Weka, the [[paper references]] located so far in the Javadoc were extracted and placed in the code.  Classes that are based on some technical paper should implement the  TechnicalInformationHandler  interface and return a customized  TechnicalInformation  instance. The format used is based on  BibTeX  and the  TechnicalInformation  class can either return a plain text string via the  toString()  method or a real  BibTeX  entry via the  toBibTex()  method. This two methods are then used to automatically update the Javadoc (see  Javadoc  further down) of a class.  Relevant classes:   weka.core.TechnicalInformation  weka.core.TechnicalInformationHandler",
            "title": "Paper reference(s)"
        },
        {
            "location": "/writing_classifier/#javadoc",
            "text": "Open-source software is only as good as its documentation. Hence, correct and up-to-date documentation is vital. So far most of the Javadoc was maintained manually, which made it hard to maintain, e.g., as soon as new options were added the Javadoc had to be changed accordingly, too. And that normally in several places:   Class description  setOptions(String[])  method   Over the time the documentation got out of sync, which made it frustrating determining what options were really relevant and active. Since a lot of the documentation is already available in the code itself, the next logical step was to automate the Javadoc generation as much as possible. In the following you will see how to structure your Javadoc to reduce maintainance. For this purpose special comment tags are used, where the content in between will be replaced automatically by the classes listed below in the  Relevant classes  section.  The indentation of the generated Javadoc depends on the indentation of the  &lt;  of the starting comment tag.  This general layout order should be used for all classes:    class description  Javadoc   globalinfo  bibtex -  if available  commandline options     setOptions  Javadoc   commandline options",
            "title": "Javadoc"
        },
        {
            "location": "/writing_classifier/#general",
            "text": "The general description for all classes displayed in the GenericObjectEditor was already in place, with the following method:   globalInfo()  The return value can be placed in the Javadoc, surrounded by the following comment tags:   <!-- globalinfo-start -->\n will be automatically replaced\n <!-- globalinfo-end -->",
            "title": "General"
        },
        {
            "location": "/writing_classifier/#paper-references_1",
            "text": "If available, the paper reference should also be listed in the Javadoc. Since the  globalInfo()  method should return a short version of the reference, it is sufficient to list the full  BibTeX  documentation:   <!-- technical-bibtex-start -->\n will be automatically replaced\n <!-- technical-bibtex-end -->  In case it is necessary to list the short, plain text version, too, one can use the following tags:   <!-- technical-plaintext-start -->\n will be automatically replaced\n <!-- technical-plaintext-end -->",
            "title": "Paper reference(s)"
        },
        {
            "location": "/writing_classifier/#options",
            "text": "To place the commandline options, use the following comment tags:   <!-- options-start -->\n will be automatically replaced\n <!-- options-end -->",
            "title": "Options"
        },
        {
            "location": "/writing_classifier/#relevant-classes_1",
            "text": "weka.core.AllJavadoc  - executes all Javadoc-producing classes  weka.core.GlobalInfoJavadoc  - updates the globalInfo tags  weka.core.OptionHandlerJavadoc  - updates the option tags  weka.core.TechnicalInformationHandlerJavadoc  - updates the technical tags (plain text and  BibTeX )",
            "title": "Relevant classes"
        },
        {
            "location": "/writing_classifier/#integration",
            "text": "After finishing the coding stage, it's time to integrate your classifier in the Weka framework, i.e., to make it available in the Explorer, Experimenter, etc.  The [[GenericObjectEditor]] article shows you how to tell Weka where to find your classifier and therefore displaying it in the  GenericObjectEditor .",
            "title": "Integration"
        },
        {
            "location": "/writing_classifier/#revisions",
            "text": "Classifiers also implement the  weka.core.RevisionHandler  interface. This provides the functionality of obtaining the  Subversion  revision from within Java. Classifiers that are not part of the official Weka distribution will have to implement the method  getRevision()  as follows, which will return a dummy revision of  1.0 :    /**\n   * Returns the revision string.\n   * \n   * @return        the revision\n   */\n  public String getRevision() {\n    return RevisionUtils.extract(\"$Revision: 1.0 $\");\n  }",
            "title": "Revisions"
        },
        {
            "location": "/writing_classifier/#testing",
            "text": "Weka provides already a test framework to ensure the basic functionality of a classifier. It is essential for the classifier to pass these tests.",
            "title": "Testing"
        },
        {
            "location": "/writing_classifier/#commandline-test",
            "text": "",
            "title": "Commandline test"
        },
        {
            "location": "/writing_classifier/#general_1",
            "text": "Use the CheckClassifier class to test your classifier from commandline:   weka.classifiers.CheckClassifier -W classname [-- additional parameters]  Only the following tests may have \"no\" as result, the others must have a \"no (OK error message)\" or \"yes\":   options  updateable classifier  weighted instances classifier  multi-instance classifier",
            "title": "General"
        },
        {
            "location": "/writing_classifier/#option-handling",
            "text": "Additionally, check the  option handling  of your classifier with the following tool from commandline:   weka.core.CheckOptionHandler -W classname [-- additional parameters]  All tests need to return  yes .",
            "title": "Option handling"
        },
        {
            "location": "/writing_classifier/#genericobjecteditor",
            "text": "The  CheckGOE  class checks whether all the properties available in the GUI have a tooltip accompanying them and whether the  globalInfo()  method is declared:   weka.core.CheckGOE -W classname [-- additional parameters]  All tests, once again, need to return  yes .",
            "title": "GenericObjectEditor"
        },
        {
            "location": "/writing_classifier/#source-code",
            "text": "Classifiers that implement the  weka.classifiers.Sourcable  interface can output Java code of their model. In order to check the generated code, one should not only compile the code, but also test it with the following test class:   weka.classifiers.CheckSource  This class takes the original Weka classifier, the generated code and the dataset used for generating the source code as parameters. It builds the Weka classifier on the dataset and compares the predictions, the ones from the Weka classifier and the ones from the generated source code, whether they are the same.  Here's an example call for  weka.classifiers.trees.Id3  and the generated class  weka.classifiers.WekaWrapper  (it wraps the actual generated code in a pseudo-classifier):   java weka.classifiers.CheckSource \\\n    -W \"weka.classifiers.trees.Id3\" \\\n    -S weka.classifiers.WekaWrapper \\\n    -t data.arff \\\n    -c last  It needs to return  Tests OK! .",
            "title": "Source code"
        },
        {
            "location": "/writing_classifier/#unit-tests",
            "text": "In order to make sure that your classifier applies to the Weka criteria, you should add your classifier to the  junit  unit test framework, i.e., by creating a Test class derived from  AbstractClassifierTest . This class uses the  CheckClassifier ,  CheckOptionHandler  and  CheckGOE  class to run a battery of tests.  How to check out the unit test framework, you can find  here .",
            "title": "Unit tests"
        },
        {
            "location": "/writing_classifier/#see-also",
            "text": "[[GenericObjectEditor|GenericObjectEditor/GenericPropertiesCreator]]  [[Paper References|HOWTO extract paper references]]",
            "title": "See also"
        },
        {
            "location": "/writing_classifier/#links",
            "text": "Build_classifier.pdf  - MindMap for implementing a new classifier  Weka API ( stable ,  developer )  Freemind  junit",
            "title": "Links"
        },
        {
            "location": "/writing_filter/",
            "text": "Note:\n This is also covered in chapter \nExtending WEKA\n of the WEKA manual.\n\n\nPackages\n\n\nA few comments about the different filter sub-packages:\n\n\n\n\n\n\nsupervised\n - contains supervised filters, i.e., filters that take class distributions into account. Must implement the \nweka.filters.SupervisedFilter\n interface.\n\n\n\n\nattribute\n - filters that work column-wise.\n\n\ninstance\n - filters that work row-wise.\n\n\n\n\n\n\n\n\nunsupervised\n - contains unsupervised filters, i.e., they work without taking any class distributions into account. Must implement the \nweka.filters.UnsupervisedFilter\n interface.\n\n\n\n\nattribute\n - filters that work column-wise.\n\n\ninstance\n - filters that work row-wise.\n\n\n\n\n\n\n\n\nChoosing the superclass\n\n\nThe base filters and interfaces are all located in the following package:\n\n\n weka.filters\n\n\n\n\nOne can basically distinguish between two different kinds of filters:\n\n\n\n\nbatch filters\n - they need to see the whole dataset before they can start processing it, which they do in one go\n\n\nstream filters\n - they can start producing output right away and the data just passes through while being modified\n\n\n\n\nAll filters are derived from the abstract superclass \nweka.filters.Filter\n.\n\n\nTo speed up development, there are also the following abstract filters, depending on the kind of classifier you want to implement:\n\n\n\n\nweka.filters.SimpleBatchFilter\n\n\nweka.filters.SimpleStreamFilter\n\n\n\n\nThese filters simplify the rather general and complex framework introduced by the abstract superclass \nweka.filters.Filter\n. One only needs to implement a couple of abstract methods that will process the actual data and override, if necessary, a few existing methods for option handling.\n\n\nFilter\n\n\nImplementation\n\n\nThe following methods are of importance for the implementation of a filter and explained in detail further down:\n\n\n\n\ngetCapabilities()\n\n\nsetInputFormat(Instances)\n\n\ngetInputFormat()\n\n\nsetOutputFormat(Instances)\n\n\ngetOutputFormat()\n\n\ninput(Instance)\n\n\nbufferInput(Instance)\n\n\npush(Instance)\n\n\noutput()\n\n\nbatchFinished()\n\n\nflushInput()\n\n\ngetRevision()\n\n\n\n\nBut only the following ones need normally be modified:\n\n\n\n\ngetCapabilities()\n\n\nsetInputFormat(Instances)\n\n\ninput(Instance)\n\n\nbatchFinished()\n\n\ngetRevision()\n\n\n\n\ngetCapabilities()\n\n\nFilters implement the \nweka.core.CapabilitiesHandler\n interface like the classifiers. This method returns what kind of data the filter is able to process. Needs to be adapted for each individual filter.\n\n\nsetInputFormat(Instances)\n\n\nWith this format, the user tells the filter what format, i.e., attributes, the input data has. This method also tests, whether the filter can actually process this data. All older Weka versions or book branch versions need to check the data manually and throw fitting exceptions, e.g., not being able to handle String attributes.\n\n\nIf the output format of the filter, i.e., the new Instances header, can be determined based alone on this information, then the method should set the output format via \nsetOutputFormat(Instances)\n and return \ntrue\n, otherwise it has to return \nfalse\n.\n\n\ngetInputFormat()\n\n\nThis method returns an Instances object containing all currently buffered Instance objects from the input queue.\n\n\nsetOutputFormat(Instances)\n\n\nsetOutputFormat(Instances)\n defines the new Instances header for the output data. For filters that work on a row-basis, there shouldn't be any changes between the input and output format. But filters that work on attributes, e.g., removing, adding, modifying, will affect this format. This method must be called with the appropriate Instances object as parameter, since all Instance objects being processed will rely on the output format.\n\n\ngetOutputFormat()\n\n\nThis method returns the currently set Instances object that defines the output format. In case \nsetOutputFormat(Instances)\n hasn't been called yet, this method will return \nnull\n.\n\n\ninput(Instance)\n\n\nThe \ninput(Instance)\n method returns \ntrue\n if the given Instance can be processed straight away and can be collected immediately via the \noutput()\n method (after adding it to the output queue via \npush(Instance)\n, of course). This is also the case if the first batch of data has been processed and the instance belongs to the second batch. Via \nisFirstBatchDone()\n one can query whether this instance is still part of the first batch or of the second.\n\n\nIf the Instance cannot be processed immediately, e.g., the filter needs to collect all the data first before doing some calculations, then it needs to be buffered with \nbufferInput(Instance)\n until \nbatchFinished()\n is called.\n\n\nbufferInput(Instance)\n\n\nIn case an Instance cannot be processed immediately, one can use this method to buffer them in the input queue. All buffered Instance objects are available via the \ngetInputFormat()\n method.\n\n\npush(Instance)\n\n\npush(Instance)\n adds the given Instance to the output queue.\n\n\noutput()\n\n\nReturns the next Instance object from the output queue and removes it from there. In case there is no Instance available this method returns \nnull\n.\n\n\nbatchFinished()\n\n\nThe \nbatchFinished()\n method signifies the end of a dataset being pushed through the filter. In case of a filter that couldn't process the data of the first batch immediately, this is the place to determine what the output format will be (and set if via \nsetOutputFormat(Instances)\n) and process the actual data. The currently available data can be retrieved with the \ngetInputFormat()\n method. After processing the data, one needs to call \nflushInput()\n to remove all the pending input data.\n\n\nflushInput()\n\n\nflushInput()\n removes all buffered Instance objects from the input queue. This method must be called after all the Instance objects have been processed in the \nbatchFinished()\n method.\n\n\nOption handling\n\n\nIf the filter should be able to handle commandline options, then the weka.core.OptionHandler interface needs to be implemented. In addition to that, the following code should be added at the end of the \nsetOptions(String[])\n method:\n\n\n if (getInputFormat() != null)\n    setInputFormat(getInputFormat());\n\n\n\n\nThis will inform the filter about changes in the options and therefore reset it.\n\n\nExamples\n\n\nThe following examples are to illustrate the filter framework. \n\n\nNote:\n unseeded random number generators like \nMath.random()\n should never be used since they will produce different results in each run and repeatable results are essential in machine learning.\n\n\nBatchFilter\n\n\nThis simple batch filter adds a new attribute called //bla// at the end of the dataset. The rows of this attribute contain only the row's index in the data. Since the batch-filter need not see all the data before creating the output format, the \nsetInputFormat(Instances)\n sets the output format and returns \ntrue\n (indicating that the output format can be queried immediately). The \nbatchFinished()\n method performs the processing of all the data.\n\n\n  import weka.core.*;\n  import weka.core.Capabilities.*;\n\n  public class BatchFilter\n    extends Filter {\n\n    public String globalInfo() {\n      return   \"A batch filter that adds an additional attribute 'bla' at the end \"\n             + \"containing the index of the processed instance. The output format \"\n             + \"can be collected immediately.\";\n    }\n\n    public Capabilities getCapabilities() {\n      Capabilities result = super.getCapabilities();\n      result.enableAllAttributes();\n      result.enableAllClasses();\n      result.enable(Capability.NO_CLASS);  // filter doesn't need class to be set\n      return result;\n    }\n\n    public boolean setInputFormat(Instances instanceInfo) throws Exception {\n      super.setInputFormat(instanceInfo);\n\n      Instances outFormat = new Instances(instanceInfo, 0);\n      outFormat.insertAttributeAt(new Attribute(\"bla\"), outFormat.numAttributes());\n      setOutputFormat(outFormat);\n\n      return true;  // output format is immediately available\n    }\n\n    public boolean batchFinished() throws Exception {\n      if (getInputFormat() = null)\n        throw new NullPointerException(\"No input instance format defined\");\n\n      Instances inst = getInputFormat();\n      Instances outFormat = getOutputFormat();\n      for (int i = 0; i < inst.numInstances(); i++) {\n        double[] newValues = new double[outFormat.numAttributes()];\n        double[] oldValues = inst.instance(i).toDoubleArray();\n        System.arraycopy(oldValues, 0, newValues, 0, oldValues.length);\n        newValues[newValues.length - 1] = i;\n        push(new Instance(1.0, newValues));\n      }\n\n      flushInput();\n      m_NewBatch = true;\n      m_FirstBatchDone = true;\n      return (numPendingOutput() != 0);\n    }\n\n    public static void main(String[] args) {\n      runFilter(new BatchFilter(), args);\n    }\n  }\n\n\n\n\nBatchFilter2\n\n\nIn contrast to the first batch filter, this one here cannot determine the output format immediately (the number of instances in the first batch is part of the attribute name now). This is done in the \nbatchFinished()\n method.\n\n\n  import weka.core.*;\n  import weka.core.Capabilities.*;\n\n  public class BatchFilter2\n    extends Filter {\n\n    public String globalInfo() {\n      return   \"A batch filter that adds an additional attribute 'bla' at the end \"\n             + \"containing the index of the processed instance. The output format \"\n             + \"cannot be collected immediately.\";\n    }\n\n    public Capabilities getCapabilities() {\n      Capabilities result = super.getCapabilities();\n      result.enableAllAttributes();\n      result.enableAllClasses();\n      result.enable(Capability.NO_CLASS);  // filter doesn't need class to be set\n      return result;\n    }\n\n    public boolean batchFinished() throws Exception {\n      if (getInputFormat() = null)\n        throw new NullPointerException(\"No input instance format defined\");\n\n      // output format still needs to be set (depends on first batch of data)\n      if (!isFirstBatchDone()) {\n        Instances outFormat = new Instances(getInputFormat(), 0);\n        outFormat.insertAttributeAt(new Attribute(\n            \"bla-\" + getInputFormat().numInstances()), outFormat.numAttributes());\n        setOutputFormat(outFormat);\n      }\n\n      Instances inst = getInputFormat();\n      Instances outFormat = getOutputFormat();\n      for (int i = 0; i < inst.numInstances(); i++) {\n        double[] newValues = new double[outFormat.numAttributes()];\n        double[] oldValues = inst.instance(i).toDoubleArray();\n        System.arraycopy(oldValues, 0, newValues, 0, oldValues.length);\n        newValues[newValues.length - 1] = i;\n        push(new Instance(1.0, newValues));\n      }\n\n      flushInput();\n      m_NewBatch = true;\n      m_FirstBatchDone = true;\n\n      return (numPendingOutput() != 0);\n    }\n\n    public static void main(String[] args) {\n      runFilter(new BatchFilter2(), args);\n    }\n  }\n\n\n\n\nBatchFilter3\n\n\nAs soon as this batch filter's first batch is done, it can process Instance objects immediately in the \ninput(Instance)\n method. It adds a new attribute which contains just a random number, but the random number generator being used is seeded with the number of instances from the first batch.\n\n\n  import weka.core.*;\n  import weka.core.Capabilities.*;\n\n  import java.util.Random;\n\n  public class BatchFilter3\n    extends Filter {\n\n    protected int m_Seed;\n    protected Random m_Random;\n\n    public String globalInfo() {\n      return   \"A batch filter that adds an attribute 'bla' at the end \"\n             + \"containing a random number. The output format cannot be collected \"\n             + \"immediately.\";\n    }\n\n    public Capabilities getCapabilities() {\n      Capabilities result = super.getCapabilities();\n      result.enableAllAttributes();\n      result.enableAllClasses();\n      result.enable(Capability.NO_CLASS);  // filter doesn't need class to be set\n      return result;\n    }\n\n    public boolean input(Instance instance) throws Exception {\n      if (getInputFormat() = null)\n        throw new NullPointerException(\"No input instance format defined\");\n\n      if (isNewBatch()) {\n        resetQueue();\n        m_NewBatch = false;\n      }\n\n      if (isFirstBatchDone())\n        convertInstance(instance);\n      else\n        bufferInput(instance);\n\n      return isFirstBatchDone();\n    }\n\n    public boolean batchFinished() throws Exception {\n      if (getInputFormat() = null)\n        throw new NullPointerException(\"No input instance format defined\");\n\n      // output format still needs to be set (random number generator is seeded\n      // with number of instances of first batch)\n      if (!isFirstBatchDone()) {\n        m_Seed = getInputFormat().numInstances();\n        Instances outFormat = new Instances(getInputFormat(), 0);\n        outFormat.insertAttributeAt(new Attribute(\n            \"bla-\" + getInputFormat().numInstances()), outFormat.numAttributes());\n        setOutputFormat(outFormat);\n      }\n\n      Instances inst = getInputFormat();\n      for (int i = 0; i < inst.numInstances(); i++) {\n        convertInstance(inst.instance(i));\n      }\n\n      flushInput();\n      m_NewBatch = true;\n      m_FirstBatchDone = true;\n      m_Random = null;\n\n      return (numPendingOutput() != 0);\n    }\n\n    protected void convertInstance(Instance instance) {\n      if (m_Random = null)\n        m_Random = new Random(m_Seed);\n      double[] newValues = new double[instance.numAttributes() + 1];\n      double[] oldValues = instance.toDoubleArray();\n      newValues[newValues.length - 1] = m_Random.nextInt();\n      System.arraycopy(oldValues, 0, newValues, 0, oldValues.length);\n      push(new Instance(1.0, newValues));\n    }\n\n    public static void main(String[] args) {\n      runFilter(new BatchFilter3(), args);\n    }\n  }\n\n\n\n\nStreamFilter\n\n\nThis stream filter adds a random number at the end of each instance of the input data. Since this doesn't rely on having access to the full data of the first batch, the output format is accessible immediately after using \nsetInputFormat(Instances)\n. All the Instance objects are immediately processed in \ninput(Instance)\n via the \nconvertInstance(Instance)\n method, which pushes them immediately to the output queue.\n\n\n  import weka.core.*;\n  import weka.core.Capabilities.*;\n\n  import java.util.Random;\n\n  public class StreamFilter\n    extends Filter {\n\n    protected Random m_Random;\n\n    public String globalInfo() {\n      return   \"A stream filter that adds an attribute 'bla' at the end \"\n             + \"containing a random number. The output format can be collected \"\n             + \"immediately.\";\n    }\n\n    public Capabilities getCapabilities() {\n      Capabilities result = super.getCapabilities();\n      result.enableAllAttributes();\n      result.enableAllClasses();\n      result.enable(Capability.NO_CLASS);  // filter doesn't need class to be set\n      return result;\n    }\n\n    public boolean setInputFormat(Instances instanceInfo) throws Exception {\n      super.setInputFormat(instanceInfo);\n\n      Instances outFormat = new Instances(instanceInfo, 0);\n      outFormat.insertAttributeAt(new Attribute(\"bla\"), outFormat.numAttributes());\n      setOutputFormat(outFormat);\n\n      m_Random = new Random(1);\n\n      return true;  // output format is immediately available\n    }\n\n    public boolean input(Instance instance) throws Exception {\n      if (getInputFormat() = null)\n        throw new NullPointerException(\"No input instance format defined\");\n\n      if (isNewBatch()) {\n        resetQueue();\n        m_NewBatch = false;\n      }\n\n      convertInstance(instance);\n\n      return true;  // can be immediately collected via output()\n    }\n\n    protected void convertInstance(Instance instance) {\n      double[] newValues = new double[instance.numAttributes() + 1];\n      double[] oldValues = instance.toDoubleArray();\n      newValues[newValues.length - 1] = m_Random.nextInt();\n      System.arraycopy(oldValues, 0, newValues, 0, oldValues.length);\n      push(new Instance(1.0, newValues));\n    }\n\n    public static void main(String[] args) {\n      runFilter(new StreamFilter(), args);\n    }\n  }\n\n\n\n\nSimpleBatchFilter\n\n\nOnly the following abstract methods need to be implemented:\n\n\n\n\nglobalInfo()\n - returns a short description of what the filter does; will be displayed in the GUI\n\n\ndetermineOutputFormat(Instances)\n - generates the new format, based on the input data\n\n\nprocess(Instances)\n - processes the whole dataset in one go\n\n\ngetRevision()\n - returns the \nSubversion\n revision information\n\n\n\n\nIf you need access to the full input dataset in \ndetermineOutputFormat(Instances)\n, then you need to also override the method \nallowAccessToFullInputFormat()\n and make it return true. \n\n\nIf more options are necessary, then the following methods need to be overridden:\n\n\n\n\nlistOptions()\n - returns an enumeration of the available options; these are printed if one calls the filter with the \n-h\n option\n\n\nsetOptions(String[])\n - parses the given option array, that were passed from commandline\n\n\ngetOptions()\n - returns an array of options, resembling the current setup of the filter\n\n\n\n\nIn the following an \nexample implementation\n that adds an additional attribute at the end, containing the index of the processed instance:\n\n\n import weka.core.*;\n import weka.core.Capabilities.*;\n import weka.filters.*;\n\n public class SimpleBatch\n   extends SimpleBatchFilter {\n\n   public String globalInfo() {\n     return   \"A simple batch filter that adds an additional attribute 'bla' at the end \"\n            + \"containing the index of the processed instance.\";\n   }\n\n   public Capabilities getCapabilities() {\n     Capabilities result = super.getCapabilities();\n     result.enableAllAttributes();\n     result.enableAllClasses();\n     result.enable(Capability.NO_CLASS);  //// filter doesn't need class to be set//\n     return result;\n   }\n\n   protected Instances determineOutputFormat(Instances inputFormat) {\n     Instances result = new Instances(inputFormat, 0);\n     result.insertAttributeAt(new Attribute(\"bla\"), result.numAttributes());\n     return result;\n   }\n\n   protected Instances process(Instances inst) {\n     Instances result = new Instances(determineOutputFormat(inst), 0);\n     for (int i = 0; i < inst.numInstances(); i++) {\n       double[] values = new double[result.numAttributes()];\n       for (int n = 0; n < inst.numAttributes(); n++)\n         values[n] = inst.instance(i).value(n);\n       values[values.length - 1] = i;\n       result.add(new Instance(1, values));\n     }\n     return result;\n   }\n\n   public static void main(String[] args) {\n     runFilter(new SimpleBatch(), args);\n   }\n }\n\n\n\n\nSimpleStreamFilter\n\n\nOnly the following abstract methods need to be implemented:\n\n\n\n\nglobalInfo()\n - returns a short description of what the filter does; will be displayed in the GUI\n\n\ndetermineOutputFormat(Instances)\n - generates the new format, based on the input data\n\n\nprocess(Instance)\nprocesses a single instance and turns it from the old format into the new one\n\n\ngetRevision()\n - returns the \nSubversion\n revision information\n\n\n\n\nThe \nreset()\n method is only used, since the random number generator needs to be re-initialized in order to obtain repeatable results.\n\n\nIf more options are necessary, then the following methods need to be overridden:\n\n\n\n\nlistOptions()\n - returns an enumeration of the available options; these are printed if one calls the filter with the \n-h\n option\n\n\nsetOptions(String[])\n - parses the given option array, that were passed from commandline\n\n\ngetOptions()\n - returns an array of options, resembling the current setup of the filter\n\n\n\n\nIn the following an \nexample implementation\n of a stream filter that adds an extra attribute at the end, which is filled with random numbers:\n\n\n import weka.core.*;\n import weka.core.Capabilities.*;\n import weka.filters.*;\n\n import java.util.Random;\n\n public class SimpleStream\n   extends SimpleStreamFilter {\n\n   protected Random m_Random;\n\n   public String globalInfo() {\n     return   \"A simple stream filter that adds an attribute 'bla' at the end \"\n            + \"containing a random number.\";\n   }\n\n   public Capabilities getCapabilities() {\n     Capabilities result = super.getCapabilities();\n     result.enableAllAttributes();\n     result.enableAllClasses();\n     result.enable(Capability.NO_CLASS);  //// filter doesn't need class to be set//\n     return result;\n   }\n\n   protected void reset() {\n     super.reset();\n     m_Random = new Random(1);\n   }\n\n   protected Instances determineOutputFormat(Instances inputFormat) {\n     Instances result = new Instances(inputFormat, 0);\n     result.insertAttributeAt(new Attribute(\"bla\"), result.numAttributes());\n     return result;\n   }\n\n   protected Instance process(Instance inst) {\n     double[] values = new double[inst.numAttributes() + 1];\n     for (int n = 0; n < inst.numAttributes(); n++)\n       values[n] = inst.value(n);\n     values[values.length - 1] = m_Random.nextInt();\n     Instance result = new Instance(1, values);\n     return result;\n   }\n\n   public static void main(String[] args) {\n     runFilter(new SimpleStream(), args);\n   }\n }\n\n\n\n\nA \nreal-world\n implementation of a stream filter is the \nMultiFilter\n (package \nweka.filters\n), which passes the data through all the filters it contains. Depending on whether all the used filters are streamable or not, it acts either as a stream filter or as batch filter.\n\n\nInternals\n\n\nSome useful methods of the filter classes:\n\n\n\n\nisNewBatch()\n - returns \ntrue\n if an instance of the filter was just instantiated via \nnew\n or a new batch was started via the \nbatchFinished()\n method.\n\n\nisFirstBatchDone()\n - returns \ntrue\n as soon as the first batch was finished via the \nbatchFinished()\n method. Useful for \nsupervised\n filters, which should not be altered after being trained with the first batch of instances.\n\n\n\n\nRevisions\n\n\nFilters also implement the \nweka.core.RevisionHandler\n interface. This provides the functionality of obtaining the \nSubversion\n revision from within Java. Filters that are not part of the official Weka distribution will have to implement the method \ngetRevision()\n as follows, which will return a dummy revision of \n1.0\n:\n\n\n  /**\n   * Returns the revision string.\n   * \n   * @return        the revision\n   */\n  public String getRevision() {\n    return RevisionUtils.extract(\"$Revision: 1.0 $\");\n  }\n\n\n\n\nIntegration\n\n\nAfter finishing the coding stage, it's time to integrate your filter in the Weka framework, i.e., to make it available in the Explorer, Experimenter, etc.\n\n\nThe [[GenericObjectEditor]] article shows you how to tell Weka where to find your filter and therefore displaying it in the \nGenericObjectEditor\n (filters work in the same fashion as classifiers, regarding the discovery).\n\n\nTesting\n\n\nWeka provides already a test framework to ensure the basic functionality of a filter. It is essential for the filter to pass these tests.\n\n\nOption handling\n\n\nIf your filter implements \nweka.core.OptionHandler\n, check the \noption handling\n of your filter with the following tool from commandline:\n\n\n weka.core.CheckOptionHandler -W classname [-- additional parameters]\n\n\n\n\nAll tests need to return \nyes\n.\n\n\nGenericObjectEditor\n\n\nThe \nCheckGOE\n class checks whether all the properties available in the GUI have a tooltip accompanying them and whether the \nglobalInfo()\n method is declared:\n\n\n weka.core.CheckGOE -W classname [-- additional parameters]\n\n\n\n\nAll tests, once again, need to return \nyes\n.\n\n\nSource code\n\n\nFilters that implement the \nweka.filters.Sourcable\n interface can output Java code of their internal representation. In order to check the generated code, one should not only compile the code, but also test it with the following test class:\n\n\n weka.filters.CheckSource\n\n\n\n\nThis class takes the original Weka filter, the generated code and the dataset used for generating the source code (and an optional class index) as parameters. It builds the Weka filter on the dataset and compares the output, the one from the Weka filter and the one from the generated source code, whether they are the same.\n\n\nHere's an example call for \nweka.filters.unsupervised.attribute.ReplaceMissingValues\n and the generated class \nweka.filters.WekaWrapper\n (it wraps the actual generated code in a pseudo-filter):\n\n\n java weka.filters.CheckSource \\\n    -W weka.filters.unsupervised.attribute.ReplaceMissingValues \\\n    -S weka.filters.WekaWrapper \\\n    -t data.arff\n\n\n\n\nIt needs to return \nTests OK!\n.\n\n\nUnit tests\n\n\nIn order to make sure that your filter applies to the Weka criteria, you should add your filter to the \njunit\n unit test framework, i.e., by creating a Test class.\n\n\nHow to check out the unit test framework, you can find \nhere\n.\n\n\nSee also\n\n\n\n\n[[GenericObjectEditor|GenericObjectEditor/GenericPropertiesCreator]]\n\n\n\n\nLinks\n\n\n\n\njunit",
            "title": " Writing your own Filter"
        },
        {
            "location": "/writing_filter/#packages",
            "text": "A few comments about the different filter sub-packages:    supervised  - contains supervised filters, i.e., filters that take class distributions into account. Must implement the  weka.filters.SupervisedFilter  interface.   attribute  - filters that work column-wise.  instance  - filters that work row-wise.     unsupervised  - contains unsupervised filters, i.e., they work without taking any class distributions into account. Must implement the  weka.filters.UnsupervisedFilter  interface.   attribute  - filters that work column-wise.  instance  - filters that work row-wise.",
            "title": "Packages"
        },
        {
            "location": "/writing_filter/#choosing-the-superclass",
            "text": "The base filters and interfaces are all located in the following package:   weka.filters  One can basically distinguish between two different kinds of filters:   batch filters  - they need to see the whole dataset before they can start processing it, which they do in one go  stream filters  - they can start producing output right away and the data just passes through while being modified   All filters are derived from the abstract superclass  weka.filters.Filter .  To speed up development, there are also the following abstract filters, depending on the kind of classifier you want to implement:   weka.filters.SimpleBatchFilter  weka.filters.SimpleStreamFilter   These filters simplify the rather general and complex framework introduced by the abstract superclass  weka.filters.Filter . One only needs to implement a couple of abstract methods that will process the actual data and override, if necessary, a few existing methods for option handling.",
            "title": "Choosing the superclass"
        },
        {
            "location": "/writing_filter/#filter",
            "text": "",
            "title": "Filter"
        },
        {
            "location": "/writing_filter/#implementation",
            "text": "The following methods are of importance for the implementation of a filter and explained in detail further down:   getCapabilities()  setInputFormat(Instances)  getInputFormat()  setOutputFormat(Instances)  getOutputFormat()  input(Instance)  bufferInput(Instance)  push(Instance)  output()  batchFinished()  flushInput()  getRevision()   But only the following ones need normally be modified:   getCapabilities()  setInputFormat(Instances)  input(Instance)  batchFinished()  getRevision()",
            "title": "Implementation"
        },
        {
            "location": "/writing_filter/#getcapabilities",
            "text": "Filters implement the  weka.core.CapabilitiesHandler  interface like the classifiers. This method returns what kind of data the filter is able to process. Needs to be adapted for each individual filter.",
            "title": "getCapabilities()"
        },
        {
            "location": "/writing_filter/#setinputformatinstances",
            "text": "With this format, the user tells the filter what format, i.e., attributes, the input data has. This method also tests, whether the filter can actually process this data. All older Weka versions or book branch versions need to check the data manually and throw fitting exceptions, e.g., not being able to handle String attributes.  If the output format of the filter, i.e., the new Instances header, can be determined based alone on this information, then the method should set the output format via  setOutputFormat(Instances)  and return  true , otherwise it has to return  false .",
            "title": "setInputFormat(Instances)"
        },
        {
            "location": "/writing_filter/#getinputformat",
            "text": "This method returns an Instances object containing all currently buffered Instance objects from the input queue.",
            "title": "getInputFormat()"
        },
        {
            "location": "/writing_filter/#setoutputformatinstances",
            "text": "setOutputFormat(Instances)  defines the new Instances header for the output data. For filters that work on a row-basis, there shouldn't be any changes between the input and output format. But filters that work on attributes, e.g., removing, adding, modifying, will affect this format. This method must be called with the appropriate Instances object as parameter, since all Instance objects being processed will rely on the output format.",
            "title": "setOutputFormat(Instances)"
        },
        {
            "location": "/writing_filter/#getoutputformat",
            "text": "This method returns the currently set Instances object that defines the output format. In case  setOutputFormat(Instances)  hasn't been called yet, this method will return  null .",
            "title": "getOutputFormat()"
        },
        {
            "location": "/writing_filter/#inputinstance",
            "text": "The  input(Instance)  method returns  true  if the given Instance can be processed straight away and can be collected immediately via the  output()  method (after adding it to the output queue via  push(Instance) , of course). This is also the case if the first batch of data has been processed and the instance belongs to the second batch. Via  isFirstBatchDone()  one can query whether this instance is still part of the first batch or of the second.  If the Instance cannot be processed immediately, e.g., the filter needs to collect all the data first before doing some calculations, then it needs to be buffered with  bufferInput(Instance)  until  batchFinished()  is called.",
            "title": "input(Instance)"
        },
        {
            "location": "/writing_filter/#bufferinputinstance",
            "text": "In case an Instance cannot be processed immediately, one can use this method to buffer them in the input queue. All buffered Instance objects are available via the  getInputFormat()  method.",
            "title": "bufferInput(Instance)"
        },
        {
            "location": "/writing_filter/#pushinstance",
            "text": "push(Instance)  adds the given Instance to the output queue.",
            "title": "push(Instance)"
        },
        {
            "location": "/writing_filter/#output",
            "text": "Returns the next Instance object from the output queue and removes it from there. In case there is no Instance available this method returns  null .",
            "title": "output()"
        },
        {
            "location": "/writing_filter/#batchfinished",
            "text": "The  batchFinished()  method signifies the end of a dataset being pushed through the filter. In case of a filter that couldn't process the data of the first batch immediately, this is the place to determine what the output format will be (and set if via  setOutputFormat(Instances) ) and process the actual data. The currently available data can be retrieved with the  getInputFormat()  method. After processing the data, one needs to call  flushInput()  to remove all the pending input data.",
            "title": "batchFinished()"
        },
        {
            "location": "/writing_filter/#flushinput",
            "text": "flushInput()  removes all buffered Instance objects from the input queue. This method must be called after all the Instance objects have been processed in the  batchFinished()  method.",
            "title": "flushInput()"
        },
        {
            "location": "/writing_filter/#option-handling",
            "text": "If the filter should be able to handle commandline options, then the weka.core.OptionHandler interface needs to be implemented. In addition to that, the following code should be added at the end of the  setOptions(String[])  method:   if (getInputFormat() != null)\n    setInputFormat(getInputFormat());  This will inform the filter about changes in the options and therefore reset it.",
            "title": "Option handling"
        },
        {
            "location": "/writing_filter/#examples",
            "text": "The following examples are to illustrate the filter framework.   Note:  unseeded random number generators like  Math.random()  should never be used since they will produce different results in each run and repeatable results are essential in machine learning.",
            "title": "Examples"
        },
        {
            "location": "/writing_filter/#batchfilter",
            "text": "This simple batch filter adds a new attribute called //bla// at the end of the dataset. The rows of this attribute contain only the row's index in the data. Since the batch-filter need not see all the data before creating the output format, the  setInputFormat(Instances)  sets the output format and returns  true  (indicating that the output format can be queried immediately). The  batchFinished()  method performs the processing of all the data.    import weka.core.*;\n  import weka.core.Capabilities.*;\n\n  public class BatchFilter\n    extends Filter {\n\n    public String globalInfo() {\n      return   \"A batch filter that adds an additional attribute 'bla' at the end \"\n             + \"containing the index of the processed instance. The output format \"\n             + \"can be collected immediately.\";\n    }\n\n    public Capabilities getCapabilities() {\n      Capabilities result = super.getCapabilities();\n      result.enableAllAttributes();\n      result.enableAllClasses();\n      result.enable(Capability.NO_CLASS);  // filter doesn't need class to be set\n      return result;\n    }\n\n    public boolean setInputFormat(Instances instanceInfo) throws Exception {\n      super.setInputFormat(instanceInfo);\n\n      Instances outFormat = new Instances(instanceInfo, 0);\n      outFormat.insertAttributeAt(new Attribute(\"bla\"), outFormat.numAttributes());\n      setOutputFormat(outFormat);\n\n      return true;  // output format is immediately available\n    }\n\n    public boolean batchFinished() throws Exception {\n      if (getInputFormat() = null)\n        throw new NullPointerException(\"No input instance format defined\");\n\n      Instances inst = getInputFormat();\n      Instances outFormat = getOutputFormat();\n      for (int i = 0; i < inst.numInstances(); i++) {\n        double[] newValues = new double[outFormat.numAttributes()];\n        double[] oldValues = inst.instance(i).toDoubleArray();\n        System.arraycopy(oldValues, 0, newValues, 0, oldValues.length);\n        newValues[newValues.length - 1] = i;\n        push(new Instance(1.0, newValues));\n      }\n\n      flushInput();\n      m_NewBatch = true;\n      m_FirstBatchDone = true;\n      return (numPendingOutput() != 0);\n    }\n\n    public static void main(String[] args) {\n      runFilter(new BatchFilter(), args);\n    }\n  }",
            "title": "BatchFilter"
        },
        {
            "location": "/writing_filter/#batchfilter2",
            "text": "In contrast to the first batch filter, this one here cannot determine the output format immediately (the number of instances in the first batch is part of the attribute name now). This is done in the  batchFinished()  method.    import weka.core.*;\n  import weka.core.Capabilities.*;\n\n  public class BatchFilter2\n    extends Filter {\n\n    public String globalInfo() {\n      return   \"A batch filter that adds an additional attribute 'bla' at the end \"\n             + \"containing the index of the processed instance. The output format \"\n             + \"cannot be collected immediately.\";\n    }\n\n    public Capabilities getCapabilities() {\n      Capabilities result = super.getCapabilities();\n      result.enableAllAttributes();\n      result.enableAllClasses();\n      result.enable(Capability.NO_CLASS);  // filter doesn't need class to be set\n      return result;\n    }\n\n    public boolean batchFinished() throws Exception {\n      if (getInputFormat() = null)\n        throw new NullPointerException(\"No input instance format defined\");\n\n      // output format still needs to be set (depends on first batch of data)\n      if (!isFirstBatchDone()) {\n        Instances outFormat = new Instances(getInputFormat(), 0);\n        outFormat.insertAttributeAt(new Attribute(\n            \"bla-\" + getInputFormat().numInstances()), outFormat.numAttributes());\n        setOutputFormat(outFormat);\n      }\n\n      Instances inst = getInputFormat();\n      Instances outFormat = getOutputFormat();\n      for (int i = 0; i < inst.numInstances(); i++) {\n        double[] newValues = new double[outFormat.numAttributes()];\n        double[] oldValues = inst.instance(i).toDoubleArray();\n        System.arraycopy(oldValues, 0, newValues, 0, oldValues.length);\n        newValues[newValues.length - 1] = i;\n        push(new Instance(1.0, newValues));\n      }\n\n      flushInput();\n      m_NewBatch = true;\n      m_FirstBatchDone = true;\n\n      return (numPendingOutput() != 0);\n    }\n\n    public static void main(String[] args) {\n      runFilter(new BatchFilter2(), args);\n    }\n  }",
            "title": "BatchFilter2"
        },
        {
            "location": "/writing_filter/#batchfilter3",
            "text": "As soon as this batch filter's first batch is done, it can process Instance objects immediately in the  input(Instance)  method. It adds a new attribute which contains just a random number, but the random number generator being used is seeded with the number of instances from the first batch.    import weka.core.*;\n  import weka.core.Capabilities.*;\n\n  import java.util.Random;\n\n  public class BatchFilter3\n    extends Filter {\n\n    protected int m_Seed;\n    protected Random m_Random;\n\n    public String globalInfo() {\n      return   \"A batch filter that adds an attribute 'bla' at the end \"\n             + \"containing a random number. The output format cannot be collected \"\n             + \"immediately.\";\n    }\n\n    public Capabilities getCapabilities() {\n      Capabilities result = super.getCapabilities();\n      result.enableAllAttributes();\n      result.enableAllClasses();\n      result.enable(Capability.NO_CLASS);  // filter doesn't need class to be set\n      return result;\n    }\n\n    public boolean input(Instance instance) throws Exception {\n      if (getInputFormat() = null)\n        throw new NullPointerException(\"No input instance format defined\");\n\n      if (isNewBatch()) {\n        resetQueue();\n        m_NewBatch = false;\n      }\n\n      if (isFirstBatchDone())\n        convertInstance(instance);\n      else\n        bufferInput(instance);\n\n      return isFirstBatchDone();\n    }\n\n    public boolean batchFinished() throws Exception {\n      if (getInputFormat() = null)\n        throw new NullPointerException(\"No input instance format defined\");\n\n      // output format still needs to be set (random number generator is seeded\n      // with number of instances of first batch)\n      if (!isFirstBatchDone()) {\n        m_Seed = getInputFormat().numInstances();\n        Instances outFormat = new Instances(getInputFormat(), 0);\n        outFormat.insertAttributeAt(new Attribute(\n            \"bla-\" + getInputFormat().numInstances()), outFormat.numAttributes());\n        setOutputFormat(outFormat);\n      }\n\n      Instances inst = getInputFormat();\n      for (int i = 0; i < inst.numInstances(); i++) {\n        convertInstance(inst.instance(i));\n      }\n\n      flushInput();\n      m_NewBatch = true;\n      m_FirstBatchDone = true;\n      m_Random = null;\n\n      return (numPendingOutput() != 0);\n    }\n\n    protected void convertInstance(Instance instance) {\n      if (m_Random = null)\n        m_Random = new Random(m_Seed);\n      double[] newValues = new double[instance.numAttributes() + 1];\n      double[] oldValues = instance.toDoubleArray();\n      newValues[newValues.length - 1] = m_Random.nextInt();\n      System.arraycopy(oldValues, 0, newValues, 0, oldValues.length);\n      push(new Instance(1.0, newValues));\n    }\n\n    public static void main(String[] args) {\n      runFilter(new BatchFilter3(), args);\n    }\n  }",
            "title": "BatchFilter3"
        },
        {
            "location": "/writing_filter/#streamfilter",
            "text": "This stream filter adds a random number at the end of each instance of the input data. Since this doesn't rely on having access to the full data of the first batch, the output format is accessible immediately after using  setInputFormat(Instances) . All the Instance objects are immediately processed in  input(Instance)  via the  convertInstance(Instance)  method, which pushes them immediately to the output queue.    import weka.core.*;\n  import weka.core.Capabilities.*;\n\n  import java.util.Random;\n\n  public class StreamFilter\n    extends Filter {\n\n    protected Random m_Random;\n\n    public String globalInfo() {\n      return   \"A stream filter that adds an attribute 'bla' at the end \"\n             + \"containing a random number. The output format can be collected \"\n             + \"immediately.\";\n    }\n\n    public Capabilities getCapabilities() {\n      Capabilities result = super.getCapabilities();\n      result.enableAllAttributes();\n      result.enableAllClasses();\n      result.enable(Capability.NO_CLASS);  // filter doesn't need class to be set\n      return result;\n    }\n\n    public boolean setInputFormat(Instances instanceInfo) throws Exception {\n      super.setInputFormat(instanceInfo);\n\n      Instances outFormat = new Instances(instanceInfo, 0);\n      outFormat.insertAttributeAt(new Attribute(\"bla\"), outFormat.numAttributes());\n      setOutputFormat(outFormat);\n\n      m_Random = new Random(1);\n\n      return true;  // output format is immediately available\n    }\n\n    public boolean input(Instance instance) throws Exception {\n      if (getInputFormat() = null)\n        throw new NullPointerException(\"No input instance format defined\");\n\n      if (isNewBatch()) {\n        resetQueue();\n        m_NewBatch = false;\n      }\n\n      convertInstance(instance);\n\n      return true;  // can be immediately collected via output()\n    }\n\n    protected void convertInstance(Instance instance) {\n      double[] newValues = new double[instance.numAttributes() + 1];\n      double[] oldValues = instance.toDoubleArray();\n      newValues[newValues.length - 1] = m_Random.nextInt();\n      System.arraycopy(oldValues, 0, newValues, 0, oldValues.length);\n      push(new Instance(1.0, newValues));\n    }\n\n    public static void main(String[] args) {\n      runFilter(new StreamFilter(), args);\n    }\n  }",
            "title": "StreamFilter"
        },
        {
            "location": "/writing_filter/#simplebatchfilter",
            "text": "Only the following abstract methods need to be implemented:   globalInfo()  - returns a short description of what the filter does; will be displayed in the GUI  determineOutputFormat(Instances)  - generates the new format, based on the input data  process(Instances)  - processes the whole dataset in one go  getRevision()  - returns the  Subversion  revision information   If you need access to the full input dataset in  determineOutputFormat(Instances) , then you need to also override the method  allowAccessToFullInputFormat()  and make it return true.   If more options are necessary, then the following methods need to be overridden:   listOptions()  - returns an enumeration of the available options; these are printed if one calls the filter with the  -h  option  setOptions(String[])  - parses the given option array, that were passed from commandline  getOptions()  - returns an array of options, resembling the current setup of the filter   In the following an  example implementation  that adds an additional attribute at the end, containing the index of the processed instance:   import weka.core.*;\n import weka.core.Capabilities.*;\n import weka.filters.*;\n\n public class SimpleBatch\n   extends SimpleBatchFilter {\n\n   public String globalInfo() {\n     return   \"A simple batch filter that adds an additional attribute 'bla' at the end \"\n            + \"containing the index of the processed instance.\";\n   }\n\n   public Capabilities getCapabilities() {\n     Capabilities result = super.getCapabilities();\n     result.enableAllAttributes();\n     result.enableAllClasses();\n     result.enable(Capability.NO_CLASS);  //// filter doesn't need class to be set//\n     return result;\n   }\n\n   protected Instances determineOutputFormat(Instances inputFormat) {\n     Instances result = new Instances(inputFormat, 0);\n     result.insertAttributeAt(new Attribute(\"bla\"), result.numAttributes());\n     return result;\n   }\n\n   protected Instances process(Instances inst) {\n     Instances result = new Instances(determineOutputFormat(inst), 0);\n     for (int i = 0; i < inst.numInstances(); i++) {\n       double[] values = new double[result.numAttributes()];\n       for (int n = 0; n < inst.numAttributes(); n++)\n         values[n] = inst.instance(i).value(n);\n       values[values.length - 1] = i;\n       result.add(new Instance(1, values));\n     }\n     return result;\n   }\n\n   public static void main(String[] args) {\n     runFilter(new SimpleBatch(), args);\n   }\n }",
            "title": "SimpleBatchFilter"
        },
        {
            "location": "/writing_filter/#simplestreamfilter",
            "text": "Only the following abstract methods need to be implemented:   globalInfo()  - returns a short description of what the filter does; will be displayed in the GUI  determineOutputFormat(Instances)  - generates the new format, based on the input data  process(Instance) processes a single instance and turns it from the old format into the new one  getRevision()  - returns the  Subversion  revision information   The  reset()  method is only used, since the random number generator needs to be re-initialized in order to obtain repeatable results.  If more options are necessary, then the following methods need to be overridden:   listOptions()  - returns an enumeration of the available options; these are printed if one calls the filter with the  -h  option  setOptions(String[])  - parses the given option array, that were passed from commandline  getOptions()  - returns an array of options, resembling the current setup of the filter   In the following an  example implementation  of a stream filter that adds an extra attribute at the end, which is filled with random numbers:   import weka.core.*;\n import weka.core.Capabilities.*;\n import weka.filters.*;\n\n import java.util.Random;\n\n public class SimpleStream\n   extends SimpleStreamFilter {\n\n   protected Random m_Random;\n\n   public String globalInfo() {\n     return   \"A simple stream filter that adds an attribute 'bla' at the end \"\n            + \"containing a random number.\";\n   }\n\n   public Capabilities getCapabilities() {\n     Capabilities result = super.getCapabilities();\n     result.enableAllAttributes();\n     result.enableAllClasses();\n     result.enable(Capability.NO_CLASS);  //// filter doesn't need class to be set//\n     return result;\n   }\n\n   protected void reset() {\n     super.reset();\n     m_Random = new Random(1);\n   }\n\n   protected Instances determineOutputFormat(Instances inputFormat) {\n     Instances result = new Instances(inputFormat, 0);\n     result.insertAttributeAt(new Attribute(\"bla\"), result.numAttributes());\n     return result;\n   }\n\n   protected Instance process(Instance inst) {\n     double[] values = new double[inst.numAttributes() + 1];\n     for (int n = 0; n < inst.numAttributes(); n++)\n       values[n] = inst.value(n);\n     values[values.length - 1] = m_Random.nextInt();\n     Instance result = new Instance(1, values);\n     return result;\n   }\n\n   public static void main(String[] args) {\n     runFilter(new SimpleStream(), args);\n   }\n }  A  real-world  implementation of a stream filter is the  MultiFilter  (package  weka.filters ), which passes the data through all the filters it contains. Depending on whether all the used filters are streamable or not, it acts either as a stream filter or as batch filter.",
            "title": "SimpleStreamFilter"
        },
        {
            "location": "/writing_filter/#internals",
            "text": "Some useful methods of the filter classes:   isNewBatch()  - returns  true  if an instance of the filter was just instantiated via  new  or a new batch was started via the  batchFinished()  method.  isFirstBatchDone()  - returns  true  as soon as the first batch was finished via the  batchFinished()  method. Useful for  supervised  filters, which should not be altered after being trained with the first batch of instances.",
            "title": "Internals"
        },
        {
            "location": "/writing_filter/#revisions",
            "text": "Filters also implement the  weka.core.RevisionHandler  interface. This provides the functionality of obtaining the  Subversion  revision from within Java. Filters that are not part of the official Weka distribution will have to implement the method  getRevision()  as follows, which will return a dummy revision of  1.0 :    /**\n   * Returns the revision string.\n   * \n   * @return        the revision\n   */\n  public String getRevision() {\n    return RevisionUtils.extract(\"$Revision: 1.0 $\");\n  }",
            "title": "Revisions"
        },
        {
            "location": "/writing_filter/#integration",
            "text": "After finishing the coding stage, it's time to integrate your filter in the Weka framework, i.e., to make it available in the Explorer, Experimenter, etc.  The [[GenericObjectEditor]] article shows you how to tell Weka where to find your filter and therefore displaying it in the  GenericObjectEditor  (filters work in the same fashion as classifiers, regarding the discovery).",
            "title": "Integration"
        },
        {
            "location": "/writing_filter/#testing",
            "text": "Weka provides already a test framework to ensure the basic functionality of a filter. It is essential for the filter to pass these tests.",
            "title": "Testing"
        },
        {
            "location": "/writing_filter/#option-handling_1",
            "text": "If your filter implements  weka.core.OptionHandler , check the  option handling  of your filter with the following tool from commandline:   weka.core.CheckOptionHandler -W classname [-- additional parameters]  All tests need to return  yes .",
            "title": "Option handling"
        },
        {
            "location": "/writing_filter/#genericobjecteditor",
            "text": "The  CheckGOE  class checks whether all the properties available in the GUI have a tooltip accompanying them and whether the  globalInfo()  method is declared:   weka.core.CheckGOE -W classname [-- additional parameters]  All tests, once again, need to return  yes .",
            "title": "GenericObjectEditor"
        },
        {
            "location": "/writing_filter/#source-code",
            "text": "Filters that implement the  weka.filters.Sourcable  interface can output Java code of their internal representation. In order to check the generated code, one should not only compile the code, but also test it with the following test class:   weka.filters.CheckSource  This class takes the original Weka filter, the generated code and the dataset used for generating the source code (and an optional class index) as parameters. It builds the Weka filter on the dataset and compares the output, the one from the Weka filter and the one from the generated source code, whether they are the same.  Here's an example call for  weka.filters.unsupervised.attribute.ReplaceMissingValues  and the generated class  weka.filters.WekaWrapper  (it wraps the actual generated code in a pseudo-filter):   java weka.filters.CheckSource \\\n    -W weka.filters.unsupervised.attribute.ReplaceMissingValues \\\n    -S weka.filters.WekaWrapper \\\n    -t data.arff  It needs to return  Tests OK! .",
            "title": "Source code"
        },
        {
            "location": "/writing_filter/#unit-tests",
            "text": "In order to make sure that your filter applies to the Weka criteria, you should add your filter to the  junit  unit test framework, i.e., by creating a Test class.  How to check out the unit test framework, you can find  here .",
            "title": "Unit tests"
        },
        {
            "location": "/writing_filter/#see-also",
            "text": "[[GenericObjectEditor|GenericObjectEditor/GenericPropertiesCreator]]",
            "title": "See also"
        },
        {
            "location": "/writing_filter/#links",
            "text": "junit",
            "title": "Links"
        }
    ]
}