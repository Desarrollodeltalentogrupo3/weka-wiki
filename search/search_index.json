{
    "docs": [
        {
            "location": "/",
            "text": "Please note, the migration from the old wiki in this new format is still work in progress!\n\n\nNew to Weka?\n\n\nHave a look at the \nFrequently Asked Questions\n (= FAQ), the \nTroubleshooting\n article or search the \nmailing list archives\n.\nDon't forget to check out the documentation on the \nWeka homepage\n and the\n\nLearning Resources\n.\n\n\nYou have questions regarding Weka?\n\n\nYou can post questions to the \nWeka mailing list\n. Please keep\nin mind that you cannot expect an immediate answer to your question(s). The\nquestions are mainly answered by volunteers, Weka users just like you.\n\n\nYou are looking for packages?\n\n\nWith Weka 3.7.2 and later, you can easily install packages through Weka's\npackage manager interface, either official ones or unofficial ones.\nHave a look at the \nPackages\n article for more information on this topic.\n\n\nYou want to contribute to the wiki?\n\n\nThe wiki is based on \nMarkdown\n articles, which are turned into static HTML using \nMkDocs\n (see \nhere\n for details on writing articles). The content of the wiki is available as \nrepository on GitHub\n. Feel free to add/update and then do a \npull request\n.\n\n\nYou found a bug?\n\n\nPlease post the bug report to the \nWeka mailing list\n. The\nfollowing information will help tracking things down:\n\n\n\n\nversion of Weka (e.g., 3.9.2)\n\n\noperating system (e.g., Windows 10 or Ubuntu 16.04 64bit)\n\n\nJava version (e.g., 1.8.0_162 64bit)\n\n\n\n\nYou can also run the following command in the SimpleCLI and attach the generate output as text file to your post:\n\n\n    java weka.core.SystemInfo",
            "title": "Home"
        },
        {
            "location": "/#new-to-weka",
            "text": "Have a look at the  Frequently Asked Questions  (= FAQ), the  Troubleshooting  article or search the  mailing list archives .\nDon't forget to check out the documentation on the  Weka homepage  and the Learning Resources .",
            "title": "New to Weka?"
        },
        {
            "location": "/#you-have-questions-regarding-weka",
            "text": "You can post questions to the  Weka mailing list . Please keep\nin mind that you cannot expect an immediate answer to your question(s). The\nquestions are mainly answered by volunteers, Weka users just like you.",
            "title": "You have questions regarding Weka?"
        },
        {
            "location": "/#you-are-looking-for-packages",
            "text": "With Weka 3.7.2 and later, you can easily install packages through Weka's\npackage manager interface, either official ones or unofficial ones.\nHave a look at the  Packages  article for more information on this topic.",
            "title": "You are looking for packages?"
        },
        {
            "location": "/#you-want-to-contribute-to-the-wiki",
            "text": "The wiki is based on  Markdown  articles, which are turned into static HTML using  MkDocs  (see  here  for details on writing articles). The content of the wiki is available as  repository on GitHub . Feel free to add/update and then do a  pull request .",
            "title": "You want to contribute to the wiki?"
        },
        {
            "location": "/#you-found-a-bug",
            "text": "Please post the bug report to the  Weka mailing list . The\nfollowing information will help tracking things down:   version of Weka (e.g., 3.9.2)  operating system (e.g., Windows 10 or Ubuntu 16.04 64bit)  Java version (e.g., 1.8.0_162 64bit)   You can also run the following command in the SimpleCLI and attach the generate output as text file to your post:      java weka.core.SystemInfo",
            "title": "You found a bug?"
        },
        {
            "location": "/faq/",
            "text": "General\n\n\n\n\nWhat are the principal release branches of Weka?\n\n\nWhere can I get old versions of WEKA?\n\n\nHow do I get the latest bugfixes?\n\n\nCan I check my CLASSPATH from within WEKA?\n\n\nWhere is my home directory located?\n\n\nCan I check how much memory is available for WEKA?\n\n\nCan I use WEKA in commercial applications?\n\n\n\n\nBasic usage\n\n\n\n\nCan I use CSV files?\n\n\nHow do I perform CSV file conversion?\n\n\nHow do I divide a dataset into training and test set?\n\n\nHow do I generate compatible train and test sets that get processed with a filter?\n\n\nHow do I perform attribute selection?\n\n\nHow do I perform clustering?\n\n\nWhere do I find visualization of classifiers, etc.?\n\n\nHow do I perform text classification?\n\n\nHow can I perform multi-instance learning in WEKA?\n\n\nHow do I perform cost-sensitive classification?\n\n\nHow do I make predictions with a trained model?\n\n\nWhy am I missing certain nominal or string values from sparse instances?\n\n\nCan I use WEKA for time series analysis?\n\n\nDoes WEKA support multi-label classification?\n\n\nHow do I perform one-class classification?\n\n\nCan I make a screenshot of a plot or graph directly in WEKA?\n\n\nHow do I use the package manager?\n\n\nWhat do I do if the package manager does not start?\n\n\n\n\nAdvanced usage\n\n\n\n\nHow can I track instances in WEKA?\n\n\nHow do I use ID attributes?\n\n\nHow do I connect to a database?\n\n\nHow do I use WEKA from the command line?\n\n\nCan I tune the parameters of a classifier?\n\n\nHow do I generate Learning curves?\n\n\nWhere can I find information regarding ROC curves?\n\n\nI have unbalanced data - now what?\n\n\nCan I run an experiment using clusterers in the Experimenter?\n\n\nHow can I use transactional data in Weka?\n\n\nHow can I use Weka with Matlab or Octave?\n\n\n\n\nCustomizing Weka\n\n\n\n\nCan I change the colors (background, axes, etc.) of the plots in WEKA?\n\n\nHow do I add a new classifier, filter, kernel, etc\n\n\n\n\nUsing third-party tools\n\n\n\n\nHow do I use libsvm in WEKA?\n\n\nThe snowball stemmers don't work, what am I doing wrong?\n\n\n\n\nDeveloping with WEKA\n\n\n\n\nWhere can I get WEKA's source code?\n\n\nHow do I compile WEKA?\n\n\nWhat is Subversion and what do I need to do to access it?\n\n\nHow do I use WEKA's classes in my own code?\n\n\nHow do I write a new classifier or filter?\n\n\nCan I compile WEKA into native code?\n\n\nCan I use WEKA from C#?\n\n\nCan I use WEKA from Python?\n\n\nCan I use WEKA from Groovy?\n\n\nSerialization is nice, but what about generating actual Java code from WEKA classes?\n\n\nHow are packages structured for the package management system?\n\n\nPluggable evaluation metrics for classification/regression\n\n\nHow can I contribute to WEKA?\n\n\n\n\nWindows\n\n\n\n\nHow do I modify the CLASSPATH?\n\n\nHow do I modify the RunWeka.bat file?\n\n\nCan I process UTF-8 datasets or files?\n\n\nHow do I run the Windows Weka installer in silent mode?\n\n\n\n\nTroubleshooting\n\n\n\n\nI have Weka download problems - what's going wrong?\n\n\nMy ARFF file doesn't load - why?\n\n\nWhat does nominal value not declared in header, read Token[X], line Y mean?\n)\n\n\nHow do I get rid of this OutOfMemoryException?\n\n\nHow do I deal with a StackOverflowError?\n\n\nWhy do I get the error message 'training and test set are not compatible'?\n\n\nCouldn't read from database: unknown data type\n\n\nTrying to add JDBC driver: ... - Error, not in CLASSPATH?\n\n\nI cannot process large datasets - any ideas?\n\n\nSee \nTroubleshooting\n article for more troubleshooting.",
            "title": "FAQ"
        },
        {
            "location": "/faq/#general",
            "text": "What are the principal release branches of Weka?  Where can I get old versions of WEKA?  How do I get the latest bugfixes?  Can I check my CLASSPATH from within WEKA?  Where is my home directory located?  Can I check how much memory is available for WEKA?  Can I use WEKA in commercial applications?",
            "title": "General"
        },
        {
            "location": "/faq/#basic-usage",
            "text": "Can I use CSV files?  How do I perform CSV file conversion?  How do I divide a dataset into training and test set?  How do I generate compatible train and test sets that get processed with a filter?  How do I perform attribute selection?  How do I perform clustering?  Where do I find visualization of classifiers, etc.?  How do I perform text classification?  How can I perform multi-instance learning in WEKA?  How do I perform cost-sensitive classification?  How do I make predictions with a trained model?  Why am I missing certain nominal or string values from sparse instances?  Can I use WEKA for time series analysis?  Does WEKA support multi-label classification?  How do I perform one-class classification?  Can I make a screenshot of a plot or graph directly in WEKA?  How do I use the package manager?  What do I do if the package manager does not start?",
            "title": "Basic usage"
        },
        {
            "location": "/faq/#advanced-usage",
            "text": "How can I track instances in WEKA?  How do I use ID attributes?  How do I connect to a database?  How do I use WEKA from the command line?  Can I tune the parameters of a classifier?  How do I generate Learning curves?  Where can I find information regarding ROC curves?  I have unbalanced data - now what?  Can I run an experiment using clusterers in the Experimenter?  How can I use transactional data in Weka?  How can I use Weka with Matlab or Octave?",
            "title": "Advanced usage"
        },
        {
            "location": "/faq/#customizing-weka",
            "text": "Can I change the colors (background, axes, etc.) of the plots in WEKA?  How do I add a new classifier, filter, kernel, etc",
            "title": "Customizing Weka"
        },
        {
            "location": "/faq/#using-third-party-tools",
            "text": "How do I use libsvm in WEKA?  The snowball stemmers don't work, what am I doing wrong?",
            "title": "Using third-party tools"
        },
        {
            "location": "/faq/#developing-with-weka",
            "text": "Where can I get WEKA's source code?  How do I compile WEKA?  What is Subversion and what do I need to do to access it?  How do I use WEKA's classes in my own code?  How do I write a new classifier or filter?  Can I compile WEKA into native code?  Can I use WEKA from C#?  Can I use WEKA from Python?  Can I use WEKA from Groovy?  Serialization is nice, but what about generating actual Java code from WEKA classes?  How are packages structured for the package management system?  Pluggable evaluation metrics for classification/regression  How can I contribute to WEKA?",
            "title": "Developing with WEKA"
        },
        {
            "location": "/faq/#windows",
            "text": "How do I modify the CLASSPATH?  How do I modify the RunWeka.bat file?  Can I process UTF-8 datasets or files?  How do I run the Windows Weka installer in silent mode?",
            "title": "Windows"
        },
        {
            "location": "/faq/#troubleshooting",
            "text": "I have Weka download problems - what's going wrong?  My ARFF file doesn't load - why?  What does nominal value not declared in header, read Token[X], line Y mean? )  How do I get rid of this OutOfMemoryException?  How do I deal with a StackOverflowError?  Why do I get the error message 'training and test set are not compatible'?  Couldn't read from database: unknown data type  Trying to add JDBC driver: ... - Error, not in CLASSPATH?  I cannot process large datasets - any ideas?  See  Troubleshooting  article for more troubleshooting.",
            "title": "Troubleshooting"
        },
        {
            "location": "/not_so_faq/",
            "text": "Associators\n\n\n\n\nHow do I use the associator GeneralizedSequentialPatterns?\n\n\n\n\nClassifiers\n\n\n\n\nWhat do those numbers mean in a J48 tree?",
            "title": "Not so FAQ"
        },
        {
            "location": "/not_so_faq/#associators",
            "text": "How do I use the associator GeneralizedSequentialPatterns?",
            "title": "Associators"
        },
        {
            "location": "/not_so_faq/#classifiers",
            "text": "What do those numbers mean in a J48 tree?",
            "title": "Classifiers"
        },
        {
            "location": "/troubleshooting/",
            "text": "Click on one of the links for more information:\n\n\n\n\nWeka download problems\n\n\nOutOfMemoryException\n\n\nStackOverflowError\n\n\njust-in-time (JIT) compiler\n\n\nCSV file conversion\n\n\nARFF file doesn't load\n\n\nError message: nominal value not declared in header, read Token[X], line Y\n\n\nSpaces in labels of ARFF files\n\n\nSingle quotes in labels of ARFF files\n\n\nCLASSPATH problems\n\n\nInstance ID\n\n\nVisualization\n\n\nMemory consumption and Garbage collector\n\n\nGUIChooser starts but not Experimenter or Explorer\n\n\nKnowledgeFlow toolbars are empty\n\n\nOSX Mountain Lion - Weka x-y-z is damaged and can't be installed. You should eject the disk image\n\n\nUbuntu 18.04: WARNING: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\n\n\n\n\nSee also the \nFrequently Asked Questions\n.",
            "title": "Troubleshooting"
        },
        {
            "location": "/learning_resources/",
            "text": "Videos\n\n\n\n\nYoutube channel of Data Mining with Weka MOOCs\n\n\n\n\nTutorials\n\n\n\n\nLearn Data Science Online\n\n\n\n\nMOOCs\n\n\n\n\nData Mining with Weka\n\n\nMore Data Mining with Weka\n\n\nAdvanced Data Mining with Weka",
            "title": "Learning resources"
        },
        {
            "location": "/learning_resources/#videos",
            "text": "Youtube channel of Data Mining with Weka MOOCs",
            "title": "Videos"
        },
        {
            "location": "/learning_resources/#tutorials",
            "text": "Learn Data Science Online",
            "title": "Tutorials"
        },
        {
            "location": "/learning_resources/#moocs",
            "text": "Data Mining with Weka  More Data Mining with Weka  Advanced Data Mining with Weka",
            "title": "MOOCs"
        },
        {
            "location": "/using_the_api/",
            "text": "Several articles describe certain aspects of using the Weka API:\n\n\n\n\nUse Weka in your Java code\n\n\nWeka Examples\n\n\nGenerating cross-validation folds\n\n\nCreating an ARFF file\n\n\nBinarize Attribute\n\n\nARFF files from Text Collections\n\n\nAdding attributes to a dataset\n\n\nSave Instances to an ARFF File\n\n\nGenerating ROC curve\n\n\nVisualizing ROC curve\n\n\nSerialization\n\n\n\n\nIt is possible to use Weka through \nJupyter notebooks\n \nas well, see the following article for more information:\n\n\n\n\nJupyter notebooks",
            "title": "Using the API"
        },
        {
            "location": "/extending_weka/",
            "text": "The following articles describe how you can extend Weka:\n\n\n\n\nWriting a new Filter\n\n\nWriting a new Classifier",
            "title": "Extending Weka"
        },
        {
            "location": "/arff/",
            "text": "Data format\n\n\nA description of the ARFF format can be found in the following articles:\n\n\n\n\nARFF (stable version)\n\n\nARFF (developer version)\n\n\n\n\nCreating an ARFF file\n\n\nHow to create an ARFF file on the fly, i.e., inside Java, you can find here:\n\n\n\n\nCreating an ARFF file\n\n\n\n\nSee also\n\n\n\n\nARFF Syntax Highlighting\n for various editors\n\n\n\n\nLinks\n\n\n\n\nARFF2DB.py\n - a Python script for importing an ARFF file into a database (similar functionality to the \nweka.core.converters.DatabaseSaver\n class)",
            "title": "ARFF Format"
        },
        {
            "location": "/arff/#data-format",
            "text": "A description of the ARFF format can be found in the following articles:   ARFF (stable version)  ARFF (developer version)",
            "title": "Data format"
        },
        {
            "location": "/arff/#creating-an-arff-file",
            "text": "How to create an ARFF file on the fly, i.e., inside Java, you can find here:   Creating an ARFF file",
            "title": "Creating an ARFF file"
        },
        {
            "location": "/arff/#see-also",
            "text": "ARFF Syntax Highlighting  for various editors",
            "title": "See also"
        },
        {
            "location": "/arff/#links",
            "text": "ARFF2DB.py  - a Python script for importing an ARFF file into a database (similar functionality to the  weka.core.converters.DatabaseSaver  class)",
            "title": "Links"
        },
        {
            "location": "/packages/",
            "text": "Weka 3.7.2 introduced support for packages, making it easy to extend Weka\nwithout having to recompile or patch the underlying Weka installation.\n\n\nHere are some pointers for using and developing packages:\n\n\n\n\nHow do I use the package manager?\n\n\nUnofficial packages\n\n\nHow are packages structured for the package management system?",
            "title": "Packages"
        },
        {
            "location": "/mailing_list/",
            "text": "The WEKA Mailing list can be found here:\n\n\n\n\nList\n for subscribing/unsubscribing to the list\n\n\nArchives\n (\nMirror 1\n, \nMirror 2\n) for searching previous posted messages\n\n\n\n\nBefore posting, please read the \nMailing List Etiquette\n.",
            "title": "Mailing list"
        },
        {
            "location": "/weka_experiment_database_utils.props/",
            "text": "File\n\n\nweka/experiment/DatabaseUtils.props\n\n\nDescription\n\n\nDefines the \nDatabases\n setup, i.e., JDBC driver information, JDBC URL, database type conversion, etc.\n\n\nVersion\n\n\n\n\n>= 3.1.3\n\n\n\n\nFields\n\n\n\n\n\n\nGeneral\n\n\n\n\n\n\njdbcDriver\n\n\nthe comma-separated list of jdbc drivers to try loading\n\n\n\n\n\n\njdbcURL\n\n\nthe JDBC URL to the database\n\n\n\n\n\n\n\n\n\n\nTable creation\n\n\n\n\n\n\nCREATE_STRING\n\n\ndatabase specific datatype, e.g., \nTEXT\n\n\n\n\n\n\nCREATE_INT\n\n\ndatabase specific datatype, e.g., \nINT\n\n\n\n\n\n\nCREATE_DOUBLE\n\n\ndatabase specific datatype, e.g., \nDOUBLE\n\n\n\n\n\n\n\n\n\n\nDatabase flags\n\n\n\n\n\n\ncheckUpperCaseNames\n\n\nnecessary if database turns column names into upper case ones, e.g., HSQLDB\n\n\n\n\n\n\ncheckLowerCaseNames\n (> 3.5.3)\n\n\nnecessary if database turns column names into lower case ones, e.g., PostgreSQL\n\n\n\n\n\n\ncheckForTable\n (> 3.5.3)\n\n\nChecks whether the tables in the query are available in the meta-data of the JDBC Connection. Some tables, like \npg_tables\n, exist but are not available through the meta-data\n\n\n\n\n\n\nsetAutoCommit\n\n\nsetting for \njava.sql.Connection.setAutoCommit(boolean)\n\n\n\n\n\n\ncreateIndex\n\n\nwhether to create a primary key \nKey_IDX\n in the results table of an experiment\n\n\n\n\n\n\n\n\n\n\nSpecial flags for DatabaseLoader/Saver (package \nweka.core.converters\n)\n\n\n\n\n\n\nnominalToStringLimit\n (>= 3.4.1)\n\n\nbeyond this limit, nominal columns are loaded as STRING attributes and no longer as NOMINAL ones\n\n\n\n\n\n\nidColumn\n (>= 3.4.1)\n\n\nunique key in table that allows ordering for incremental loading\n\n\n\n\n\n\nKeywords\n (> 3.5.8, > 3.6.0)\n\n\nlists all the reserved keywords of the current database type\n\n\ndefault: \nAND,ASC,BY,DESC,FROM,GROUP,INSERT,ORDER,SELECT,UPDATE,WHERE\n\n\n\n\n\n\nKeywordsMaskChar\n (> 3.5.8, > 3.6.0)\n\n\nthe character to append to attribute names/table names that would be interpreted as keywords by the database, in order to avoid exceptions when executing SQL commands\ndefaut: \n_\n\n\n\n\n\n\n\n\n\n\nDatabase type mapping\n\n\nIn order to import the data from database correctly into Weka, one has to specify what JDBC datatype corresponds to what Java SQL retrieval method. Here's an overview of how the Java types are mapped to Weka's attribute types:\n\n\n\n\n\n\n\n\nJava type\n\n\nJava method\n\n\nIdentifier\n\n\nWeka attribute type\n\n\nVersion\n\n\n\n\n\n\n\n\n\n\nString\n\n\ngetString()\n\n\n0\n\n\nnominal\n\n\n\n\n\n\n\n\nboolean\n\n\ngetBoolean()\n\n\n1\n\n\nnominal\n\n\n\n\n\n\n\n\ndouble\n\n\ngetDouble()\n\n\n2\n\n\nnumeric\n\n\n\n\n\n\n\n\nbyte\n\n\ngetByte()\n\n\n3\n\n\nnumeric\n\n\n\n\n\n\n\n\nshort\n\n\ngetByte()\n\n\n4\n\n\nnumeric\n\n\n\n\n\n\n\n\nint\n\n\ngetInteger()\n\n\n5\n\n\nnumeric\n\n\n\n\n\n\n\n\nlong\n\n\ngetLong()\n\n\n6\n\n\nnumeric\n\n\n\n\n\n\n\n\nfloat\n\n\ngetFloat()\n\n\n7\n\n\nnumeric\n\n\n\n\n\n\n\n\ndate\n\n\ngetDate()\n\n\n8\n\n\ndate\n\n\n\n\n\n\n\n\ntext\n\n\ngetString()\n\n\n9\n\n\nstring\n\n\n>3.5.5\n\n\n\n\n\n\ntime\n\n\ngetTime()\n\n\n10\n\n\nstring\n\n\n>3.5.8\n\n\n\n\n\n\n\n\nIn the props file one lists now the type names that the database returns and what Java type it represents (via the identifier), e.g.:\n\n\n CHAR=0\n VARCHAR=0\n\n\n\n\nCHAR\n and \nVARCHAR\n are both String types, hence they are interpreted as \nString\n (identifier \n0\n)\n\n\nNote:\n in case database types have blanks, one needs to replace those blanks with an underscore, e.g., \nDOUBLE PRECISION\n must be listed like this:\n\n\n DOUBLE_PRECISION=2\n\n\n\n\nSee also\n\n\n\n\nDatabases\n\n\nProperties file",
            "title": " WEKA experiment DatabaseUtils.props"
        },
        {
            "location": "/weka_experiment_database_utils.props/#file",
            "text": "weka/experiment/DatabaseUtils.props",
            "title": "File"
        },
        {
            "location": "/weka_experiment_database_utils.props/#description",
            "text": "Defines the  Databases  setup, i.e., JDBC driver information, JDBC URL, database type conversion, etc.",
            "title": "Description"
        },
        {
            "location": "/weka_experiment_database_utils.props/#version",
            "text": ">= 3.1.3",
            "title": "Version"
        },
        {
            "location": "/weka_experiment_database_utils.props/#fields",
            "text": "General    jdbcDriver  the comma-separated list of jdbc drivers to try loading    jdbcURL  the JDBC URL to the database      Table creation    CREATE_STRING  database specific datatype, e.g.,  TEXT    CREATE_INT  database specific datatype, e.g.,  INT    CREATE_DOUBLE  database specific datatype, e.g.,  DOUBLE      Database flags    checkUpperCaseNames  necessary if database turns column names into upper case ones, e.g., HSQLDB    checkLowerCaseNames  (> 3.5.3)  necessary if database turns column names into lower case ones, e.g., PostgreSQL    checkForTable  (> 3.5.3)  Checks whether the tables in the query are available in the meta-data of the JDBC Connection. Some tables, like  pg_tables , exist but are not available through the meta-data    setAutoCommit  setting for  java.sql.Connection.setAutoCommit(boolean)    createIndex  whether to create a primary key  Key_IDX  in the results table of an experiment      Special flags for DatabaseLoader/Saver (package  weka.core.converters )    nominalToStringLimit  (>= 3.4.1)  beyond this limit, nominal columns are loaded as STRING attributes and no longer as NOMINAL ones    idColumn  (>= 3.4.1)  unique key in table that allows ordering for incremental loading    Keywords  (> 3.5.8, > 3.6.0)  lists all the reserved keywords of the current database type  default:  AND,ASC,BY,DESC,FROM,GROUP,INSERT,ORDER,SELECT,UPDATE,WHERE    KeywordsMaskChar  (> 3.5.8, > 3.6.0)  the character to append to attribute names/table names that would be interpreted as keywords by the database, in order to avoid exceptions when executing SQL commands\ndefaut:  _",
            "title": "Fields"
        },
        {
            "location": "/weka_experiment_database_utils.props/#database-type-mapping",
            "text": "In order to import the data from database correctly into Weka, one has to specify what JDBC datatype corresponds to what Java SQL retrieval method. Here's an overview of how the Java types are mapped to Weka's attribute types:     Java type  Java method  Identifier  Weka attribute type  Version      String  getString()  0  nominal     boolean  getBoolean()  1  nominal     double  getDouble()  2  numeric     byte  getByte()  3  numeric     short  getByte()  4  numeric     int  getInteger()  5  numeric     long  getLong()  6  numeric     float  getFloat()  7  numeric     date  getDate()  8  date     text  getString()  9  string  >3.5.5    time  getTime()  10  string  >3.5.8     In the props file one lists now the type names that the database returns and what Java type it represents (via the identifier), e.g.:   CHAR=0\n VARCHAR=0  CHAR  and  VARCHAR  are both String types, hence they are interpreted as  String  (identifier  0 )  Note:  in case database types have blanks, one needs to replace those blanks with an underscore, e.g.,  DOUBLE PRECISION  must be listed like this:   DOUBLE_PRECISION=2",
            "title": "Database type mapping"
        },
        {
            "location": "/weka_experiment_database_utils.props/#see-also",
            "text": "Databases  Properties file",
            "title": "See also"
        },
        {
            "location": "/subversion/",
            "text": "General\n\n\nThe Weka \nSubversion\n repository is accessible and browseable via the following URL:\n\n\n\n\nhttps://svn.cms.waikato.ac.nz/svn/weka/\n\n\n\n\nA Subversion repository has usually the following layout:\n\n\n root\n  |\n  +- trunk\n  |\n  +- tags\n  |\n  +- branches\n\n\n\n\nWhere \ntrunk\n contains the \nmain trunk\n of the development, \ntags\n snapshots in time of the repository (e.g., when a new version got released) and \nbranches\n development branches that forked off the main trunk at some stage (e.g., legacy versions that still get bugfixed).\n\n\nSource code\n\n\nThe latest version of the Weka source code can be obtained with this URL:\n\n\nhttps://svn.cms.waikato.ac.nz/svn/weka/trunk/weka\n\n\nIf you want to obtain the source code of the book version, use this URL:\n\n\nhttps://svn.cms.waikato.ac.nz/svn/weka/branches/stable-3-8/\n\n\nSpecific version\n\n\nWhenever a release of Weka is generated, the repository gets \ntagged\n:\n\n\ndev-X-Y-Z\n\n\nthe tag for a release of the developer version, e.g., \ndev-3-9-2\n for Weka 3.9.2\n\n\nhttps://svn.cms.waikato.ac.nz/svn/weka/tags/dev-3-9-2\n\n\nstable-X-Y-Z\n\n\nthe tag for a release of a stable version. The book version is one of those stable versions, e.g., \nstable-3-8-2\n for Weka 3.8.2.\n\n\nhttps://svn.cms.waikato.ac.nz/svn/weka/tags/stable-3-8-2\n\n\nJUnit\n\n\nWeka's JUnit tests are no longer a separate module (as it was the case before the migration to Subversion). They are now located in the \nsrc/test\n directory of the Weka source code tree.\n\n\nCommandline\n\n\nModern Linux distributions already come with Subversion either pre-installed or easily installed via the package manager of the distribution. If that shouldn't be case, or if you are using Windows, you have to download the appropriate client from [http://subversion.tigris.org/ Subversion's homepage].\n\n\nA checkout of the current developer version of Weka looks like this:\n\n\nsvn co https://svn.cms.waikato.ac.nz/svn/weka/trunk/weka\n\n\n\n\nYou can also obtain the source code for a specific date. The \n-r\n option of the \nsvn\n command-line client can also take dates (format: YYYYMMDD) instead of only revision numbers. In order to distinguish the dates from revision numbers, you have to enclose the date within curly brackets:\n\n\nsvn co -r {20180616} https://svn.cms.waikato.ac.nz/svn/weka/trunk/weka\n\n\n\n\nLinks\n\n\n\n\nSubversion on WikiPedia\n\n\nSubversion homepage\n\n\nJUnit homepage",
            "title": " Subversion"
        },
        {
            "location": "/subversion/#general",
            "text": "The Weka  Subversion  repository is accessible and browseable via the following URL:   https://svn.cms.waikato.ac.nz/svn/weka/   A Subversion repository has usually the following layout:   root\n  |\n  +- trunk\n  |\n  +- tags\n  |\n  +- branches  Where  trunk  contains the  main trunk  of the development,  tags  snapshots in time of the repository (e.g., when a new version got released) and  branches  development branches that forked off the main trunk at some stage (e.g., legacy versions that still get bugfixed).",
            "title": "General"
        },
        {
            "location": "/subversion/#source-code",
            "text": "The latest version of the Weka source code can be obtained with this URL:  https://svn.cms.waikato.ac.nz/svn/weka/trunk/weka  If you want to obtain the source code of the book version, use this URL:  https://svn.cms.waikato.ac.nz/svn/weka/branches/stable-3-8/",
            "title": "Source code"
        },
        {
            "location": "/subversion/#specific-version",
            "text": "Whenever a release of Weka is generated, the repository gets  tagged :  dev-X-Y-Z  the tag for a release of the developer version, e.g.,  dev-3-9-2  for Weka 3.9.2  https://svn.cms.waikato.ac.nz/svn/weka/tags/dev-3-9-2  stable-X-Y-Z  the tag for a release of a stable version. The book version is one of those stable versions, e.g.,  stable-3-8-2  for Weka 3.8.2.  https://svn.cms.waikato.ac.nz/svn/weka/tags/stable-3-8-2",
            "title": "Specific version"
        },
        {
            "location": "/subversion/#junit",
            "text": "Weka's JUnit tests are no longer a separate module (as it was the case before the migration to Subversion). They are now located in the  src/test  directory of the Weka source code tree.",
            "title": "JUnit"
        },
        {
            "location": "/subversion/#commandline",
            "text": "Modern Linux distributions already come with Subversion either pre-installed or easily installed via the package manager of the distribution. If that shouldn't be case, or if you are using Windows, you have to download the appropriate client from [http://subversion.tigris.org/ Subversion's homepage].  A checkout of the current developer version of Weka looks like this:  svn co https://svn.cms.waikato.ac.nz/svn/weka/trunk/weka  You can also obtain the source code for a specific date. The  -r  option of the  svn  command-line client can also take dates (format: YYYYMMDD) instead of only revision numbers. In order to distinguish the dates from revision numbers, you have to enclose the date within curly brackets:  svn co -r {20180616} https://svn.cms.waikato.ac.nz/svn/weka/trunk/weka",
            "title": "Commandline"
        },
        {
            "location": "/subversion/#links",
            "text": "Subversion on WikiPedia  Subversion homepage  JUnit homepage",
            "title": "Links"
        },
        {
            "location": "/get_latest_bugfixes/",
            "text": "Weka is actively developed, that means that bugs are fixed and new functionality is added (only to the developer version) all the time. Every now and then (about every 6-12 months), when there was a sufficiently large number of improvements or fixes, a release is made and uploaded to \nSourceforget.net\n.\n\n\nIf you don't want to wait that long, you have two options:\n\n\n\n\n\n\nGet the latest source code from \nSubversion\n and compile it yourself. See the following articles for more information\n\n\n\n\nobtaining the source code from Subversion\n, either book or developer version\n\n\ncompiling the source code\n\n\n\n\n\n\n\n\nDownload a \nsnapshot\n from the download section of the \nWeka homepage\n. Snapshots for book and developer version are generated automatically every night, based on the current Subversion code. The \nZIP files\n have the same content as a release, i.e., compiled classes (= weka.jar), source code (= weka-src.jar), Javadoc and other documentation.\n\n\n\n\n\n\nNote:\n compare the timestamp of the \nWeka Mailing List\n post that reports a bugfix with the one of the snapshot to make sure the bugfix is already included in the snapshot.",
            "title": " Get latest Bugfixes"
        },
        {
            "location": "/compiling_weka/",
            "text": "There are several ways of compiling the Weka source code:\n\n\n\n\nwith \nant\n\n\n\n\ntakes care of compiling all the necessary classes and easily generates jar archives\n\n\n\n\nwith \nmaven\n\n\n\n\nsimilar to ant\n\n\n\n\nwith an IDE, like IntelliJ IDEA, Eclipse or NetBeans\n\n\n\n\ncan be very helpful for debugging tricky bugs",
            "title": " Compiling Weka"
        },
        {
            "location": "/ant/",
            "text": "What is ANT? This is how the ANT \nhomepage\n defines its tool:\n\n\nApache Ant is a Java-based build tool. In theory, it is kind of like Make, but without Make's wrinkles.\n\n\nBasics\n\n\n\n\nthe ANT build file is based on \nXML\n\n\nthe usual name for the build file is \nbuild.xml\n\n\ninvocation - the usual build file needs not be specified explicitly, if it's in the current directory; if not target is specified, the default one is used\n\n\n\n\nant [-f <build-file>] [<target>]\n\n\n\n\ndisplaying all the available targets of a build file\n\n\n\n\nant [-f <build-file>] -projecthelp\n\n\nWeka and ANT\n\n\n\n\na build file for Weka is available from \nsubversion\n (it has been included in the \nweka-src.jar\n since version 3.4.8 and 3.5.3)\n\n\nit is located in the \nweka\n directory\n\n\n\n\nsome targets of interest\n\n\n\n\n\n\nclean\n - Removes the build, dist and reports directories; also any class files in the source tree\n\n\n\n\ncompile\n - Compile weka and deposit class files in \n${path_modifier}/build/classes\n\n\ndocs\n - Make javadocs into {${path_modifier}/doc}}\n\n\nexejar\n - Create an executable jar file in \n${path_modifier}/dist\n\n\n\n\nLinks\n\n\n\n\nANT homepage\n\n\nXML",
            "title": " Ant"
        },
        {
            "location": "/ant/#basics",
            "text": "the ANT build file is based on  XML  the usual name for the build file is  build.xml  invocation - the usual build file needs not be specified explicitly, if it's in the current directory; if not target is specified, the default one is used   ant [-f <build-file>] [<target>]   displaying all the available targets of a build file   ant [-f <build-file>] -projecthelp",
            "title": "Basics"
        },
        {
            "location": "/ant/#weka-and-ant",
            "text": "a build file for Weka is available from  subversion  (it has been included in the  weka-src.jar  since version 3.4.8 and 3.5.3)  it is located in the  weka  directory   some targets of interest    clean  - Removes the build, dist and reports directories; also any class files in the source tree   compile  - Compile weka and deposit class files in  ${path_modifier}/build/classes  docs  - Make javadocs into {${path_modifier}/doc}}  exejar  - Create an executable jar file in  ${path_modifier}/dist",
            "title": "Weka and ANT"
        },
        {
            "location": "/ant/#links",
            "text": "ANT homepage  XML",
            "title": "Links"
        },
        {
            "location": "/maven/",
            "text": "Maven\n is another build tool. But unlike \nAnt\n, it is a more high-level tool. Though its configuration file, \npom.xml\n is written in XML as well, Maven uses a different approach to the build process. In Ant, you tell it where to find Java classes for compilation, what libraries to compile against, where to put the compiled ones and then how to combine them into a jar. With Maven, you only specify dependent libraries, a compile and a jar plugin and maybe tweak the options a bit. For this to work, Maven enforces a strict \ndirectory structure\n (though you can tweak that, if you need to).\n\n\nSo why another build tool?\n\n\nWhereas Ant scripts quite often create a \nfat jar\n, i.e., a jar that contains not only the project's code, but also the contain of libraries the code was compiled against. Handy if you only want to have a single jar. However, this is a nightmare, if you need to update a single library, but all you have is a single, enormous jar. Maven handles \ndependencies automatically\n, relying on libraries (they call them artifacts) to be publicly available, e.g., on \nMaven Central\n. It allows you to use newer versions of libraries than defined by the dependent libraries (e.g., critical bug fixes), without having to modify any jars manually. Though Maven can also generate \nfar jar\n files, it is not considered good practice, as it defeats Maven's automatic version resolution.\n\n\nIn order to make Weka, and most of its packages, available to a wider audience (e.g., other software developers), we also publish on Maven Central.\n\n\nCompiling\n\n\nFor compiling Weka, you would issue a command like this (in the same directory as \npom.xml\n):\n\n\nmvn clean install\n\n\n\n\nIf you don't want the tests to run, use this:\n\n\nmvn clean install -DskipTests=true",
            "title": " Maven"
        },
        {
            "location": "/maven/#so-why-another-build-tool",
            "text": "Whereas Ant scripts quite often create a  fat jar , i.e., a jar that contains not only the project's code, but also the contain of libraries the code was compiled against. Handy if you only want to have a single jar. However, this is a nightmare, if you need to update a single library, but all you have is a single, enormous jar. Maven handles  dependencies automatically , relying on libraries (they call them artifacts) to be publicly available, e.g., on  Maven Central . It allows you to use newer versions of libraries than defined by the dependent libraries (e.g., critical bug fixes), without having to modify any jars manually. Though Maven can also generate  far jar  files, it is not considered good practice, as it defeats Maven's automatic version resolution.  In order to make Weka, and most of its packages, available to a wider audience (e.g., other software developers), we also publish on Maven Central.",
            "title": "So why another build tool?"
        },
        {
            "location": "/maven/#compiling",
            "text": "For compiling Weka, you would issue a command like this (in the same directory as  pom.xml ):  mvn clean install  If you don't want the tests to run, use this:  mvn clean install -DskipTests=true",
            "title": "Compiling"
        },
        {
            "location": "/snapshots/",
            "text": "See \nHow to get the latest bugfixes?",
            "title": " Snapsnots"
        },
        {
            "location": "/arff_stable/",
            "text": "An ARFF (Attribute-Relation File Format) file is an ASCII text file that describes a list of instances sharing a set of attributes. \n\n\nOverview\n\n\nARFF files have two distinct sections. The first section is the \nHeader\n information, which is followed the \nData\n information.\n\n\nThe \nHeader\n of the ARFF file contains the name of the relation, a list of the attributes (the columns in the data), and their types. An example header on the standard IRIS dataset looks like this:\n\n\n   % 1. Title: Iris Plants Database\n   % \n   % 2. Sources:\n   %      (a) Creator: R.A. Fisher\n   %      (b) Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n   %      (c) Date: July, 1988\n   % \n   @RELATION iris\n\n   @ATTRIBUTE sepallength  NUMERIC\n   @ATTRIBUTE sepalwidth   NUMERIC\n   @ATTRIBUTE petallength  NUMERIC\n   @ATTRIBUTE petalwidth   NUMERIC\n   @ATTRIBUTE class        {Iris-setosa,Iris-versicolor,Iris-virginica}\n\n\n\n\nThe \nData\n of the ARFF file looks like the following:\n\n\n   @DATA\n   5.1,3.5,1.4,0.2,Iris-setosa\n   4.9,3.0,1.4,0.2,Iris-setosa\n   4.7,3.2,1.3,0.2,Iris-setosa\n   4.6,3.1,1.5,0.2,Iris-setosa\n   5.0,3.6,1.4,0.2,Iris-setosa\n   5.4,3.9,1.7,0.4,Iris-setosa\n   4.6,3.4,1.4,0.3,Iris-setosa\n   5.0,3.4,1.5,0.2,Iris-setosa\n   4.4,2.9,1.4,0.2,Iris-setosa\n   4.9,3.1,1.5,0.1,Iris-setosa\n\n\n\n\nLines that begin with a \n%\n are comments. The \n@RELATION\n, \n@ATTRIBUTE\n and \n@DATA\n declarations are case insensitive.\n\n\nExamples\n\n\nSeveral well-known machine learning datasets are distributed with Weka in the \n$WEKAHOME/data\n directory as ARFF files.\n\n\nThe ARFF Header Section\n\n\nThe ARFF Header section of the file contains the relation declaration and attribute declarations.\n\n\nThe @relation Declaration\n\n\nThe relation name is defined as the first line in the ARFF file. The format is:\n\n\n    @relation [relation-name]\n\n\n\n\nwhere \n[relation-name]\n is a string. The string must be quoted if the name includes spaces. Furthermore, relation names or attribute names (see below) cannot begin with\n\n\n\n\na character below \\u0021\n\n\n'{', '}', ',', or '%' \n\n\n\n\nMoreover, it can only begin with a single or double quote if there is a corresponding quote at the end of the name.\n\n\nThe @attribute Declarations\n\n\nAttribute declarations take the form of an ordered sequence of \n@attribute\n statements. Each attribute in the data set has its own \n@attribute\n statement which uniquely defines the name of that attribute and its data type. The order the attributes are declared indicates the column position in the data section of the file. For example, if an attribute is the third one declared then Weka expects that all that attributes values will be found in the third comma delimited column.\n\n\nThe format for the \n@attribute\n statement is:\n\n\n    @attribute [attribute-name] [datatype]\n\n\n\n\nwhere the \n[attribute-name]\n must adhere to the constraints specified in the above section on the @relation declaration.\n\n\nThe [datatype] can be any of the four types supported by Weka:\n\n\n\n\nnumeric\n\n\ninteger\n is treated as \nnumeric\n\n\nreal\n is treated as \nnumeric\n\n\n[nominal-specification]\n\n\nstring\n\n\ndate\n [date-format]\n\n\nrelational\n for multi-instance data (for future use)\n\n\n\n\nwhere \n[nominal-specification]\n and \n[date-format]\n are defined below. The keywords \nnumeric\n, \nreal\n, \ninteger\n, \nstring\n and \ndate\n are case insensitive.\n\n\nNumeric attributes\n\n\nNumeric attributes can be real or integer numbers.\n\n\nNominal attributes\n\n\nNominal values are defined by providing an [nominal-specification] listing the possible values: {[nominal-name1], [nominal-name2], [nominal-name3], ...}\n\n\nFor example, the class value of the Iris dataset can be defined as follows:\n\n\n    @ATTRIBUTE class        {Iris-setosa,Iris-versicolor,Iris-virginica}\n\n\n\n\nValues that contain spaces must be quoted.\n\n\nString attributes\n\n\nString attributes allow us to create attributes containing arbitrary textual values. This is very useful in text-mining applications, as we can create datasets with string attributes, then write Weka Filters to manipulate strings (like \nStringToWordVectorFilter\n). String attributes are declared as follows:\n\n\n    @ATTRIBUTE LCC    string\n\n\n\n\nDate attributes\n\n\nDate attribute declarations take the form:\n\n\n    @attribute [name] date [[date-format]]\n\n\n\n\nwhere [name] is the name for the attribute and \n[date-format]\n is an optional string specifying how date values should be parsed and printed (this is the same format used by \nSimpleDateFormat\n). The default format string accepts the ISO-8601 combined date and time format: \nyyyy-MM-dd'T'HH:mm:ss\n. Check out the Javadoc of the \njava.text.SimpleDateFormat\n class for supported character patterns.\n\n\nDates must be specified in the data section as the corresponding string representations of the date/time (see example below).\n\n\nRelational attributes\n\n\nRelational attribute declarations take the form:\n\n\n    @attribute [name] relational\n      [further attribute definitions]\n    @end [name]\n\n\n\n\nFor the multi-instance dataset \nMUSK1\n the definition would look like this (\n\"...\"\n denotes an omission):\n\n\n @attribute molecule_name {MUSK-jf78,...,NON-MUSK-199}\n @attribute bag relational\n   @attribute f1 numeric\n   ...\n   @attribute f166 numeric\n @end bag\n @attribute class {0,1}\n ...\n\n\n\n\nThe ARFF Data Section\n\n\nThe ARFF Data section of the file contains the data declaration line and the actual instance lines.\n\n\nThe @data Declaration\n\n\nThe \n@data\n declaration is a single line denoting the start of the data segment in the file. The format is:\n\n\n    @data\n\n\n\n\nThe instance data\n\n\nEach instance is represented on a single line, with carriage returns denoting the end of the instance.  A percent sign (%) introduces a comment, which continues to the end of the line.\n\n\nAttribute values for each instance can be delimited by commas or tabs. A comma/tab may be followed by zero or more spaces. Attribute values must appear in the order in which they were declared in the header section (i.e., the data corresponding to the nth \n@attribute\n declaration is always the nth field of the attribute).\n\n\nA missing value is represented by a single question mark, as in:\n\n\n    @data\n    4.4,?,1.5,?,Iris-setosa\n\n\n\n\nValues of string and nominal attributes are case sensitive, and any that contain space or the comment-delimiter character % must be quoted.  (The code suggests that double-quotes are acceptable and that a backslash will escape individual characters.)  An example follows:\n\n\n    @relation LCCvsLCSH\n\n    @attribute LCC string\n    @attribute LCSH string\n\n    @data\n    AG5,   'Encyclopedias and dictionaries.;Twentieth century.'\n    AS262, 'Science -- Soviet Union -- History.'\n    AE5,   'Encyclopedias and dictionaries.'\n    AS281, 'Astronomy, Assyro-Babylonian.;Moon -- Phases.'\n    AS281, 'Astronomy, Assyro-Babylonian.;Moon -- Tables.'\n\n\n\n\nDates must be specified in the data section using the string representation specified in the attribute declaration. For example:\n\n\n    @RELATION Timestamps\n\n    @ATTRIBUTE timestamp DATE \"yyyy-MM-dd HH:mm:ss\"\n\n    @DATA \n    \"2001-04-03 12:12:12\"\n    \"2001-05-03 12:59:55\"\n\n\n\n\nRelational data must be enclosed within double quotes \n\"\n. For example an instance of the \nMUSK1\n dataset (\n\"...\"\n denotes an omission):\n\n\n MUSK-188,\"42,...,30\",1\n\n\n\n\nSparse ARFF files\n\n\nSparse ARFF files are very similar to ARFF files, but data with value 0 are not be explicitly represented.\n\n\nSparse ARFF files have the same header (i.e \n@relation\n and \n@attribute\n tags) but the data section is different. Instead of representing each value in order, like this:\n\n\n    @data\n    0, X, 0, Y, \"class A\"\n    0, 0, W, 0, \"class B\"\n\n\n\n\nthe non-zero attributes are explicitly identified by attribute number and their value stated, like this:\n\n\n    @data\n    {1 X, 3 Y, 4 \"class A\"}\n    {2 W, 4 \"class B\"}\n\n\n\n\nEach instance is surrounded by curly braces, and the format for each entry is: [index] [space] [value] where index is the attribute index (starting from 0).\n\n\nNote that the omitted values in a sparse instance are \n0\n, they are not \"missing\" values! If a value is unknown, you must explicitly represent it with a question mark (?).\n\n\nWarning:\n There is a known problem saving SparseInstance objects from datasets that have string attributes. In Weka, string and nominal data values are stored as numbers; these numbers act as indexes into an array of possible attribute values (this is very efficient). However, the first string value is assigned index 0: this means that, internally, this value is stored as a 0. When a SparseInstance is written, string instances with internal value 0 are not output, so their string value is lost (and when the arff file is read again, the default value 0 is the index of a different string value, so the attribute value appears to change). To get around this problem, add a dummy string value at index 0 that is never used whenever you declare string attributes that are likely to be used in SparseInstance objects and saved as Sparse ARFF files.\n\n\nInstance weights in ARFF files\n\n\nA weight can be associated with an instance in a standard ARFF file by appending it to the end of the line for that instance and enclosing the value in curly braces. E.g:\n\n\n    @data\n    0, X, 0, Y, \"class A\", {5}\n\n\n\n\nFor a sparse instance, this example would look like:\n\n\n    @data\n    {1 X, 3 Y, 4 \"class A\"}, {5}\n\n\n\n\nNote that any instance without a weight value specified is assumed to have a weight of 1 for backwards compatibility.\n\n\nSee also\n\n\n\n\n[[Add weights to dataset]]\n\n\nARFF Syntax Highlighting\n for various editors\n\n\n\n\nLinks\n\n\n\n\nISO-8601\n\n\nJavadoc of \njava.text.SimpleDateFormat\n (lists the supported character patterns)\n\n\nANTLR syntax by Staal A. Vinterbo \narff.g",
            "title": " ARFF (stable version)"
        },
        {
            "location": "/arff_stable/#overview",
            "text": "ARFF files have two distinct sections. The first section is the  Header  information, which is followed the  Data  information.  The  Header  of the ARFF file contains the name of the relation, a list of the attributes (the columns in the data), and their types. An example header on the standard IRIS dataset looks like this:     % 1. Title: Iris Plants Database\n   % \n   % 2. Sources:\n   %      (a) Creator: R.A. Fisher\n   %      (b) Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n   %      (c) Date: July, 1988\n   % \n   @RELATION iris\n\n   @ATTRIBUTE sepallength  NUMERIC\n   @ATTRIBUTE sepalwidth   NUMERIC\n   @ATTRIBUTE petallength  NUMERIC\n   @ATTRIBUTE petalwidth   NUMERIC\n   @ATTRIBUTE class        {Iris-setosa,Iris-versicolor,Iris-virginica}  The  Data  of the ARFF file looks like the following:     @DATA\n   5.1,3.5,1.4,0.2,Iris-setosa\n   4.9,3.0,1.4,0.2,Iris-setosa\n   4.7,3.2,1.3,0.2,Iris-setosa\n   4.6,3.1,1.5,0.2,Iris-setosa\n   5.0,3.6,1.4,0.2,Iris-setosa\n   5.4,3.9,1.7,0.4,Iris-setosa\n   4.6,3.4,1.4,0.3,Iris-setosa\n   5.0,3.4,1.5,0.2,Iris-setosa\n   4.4,2.9,1.4,0.2,Iris-setosa\n   4.9,3.1,1.5,0.1,Iris-setosa  Lines that begin with a  %  are comments. The  @RELATION ,  @ATTRIBUTE  and  @DATA  declarations are case insensitive.",
            "title": "Overview"
        },
        {
            "location": "/arff_stable/#examples",
            "text": "Several well-known machine learning datasets are distributed with Weka in the  $WEKAHOME/data  directory as ARFF files.",
            "title": "Examples"
        },
        {
            "location": "/arff_stable/#the-arff-header-section",
            "text": "The ARFF Header section of the file contains the relation declaration and attribute declarations.",
            "title": "The ARFF Header Section"
        },
        {
            "location": "/arff_stable/#the-relation-declaration",
            "text": "The relation name is defined as the first line in the ARFF file. The format is:      @relation [relation-name]  where  [relation-name]  is a string. The string must be quoted if the name includes spaces. Furthermore, relation names or attribute names (see below) cannot begin with   a character below \\u0021  '{', '}', ',', or '%'    Moreover, it can only begin with a single or double quote if there is a corresponding quote at the end of the name.",
            "title": "The @relation Declaration"
        },
        {
            "location": "/arff_stable/#the-attribute-declarations",
            "text": "Attribute declarations take the form of an ordered sequence of  @attribute  statements. Each attribute in the data set has its own  @attribute  statement which uniquely defines the name of that attribute and its data type. The order the attributes are declared indicates the column position in the data section of the file. For example, if an attribute is the third one declared then Weka expects that all that attributes values will be found in the third comma delimited column.  The format for the  @attribute  statement is:      @attribute [attribute-name] [datatype]  where the  [attribute-name]  must adhere to the constraints specified in the above section on the @relation declaration.  The [datatype] can be any of the four types supported by Weka:   numeric  integer  is treated as  numeric  real  is treated as  numeric  [nominal-specification]  string  date  [date-format]  relational  for multi-instance data (for future use)   where  [nominal-specification]  and  [date-format]  are defined below. The keywords  numeric ,  real ,  integer ,  string  and  date  are case insensitive.",
            "title": "The @attribute Declarations"
        },
        {
            "location": "/arff_stable/#numeric-attributes",
            "text": "Numeric attributes can be real or integer numbers.",
            "title": "Numeric attributes"
        },
        {
            "location": "/arff_stable/#nominal-attributes",
            "text": "Nominal values are defined by providing an [nominal-specification] listing the possible values: {[nominal-name1], [nominal-name2], [nominal-name3], ...}  For example, the class value of the Iris dataset can be defined as follows:      @ATTRIBUTE class        {Iris-setosa,Iris-versicolor,Iris-virginica}  Values that contain spaces must be quoted.",
            "title": "Nominal attributes"
        },
        {
            "location": "/arff_stable/#string-attributes",
            "text": "String attributes allow us to create attributes containing arbitrary textual values. This is very useful in text-mining applications, as we can create datasets with string attributes, then write Weka Filters to manipulate strings (like  StringToWordVectorFilter ). String attributes are declared as follows:      @ATTRIBUTE LCC    string",
            "title": "String attributes"
        },
        {
            "location": "/arff_stable/#date-attributes",
            "text": "Date attribute declarations take the form:      @attribute [name] date [[date-format]]  where [name] is the name for the attribute and  [date-format]  is an optional string specifying how date values should be parsed and printed (this is the same format used by  SimpleDateFormat ). The default format string accepts the ISO-8601 combined date and time format:  yyyy-MM-dd'T'HH:mm:ss . Check out the Javadoc of the  java.text.SimpleDateFormat  class for supported character patterns.  Dates must be specified in the data section as the corresponding string representations of the date/time (see example below).",
            "title": "Date attributes"
        },
        {
            "location": "/arff_stable/#relational-attributes",
            "text": "Relational attribute declarations take the form:      @attribute [name] relational\n      [further attribute definitions]\n    @end [name]  For the multi-instance dataset  MUSK1  the definition would look like this ( \"...\"  denotes an omission):   @attribute molecule_name {MUSK-jf78,...,NON-MUSK-199}\n @attribute bag relational\n   @attribute f1 numeric\n   ...\n   @attribute f166 numeric\n @end bag\n @attribute class {0,1}\n ...",
            "title": "Relational attributes"
        },
        {
            "location": "/arff_stable/#the-arff-data-section",
            "text": "The ARFF Data section of the file contains the data declaration line and the actual instance lines.",
            "title": "The ARFF Data Section"
        },
        {
            "location": "/arff_stable/#the-data-declaration",
            "text": "The  @data  declaration is a single line denoting the start of the data segment in the file. The format is:      @data",
            "title": "The @data Declaration"
        },
        {
            "location": "/arff_stable/#the-instance-data",
            "text": "Each instance is represented on a single line, with carriage returns denoting the end of the instance.  A percent sign (%) introduces a comment, which continues to the end of the line.  Attribute values for each instance can be delimited by commas or tabs. A comma/tab may be followed by zero or more spaces. Attribute values must appear in the order in which they were declared in the header section (i.e., the data corresponding to the nth  @attribute  declaration is always the nth field of the attribute).  A missing value is represented by a single question mark, as in:      @data\n    4.4,?,1.5,?,Iris-setosa  Values of string and nominal attributes are case sensitive, and any that contain space or the comment-delimiter character % must be quoted.  (The code suggests that double-quotes are acceptable and that a backslash will escape individual characters.)  An example follows:      @relation LCCvsLCSH\n\n    @attribute LCC string\n    @attribute LCSH string\n\n    @data\n    AG5,   'Encyclopedias and dictionaries.;Twentieth century.'\n    AS262, 'Science -- Soviet Union -- History.'\n    AE5,   'Encyclopedias and dictionaries.'\n    AS281, 'Astronomy, Assyro-Babylonian.;Moon -- Phases.'\n    AS281, 'Astronomy, Assyro-Babylonian.;Moon -- Tables.'  Dates must be specified in the data section using the string representation specified in the attribute declaration. For example:      @RELATION Timestamps\n\n    @ATTRIBUTE timestamp DATE \"yyyy-MM-dd HH:mm:ss\"\n\n    @DATA \n    \"2001-04-03 12:12:12\"\n    \"2001-05-03 12:59:55\"  Relational data must be enclosed within double quotes  \" . For example an instance of the  MUSK1  dataset ( \"...\"  denotes an omission):   MUSK-188,\"42,...,30\",1",
            "title": "The instance data"
        },
        {
            "location": "/arff_stable/#sparse-arff-files",
            "text": "Sparse ARFF files are very similar to ARFF files, but data with value 0 are not be explicitly represented.  Sparse ARFF files have the same header (i.e  @relation  and  @attribute  tags) but the data section is different. Instead of representing each value in order, like this:      @data\n    0, X, 0, Y, \"class A\"\n    0, 0, W, 0, \"class B\"  the non-zero attributes are explicitly identified by attribute number and their value stated, like this:      @data\n    {1 X, 3 Y, 4 \"class A\"}\n    {2 W, 4 \"class B\"}  Each instance is surrounded by curly braces, and the format for each entry is: [index] [space] [value] where index is the attribute index (starting from 0).  Note that the omitted values in a sparse instance are  0 , they are not \"missing\" values! If a value is unknown, you must explicitly represent it with a question mark (?).  Warning:  There is a known problem saving SparseInstance objects from datasets that have string attributes. In Weka, string and nominal data values are stored as numbers; these numbers act as indexes into an array of possible attribute values (this is very efficient). However, the first string value is assigned index 0: this means that, internally, this value is stored as a 0. When a SparseInstance is written, string instances with internal value 0 are not output, so their string value is lost (and when the arff file is read again, the default value 0 is the index of a different string value, so the attribute value appears to change). To get around this problem, add a dummy string value at index 0 that is never used whenever you declare string attributes that are likely to be used in SparseInstance objects and saved as Sparse ARFF files.",
            "title": "Sparse ARFF files"
        },
        {
            "location": "/arff_stable/#instance-weights-in-arff-files",
            "text": "A weight can be associated with an instance in a standard ARFF file by appending it to the end of the line for that instance and enclosing the value in curly braces. E.g:      @data\n    0, X, 0, Y, \"class A\", {5}  For a sparse instance, this example would look like:      @data\n    {1 X, 3 Y, 4 \"class A\"}, {5}  Note that any instance without a weight value specified is assumed to have a weight of 1 for backwards compatibility.",
            "title": "Instance weights in ARFF files"
        },
        {
            "location": "/arff_stable/#see-also",
            "text": "[[Add weights to dataset]]  ARFF Syntax Highlighting  for various editors",
            "title": "See also"
        },
        {
            "location": "/arff_stable/#links",
            "text": "ISO-8601  Javadoc of  java.text.SimpleDateFormat  (lists the supported character patterns)  ANTLR syntax by Staal A. Vinterbo  arff.g",
            "title": "Links"
        },
        {
            "location": "/arff_developer/",
            "text": "An ARFF (Attribute-Relation File Format) file is an ASCII text file that describes a list of instances sharing a set of attributes. \n\n\nOverview\n\n\nARFF files have two distinct sections. The first section is the \nHeader\n information, which is followed the \nData\n information.\n\n\nThe \nHeader\n of the ARFF file contains the name of the relation, a list of the attributes (the columns in the data), and their types. An example header on the standard IRIS dataset looks like this:\n\n\n   % 1. Title: Iris Plants Database\n   % \n   % 2. Sources:\n   %      (a) Creator: R.A. Fisher\n   %      (b) Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n   %      (c) Date: July, 1988\n   % \n   @RELATION iris\n\n   @ATTRIBUTE sepallength  NUMERIC\n   @ATTRIBUTE sepalwidth   NUMERIC\n   @ATTRIBUTE petallength  NUMERIC\n   @ATTRIBUTE petalwidth   NUMERIC\n   @ATTRIBUTE class        {Iris-setosa,Iris-versicolor,Iris-virginica}\n\n\n\n\nThe \nData\n of the ARFF file looks like the following:\n\n\n   @DATA\n   5.1,3.5,1.4,0.2,Iris-setosa\n   4.9,3.0,1.4,0.2,Iris-setosa\n   4.7,3.2,1.3,0.2,Iris-setosa\n   4.6,3.1,1.5,0.2,Iris-setosa\n   5.0,3.6,1.4,0.2,Iris-setosa\n   5.4,3.9,1.7,0.4,Iris-setosa\n   4.6,3.4,1.4,0.3,Iris-setosa\n   5.0,3.4,1.5,0.2,Iris-setosa\n   4.4,2.9,1.4,0.2,Iris-setosa\n   4.9,3.1,1.5,0.1,Iris-setosa\n\n\n\n\nLines that begin with a \n%\n are comments. The \n@RELATION\n, \n@ATTRIBUTE\n and \n@DATA\n declarations are case insensitive.\n\n\nExamples\n\n\nSeveral well-known machine learning datasets are distributed with Weka in the \n$WEKAHOME/data\n directory as ARFF files.\n\n\nThe ARFF Header Section\n\n\nThe ARFF Header section of the file contains the relation declaration and attribute declarations.\n\n\nThe @relation Declaration\n\n\nThe relation name is defined as the first line in the ARFF file. The format is:\n\n\n    @relation [relation-name]\n\n\n\n\nwhere \n[relation-name]\n is a string. The string must be quoted if the name includes spaces. Furthermore, relation names or attribute names (see below) cannot begin with\n\n\n\n\na character below \\u0021\n\n\n'{', '}', ',', or '%' \n\n\n\n\nMoreover, it can only begin with a single or double quote if there is a corresponding quote at the end of the name.\n\n\n== The @attribute Declarations ==\nAttribute declarations take the form of an ordered sequence of \n@attribute\n statements. Each attribute in the data set has its own \n@attribute\n statement which uniquely defines the name of that attribute and its data type. The order the attributes are declared indicates the column position in the data section of the file. For example, if an attribute is the third one declared then Weka expects that all that attributes values will be found in the third comma delimited column.\n\n\nThe format for the \n@attribute\n statement is:\n\n\n    @attribute [attribute-name] [datatype]\n\n\n\n\nwhere the \n[attribute-name]\n must adhere to the constraints specified in the above section on the @relation declaration.\n\n\nThe [datatype] can be any of the four types supported by Weka:\n\n\n\n\nnumeric\n\n\ninteger\n is treated as \nnumeric\n\n\nreal\n is treated as \nnumeric\n\n\n[nominal-specification]\n\n\nstring\n\n\ndate\n [date-format]\n\n\nrelational\n for multi-instance data (for future use)\n\n\n\n\nwhere \n[nominal-specification]\n and \n[date-format]\n are defined below. The keywords \nnumeric\n, \nreal\n, \ninteger\n, \nstring\n and \ndate\n are case insensitive.\n\n\nNumeric attributes\n\n\nNumeric attributes can be real or integer numbers.\n\n\nNominal attributes\n\n\nNominal values are defined by providing an [nominal-specification] listing the possible values: {[nominal-name1], [nominal-name2], [nominal-name3], ...}\n\n\nFor example, the class value of the Iris dataset can be defined as follows:\n\n\n    @ATTRIBUTE class        {Iris-setosa,Iris-versicolor,Iris-virginica}\n\n\n\n\nValues that contain spaces must be quoted.\n\n\nString attributes\n\n\nString attributes allow us to create attributes containing arbitrary textual values. This is very useful in text-mining applications, as we can create datasets with string attributes, then write Weka Filters to manipulate strings (like \nStringToWordVectorFilter\n). String attributes are declared as follows:\n\n\n    @ATTRIBUTE LCC    string\n\n\n\n\nDate attributes\n\n\nDate attribute declarations take the form:\n\n\n    @attribute [name] date [[date-format]]\n\n\n\n\nwhere [name] is the name for the attribute and \n[date-format]\n is an optional string specifying how date values should be parsed and printed (this is the same format used by \nSimpleDateFormat\n). The default format string accepts the ISO-8601 combined date and time format: \nyyyy-MM-dd'T'HH:mm:ss\n. Check out the Javadoc of the \njava.text.SimpleDateFormat\n class for supported character patterns.\n\n\nDates must be specified in the data section as the corresponding string representations of the date/time (see example below).\n\n\nRelational attributes\n\n\nRelational attribute declarations take the form:\n\n\n    @attribute [name] relational\n      [further attribute definitions]\n    @end [name]\n\n\n\n\nFor the multi-instance dataset \nMUSK1\n the definition would look like this (\n\"...\"\n denotes an omission):\n\n\n @attribute molecule_name {MUSK-jf78,...,NON-MUSK-199}\n @attribute bag relational\n   @attribute f1 numeric\n   ...\n   @attribute f166 numeric\n @end bag\n @attribute class {0,1}\n ...\n\n\n\n\nThe ARFF Data Section\n\n\nThe ARFF Data section of the file contains the data declaration line and the actual instance lines.\n\n\nThe @data Declaration\n\n\nThe \n@data\n declaration is a single line denoting the start of the data segment in the file. The format is:\n\n\n    @data\n\n\n\n\nThe instance data\n\n\nEach instance is represented on a single line, with carriage returns denoting the end of the instance.  A percent sign (%) introduces a comment, which continues to the end of the line.\n\n\nAttribute values for each instance can be delimited by commas or tabs. A comma/tab may be followed by zero or more spaces. Attribute values must appear in the order in which they were declared in the header section (i.e., the data corresponding to the nth \n@attribute\n declaration is always the nth field of the attribute).\n\n\nA missing value is represented by a single question mark, as in:\n\n\n    @data\n    4.4,?,1.5,?,Iris-setosa\n\n\n\n\nValues of string and nominal attributes are case sensitive, and any that contain space or the comment-delimiter character % must be quoted.  (The code suggests that double-quotes are acceptable and that a backslash will escape individual characters.)  An example follows:\n\n\n    @relation LCCvsLCSH\n\n    @attribute LCC string\n    @attribute LCSH string\n\n    @data\n    AG5,   'Encyclopedias and dictionaries.;Twentieth century.'\n    AS262, 'Science -- Soviet Union -- History.'\n    AE5,   'Encyclopedias and dictionaries.'\n    AS281, 'Astronomy, Assyro-Babylonian.;Moon -- Phases.'\n    AS281, 'Astronomy, Assyro-Babylonian.;Moon -- Tables.'\n\n\n\n\nDates must be specified in the data section using the string representation specified in the attribute declaration. For example:\n\n\n    @RELATION Timestamps\n\n    @ATTRIBUTE timestamp DATE \"yyyy-MM-dd HH:mm:ss\"\n\n    @DATA \n    \"2001-04-03 12:12:12\"\n    \"2001-05-03 12:59:55\"\n\n\n\n\nRelational data must be enclosed within double quotes \n\"\n. For example an instance of the \nMUSK1\n dataset (\n\"...\"\n denotes an omission):\n\n\nMUSK-188,\"42,...,30\",1\n\n\n\n\nSparse ARFF files\n\n\nSparse ARFF files are very similar to ARFF files, but data with value 0 are not be explicitly represented.\n\n\nSparse ARFF files have the same header (i.e \n@relation\n and \n@attribute\n tags) but the data section is different. Instead of representing each value in order, like this:\n\n\n    @data\n    0, X, 0, Y, \"class A\"\n    0, 0, W, 0, \"class B\"\n\n\n\n\nthe non-zero attributes are explicitly identified by attribute number and their value stated, like this:\n\n\n    @data\n    {1 X, 3 Y, 4 \"class A\"}\n    {2 W, 4 \"class B\"}\n\n\n\n\nEach instance is surrounded by curly braces, and the format for each entry is: [index] [space] [value] where index is the attribute index (starting from 0).\n\n\nNote that the omitted values in a sparse instance are \n0\n, they are not \"missing\" values! If a value is unknown, you must explicitly represent it with a question mark (?).\n\n\nWarning:\n There is a known problem saving SparseInstance objects from datasets that have string attributes. In Weka, string and nominal data values are stored as numbers; these numbers act as indexes into an array of possible attribute values (this is very efficient). However, the first string value is assigned index 0: this means that, internally, this value is stored as a 0. When a SparseInstance is written, string instances with internal value 0 are not output, so their string value is lost (and when the arff file is read again, the default value 0 is the index of a different string value, so the attribute value appears to change). To get around this problem, add a dummy string value at index 0 that is never used whenever you declare string attributes that are likely to be used in SparseInstance objects and saved as Sparse ARFF files.\n\n\nInstance weights in ARFF files\n\n\nA weight can be associated with an instance in a standard ARFF file by appending it to the end of the line for that instance and enclosing the value in curly braces. E.g:\n\n\n    @data\n    0, X, 0, Y, \"class A\", {5}\n\n\n\n\nFor a sparse instance, this example would look like:\n\n\n    @data\n    {1 X, 3 Y, 4 \"class A\"}, {5}\n\n\n\n\nNote that any instance without a weight value specified is assumed to have a weight of 1 for backwards compatibility.\n\n\nSee also\n\n\n\n\n[[Add weights to dataset]]\n\n\nARFF Syntax Highlighting\n for various editors\n\n\n\n\nLinks\n\n\n\n\nISO 8601\n\n\nJavadoc of \njava.text.SimpleDateFormat\n (lists the supported character patterns)\n\n\nANTLR syntax by Staal A. Vinterbo \narff.g",
            "title": " ARFF (developer version)"
        },
        {
            "location": "/arff_developer/#overview",
            "text": "ARFF files have two distinct sections. The first section is the  Header  information, which is followed the  Data  information.  The  Header  of the ARFF file contains the name of the relation, a list of the attributes (the columns in the data), and their types. An example header on the standard IRIS dataset looks like this:     % 1. Title: Iris Plants Database\n   % \n   % 2. Sources:\n   %      (a) Creator: R.A. Fisher\n   %      (b) Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n   %      (c) Date: July, 1988\n   % \n   @RELATION iris\n\n   @ATTRIBUTE sepallength  NUMERIC\n   @ATTRIBUTE sepalwidth   NUMERIC\n   @ATTRIBUTE petallength  NUMERIC\n   @ATTRIBUTE petalwidth   NUMERIC\n   @ATTRIBUTE class        {Iris-setosa,Iris-versicolor,Iris-virginica}  The  Data  of the ARFF file looks like the following:     @DATA\n   5.1,3.5,1.4,0.2,Iris-setosa\n   4.9,3.0,1.4,0.2,Iris-setosa\n   4.7,3.2,1.3,0.2,Iris-setosa\n   4.6,3.1,1.5,0.2,Iris-setosa\n   5.0,3.6,1.4,0.2,Iris-setosa\n   5.4,3.9,1.7,0.4,Iris-setosa\n   4.6,3.4,1.4,0.3,Iris-setosa\n   5.0,3.4,1.5,0.2,Iris-setosa\n   4.4,2.9,1.4,0.2,Iris-setosa\n   4.9,3.1,1.5,0.1,Iris-setosa  Lines that begin with a  %  are comments. The  @RELATION ,  @ATTRIBUTE  and  @DATA  declarations are case insensitive.",
            "title": "Overview"
        },
        {
            "location": "/arff_developer/#examples",
            "text": "Several well-known machine learning datasets are distributed with Weka in the  $WEKAHOME/data  directory as ARFF files.",
            "title": "Examples"
        },
        {
            "location": "/arff_developer/#the-arff-header-section",
            "text": "The ARFF Header section of the file contains the relation declaration and attribute declarations.",
            "title": "The ARFF Header Section"
        },
        {
            "location": "/arff_developer/#the-relation-declaration",
            "text": "The relation name is defined as the first line in the ARFF file. The format is:      @relation [relation-name]  where  [relation-name]  is a string. The string must be quoted if the name includes spaces. Furthermore, relation names or attribute names (see below) cannot begin with   a character below \\u0021  '{', '}', ',', or '%'    Moreover, it can only begin with a single or double quote if there is a corresponding quote at the end of the name.  == The @attribute Declarations ==\nAttribute declarations take the form of an ordered sequence of  @attribute  statements. Each attribute in the data set has its own  @attribute  statement which uniquely defines the name of that attribute and its data type. The order the attributes are declared indicates the column position in the data section of the file. For example, if an attribute is the third one declared then Weka expects that all that attributes values will be found in the third comma delimited column.  The format for the  @attribute  statement is:      @attribute [attribute-name] [datatype]  where the  [attribute-name]  must adhere to the constraints specified in the above section on the @relation declaration.  The [datatype] can be any of the four types supported by Weka:   numeric  integer  is treated as  numeric  real  is treated as  numeric  [nominal-specification]  string  date  [date-format]  relational  for multi-instance data (for future use)   where  [nominal-specification]  and  [date-format]  are defined below. The keywords  numeric ,  real ,  integer ,  string  and  date  are case insensitive.",
            "title": "The @relation Declaration"
        },
        {
            "location": "/arff_developer/#numeric-attributes",
            "text": "Numeric attributes can be real or integer numbers.",
            "title": "Numeric attributes"
        },
        {
            "location": "/arff_developer/#nominal-attributes",
            "text": "Nominal values are defined by providing an [nominal-specification] listing the possible values: {[nominal-name1], [nominal-name2], [nominal-name3], ...}  For example, the class value of the Iris dataset can be defined as follows:      @ATTRIBUTE class        {Iris-setosa,Iris-versicolor,Iris-virginica}  Values that contain spaces must be quoted.",
            "title": "Nominal attributes"
        },
        {
            "location": "/arff_developer/#string-attributes",
            "text": "String attributes allow us to create attributes containing arbitrary textual values. This is very useful in text-mining applications, as we can create datasets with string attributes, then write Weka Filters to manipulate strings (like  StringToWordVectorFilter ). String attributes are declared as follows:      @ATTRIBUTE LCC    string",
            "title": "String attributes"
        },
        {
            "location": "/arff_developer/#date-attributes",
            "text": "Date attribute declarations take the form:      @attribute [name] date [[date-format]]  where [name] is the name for the attribute and  [date-format]  is an optional string specifying how date values should be parsed and printed (this is the same format used by  SimpleDateFormat ). The default format string accepts the ISO-8601 combined date and time format:  yyyy-MM-dd'T'HH:mm:ss . Check out the Javadoc of the  java.text.SimpleDateFormat  class for supported character patterns.  Dates must be specified in the data section as the corresponding string representations of the date/time (see example below).",
            "title": "Date attributes"
        },
        {
            "location": "/arff_developer/#relational-attributes",
            "text": "Relational attribute declarations take the form:      @attribute [name] relational\n      [further attribute definitions]\n    @end [name]  For the multi-instance dataset  MUSK1  the definition would look like this ( \"...\"  denotes an omission):   @attribute molecule_name {MUSK-jf78,...,NON-MUSK-199}\n @attribute bag relational\n   @attribute f1 numeric\n   ...\n   @attribute f166 numeric\n @end bag\n @attribute class {0,1}\n ...",
            "title": "Relational attributes"
        },
        {
            "location": "/arff_developer/#the-arff-data-section",
            "text": "The ARFF Data section of the file contains the data declaration line and the actual instance lines.",
            "title": "The ARFF Data Section"
        },
        {
            "location": "/arff_developer/#the-data-declaration",
            "text": "The  @data  declaration is a single line denoting the start of the data segment in the file. The format is:      @data",
            "title": "The @data Declaration"
        },
        {
            "location": "/arff_developer/#the-instance-data",
            "text": "Each instance is represented on a single line, with carriage returns denoting the end of the instance.  A percent sign (%) introduces a comment, which continues to the end of the line.  Attribute values for each instance can be delimited by commas or tabs. A comma/tab may be followed by zero or more spaces. Attribute values must appear in the order in which they were declared in the header section (i.e., the data corresponding to the nth  @attribute  declaration is always the nth field of the attribute).  A missing value is represented by a single question mark, as in:      @data\n    4.4,?,1.5,?,Iris-setosa  Values of string and nominal attributes are case sensitive, and any that contain space or the comment-delimiter character % must be quoted.  (The code suggests that double-quotes are acceptable and that a backslash will escape individual characters.)  An example follows:      @relation LCCvsLCSH\n\n    @attribute LCC string\n    @attribute LCSH string\n\n    @data\n    AG5,   'Encyclopedias and dictionaries.;Twentieth century.'\n    AS262, 'Science -- Soviet Union -- History.'\n    AE5,   'Encyclopedias and dictionaries.'\n    AS281, 'Astronomy, Assyro-Babylonian.;Moon -- Phases.'\n    AS281, 'Astronomy, Assyro-Babylonian.;Moon -- Tables.'  Dates must be specified in the data section using the string representation specified in the attribute declaration. For example:      @RELATION Timestamps\n\n    @ATTRIBUTE timestamp DATE \"yyyy-MM-dd HH:mm:ss\"\n\n    @DATA \n    \"2001-04-03 12:12:12\"\n    \"2001-05-03 12:59:55\"  Relational data must be enclosed within double quotes  \" . For example an instance of the  MUSK1  dataset ( \"...\"  denotes an omission):  MUSK-188,\"42,...,30\",1",
            "title": "The instance data"
        },
        {
            "location": "/arff_developer/#sparse-arff-files",
            "text": "Sparse ARFF files are very similar to ARFF files, but data with value 0 are not be explicitly represented.  Sparse ARFF files have the same header (i.e  @relation  and  @attribute  tags) but the data section is different. Instead of representing each value in order, like this:      @data\n    0, X, 0, Y, \"class A\"\n    0, 0, W, 0, \"class B\"  the non-zero attributes are explicitly identified by attribute number and their value stated, like this:      @data\n    {1 X, 3 Y, 4 \"class A\"}\n    {2 W, 4 \"class B\"}  Each instance is surrounded by curly braces, and the format for each entry is: [index] [space] [value] where index is the attribute index (starting from 0).  Note that the omitted values in a sparse instance are  0 , they are not \"missing\" values! If a value is unknown, you must explicitly represent it with a question mark (?).  Warning:  There is a known problem saving SparseInstance objects from datasets that have string attributes. In Weka, string and nominal data values are stored as numbers; these numbers act as indexes into an array of possible attribute values (this is very efficient). However, the first string value is assigned index 0: this means that, internally, this value is stored as a 0. When a SparseInstance is written, string instances with internal value 0 are not output, so their string value is lost (and when the arff file is read again, the default value 0 is the index of a different string value, so the attribute value appears to change). To get around this problem, add a dummy string value at index 0 that is never used whenever you declare string attributes that are likely to be used in SparseInstance objects and saved as Sparse ARFF files.",
            "title": "Sparse ARFF files"
        },
        {
            "location": "/arff_developer/#instance-weights-in-arff-files",
            "text": "A weight can be associated with an instance in a standard ARFF file by appending it to the end of the line for that instance and enclosing the value in curly braces. E.g:      @data\n    0, X, 0, Y, \"class A\", {5}  For a sparse instance, this example would look like:      @data\n    {1 X, 3 Y, 4 \"class A\"}, {5}  Note that any instance without a weight value specified is assumed to have a weight of 1 for backwards compatibility.",
            "title": "Instance weights in ARFF files"
        },
        {
            "location": "/arff_developer/#see-also",
            "text": "[[Add weights to dataset]]  ARFF Syntax Highlighting  for various editors",
            "title": "See also"
        },
        {
            "location": "/arff_developer/#links",
            "text": "ISO 8601  Javadoc of  java.text.SimpleDateFormat  (lists the supported character patterns)  ANTLR syntax by Staal A. Vinterbo  arff.g",
            "title": "Links"
        },
        {
            "location": "/arff_syntax/",
            "text": "Here you can find syntax highlightings for various editors:\n\n\nEmacs\n\n\nAdd the code from the \narff.emacs\n file into your startup file.\n\n\nNotepad++\n\n\n\n\nCopy the contents of tag of the \narff.notepadplus\n file into your \n%APPDATA%\\Notepad++\\userDefineLang.xml\n file.\n\n\n(Ensure that you maintain the XML structure). If \nuserDefineLang.xml\n does not exist, simply rename the \narff.notepadplus\n file to \nuserDefineLang.xml\n\n\n\n\nTextPad\n\n\n\n\nCopy the file \narff.syn\n into your \n<TEXTPAD-DIR>/system\n directory.\n\n\nThen run the wizard for adding a new document class (Configure -> New Document Class...).\n\n\n\n\nUltraedit\n\n\n\n\nJust copy/paste the content of the file \narff.ultraedit\n in your \n<ULTRAEDIT-DIR>/WORDFILE.TXT\n file.\n\n\nAdjust the \n/Lnn\n language number that it fits into the numbering of your current settings.\n\n\n\n\nvim/gvim\n\n\n\n\nSave the file \narff.vim\n in your \n$HOME/.vim/syntax\n directory.\n\n\nYou can enable the syntax with \n:set syntax=arff\n.\n\n\n\n\nLinks\n\n\n\n\nEmacs homepage\n\n\nNotepad++ homepage\n\n\nTextPad homepage\n\n\nUltraedit homepage\n\n\nvim homepage",
            "title": " ARFF Syntax Highlighting"
        },
        {
            "location": "/arff_syntax/#emacs",
            "text": "Add the code from the  arff.emacs  file into your startup file.",
            "title": "Emacs"
        },
        {
            "location": "/arff_syntax/#notepad",
            "text": "Copy the contents of tag of the  arff.notepadplus  file into your  %APPDATA%\\Notepad++\\userDefineLang.xml  file.  (Ensure that you maintain the XML structure). If  userDefineLang.xml  does not exist, simply rename the  arff.notepadplus  file to  userDefineLang.xml",
            "title": "Notepad++"
        },
        {
            "location": "/arff_syntax/#textpad",
            "text": "Copy the file  arff.syn  into your  <TEXTPAD-DIR>/system  directory.  Then run the wizard for adding a new document class (Configure -> New Document Class...).",
            "title": "TextPad"
        },
        {
            "location": "/arff_syntax/#ultraedit",
            "text": "Just copy/paste the content of the file  arff.ultraedit  in your  <ULTRAEDIT-DIR>/WORDFILE.TXT  file.  Adjust the  /Lnn  language number that it fits into the numbering of your current settings.",
            "title": "Ultraedit"
        },
        {
            "location": "/arff_syntax/#vimgvim",
            "text": "Save the file  arff.vim  in your  $HOME/.vim/syntax  directory.  You can enable the syntax with  :set syntax=arff .",
            "title": "vim/gvim"
        },
        {
            "location": "/arff_syntax/#links",
            "text": "Emacs homepage  Notepad++ homepage  TextPad homepage  Ultraedit homepage  vim homepage",
            "title": "Links"
        },
        {
            "location": "/classpath/",
            "text": "The \nCLASSPATH\n environment variable tells Java where to look for classes. Since Java does the search in a ''first-come-first-serve'' kind of manner, you'll have to take care where and what to put in your CLASSPATH. I, personally, never use the environment variable, since I'm working often on a project in different versions in parallel. The CLASSPATH would just mess up things, if you're not careful (or just forget to remove an entry). [[ANT]] offers a nice way for building (and separating source code and class files) Java projects.\nBut still, if you're only working on totally separate projects, it might be easiest for you to use the environment variable.\n\n\nSetting the CLASSPATH\n\n\nIn the following we add the \nmysql-connector-java-5.1.6-bin.jar\n to our \nCLASSPATH\n variable (this works for any other jar archive) to make it possible to access MySQL \nDatabases\n via JDBC.\n\n\nWindows\n\n\nWe assume that the \nmysql-connector-java-5.1.6-bin.jar\n archive is located in the following directory:\n\n\nC:\\Program Files\\Weka-3-8\n\n\n\n\nIn the \nControl Panel\n click on \nSystem\n (or right click on \nThis PC\n and select \nProperties\n) and then go to the \nAdvanced\n tab. There you will find a button called \nEnvironment Variables\n, click it.\n\n\nDepending on, whether you're the only person using this computer or it is a lab computer shared by many, you can either create a new system-wide (you are the only user) environment variable or a user dependent one (recommended for multi-user machines). Enter the following name for the variable\n\n\nCLASSPATH\n\n\n\n\nand add this value\n\n\nC:\\Program Files\\Weka-3-8\\mysql-connector-java-5.1.6-bin.jar\n\n\n\n\nIf you want to add additional jars, you'll have to separate them with the path separator, the semicolon \n;\n (no spaces!).\n\n\nUnix/Linux\n\n\nI assume, that the mysql jar is located in the following directory:\n\n\n/home/johndoe/jars/\n\n\n\n\nOpen a shell and execute the following command, depending on the shell you're using:\n\n\n\n\nbash\n\n\n\n\nexport CLASSPATH=$CLASSPATH:/home/johndoe/jars/mysql-connector-java-5.1.6-bin.jar\n\n\n\n\nc shell\n\n\n\n\nsetenv CLASSPATH $CLASSPATH:/home/johndoe/jars/mysql-connector-java-5.1.6-bin.jar\n\n\nUnix/Linux uses the colon \n:\n as path separator, in contrast to Windows, which uses the semicolon \n;\n.\n\n\nNote:\n the prefixing with \n$CLASSPATH\n adds the mysql jar at the end of the currently existing \nCLASSPATH\n.\n\n\nCygwin\n\n\nThe process is like with Unix/Linux systems, but since the host system is Win32 and therefore the Java installation also a Windows application, you'll have to use the semicolon \n;\n as separator for several jars.",
            "title": " CLASSPATH"
        },
        {
            "location": "/classpath/#setting-the-classpath",
            "text": "In the following we add the  mysql-connector-java-5.1.6-bin.jar  to our  CLASSPATH  variable (this works for any other jar archive) to make it possible to access MySQL  Databases  via JDBC.",
            "title": "Setting the CLASSPATH"
        },
        {
            "location": "/classpath/#windows",
            "text": "We assume that the  mysql-connector-java-5.1.6-bin.jar  archive is located in the following directory:  C:\\Program Files\\Weka-3-8  In the  Control Panel  click on  System  (or right click on  This PC  and select  Properties ) and then go to the  Advanced  tab. There you will find a button called  Environment Variables , click it.  Depending on, whether you're the only person using this computer or it is a lab computer shared by many, you can either create a new system-wide (you are the only user) environment variable or a user dependent one (recommended for multi-user machines). Enter the following name for the variable  CLASSPATH  and add this value  C:\\Program Files\\Weka-3-8\\mysql-connector-java-5.1.6-bin.jar  If you want to add additional jars, you'll have to separate them with the path separator, the semicolon  ;  (no spaces!).",
            "title": "Windows"
        },
        {
            "location": "/classpath/#unixlinux",
            "text": "I assume, that the mysql jar is located in the following directory:  /home/johndoe/jars/  Open a shell and execute the following command, depending on the shell you're using:   bash   export CLASSPATH=$CLASSPATH:/home/johndoe/jars/mysql-connector-java-5.1.6-bin.jar   c shell   setenv CLASSPATH $CLASSPATH:/home/johndoe/jars/mysql-connector-java-5.1.6-bin.jar  Unix/Linux uses the colon  :  as path separator, in contrast to Windows, which uses the semicolon  ; .  Note:  the prefixing with  $CLASSPATH  adds the mysql jar at the end of the currently existing  CLASSPATH .",
            "title": "Unix/Linux"
        },
        {
            "location": "/classpath/#cygwin",
            "text": "The process is like with Unix/Linux systems, but since the host system is Win32 and therefore the Java installation also a Windows application, you'll have to use the semicolon  ;  as separator for several jars.",
            "title": "Cygwin"
        },
        {
            "location": "/packages/manager/",
            "text": "Usually, the term \"package\" is used to refer to Java's concept of organizing classes. From version 3.7.2, Weka has the concept of a package as a bundle of additional functionality, separate from that supplied in the main weka.jar file. A package consists of various jar files, documentation, meta data, and possibly source code. Many learning algorithms and tools that were present in earlier versions of Weka have become separate packages from version 3.7.2. This simplifies the core Weka system and allows users to install just what they need or are interested in. It also provides a simple mechanism for people to use when contributing to Weka. There are a number of packages available for Weka that add learning schemes or extend the functionality of the core system in some fashion. Many are provided by the Weka team and others are from third parties.\n\n\nWeka includes a facility for the management of packages and a mechanism to load them dynamically at runtime. There is both a command-line and GUI package manager. If the package manager does not start when you try to run it, take a look at \nthis\n page.\n\n\nCommand line package management\n\n\nAssuming that the weka.jar file is in the classpath, the package manager can be accessed by typing:\n\n\n java weka.core.WekaPackageManager\n\n\n\n\nSupplying no options will print the usage information:\n\n\nUsage: weka.core.WekaPackageManager [option]\nOptions:\n  -list-packages <all | installed | available>\n  -package-info <repository | installed | archive> packageName\n  -install-package <packageName | packageZip | URL> [version]\n  -uninstall-package <packageName>\n  -refresh-cache\n\n\n\n\n\n\nWeka 3.7.8 and snapshot builds of the developer version of Weka after September 24 2012 now offer a completely \"offline\" mode that involves no attempts to connect to the internet. This mode can be used to install package zip files that the user already has on the file system, and to browse already installed packages. This mode can be accessed from the command line package manager by specifying the \"-offline\" option. Alternatively, the property weka.packageManager.offline=true can be provided to the Java virtual machine on the command line or in a properties file (see the section on properties below).\n\n\n\n\nInformation (meta data) about packages is stored on a web server hosted on Sourceforge. The first time the package manager is run, for a new installation of Weka, there will be a short delay while the system downloads and stores a cache of the meta data from the server. Maintaining a cache speeds up the process of browsing the package information. From time to time you should update the local cache of package meta data in order to get the latest information on packages from the server. This can be achieved by supplying the \n-refresh-cache\n option.\n\n\nThe \n-list-packages\n option will, as the name suggests, print information (version numbers and short descriptions) about various packages. The option must be followed by one of three keywords:\n\n\n\n\nall\n will print information on all packages that the system knows about\n\n\ninstalled\n will print information on all packages that are installed locally\n\n\navailable\n will print information on all packages that are not installed\n\n\n\n\nThe following shows an example of listing all packages installed locally:\n\n\njava weka.core.WekaPackageManager -list-packages installed\n\nInstalled    Repository    Package\n=========    ==========    =======\n1.0.0        1.0.0         DTNB: Class for building and using a decision table/naive bayes hybrid classifier.\n1.0.0        1.0.0         massiveOnlineAnalysis: MOA (Massive On-line Analysis).\n1.0.0        1.0.0         multiInstanceFilters: A collection of filters for manipulating multi-instance data.\n1.0.0        1.0.0         naiveBayesTree: Class for generating a decision tree with naive Bayes classifiers at the leaves.\n1.0.0        1.0.0         scatterPlot3D: A visualization component for displaying a 3D scatter plot of the data using Java 3D.\n\n\n\n\nThe \n-package-info\n command lists information about a package given its name. The command is followed by one of three keywords and then the name of a package:\n\n\n\n\nrepository\n will print info from the repository for the named package\n\n\ninstalled\n will print info on the installed version of the named package\n\n\narchive\n will print info for a package stored in a zip archive. In this case, the \u201carchive\u201d keyword must be followed by the path to an package zip archive file rather than just the name of a package\n\n\n\n\nThe following shows an example of listing information for the \u201cisotonicRegression\u201d package from the server:\n\n\njava weka.core.WekaPackageManager -package-info repository isotonicRegression\nDescription:Learns an isotonic regression model. Picks the attribute that results in the lowest squared error. Missing values are not allowed. Can only deal with numeric attributes. Considers the monotonically increasing case as well as the monotonically decreasing case.\nVersion:1.0.0\nPackageURL:http://prdownloads.sourceforge.net/weka/isotonicRegression1.0.0.zip?download\nAuthor:Eibe Frank\nPackageName:isotonicRegression\nTitle:Learns an isotonic regression model.\nDate:2009-09-10\nURL:http://weka.sourceforge.net/doc.dev/weka/classifiers/IsotonicRegression.html\nCategory:Regression\nDepends:weka (>=3.7.1)\nLicense:GPL 2.0\nMaintainer:Weka team <wekalist@list.scms.waikato.ac.nz>\n\n\n\n\nThe \n-install-package\n command allows a package to be installed from one of three locations:\n\n\n\n\nspecifying a name of a package will install the package using the information in the package description meta data stored on the server. If no version number is given, then the latest available version of the package is installed.\n\n\nproviding a path to a zip file will attempt to unpack and install the archive as a Weka package\n\n\nproviding a URL (beginning with \nhttp://\n) to a package zip file on the web will download and attempt to install the zip file as a Weka package\n\n\n\n\nThe \nuninstall-package\n command will uninstall the named package. Of course, the named package has to be installed for this command to have any effect!\n\n\nRunning installed learning algorithms\n\n\nRunning learning algorithms that come with the main weka distribution (i.e. are contained in the weka.jar file) was covered earlier in the [[Primer]]. But what about algorithms from packages that you\u2019ve installed using the package manager? We don\u2019t want to have to add a ton of jar files to our classpath every time we wan\u2019t to run a particular algorithm. Fortunately, we don\u2019t have to. Weka has a mechanism to load installed packages dynamically at run time. This means that newly installed packages are available in Weka's GUIs immediately.\n\n\nWhat about running algorithms from packages on the command line I hear you ask? We can run a named algorithm by using the Run command:\n\n\n java weka.Run\n\n\n\n\nIf no arguments are supplied, then Run outputs the following usage information:\n\n\n Usage:\n     weka.Run [-no-scan] [-no-load] <scheme name [scheme options]>\n\n\n\n\nThe Run command supports sub-string matching, so you can run a classifier (such as J48) like so:\n\n\n java weka.Run J48\n\n\n\n\nWhen there are multiple matches on a supplied scheme name you will be presented with a list. For example:\n\n\n java weka.Run NaiveBayes\n\n Select a scheme to run, or <return> to exit:\n     1) weka.classifiers.bayes.ComplementNaiveBayes\n     2) weka.classifiers.bayes.NaiveBayes\n     3) weka.classifiers.bayes.NaiveBayesMultinomial\n     4) weka.classifiers.bayes.NaiveBayesMultinomialUpdateable\n     5) weka.classifiers.bayes.NaiveBayesSimple\n     6) weka.classifiers.bayes.NaiveBayesUpdateable\n\n Enter a number >\n\n\n\n\nYou can turn off the scanning of packages and sub-string matching by providing the \n-no-scan\n option. This is useful when using the \nRun\n command in a script. In this case, you need to specify the fully qualified name of the algorithm to use. E.g.\n\n\n java weka.Run -no-scan weka.classifiers.bayes.NaiveBayes\n\n\n\n\nTo reduce startup time you can also turn off the dynamic loading of installed packages by specifying the \n-no-load\n option. In this case, you will need to explicitly include any packaged algorithms in your classpath if you plan to use them. E.g.\n\n\njava -classpath ./weka.jar:$HOME/wekafiles/packages/DTNB/DTNB.jar rweka.Run -no-load -no-scan weka.classifiers.rules.DTNB\n\n\n\n\nGUI package manager\n\n\nAs well as a command line client, there is also a graphical interface to Weka\u2019s package management system. This is available from the \nTools\n menu in the \nGUIChooser\n. All the functionality available in the command line client to the package management system is available in the GUI version, along with the ability to install and uninstall multiple packages in one hit.\n\n\n\n\nThe package manager\u2019s window is split horizontally into two parts: at the top is a list of packages and at the bottom is a mini browser that can be used to display information on the currently selected package.\n\n\nThe package list shows the name of a package, its category, the currently installed version (if installed), the latest version available via the repository and whether the package has been loaded or not. This list may be sorted by either package name or category by clicking on the appropriate column header. A second click on the same header reverses the sort order. Three radio buttons in the upper left of the window can be used to filter what is displayed in the list. All packages (default), all available packages (i.e. those not yet installed) or only installed packages can be displayed.\n\n\nIf multiple versions of a package are available, they can be accessed by clicking on an entry in the \u201cRepository version\u201d column:\n\n\n\n\nInstalling and removing packages\n\n\nAt the very top of the window are three buttons. On the left-hand side is a button that can be used to refresh the cached copy of the package repository meta data. The first time that the package manager (GUI or command line) is used there will be a short delay as the initial cache is established. \n\n\nNOTE: Weka (3.7.2) will not automatically check for new information at the central repository, so it is a good idea to refresh the local cache regularly. From Weka 3.7.3 the package manager will notify you if there are new packages available at the central repository\n.\n\n\nThe two buttons at the top right are used to install and remove packages repspectively. Multiple packages may be installed/removed by using a shift-left-click combination to select a range and/or by using a command-left-click combination to add to the selection. Underneath the install and uninstall but- tons is a checkbox that can be enabled to ignore any dependencies required by selected packages and any conflicts that may occur. Installing packages while this checkbox is selected will '''not''' install required dependencies.\n\n\nSome packages may have additional information on how to complete the installation or special instructions that gets displayed when the package is installed:\n\n\n\n\nUsually it is not necessary to restart Weka after packages have been installed\u2014the changes should be available immediately. An exception is when upgrading a package that is already installed. If in doubt, restart Weka.\n\n\nUnofficial packages\n\n\nThe package list shows those packages that have their meta data stored in Weka\u2019s central meta data repository. These packages are \u201cofficial\u201d Weka packages and the Weka team as verified that they appear to provide what is advertised (and do not contain malicious code or malware).\n\n\nIt is also possible to install an \nunofficial\n package that has not gone through the process of become official. Unofficial packages might be provided, for exam- ple, by researchers who want to make experimental algorithms quickly available to the community for feedback. Unofficial packages can be installed by clicking the \u201cFile/url\u201d button on the top-right of the package manager window. This will bring up an \u201cUnnoficial package install\u201d dialog where the user can browse their file system for a package zip file or directly enter an URL to the package zip file. Note that no dependency checking is done for unofficial packages.\n\n\nUsing a HTTP proxy\n\n\nBoth the GUI and command line package managers can operate via a http proxy. To do so, start Weka from the command line and supply property values for the proxy host and port:\n\n\n java -Dhttp.proxyHost=some.proxy.somewhere.net -Dhttp.proxyPort=port weka.gui.GUIChooser\n\n\n\n\nIf your proxy requires authentication, then Weka will present a GUI dialog where you can enter your username and password. If you are running on a headless environment, then two more (non-standard) properties can be supplied:\n\n\n -Dhttp.proxyUser=some_user_name -Dhttp.proxyPassword=some_password\n\n\n\n\nUsing an alternative central package meta data repository\n\n\nBy default, both the command-line and GUI package managers use the central package meta data repository hosted on Sourceforge. In the unlikely event that this site is unavailable for one reason or another, it is possible to point the package management system at an alternative repository. This mechanism allows a temporary backup of the official repostory to be accessed, local mirrors to be established and alternative repositories to be set up for use etc.\n\n\nAn alternative repository can be specified by setting a Java property:\n\n\nweka.core.wekaPackageRepositoryURL=http://some.mirror.somewhere\n\n\n\n\nThis can either be set when starting Weka from the command line with the \n-D\n flag, or it can be placed into a file called \u201cPackageRepository.props\u201d in \n$WEKA_HOME/props\n. The default value of \nWEKA_HOME\n is \nuser.home/wekafiles\n, where \nuser.home\n is the user\u2019s home directory. More information on how and where Weka stores configuration information is given in the [[How are packages structured for the package management system?|how are package structured for the package management system?]] article.\n\n\nPackage manager property file\n\n\nAs mentioned in the previous section, an alternative package meta data repository can be specified by placing an entry in the PackageRepository.props file in \n$WEKA_HOME/props\n. From Weka 3.7.8 (and snapshot builds after 24 September 2012), the package manager also looks for properties placed in \n$WEKA_HOME/props/PackageManager.props\n. The current set of properties that can be set are:\n\n\nweka.core.wekaPackageRepositoryURL=http://some.mirror.somewhere\nweka.packageManager.offline=[true | false]\nweka.packageManager.loadPackages=[true | false]\nweka.pluginManager.disable=com.funky.FunkyExplorerPluginTab\n\n\n\n\nThe default for offline mode (if unspecified) is \nfalse\n and for loadPackages is \ntrue\n. The weka.pluginManager.disable property can be used to specify a comma-separated list of fully qualified class names to \"disable\" in the GUI. This can be used to make problematic components unavailable in the GUI without having to prevent the entire package that contains them from being loaded. E.g. \"funkyPackage\" might provide several classifiers and a special Explorer plugin tab for visualization. Suppose, for example, that the plugin Explorer tab has issues with certain data sets and causes annoying exceptions to be generated (or perhaps in the worst cases crashes the Explorer!). In this case we might want to use the classifiers provided by the package and just disable the Explorer plugin. Listing the fully qualified name of the Explorer plugin as a member of the comma-separated list associated with the weka.pluginManager.disable property will achieve this.",
            "title": " Package manager"
        },
        {
            "location": "/packages/manager/#command-line-package-management",
            "text": "Assuming that the weka.jar file is in the classpath, the package manager can be accessed by typing:   java weka.core.WekaPackageManager  Supplying no options will print the usage information:  Usage: weka.core.WekaPackageManager [option]\nOptions:\n  -list-packages <all | installed | available>\n  -package-info <repository | installed | archive> packageName\n  -install-package <packageName | packageZip | URL> [version]\n  -uninstall-package <packageName>\n  -refresh-cache   Weka 3.7.8 and snapshot builds of the developer version of Weka after September 24 2012 now offer a completely \"offline\" mode that involves no attempts to connect to the internet. This mode can be used to install package zip files that the user already has on the file system, and to browse already installed packages. This mode can be accessed from the command line package manager by specifying the \"-offline\" option. Alternatively, the property weka.packageManager.offline=true can be provided to the Java virtual machine on the command line or in a properties file (see the section on properties below).   Information (meta data) about packages is stored on a web server hosted on Sourceforge. The first time the package manager is run, for a new installation of Weka, there will be a short delay while the system downloads and stores a cache of the meta data from the server. Maintaining a cache speeds up the process of browsing the package information. From time to time you should update the local cache of package meta data in order to get the latest information on packages from the server. This can be achieved by supplying the  -refresh-cache  option.  The  -list-packages  option will, as the name suggests, print information (version numbers and short descriptions) about various packages. The option must be followed by one of three keywords:   all  will print information on all packages that the system knows about  installed  will print information on all packages that are installed locally  available  will print information on all packages that are not installed   The following shows an example of listing all packages installed locally:  java weka.core.WekaPackageManager -list-packages installed\n\nInstalled    Repository    Package\n=========    ==========    =======\n1.0.0        1.0.0         DTNB: Class for building and using a decision table/naive bayes hybrid classifier.\n1.0.0        1.0.0         massiveOnlineAnalysis: MOA (Massive On-line Analysis).\n1.0.0        1.0.0         multiInstanceFilters: A collection of filters for manipulating multi-instance data.\n1.0.0        1.0.0         naiveBayesTree: Class for generating a decision tree with naive Bayes classifiers at the leaves.\n1.0.0        1.0.0         scatterPlot3D: A visualization component for displaying a 3D scatter plot of the data using Java 3D.  The  -package-info  command lists information about a package given its name. The command is followed by one of three keywords and then the name of a package:   repository  will print info from the repository for the named package  installed  will print info on the installed version of the named package  archive  will print info for a package stored in a zip archive. In this case, the \u201carchive\u201d keyword must be followed by the path to an package zip archive file rather than just the name of a package   The following shows an example of listing information for the \u201cisotonicRegression\u201d package from the server:  java weka.core.WekaPackageManager -package-info repository isotonicRegression\nDescription:Learns an isotonic regression model. Picks the attribute that results in the lowest squared error. Missing values are not allowed. Can only deal with numeric attributes. Considers the monotonically increasing case as well as the monotonically decreasing case.\nVersion:1.0.0\nPackageURL:http://prdownloads.sourceforge.net/weka/isotonicRegression1.0.0.zip?download\nAuthor:Eibe Frank\nPackageName:isotonicRegression\nTitle:Learns an isotonic regression model.\nDate:2009-09-10\nURL:http://weka.sourceforge.net/doc.dev/weka/classifiers/IsotonicRegression.html\nCategory:Regression\nDepends:weka (>=3.7.1)\nLicense:GPL 2.0\nMaintainer:Weka team <wekalist@list.scms.waikato.ac.nz>  The  -install-package  command allows a package to be installed from one of three locations:   specifying a name of a package will install the package using the information in the package description meta data stored on the server. If no version number is given, then the latest available version of the package is installed.  providing a path to a zip file will attempt to unpack and install the archive as a Weka package  providing a URL (beginning with  http:// ) to a package zip file on the web will download and attempt to install the zip file as a Weka package   The  uninstall-package  command will uninstall the named package. Of course, the named package has to be installed for this command to have any effect!",
            "title": "Command line package management"
        },
        {
            "location": "/packages/manager/#running-installed-learning-algorithms",
            "text": "Running learning algorithms that come with the main weka distribution (i.e. are contained in the weka.jar file) was covered earlier in the [[Primer]]. But what about algorithms from packages that you\u2019ve installed using the package manager? We don\u2019t want to have to add a ton of jar files to our classpath every time we wan\u2019t to run a particular algorithm. Fortunately, we don\u2019t have to. Weka has a mechanism to load installed packages dynamically at run time. This means that newly installed packages are available in Weka's GUIs immediately.  What about running algorithms from packages on the command line I hear you ask? We can run a named algorithm by using the Run command:   java weka.Run  If no arguments are supplied, then Run outputs the following usage information:   Usage:\n     weka.Run [-no-scan] [-no-load] <scheme name [scheme options]>  The Run command supports sub-string matching, so you can run a classifier (such as J48) like so:   java weka.Run J48  When there are multiple matches on a supplied scheme name you will be presented with a list. For example:   java weka.Run NaiveBayes\n\n Select a scheme to run, or <return> to exit:\n     1) weka.classifiers.bayes.ComplementNaiveBayes\n     2) weka.classifiers.bayes.NaiveBayes\n     3) weka.classifiers.bayes.NaiveBayesMultinomial\n     4) weka.classifiers.bayes.NaiveBayesMultinomialUpdateable\n     5) weka.classifiers.bayes.NaiveBayesSimple\n     6) weka.classifiers.bayes.NaiveBayesUpdateable\n\n Enter a number >  You can turn off the scanning of packages and sub-string matching by providing the  -no-scan  option. This is useful when using the  Run  command in a script. In this case, you need to specify the fully qualified name of the algorithm to use. E.g.   java weka.Run -no-scan weka.classifiers.bayes.NaiveBayes  To reduce startup time you can also turn off the dynamic loading of installed packages by specifying the  -no-load  option. In this case, you will need to explicitly include any packaged algorithms in your classpath if you plan to use them. E.g.  java -classpath ./weka.jar:$HOME/wekafiles/packages/DTNB/DTNB.jar rweka.Run -no-load -no-scan weka.classifiers.rules.DTNB",
            "title": "Running installed learning algorithms"
        },
        {
            "location": "/packages/manager/#gui-package-manager",
            "text": "As well as a command line client, there is also a graphical interface to Weka\u2019s package management system. This is available from the  Tools  menu in the  GUIChooser . All the functionality available in the command line client to the package management system is available in the GUI version, along with the ability to install and uninstall multiple packages in one hit.   The package manager\u2019s window is split horizontally into two parts: at the top is a list of packages and at the bottom is a mini browser that can be used to display information on the currently selected package.  The package list shows the name of a package, its category, the currently installed version (if installed), the latest version available via the repository and whether the package has been loaded or not. This list may be sorted by either package name or category by clicking on the appropriate column header. A second click on the same header reverses the sort order. Three radio buttons in the upper left of the window can be used to filter what is displayed in the list. All packages (default), all available packages (i.e. those not yet installed) or only installed packages can be displayed.  If multiple versions of a package are available, they can be accessed by clicking on an entry in the \u201cRepository version\u201d column:",
            "title": "GUI package manager"
        },
        {
            "location": "/packages/manager/#installing-and-removing-packages",
            "text": "At the very top of the window are three buttons. On the left-hand side is a button that can be used to refresh the cached copy of the package repository meta data. The first time that the package manager (GUI or command line) is used there will be a short delay as the initial cache is established.   NOTE: Weka (3.7.2) will not automatically check for new information at the central repository, so it is a good idea to refresh the local cache regularly. From Weka 3.7.3 the package manager will notify you if there are new packages available at the central repository .  The two buttons at the top right are used to install and remove packages repspectively. Multiple packages may be installed/removed by using a shift-left-click combination to select a range and/or by using a command-left-click combination to add to the selection. Underneath the install and uninstall but- tons is a checkbox that can be enabled to ignore any dependencies required by selected packages and any conflicts that may occur. Installing packages while this checkbox is selected will '''not''' install required dependencies.  Some packages may have additional information on how to complete the installation or special instructions that gets displayed when the package is installed:   Usually it is not necessary to restart Weka after packages have been installed\u2014the changes should be available immediately. An exception is when upgrading a package that is already installed. If in doubt, restart Weka.",
            "title": "Installing and removing packages"
        },
        {
            "location": "/packages/manager/#unofficial-packages",
            "text": "The package list shows those packages that have their meta data stored in Weka\u2019s central meta data repository. These packages are \u201cofficial\u201d Weka packages and the Weka team as verified that they appear to provide what is advertised (and do not contain malicious code or malware).  It is also possible to install an  unofficial  package that has not gone through the process of become official. Unofficial packages might be provided, for exam- ple, by researchers who want to make experimental algorithms quickly available to the community for feedback. Unofficial packages can be installed by clicking the \u201cFile/url\u201d button on the top-right of the package manager window. This will bring up an \u201cUnnoficial package install\u201d dialog where the user can browse their file system for a package zip file or directly enter an URL to the package zip file. Note that no dependency checking is done for unofficial packages.",
            "title": "Unofficial packages"
        },
        {
            "location": "/packages/manager/#using-a-http-proxy",
            "text": "Both the GUI and command line package managers can operate via a http proxy. To do so, start Weka from the command line and supply property values for the proxy host and port:   java -Dhttp.proxyHost=some.proxy.somewhere.net -Dhttp.proxyPort=port weka.gui.GUIChooser  If your proxy requires authentication, then Weka will present a GUI dialog where you can enter your username and password. If you are running on a headless environment, then two more (non-standard) properties can be supplied:   -Dhttp.proxyUser=some_user_name -Dhttp.proxyPassword=some_password",
            "title": "Using a HTTP proxy"
        },
        {
            "location": "/packages/manager/#using-an-alternative-central-package-meta-data-repository",
            "text": "By default, both the command-line and GUI package managers use the central package meta data repository hosted on Sourceforge. In the unlikely event that this site is unavailable for one reason or another, it is possible to point the package management system at an alternative repository. This mechanism allows a temporary backup of the official repostory to be accessed, local mirrors to be established and alternative repositories to be set up for use etc.  An alternative repository can be specified by setting a Java property:  weka.core.wekaPackageRepositoryURL=http://some.mirror.somewhere  This can either be set when starting Weka from the command line with the  -D  flag, or it can be placed into a file called \u201cPackageRepository.props\u201d in  $WEKA_HOME/props . The default value of  WEKA_HOME  is  user.home/wekafiles , where  user.home  is the user\u2019s home directory. More information on how and where Weka stores configuration information is given in the [[How are packages structured for the package management system?|how are package structured for the package management system?]] article.",
            "title": "Using an alternative central package meta data repository"
        },
        {
            "location": "/packages/manager/#package-manager-property-file",
            "text": "As mentioned in the previous section, an alternative package meta data repository can be specified by placing an entry in the PackageRepository.props file in  $WEKA_HOME/props . From Weka 3.7.8 (and snapshot builds after 24 September 2012), the package manager also looks for properties placed in  $WEKA_HOME/props/PackageManager.props . The current set of properties that can be set are:  weka.core.wekaPackageRepositoryURL=http://some.mirror.somewhere\nweka.packageManager.offline=[true | false]\nweka.packageManager.loadPackages=[true | false]\nweka.pluginManager.disable=com.funky.FunkyExplorerPluginTab  The default for offline mode (if unspecified) is  false  and for loadPackages is  true . The weka.pluginManager.disable property can be used to specify a comma-separated list of fully qualified class names to \"disable\" in the GUI. This can be used to make problematic components unavailable in the GUI without having to prevent the entire package that contains them from being loaded. E.g. \"funkyPackage\" might provide several classifiers and a special Explorer plugin tab for visualization. Suppose, for example, that the plugin Explorer tab has issues with certain data sets and causes annoying exceptions to be generated (or perhaps in the worst cases crashes the Explorer!). In this case we might want to use the classifiers provided by the package and just disable the Explorer plugin. Listing the fully qualified name of the Explorer plugin as a member of the comma-separated list associated with the weka.pluginManager.disable property will achieve this.",
            "title": "Package manager property file"
        },
        {
            "location": "/packages/unofficial/",
            "text": "There are a number of packages for WEKA 3.8 on the internet that are not listed in the \"official\" WEKA package repository. These packages can nevertheless be easily installed via the package manager in WEKA 3.8 (available via the Tools menu in WEKA's GUIChooser) by providing the URL for the package .zip file.\n\n\nBelow is an (incomplete list) of packages that are available.\n\n\nPreprocessing\n\n\n\n\ndataset-weights\n -- filters for setting attribute and instance weights using various methods.\n\n\nmissing-values-imputation\n -- various methods for imputing missing values using a filter.\n\n\nmxexpression\n -- filter for updating a target attribute using a mathematical expression.\n\n\n\n\nClassification\n\n\n\n\nJava neural network package\n -- Java (convolutional or fully-connected) neural network implementation with plugin for \nWeka\n. Uses dropout and rectified linear units. Implementation is multithreaded and uses \nMTJ\n matrix library with native libs for performance.\n\n\nHMMWeka\n -- This library makes Hidden Markov Model machine learning available in Weka.\n\n\nCollective classification\n -- Algorithms around semi-supervised learning and collective classification.\n\n\nBagging ensemble selection\n -- Bagging Ensemble Selection - a new ensemble learning strategy.\n\n\nDataSqueezer\n -- Efficient rule builder that generates a set of production rules from labeled input data. It can handle missing data and has log-linear asymptotic complexity with the number of training examples.\n\n\nmiDS\n -- mi-DS is a multiple-Instance learning supervised algorithm based on the DataSqueezer algorithm.\n\n\nLibD3C\n -- Ensemble classifiers with a clustering and dynamic selection strategy.\n\n\nICRM\n -- An Interpretable Classification Rule Mining Algorithm.\n\n\ntclass\n -- TClass is a supervised learner for multivariate time series, originally developed by \nWaleed Kadous\n.\n\n\nwekaclassalgos\n -- collection of artificial neural network (ANN) algorithms and artificial immune system (AIS) algorithms, originally developed by \nJason Brownlee\n.\n\n\nmxexpression\n -- classifier for making predictions using a mathematical expression.\n\n\n\n\nClustering\n\n\n\n\nAPCluster\n -- Affinity propagation algorithm for clustering, used especially in bioinformatics and computer vision.\n\n\nFast Optics\n -- Fast Implementation of OPTICS algorithm using random projections for Euclidean distances.\n\n\n\n\nSimilarity functions\n\n\n\n\nwekabiosimilarity\n -- implements several measures to compare binary feature vectors; and, additionally, extrapolates those measures to work with multi-value, string and numerical feature vectors.\n\n\n\n\nDiscretization\n\n\n\n\nur-CAIM\n -- Improved CAIM Discretization for Unbalanced and Balanced Data.\n\n\nCAIM\n -- Class-Attribute Interdependence Maximization algorithm: discretizes a continuous feature into a number of intervals. This is done by using class information, without requiring the user to provide this number.\n\n\n\n\nFeature selection\n\n\n\n\nRSARSubsetEval\n -- Rough set feature selection.\n\n\n\n\nFrequent pattern mining\n\n\n\n\nXApriori\n --Available case analysis modification of Apriori frequent pattern mining algorithm.\n\n\n\n\nStemming\n\n\n\n\nSnowball stemmers\n -- Contains the actual snowball stemmer algorithms to make the Snowball stemmer wrapper in Weka work.\n\n\nPTStemmer\n -- Wrapper for \nPedro Oliveira's stemmer library\n for Portuguese.\n\n\n\n\nText mining\n\n\n\n\nnlp\n -- Contains components for natural language processing, eg part-of-speech tagging filter and Penn Tree Bank tokenizer. Makes use of the \nStanford Parser\n (parser models need to be downloaded separately).\n\n\n\n\nVisualization\n\n\n\n\ngraphviz-treevisualize\n -- Generating nice graphs in the Explorer from trees (eg J48) using the GraphViz executables.\n\n\nconfusionmatrix\n -- Various visualizations of confusion matrices in the Explorer.\n\n\nserialized-model-viewer\n -- Adds a standalone tab to the Explorer that allows the user to load a serialized model and view its content as text (simply uses the objects' \ntoString()\n method).\n\n\n\n\nParameter optimization\n\n\n\n\nmultisearch\n -- Meta-classifier similar to GridSearch, but for optimizing arbitrary number of parameters.\n\n\n\n\nOthers\n\n\n\n\nscreencast4j\n -- Allows you to record sound, webcam and screen feeds, storing them in separate files to be combined into a screencast using a \nvideo editor\n. This screencast you can then share on YouTube, for instance.\n\n\ncommand-to-code\n -- Turns command-lines (eg of classifiers or filters) into various Java code snippets.\n\n\njshell-scripting\n -- Allows scripting in Java, using \njshell",
            "title": " Unofficial packages"
        },
        {
            "location": "/packages/unofficial/#preprocessing",
            "text": "dataset-weights  -- filters for setting attribute and instance weights using various methods.  missing-values-imputation  -- various methods for imputing missing values using a filter.  mxexpression  -- filter for updating a target attribute using a mathematical expression.",
            "title": "Preprocessing"
        },
        {
            "location": "/packages/unofficial/#classification",
            "text": "Java neural network package  -- Java (convolutional or fully-connected) neural network implementation with plugin for  Weka . Uses dropout and rectified linear units. Implementation is multithreaded and uses  MTJ  matrix library with native libs for performance.  HMMWeka  -- This library makes Hidden Markov Model machine learning available in Weka.  Collective classification  -- Algorithms around semi-supervised learning and collective classification.  Bagging ensemble selection  -- Bagging Ensemble Selection - a new ensemble learning strategy.  DataSqueezer  -- Efficient rule builder that generates a set of production rules from labeled input data. It can handle missing data and has log-linear asymptotic complexity with the number of training examples.  miDS  -- mi-DS is a multiple-Instance learning supervised algorithm based on the DataSqueezer algorithm.  LibD3C  -- Ensemble classifiers with a clustering and dynamic selection strategy.  ICRM  -- An Interpretable Classification Rule Mining Algorithm.  tclass  -- TClass is a supervised learner for multivariate time series, originally developed by  Waleed Kadous .  wekaclassalgos  -- collection of artificial neural network (ANN) algorithms and artificial immune system (AIS) algorithms, originally developed by  Jason Brownlee .  mxexpression  -- classifier for making predictions using a mathematical expression.",
            "title": "Classification"
        },
        {
            "location": "/packages/unofficial/#clustering",
            "text": "APCluster  -- Affinity propagation algorithm for clustering, used especially in bioinformatics and computer vision.  Fast Optics  -- Fast Implementation of OPTICS algorithm using random projections for Euclidean distances.",
            "title": "Clustering"
        },
        {
            "location": "/packages/unofficial/#similarity-functions",
            "text": "wekabiosimilarity  -- implements several measures to compare binary feature vectors; and, additionally, extrapolates those measures to work with multi-value, string and numerical feature vectors.",
            "title": "Similarity functions"
        },
        {
            "location": "/packages/unofficial/#discretization",
            "text": "ur-CAIM  -- Improved CAIM Discretization for Unbalanced and Balanced Data.  CAIM  -- Class-Attribute Interdependence Maximization algorithm: discretizes a continuous feature into a number of intervals. This is done by using class information, without requiring the user to provide this number.",
            "title": "Discretization"
        },
        {
            "location": "/packages/unofficial/#feature-selection",
            "text": "RSARSubsetEval  -- Rough set feature selection.",
            "title": "Feature selection"
        },
        {
            "location": "/packages/unofficial/#frequent-pattern-mining",
            "text": "XApriori  --Available case analysis modification of Apriori frequent pattern mining algorithm.",
            "title": "Frequent pattern mining"
        },
        {
            "location": "/packages/unofficial/#stemming",
            "text": "Snowball stemmers  -- Contains the actual snowball stemmer algorithms to make the Snowball stemmer wrapper in Weka work.  PTStemmer  -- Wrapper for  Pedro Oliveira's stemmer library  for Portuguese.",
            "title": "Stemming"
        },
        {
            "location": "/packages/unofficial/#text-mining",
            "text": "nlp  -- Contains components for natural language processing, eg part-of-speech tagging filter and Penn Tree Bank tokenizer. Makes use of the  Stanford Parser  (parser models need to be downloaded separately).",
            "title": "Text mining"
        },
        {
            "location": "/packages/unofficial/#visualization",
            "text": "graphviz-treevisualize  -- Generating nice graphs in the Explorer from trees (eg J48) using the GraphViz executables.  confusionmatrix  -- Various visualizations of confusion matrices in the Explorer.  serialized-model-viewer  -- Adds a standalone tab to the Explorer that allows the user to load a serialized model and view its content as text (simply uses the objects'  toString()  method).",
            "title": "Visualization"
        },
        {
            "location": "/packages/unofficial/#parameter-optimization",
            "text": "multisearch  -- Meta-classifier similar to GridSearch, but for optimizing arbitrary number of parameters.",
            "title": "Parameter optimization"
        },
        {
            "location": "/packages/unofficial/#others",
            "text": "screencast4j  -- Allows you to record sound, webcam and screen feeds, storing them in separate files to be combined into a screencast using a  video editor . This screencast you can then share on YouTube, for instance.  command-to-code  -- Turns command-lines (eg of classifiers or filters) into various Java code snippets.  jshell-scripting  -- Allows scripting in Java, using  jshell",
            "title": "Others"
        },
        {
            "location": "/packages/structure/",
            "text": "Articles such as \nHow do I use WEKA's classes in my own code?\n and \nHow do I write a new classifier or filter?\n describe how to extend Weka to add your own learning algorithms and so forth. This article describes how such enhancements can be assembled into a \npackage\n that can be accessed via Weka\u2019s \npackage management\n system. Bundling your enhancements in a package makes it easy to share with other Weka users.\n\n\nIn this article we refer to a \npackage\n as an archive containing various resources such as compiled code, source code, javadocs, package description files (meta data), third-party libraries and configuration property files. Not all of the preceding may be in a given package, and there may be other resources included as well. This concept of a \npackage\n is quite different to that of a Java packages, which simply define how classes are arranged hierarchically.\n\n\nWhere does WEKA store packages and other configuration stuff?\n\n\nBy default, Weka stores packages and other information in \n$WEKA_HOME\n. The default location for \nWEKA_HOME\n is \nuser.home/wekafiles\n, where \nuser.home\n is the user\u2019s home directory. You can change the default location for \nWEKA_HOME\n by setting this either as an evironment variable for your platform, or by specifying it as a Java property when starting Weka. E.g.:\n\n\nexport WEKA_HOME=/home/somewhere/weka_bits_and_bobs\n\n\n\n\nwill set the directory that Weka uses to \n/home/somewhere/weka_bits_and_bobs\n under the LINUX operating system.\n\n\nThe same thing can be accomplished when starting Weka by specifying a Java property on the command line, E.g.:\n\n\njava -DWEKA_HOME=/home/somewhere/weka_bits_and_bobs -jar weka.jar\n\n\n\n\nInside \n$WEKA_HOME\n you will find the main weka log file (weka.log) and a number of directories:\n\n\n\n\npackages\n holds installed packages. Each package is contained its own subdirectory.\n\n\nprops\n holds various Java property files used by Weka. This directory replaces the user\u2019s home directory (used in earlier releases of Weka) as one of the locations checked by Weka for properties files (such as \nDatabaseUtils.props\n). Weka first checks, in order, the current directory (i.e. the directory that Weka is launched from), then \n$WEKA_HOME/props\n and finally the \nweka.jar\n file for property files.\n\n\nrepCache\n holds the cached copy of the meta data from the central package repository. If the contents of this directory get corrupted it can be safely deleted and Weka will recreate it on the next restart.\n\n\nsystemDialogs\n holds marker files that are created when you check \nDon\u2019t show this again\n in various system popup dialogs. Removing this directory or its contents will cause Weka to display those prompts anew.\n\n\n\n\nAnatomy of a package\n\n\nA Weka package is a zip archive that must unpack to the current directory. For example, the \nDTNB\n package contains the decision table naive Bayes hybrid classifier and is delivered in a file called \nDTNB.zip\n. When unpacked this zip file creates the following directory structure:\n\n\n   <current directory>\n     +-DTNB.jar\n     +-Description.props\n     +-build_package.xml\n     +-src\n     |   +-main\n     |   |   +-java\n     |   |       +-weka\n     |   |           +-classifiers\n     |   |               +-rules\n     |   |                   +-DTNB.java\n     |   +-test\n     |       +-java\n     |           +-weka\n     |               +-classifiers\n     |                   +-rules\n     |                       +-DTNBTest.java\n     +-lib\n     +-doc\n\n\n\n\nWhen installing, the package manager will use the value of the \"PackageName\" field in the Description.props file (see below) to create a directory in \n$WEKA_HOME/packages\n to hold the package contents. The contents of the \ndoc\n directory have not been shown in the diagram above, but this directory contains javadoc for the \nDTNB\n class. A package must have a \nDescription.props\n file and contain at least one jar file with compiled Java classes. The package manager will attempt to load all jar files that it finds in the root directory and the \nlib\n directory. Other files are optional, but if the package is open-source then it is nice to include the source code and an ant build file that can be used to compile the code. Template versions of the \nDescription.props\n file and\n\nbuild_package.xml\n file are available from the Weka site and here.\n\n\n==The description file==\nA valid package must contain a \nDescription.props\n file that provides meta data on the package. Identical files are stored at the central package repository and the local cache maintained by the package manager. The package manager uses these files to compare what is installed to what is available and resolve dependencies.\n\n\nThe \nDescription.props\n contains basic information on the package in the following format:\n\n\n# Template Description file for a Weka package\n\n# Package name (required)\nPackageName=funkyPackage\n\n# Version (required)\nVersion=1.0.0\n\n#Date (year-month-day)\nDate=2010-01-01\n\n# Title (required)\nTitle=My cool algorithm\n\n# Category (recommended)\nCategory=Classification\n\n# Author (required)\nAuthor=Joe Dev <joe@somewhere.net>,Dev2 <dev2@somewhereelse.net>\n\n# Maintainer (required)\nMaintainer=Joe Dev <joe@somewhere.net>\n\n# License (required)\nLicense=GPL 2.0|Mozilla\n\n# Description (required)\nDescription=This package contains the famous Funky Classifer that performs \\\n truely funky prediction.\n\n# Changes and/or bug fixes in this package (optional)\nChanges=Fixed a serious bug that affected overall coolness of the Funky Classifier\n\n# Package URL for obtaining the package archive (required)\nPackageURL=http://somewhere.net/weka/funkyPackage.zip\n\n# URL for further information\nURL=http://somewhere.net/funkyResearchInfo.html\n\n# Enhances various other packages?\nEnhances=packageName1,packageName2,...\n\n# Related to other packages?\nRelated=packageName1,packageName2,...\n\n# Dependencies (required; format: packageName (equality/inequality version_number)\nDepends=weka (>=3.7.1), packageName1 (=x.y.z), packageName2 (>u.v.w|<=x.y.z),...\n\n\n\n\nThe \nPackageName\n and \nVersion\n give the name of the package and version number respectively. The name can consist of letters, numbers, and the dot character. It should not start with a dot and should not contain any spaces. The version number is a sequence of three non-negative integers separated by single \n.\n or \n-\n characters.\n\n\nThe \nTitle\n field should give a one sentence description of the package. The \nDescription\n field can give a longer description of the package spaning multiple sentences. It may include technical references and can use HTML markup.\n\n\nThe \nCategory\n field is strongly recommended as this information is displayed on both the repository web site and in the GUI package manager client. In the latter, the user can sort the packages on the basis of the category field. It is recommended that an existing category be assigned if possible. Some examples include (Classification, Text classification, Ensemble learning, Regression, Clustering, Associations, Preprocessing, Visualization, Explorer, Experimenter, KnowledgeFlow).\n\n\nThe \nAuthor\n field describes who wrote the package and may include multiple names (separated by commas). Email addresses may be given in angle brackets after each name. The field is intended for human readers and no email addresses are automatically extracted.\n\n\nThe \nMaintainer\n field lists who maintains the package and should include a single email address, enclosed in angle brackets, for sending bug reports to.\n\n\nThe \nLicense\n field lists the license(s) that apply to the package. This field may contain the short specification of a license (such as LGPL, GPL 2.0 etc.) or the string \nfile LICENSE\n, where \nLICENSE\n exists as a file in the top-level directory of the package. The string \nUnlimited\n may be supplied to indicate that there are no restrictions on distribution or use aside from those imposed by relevant laws.\n\n\nThe \nPackageURL\n field lists valid URL that points to the package zip file. This URL is used by the package manager to download and install the package.\n\n\nThe optional \nDepends\n field gives a comma separated list of packages which this package depends on. The name of a package is optionally followed by a version number constraint enclosed in parenthesis. Valid operators for version number constraints include =, <, >, <=, >=. The keyword \nweka\n is reserved to refer to the base Weka system and can be used to indicate a dependency on\n  a particular version of Weka. For example:\n\n\nDepends=weka (>=3.7.2), DTNB (=1.0.0)\n\n\n\n\nstates that this package requires Weka 3.7.2 or higher and version 1.0.0 of the package DTNB.\n\n\nDepends=weka (>3.7.1|<3.8.0)\n\n\n\n\nstates that this package requires a version of Weka between 3.7.1 and 3.8.0.\n\n\nDepends=DTNB (<1.5.0|>=2.0.1)\n\n\n\n\nstates that this package requires that a version of the DTNB package be installed that is either less than version 1.5.0 or greater than or equal to version 2.0.1.\n\n\nIf there is no version number constraint following a package name, the package manager assumes that the latest version of the dependent package is suitable.\n\n\nThe optional \nURL\n field gives a URL at which the user can find additional online information about the package or its constituent algorithms.\n\n\nThe optional \nEnhances\n field can be used to indicate which other packages this package is based on (i.e. if it extends methods/algorithms from another\n  package in some fashion).\n\n\nThe optional \nRelated\n field is similar to the \nEnhances\n field. It can be used to point the user to other packages that are related in some fashion to this one.\n\n\nThe optional \nChanges\n field should be used to indicate what changes/bug fixes are included in the current release of the package.\n\n\nThere are several other fields that can be used to provide information to assist the user with completing installation (if it can\u2019t be completely accomplished with the package zip file) or display error messages if necessary components are missing:\n\n\nMessageToDisplayOnInstallation=Funky package requires some extra\\n\\\n stuff to be installed after installing this package. You will\\n\\\n need to blah, blah, blah in order to blah, blah, blah...\n\nDoNotLoadIfFileNotPresent=lib/someLibrary.jar,otherStuff/important,...\n\nDoNotLoadIfFileNotPresentMessage=funkyPackage can't be loaded because some \\\n funky libraries are missing. Please download funkyLibrary.jar from \\\n http://www.funky.com and install in $WEKA_HOME/packages/funkyPackage/lib\n\nDoNotLoadIfClassNotPresent=com.some.class.from.some.Where,org.some.other.Class,...\n\nDoNotLoadIfClassNotPresentMessage=funkyPackage can't be loaded because \\\n com.funky.FunkyClass can't be instantiated. Have you downloaded and run \\\n the funky software installer for your platform?\n\n\n\n\nThe optional \nMessageToDisplayOnInstallation\n field allows you to specify special instructions to the user in order to help them complete the intallation manually. This message gets displayed on the console, written to the log and appears in a pop-up information dialog if using the GUI package manager. It should include \n\\n\n in order to avoid long lines when displayed in a GUI pop-up dialog.\n\n\nThe optional \nDoNotLoadIfFileNotPresent\n field can be used to prevent Weka from loading the package if the named ''files'' and/or ''directories'' are not present in the package\u2019s installation directory. An example is the massiveOnlineAnalysis package. This package is a connector only package and does not include the MOA library. Users of this package must download the moa.jar file separately and copy it to the package\u2019s \nlib\n directory manually. Multiple files and directories can be specified as a comma separated list. All paths are relative to the top-level directory of the package. \nIMPORTANT\n: use forward slashes as separator characters, as these are portable accross all platforms. The \nDoNotLoadIfFileNotPresentMessage\n field can be used to supply an optional message to display to the user if Weka detects that a file or directory is missing from the package. This message will be displayed on the console and in the log.\n\n\nThe optional \nDoNotLoadIfClassNotPresent\n field can be used to prevent Weka from loading the package if the named ''class(es)'' can\u2019t be instantiated. This is useful for packages that rely on stuff that has to be installed manually by the user. For example, Java3D is a separate download on all platforms except for OSX, and installs itself into the system JRE/JDK. The \nDoNotLoadIfClassNotPresentMessage\n field can be used to supply an optional message to display to the user if Weka detects that a class can\u2019t be instantiated. Again, this will be displayed on the console and in the log.\n\n\nNew in Weka 3.9.2 and 3.8.2 is the ability to constrain the OS and architecture that a package can be installed and loaded on. Two new fields are used for this:\n\n\n# Specify which OS's the package can operate with. Omitting this entry indicates no restrictions on OS. (optional)\nOSName=Windows,Mac,Linux\n\n# Specify which architecture the package can operate with. Omitting this entry indicates no restriction. (optional)\nOSArch=64\n\n\n\n\nEntries in the OSName field are compared against the value of the Java property \"os.name\" using a String.toLowerCase().contains() operation. Any single match indicates a pass. If an OSName field exists in the Description.props, and it matches, then the optional OSArch field is examined. If present, values in the OSArch list are compared against the value of the Java property \"os.arch\". The special OSArch entries \"32\" and \"64\" are tested using a String().contains() operation; all other entries are compared using String.equalsIgnoreCase().\n\n\nAdditional configuration files\n\n\nCertain types of packages may require additional configuration files to be present as part of the package. The last chapter covered various ways in which Weka can be extended without having to alter the core Weka code. These plugin mechanisms have been subsumed by the package management system, so some of the configuration property files they require must be present in the package\u2019s top-level directory if the package in question contains such a plugin. Examples include additional tabs for the Explorer, mappings to custom property editors for Weka\u2019s GenericObjectEditor and Knowledge Flow plugins. Here are some examples:\n\n\nThe scatterPlot3D package adds a new tab to the Explorer. In order to accomplish this a property has to be set in the Explorer.props file (which contains default values for the Explorer) in order to tell Weka to instantiate and display the new panel. The scatterPlot3D file includes an \nExplorer.props\n file in its top-level directory that has the following contents:\n\n\n# Explorer.props file. Adds the Explorer3DPanel to the Tabs key.\nTabs=weka.gui.explorer.Explorer3DPanel\nTabsPolicy=append\n\n\n\n\nThis property file is read by the package management system when the package is loaded and any key-value pairs are added to existing Explorer properties that have been loaded by the system at startup. If the key already exists in the Explorer properties, then the package has the option to either replace (i.e. overwrite) or append to the existing value. This can be specified with the \"TabsPolicy\" key. In this case, the value \nweka.gui.explorer.Explorer3DPanel\n is appended to any existing value associated with the \"Tabs\" key. \nExplorer3DPanel\n gets instantiated and added as a new tab when the Explorer starts.\n\n\nAnother example is the kfGroovy package. This package adds a plugin component to Weka\u2019s Knowledge Flow that allows a Knowledge Flow step to be implemented and compiled dynamically at runtime as a Groovy script. In order for the Knowledge Flow to make the new step appear in its \nPlugins\n toolbar, there needs to be a \nBeans.props\n file in the package\u2019s top level directory. In the case of kfGroovy, this property file has the following contents:\n\n\n# Specifies that this component goes into the Plugins toolbar\nweka.gui.beans.KnowledgeFlow.Plugins=org.pentaho.dm.kf.GroovyComponent\n\n\n\n\nThe new pluggable evaluation metrics for classification/regression (from Weka 3.7.8 and nightly developer snapshots from 15-11-2012) are managed by the PluginManager class. To tell PluginManager that your package provides a new evaluation metric you need to provide a \"PluginManager.props\" file in the package's top level directory. For example, a hypothetical bobsMetric package might declare a new \"Area under Bob curve\" metric like so:\n\n\n# Specify a new plugin Evaluation metric\nweka.classifiers.evaluation.AbstractEvaluationMetric=weka.classifiers.evaluation.BobsAUC\n\n\n\n\nContributing a package\n\n\nIf you have created a package for Weka then there are two options for making it available to the community. In both cases, hosting the package\u2019s zip archive is the responsibility of the contributer.\n\n\nThe first, and official, route is to contact the current Weka maintainer (normally also the admin of the WEKA homepage) and supply your package\u2019s \nDescription.props\n file. The Weka team will then test downloading and using your package to make sure that there are no obvious problems with what has been specified in the \nDescription.props\n file and that the software runs and does not contain any malware/malicious code. If all is well, then the package will become an \nofficial\n Weka package and the central package repository meta data will be updated with the package\u2019s \nDescription.props file\n. \n\n\nResponsibility for maintaining and supporting the package resides with the contributer\n.\n\n\nThe second, and unofficial, route is to simply make the package\u2019s zip archive available on the web somewhere and advertise it yourself. Although users will not be able to browse it\u2019s description in the official package repository, they will be able to download and install it directly from your URL by using the command line version of the package manager. This route could be attractive for people who have published a new algorithm and want to quiclky make a beta version available for others to try without having to go through the official route.\n\n\nCreating a mirror of the package meta data repository\n\n\nIn this section we discuss an easy approach to setting up and maintaining a mirror of the package meta data repository. Having a local mirror may provide faster access times than to that of the official repository on Sourceforge. Extending this approach to the creation of an alternative central repository (hosting packages not available at the official repository) should be straight forward.\n\n\nJust about everything necessary for creating a mirror exists in the local meta data cache created by Weka\u2019s package management system. This cache resides at \n$WEKA_HOME/repCache\n. The only thing missing (in Weka 3.7.2) for a complete mirror is the file \nimages.txt\n, that lists all the image files used in the html index files. This file contains the following two lines:\n\n\nTitle-Bird-Header.gif\npentaho_logo_rgb_sm.png\n\n\n\n\nimages.txt\n is downloaded automatically by the package management system in Weka 3.7.3 and higher.\n\n\nTo create a mirror:\n1. Copy the contents of \n$WEKA_HOME/repCache\n to a temporary directory. For the purposes of this example we\u2019ll call it \ntempRep\n\n2. Change directory into \ntempRep\n and run \njava weka.core.RepositoryIndexGenerator .\n. Don't forget the \".\" after the command (this tells \nRepoistoryIndexGenerator\n to operate on the current directory)\n3. Change directory to the parent of \ntempRep\n and synchronize its contents to wherever your web server is located (this is easy via rsync under Nix-like operating systems).\n\n\nRepositoryIndexGenerator\n automatically creates the main index.html file, all the package index.html files and html files correpsonding to all version prop files for each package. It will also create \npackageList.txt\n and \nnumPackages.txt\n files.\n\n\nIMPORTANT\n: Make sure that all the files in \ntempRep\n are world readable. It is easy to make packages available that are not part of the official Weka repository. Assuming you want to add a package called \nfunkyPackage\n (as specified by the \nPackageName\n field in the \nDescription.props\n file):\n\n\n\n\nCreate a directory called \nfunkyPackage\n in \ntempRep\n\n\nCopy the \nDescription.props\n file to \ntempRep/funkyPackage/Latest.props\n\n\nCopy the \nDescription.props file\n to \ntempRep/funkyPackage/<version number>.props\n, where \nversion number\n is the version number specified in the \nVersion\n field of \nDescription.props\n\n\nRun \nRepositoryIndexGenerator\n as described previously and sync \ntempRep\n to your web server\n\n\n\n\nAdding a new version of an existing package is very similar to what has already been described. All that is required is that the new \nDescription.props\n file corresponding to the new version is copied to \nLatest.props\n and to \n<version numer>.props\n in the package\u2019s folder. Running \nRepositoryIndexGenerator\n will ensure that all necessary html files are created and supporting text files are updated.\n\n\nAutomating the mirroring process would simply involve using your OS\u2019s scheduler to execute a script that:\n\n\n\n\nRuns \nweka.core.WekaPackageManager -refresh-cache\n\n\nrsyncs \n$WEKA_HOME/repCache\n to \ntempRep\n\n\nRuns \nweka.core.RepoistoryIndexGenerator\n\n\nrsyncs \ntempRep\n to your web server",
            "title": " How are packages structured for the package management system?"
        },
        {
            "location": "/packages/structure/#where-does-weka-store-packages-and-other-configuration-stuff",
            "text": "By default, Weka stores packages and other information in  $WEKA_HOME . The default location for  WEKA_HOME  is  user.home/wekafiles , where  user.home  is the user\u2019s home directory. You can change the default location for  WEKA_HOME  by setting this either as an evironment variable for your platform, or by specifying it as a Java property when starting Weka. E.g.:  export WEKA_HOME=/home/somewhere/weka_bits_and_bobs  will set the directory that Weka uses to  /home/somewhere/weka_bits_and_bobs  under the LINUX operating system.  The same thing can be accomplished when starting Weka by specifying a Java property on the command line, E.g.:  java -DWEKA_HOME=/home/somewhere/weka_bits_and_bobs -jar weka.jar  Inside  $WEKA_HOME  you will find the main weka log file (weka.log) and a number of directories:   packages  holds installed packages. Each package is contained its own subdirectory.  props  holds various Java property files used by Weka. This directory replaces the user\u2019s home directory (used in earlier releases of Weka) as one of the locations checked by Weka for properties files (such as  DatabaseUtils.props ). Weka first checks, in order, the current directory (i.e. the directory that Weka is launched from), then  $WEKA_HOME/props  and finally the  weka.jar  file for property files.  repCache  holds the cached copy of the meta data from the central package repository. If the contents of this directory get corrupted it can be safely deleted and Weka will recreate it on the next restart.  systemDialogs  holds marker files that are created when you check  Don\u2019t show this again  in various system popup dialogs. Removing this directory or its contents will cause Weka to display those prompts anew.",
            "title": "Where does WEKA store packages and other configuration stuff?"
        },
        {
            "location": "/packages/structure/#anatomy-of-a-package",
            "text": "A Weka package is a zip archive that must unpack to the current directory. For example, the  DTNB  package contains the decision table naive Bayes hybrid classifier and is delivered in a file called  DTNB.zip . When unpacked this zip file creates the following directory structure:     <current directory>\n     +-DTNB.jar\n     +-Description.props\n     +-build_package.xml\n     +-src\n     |   +-main\n     |   |   +-java\n     |   |       +-weka\n     |   |           +-classifiers\n     |   |               +-rules\n     |   |                   +-DTNB.java\n     |   +-test\n     |       +-java\n     |           +-weka\n     |               +-classifiers\n     |                   +-rules\n     |                       +-DTNBTest.java\n     +-lib\n     +-doc  When installing, the package manager will use the value of the \"PackageName\" field in the Description.props file (see below) to create a directory in  $WEKA_HOME/packages  to hold the package contents. The contents of the  doc  directory have not been shown in the diagram above, but this directory contains javadoc for the  DTNB  class. A package must have a  Description.props  file and contain at least one jar file with compiled Java classes. The package manager will attempt to load all jar files that it finds in the root directory and the  lib  directory. Other files are optional, but if the package is open-source then it is nice to include the source code and an ant build file that can be used to compile the code. Template versions of the  Description.props  file and build_package.xml  file are available from the Weka site and here.  ==The description file==\nA valid package must contain a  Description.props  file that provides meta data on the package. Identical files are stored at the central package repository and the local cache maintained by the package manager. The package manager uses these files to compare what is installed to what is available and resolve dependencies.  The  Description.props  contains basic information on the package in the following format:  # Template Description file for a Weka package\n\n# Package name (required)\nPackageName=funkyPackage\n\n# Version (required)\nVersion=1.0.0\n\n#Date (year-month-day)\nDate=2010-01-01\n\n# Title (required)\nTitle=My cool algorithm\n\n# Category (recommended)\nCategory=Classification\n\n# Author (required)\nAuthor=Joe Dev <joe@somewhere.net>,Dev2 <dev2@somewhereelse.net>\n\n# Maintainer (required)\nMaintainer=Joe Dev <joe@somewhere.net>\n\n# License (required)\nLicense=GPL 2.0|Mozilla\n\n# Description (required)\nDescription=This package contains the famous Funky Classifer that performs \\\n truely funky prediction.\n\n# Changes and/or bug fixes in this package (optional)\nChanges=Fixed a serious bug that affected overall coolness of the Funky Classifier\n\n# Package URL for obtaining the package archive (required)\nPackageURL=http://somewhere.net/weka/funkyPackage.zip\n\n# URL for further information\nURL=http://somewhere.net/funkyResearchInfo.html\n\n# Enhances various other packages?\nEnhances=packageName1,packageName2,...\n\n# Related to other packages?\nRelated=packageName1,packageName2,...\n\n# Dependencies (required; format: packageName (equality/inequality version_number)\nDepends=weka (>=3.7.1), packageName1 (=x.y.z), packageName2 (>u.v.w|<=x.y.z),...  The  PackageName  and  Version  give the name of the package and version number respectively. The name can consist of letters, numbers, and the dot character. It should not start with a dot and should not contain any spaces. The version number is a sequence of three non-negative integers separated by single  .  or  -  characters.  The  Title  field should give a one sentence description of the package. The  Description  field can give a longer description of the package spaning multiple sentences. It may include technical references and can use HTML markup.  The  Category  field is strongly recommended as this information is displayed on both the repository web site and in the GUI package manager client. In the latter, the user can sort the packages on the basis of the category field. It is recommended that an existing category be assigned if possible. Some examples include (Classification, Text classification, Ensemble learning, Regression, Clustering, Associations, Preprocessing, Visualization, Explorer, Experimenter, KnowledgeFlow).  The  Author  field describes who wrote the package and may include multiple names (separated by commas). Email addresses may be given in angle brackets after each name. The field is intended for human readers and no email addresses are automatically extracted.  The  Maintainer  field lists who maintains the package and should include a single email address, enclosed in angle brackets, for sending bug reports to.  The  License  field lists the license(s) that apply to the package. This field may contain the short specification of a license (such as LGPL, GPL 2.0 etc.) or the string  file LICENSE , where  LICENSE  exists as a file in the top-level directory of the package. The string  Unlimited  may be supplied to indicate that there are no restrictions on distribution or use aside from those imposed by relevant laws.  The  PackageURL  field lists valid URL that points to the package zip file. This URL is used by the package manager to download and install the package.  The optional  Depends  field gives a comma separated list of packages which this package depends on. The name of a package is optionally followed by a version number constraint enclosed in parenthesis. Valid operators for version number constraints include =, <, >, <=, >=. The keyword  weka  is reserved to refer to the base Weka system and can be used to indicate a dependency on   a particular version of Weka. For example:  Depends=weka (>=3.7.2), DTNB (=1.0.0)  states that this package requires Weka 3.7.2 or higher and version 1.0.0 of the package DTNB.  Depends=weka (>3.7.1|<3.8.0)  states that this package requires a version of Weka between 3.7.1 and 3.8.0.  Depends=DTNB (<1.5.0|>=2.0.1)  states that this package requires that a version of the DTNB package be installed that is either less than version 1.5.0 or greater than or equal to version 2.0.1.  If there is no version number constraint following a package name, the package manager assumes that the latest version of the dependent package is suitable.  The optional  URL  field gives a URL at which the user can find additional online information about the package or its constituent algorithms.  The optional  Enhances  field can be used to indicate which other packages this package is based on (i.e. if it extends methods/algorithms from another   package in some fashion).  The optional  Related  field is similar to the  Enhances  field. It can be used to point the user to other packages that are related in some fashion to this one.  The optional  Changes  field should be used to indicate what changes/bug fixes are included in the current release of the package.  There are several other fields that can be used to provide information to assist the user with completing installation (if it can\u2019t be completely accomplished with the package zip file) or display error messages if necessary components are missing:  MessageToDisplayOnInstallation=Funky package requires some extra\\n\\\n stuff to be installed after installing this package. You will\\n\\\n need to blah, blah, blah in order to blah, blah, blah...\n\nDoNotLoadIfFileNotPresent=lib/someLibrary.jar,otherStuff/important,...\n\nDoNotLoadIfFileNotPresentMessage=funkyPackage can't be loaded because some \\\n funky libraries are missing. Please download funkyLibrary.jar from \\\n http://www.funky.com and install in $WEKA_HOME/packages/funkyPackage/lib\n\nDoNotLoadIfClassNotPresent=com.some.class.from.some.Where,org.some.other.Class,...\n\nDoNotLoadIfClassNotPresentMessage=funkyPackage can't be loaded because \\\n com.funky.FunkyClass can't be instantiated. Have you downloaded and run \\\n the funky software installer for your platform?  The optional  MessageToDisplayOnInstallation  field allows you to specify special instructions to the user in order to help them complete the intallation manually. This message gets displayed on the console, written to the log and appears in a pop-up information dialog if using the GUI package manager. It should include  \\n  in order to avoid long lines when displayed in a GUI pop-up dialog.  The optional  DoNotLoadIfFileNotPresent  field can be used to prevent Weka from loading the package if the named ''files'' and/or ''directories'' are not present in the package\u2019s installation directory. An example is the massiveOnlineAnalysis package. This package is a connector only package and does not include the MOA library. Users of this package must download the moa.jar file separately and copy it to the package\u2019s  lib  directory manually. Multiple files and directories can be specified as a comma separated list. All paths are relative to the top-level directory of the package.  IMPORTANT : use forward slashes as separator characters, as these are portable accross all platforms. The  DoNotLoadIfFileNotPresentMessage  field can be used to supply an optional message to display to the user if Weka detects that a file or directory is missing from the package. This message will be displayed on the console and in the log.  The optional  DoNotLoadIfClassNotPresent  field can be used to prevent Weka from loading the package if the named ''class(es)'' can\u2019t be instantiated. This is useful for packages that rely on stuff that has to be installed manually by the user. For example, Java3D is a separate download on all platforms except for OSX, and installs itself into the system JRE/JDK. The  DoNotLoadIfClassNotPresentMessage  field can be used to supply an optional message to display to the user if Weka detects that a class can\u2019t be instantiated. Again, this will be displayed on the console and in the log.  New in Weka 3.9.2 and 3.8.2 is the ability to constrain the OS and architecture that a package can be installed and loaded on. Two new fields are used for this:  # Specify which OS's the package can operate with. Omitting this entry indicates no restrictions on OS. (optional)\nOSName=Windows,Mac,Linux\n\n# Specify which architecture the package can operate with. Omitting this entry indicates no restriction. (optional)\nOSArch=64  Entries in the OSName field are compared against the value of the Java property \"os.name\" using a String.toLowerCase().contains() operation. Any single match indicates a pass. If an OSName field exists in the Description.props, and it matches, then the optional OSArch field is examined. If present, values in the OSArch list are compared against the value of the Java property \"os.arch\". The special OSArch entries \"32\" and \"64\" are tested using a String().contains() operation; all other entries are compared using String.equalsIgnoreCase().",
            "title": "Anatomy of a package"
        },
        {
            "location": "/packages/structure/#additional-configuration-files",
            "text": "Certain types of packages may require additional configuration files to be present as part of the package. The last chapter covered various ways in which Weka can be extended without having to alter the core Weka code. These plugin mechanisms have been subsumed by the package management system, so some of the configuration property files they require must be present in the package\u2019s top-level directory if the package in question contains such a plugin. Examples include additional tabs for the Explorer, mappings to custom property editors for Weka\u2019s GenericObjectEditor and Knowledge Flow plugins. Here are some examples:  The scatterPlot3D package adds a new tab to the Explorer. In order to accomplish this a property has to be set in the Explorer.props file (which contains default values for the Explorer) in order to tell Weka to instantiate and display the new panel. The scatterPlot3D file includes an  Explorer.props  file in its top-level directory that has the following contents:  # Explorer.props file. Adds the Explorer3DPanel to the Tabs key.\nTabs=weka.gui.explorer.Explorer3DPanel\nTabsPolicy=append  This property file is read by the package management system when the package is loaded and any key-value pairs are added to existing Explorer properties that have been loaded by the system at startup. If the key already exists in the Explorer properties, then the package has the option to either replace (i.e. overwrite) or append to the existing value. This can be specified with the \"TabsPolicy\" key. In this case, the value  weka.gui.explorer.Explorer3DPanel  is appended to any existing value associated with the \"Tabs\" key.  Explorer3DPanel  gets instantiated and added as a new tab when the Explorer starts.  Another example is the kfGroovy package. This package adds a plugin component to Weka\u2019s Knowledge Flow that allows a Knowledge Flow step to be implemented and compiled dynamically at runtime as a Groovy script. In order for the Knowledge Flow to make the new step appear in its  Plugins  toolbar, there needs to be a  Beans.props  file in the package\u2019s top level directory. In the case of kfGroovy, this property file has the following contents:  # Specifies that this component goes into the Plugins toolbar\nweka.gui.beans.KnowledgeFlow.Plugins=org.pentaho.dm.kf.GroovyComponent  The new pluggable evaluation metrics for classification/regression (from Weka 3.7.8 and nightly developer snapshots from 15-11-2012) are managed by the PluginManager class. To tell PluginManager that your package provides a new evaluation metric you need to provide a \"PluginManager.props\" file in the package's top level directory. For example, a hypothetical bobsMetric package might declare a new \"Area under Bob curve\" metric like so:  # Specify a new plugin Evaluation metric\nweka.classifiers.evaluation.AbstractEvaluationMetric=weka.classifiers.evaluation.BobsAUC",
            "title": "Additional configuration files"
        },
        {
            "location": "/packages/structure/#contributing-a-package",
            "text": "If you have created a package for Weka then there are two options for making it available to the community. In both cases, hosting the package\u2019s zip archive is the responsibility of the contributer.  The first, and official, route is to contact the current Weka maintainer (normally also the admin of the WEKA homepage) and supply your package\u2019s  Description.props  file. The Weka team will then test downloading and using your package to make sure that there are no obvious problems with what has been specified in the  Description.props  file and that the software runs and does not contain any malware/malicious code. If all is well, then the package will become an  official  Weka package and the central package repository meta data will be updated with the package\u2019s  Description.props file .   Responsibility for maintaining and supporting the package resides with the contributer .  The second, and unofficial, route is to simply make the package\u2019s zip archive available on the web somewhere and advertise it yourself. Although users will not be able to browse it\u2019s description in the official package repository, they will be able to download and install it directly from your URL by using the command line version of the package manager. This route could be attractive for people who have published a new algorithm and want to quiclky make a beta version available for others to try without having to go through the official route.",
            "title": "Contributing a package"
        },
        {
            "location": "/packages/structure/#creating-a-mirror-of-the-package-meta-data-repository",
            "text": "In this section we discuss an easy approach to setting up and maintaining a mirror of the package meta data repository. Having a local mirror may provide faster access times than to that of the official repository on Sourceforge. Extending this approach to the creation of an alternative central repository (hosting packages not available at the official repository) should be straight forward.  Just about everything necessary for creating a mirror exists in the local meta data cache created by Weka\u2019s package management system. This cache resides at  $WEKA_HOME/repCache . The only thing missing (in Weka 3.7.2) for a complete mirror is the file  images.txt , that lists all the image files used in the html index files. This file contains the following two lines:  Title-Bird-Header.gif\npentaho_logo_rgb_sm.png  images.txt  is downloaded automatically by the package management system in Weka 3.7.3 and higher.  To create a mirror:\n1. Copy the contents of  $WEKA_HOME/repCache  to a temporary directory. For the purposes of this example we\u2019ll call it  tempRep \n2. Change directory into  tempRep  and run  java weka.core.RepositoryIndexGenerator . . Don't forget the \".\" after the command (this tells  RepoistoryIndexGenerator  to operate on the current directory)\n3. Change directory to the parent of  tempRep  and synchronize its contents to wherever your web server is located (this is easy via rsync under Nix-like operating systems).  RepositoryIndexGenerator  automatically creates the main index.html file, all the package index.html files and html files correpsonding to all version prop files for each package. It will also create  packageList.txt  and  numPackages.txt  files.  IMPORTANT : Make sure that all the files in  tempRep  are world readable. It is easy to make packages available that are not part of the official Weka repository. Assuming you want to add a package called  funkyPackage  (as specified by the  PackageName  field in the  Description.props  file):   Create a directory called  funkyPackage  in  tempRep  Copy the  Description.props  file to  tempRep/funkyPackage/Latest.props  Copy the  Description.props file  to  tempRep/funkyPackage/<version number>.props , where  version number  is the version number specified in the  Version  field of  Description.props  Run  RepositoryIndexGenerator  as described previously and sync  tempRep  to your web server   Adding a new version of an existing package is very similar to what has already been described. All that is required is that the new  Description.props  file corresponding to the new version is copied to  Latest.props  and to  <version numer>.props  in the package\u2019s folder. Running  RepositoryIndexGenerator  will ensure that all necessary html files are created and supporting text files are updated.  Automating the mirroring process would simply involve using your OS\u2019s scheduler to execute a script that:   Runs  weka.core.WekaPackageManager -refresh-cache  rsyncs  $WEKA_HOME/repCache  to  tempRep  Runs  weka.core.RepoistoryIndexGenerator  rsyncs  tempRep  to your web server",
            "title": "Creating a mirror of the package meta data repository"
        },
        {
            "location": "/not_so_faq/gsp/",
            "text": "Class\n\n\nweka.associators.GeneralizedSequentialPatterns\n\n\nPublication\n\n\nRamakrishnan Srikant, Rakesh Agrawal (1996). Mining Sequential Patterns: Generalizations and Performance Improvements.\n\n\nDownloads\n\n\n\n\nGeneralizedSequentialPattern_example.arff",
            "title": " Generalized Sequential Patterns"
        },
        {
            "location": "/not_so_faq/gsp/#class",
            "text": "weka.associators.GeneralizedSequentialPatterns",
            "title": "Class"
        },
        {
            "location": "/not_so_faq/gsp/#publication",
            "text": "Ramakrishnan Srikant, Rakesh Agrawal (1996). Mining Sequential Patterns: Generalizations and Performance Improvements.",
            "title": "Publication"
        },
        {
            "location": "/not_so_faq/gsp/#downloads",
            "text": "GeneralizedSequentialPattern_example.arff",
            "title": "Downloads"
        },
        {
            "location": "/not_so_faq/j48_numbers/",
            "text": "J48 pruned tree\nnode-caps = yes\n| deg-malig = 1: recurrence-events (1.01/0.4)\n| deg-malig = 2: no-recurrence-events (26.2/8.0)\n| deg-malig = 3: recurrence-events (30.4/7.4)\nnode-caps = no: no-recurrence-events (228.39/53.4)\n\n\n\n\nThe \nfirst\n number is the total number of instances (weight of instances) reaching the leaf. The \nsecond\n number is the number (weight) of those instances that are misclassified.\n\n\nIf your data has \nmissing\n attribute values then you will end up with \nfractional\n instances at the leafs. When splitting on an attribute where some of the training instances have missing values, J48 will divide a training instance with a missing value for the split attribute up into fractional parts proportional to the frequencies of the observed non-missing values. This is discussed in the Witten & Frank Data Mining book as well as Ross Quinlan's original publications on C4.5.",
            "title": " What do those numbers mean in a J48 tree?"
        },
        {
            "location": "/faqs/different_versions/",
            "text": "Refer to \nHistory\n for a tabular overview of all Weka releases.\n\n\nSeveral branches are associated with the 1st, 2nd, 3rd, and \n4th\n edition of the book \nData Mining: Practical Machine Learning Tools and Techniques\n by \nIan H. Witten\n and \nEibe Frank\n, joined by \nMark Hall\n for the 3rd edition and \nChris Pal\n for the 4th edition.\n\n\nOnce created, non-development branches receive bug fixes, but no new features (classifiers, filters, etc.).\n\n\n\n\n\n\n\n\nVersion name\n\n\nMost recent base number\n\n\nAssociated with book edition\n\n\n\n\n\n\n\n\n\n\nBook 1st ed. version\n\n\n3.0.x\n\n\n1st edition\n\n\n\n\n\n\nOld GUI version\n\n\n3.2.x\n\n\nnone\n\n\n\n\n\n\nBook 2nd ed. version\n\n\n3.4.x\n\n\n2nd edition\n\n\n\n\n\n\nBook 3rd ed. version\n\n\n3.6.x\n\n\n3rd edition\n\n\n\n\n\n\nBook 4th ed. version\n\n\n3.8.x\n\n\n4th edition\n\n\n\n\n\n\nDevelopment version\n\n\n3.9.x\n\n\nnone\n\n\n\n\n\n\n\n\nFor \ncontributions\n, you should always develop against the developer version.",
            "title": " What are the principal release branches of Weka?"
        },
        {
            "location": "/faqs/old_versions/",
            "text": "If you need a specific version of WEKA, e.g., due to some third-party tools, go \nWEKA's project page\n on \nSourceforge.net\n. In the \nFiles section\n you have access to all the releases ever made.",
            "title": " Where can I get old versions of WEKA?"
        },
        {
            "location": "/faqs/latest_bugfixes/",
            "text": "The article \nHow to get the latest bugfixes\n explains it in detail (it's basically either obtaining the source code from \nSubversion\n and compiling it yourself or getting a snapshot from the download section of the \nWEKA homepage\n).",
            "title": " How do I get the latest bugfixes?"
        },
        {
            "location": "/faqs/contribution/",
            "text": "Information on how to contribute to WEKA can be found in the \nContributing a package\n section of the \nHow are packages structured for the package management system?\n article. The conditions for new classifiers (schemes in general) are that, firstly, they have to be published in the proceedings of a renowned conference (e.g., ICML) or as an article of respected journal (e.g., Machine Learning) and, secondly, that they outperform other standard schemes (e.g., J48/C4.5).\n\n\nBut please bear in mind, that we don't have a lot of man power, i.e., being the WEKA maintainer is \nNOT\n a full-time position.",
            "title": " How can I contribute to WEKA?"
        },
        {
            "location": "/faqs/package_manager_doesnt_start/",
            "text": "The most likely reason for this is that your computer does not have direct access to the Internet and Java needs to be told to use a proxy server to access the web. The best way to achieve this is to configure an environment variable that provides the proxy details, e.g.,\n\n\n_JAVA_OPTIONS\n\n\n\n\nwhich is read by Oracle Java virtual machines. There is more information on this variable \nhere\n. Information on how to set environment variables in Windows is \nhere\n. For Mac users, there is a nice program to set environment variables available \nhere\n.\n\n\nSet the value of this variable to\n\n\n-Dhttp.proxyHost=some.proxy.somewhere.net -Dhttp.proxyPort=port\n\n\n\n\nwhere \nsome.proxy.somewhere.net\n needs to be replaced by the name of your proxy server and \nport\n needs to be replaced by the appropriate port number on the proxy server. Your IT department should be able to give you these details.\n\n\nThis should allow the package manager to connect to the website that hosts the package meta-information. However, if the package manager still cannot connect to the Internet, you can also force it to run in offline mode, by setting the above environment variable to\n\n\n-Dweka.packageManager.offline=true\n\n\n\n\nThen, you can download package .zip files manually via your web browser, by navigating to\n\n\nhttp://weka.sourceforge.net/packageMetaData/\n\n\nclicking on the link for the package you want to install, then clicking on \nLatest\n, and finally clicking on the URL given next to \nPackageURL\n.\n\n\nOnce you have downloaded the package .zip file, open the WEKA package manager, and click on the \nFile/URL\n button in the top-right corner of the package manager window (in the \nUnofficial\n panel). Then navigate to your package .zip file and install it.\n\n\nIf you are running Weka in \noffline\n mode, and the packages you are installing have some dependencies on one another, then there can still be some problems due to Weka not being able to verify the dependencies by checking against the central repository. This is usually a problem in the case where Weka has never been able to connect to the internet and thus has not downloaded and established a cache of the central package metadata repository. Fortunately there is a simple work-around to this, as long as you can access the internet via a web browser:\n\n\n\n\nUsing your web browser, download \nhttp://weka.sourceforge.net/packageMetaData/repo.zip\n\n\nIf it doesn't already exist, create the directory \n~/wekafiles/repCache\n\n\nCopy the downloaded \nrepo.zip\n into \n~/wekafiles/repCache\n and unzip it there\n\n\nStart Weka (use the \nweka.packageManager.offline=true\n property to speed up the startup process; see [http://weka.wikispaces.com/How+do+I+use+the+package+manager%3F#Package%20manager%20property%20file] for info)",
            "title": " Weka package manager does not start"
        },
        {
            "location": "/faqs/arff_does_not_load/",
            "text": "One way to figure out why \nARFF\n files are failing to load is to give them to the \nweka.core.Instances\n class. In the SimpleCLI or in the terminal, type the following:\n\n\n java weka.core.Instances filename.arff\n\n\n\n\nwhere you substitute \nfilename\n for the actual name of your file. This should return an error if there is a problem reading the file, or show some statistics if the file is ok. The error message you get should give some indication of what is wrong.\n\n\nnominal value not declared in header, read Token[X], line Y\n\n\nIf you get this error message than you seem to have declared a nominal attribute in the ARFF header section, but WEKA came across a value (\n\"X\"\n) in the data (in line \nY\n) for this particular attribute that wasn't listed as possible value.\n\n\nAll\n nominal values that appear in the data must be declared in the header.",
            "title": " ARFF file does not load"
        },
        {
            "location": "/faqs/arff_does_not_load/#nominal-value-not-declared-in-header-read-tokenx-line-y",
            "text": "If you get this error message than you seem to have declared a nominal attribute in the ARFF header section, but WEKA came across a value ( \"X\" ) in the data (in line  Y ) for this particular attribute that wasn't listed as possible value.  All  nominal values that appear in the data must be declared in the header.",
            "title": "nominal value not declared in header, read Token[X], line Y"
        },
        {
            "location": "/faqs/commercial_applications/",
            "text": "WEKA is licensed under the GNU General Public license (\nGPL 2.0 for Weka 3.6\n) and (\nGPL 3.0 for Weka > 3.7.5\n). Any derivative work obtained under this license must be licensed under the GPL if this derivative work is distributed to a third party.\n\n\nFor commercial projects that require the ability to distribute WEKA code as part of a program that cannot be distributed under the GPL, it may be possible to purchase an appropriate license from the copyright holders listed in the corresponding Java classes.\n\n\nThe copyright for most WEKA code is owned by the University of Waikato. For information on licenses for this code please contact \nWaikatoLink\n , the commercialization unit of the \nUniversity of Waikato\n, by sending email to weka-enquiries at waikatolink.co.nz.",
            "title": " Can I use WEKA in commercial applications?"
        },
        {
            "location": "/faqs/ubuntu_1804_blas_warning/",
            "text": "When running Ubuntu 18.04, you might see the following warning message(s) in the console:\n\n\nApr 03, 2019 5:40:10 PM com.github.fommil.netlib.BLAS <clinit>\nWARNING: Failed to load implementation from: com.github.fommil.netlib.NativeSystemBLAS\nApr 03, 2019 5:40:10 PM com.github.fommil.netlib.BLAS <clinit>\nWARNING: Failed to load implementation from: com.github.fommil.netlib.NativeRefBLAS\nApr 03, 2019 5:40:10 PM com.github.fommil.netlib.LAPACK <clinit>\nWARNING: Failed to load implementation from: com.github.fommil.netlib.NativeSystemLAPACK\nApr 03, 2019 5:40:10 PM com.github.fommil.netlib.LAPACK <clinit>\nWARNING: Failed to load implementation from: com.github.fommil.netlib.NativeRefLAPACK\n\n\n\n\nYou can easily fix this by installing the missing dependencies with this command:\n\n\nsudo apt-get install libgfortran-6-dev",
            "title": " Ubuntu 18:08 BLAS Warning"
        },
        {
            "location": "/faqs/check_classpath_within_weka/",
            "text": "Yes, you can. Just start up the \nSimpleCLI\n and issue the following command:\n\n\njava weka.core.SystemInfo\n\n\n\n\nLook for the property \njava.class.path\n, which lists the \nCLASSPATH\n WEKA was started with.",
            "title": " Can I check my CLASSPATH from within WEKA?"
        },
        {
            "location": "/faqs/home_directory_location/",
            "text": "Where a user's home directory is located varies from platform to platform and among the users on a single computer. But the actual location of the home directory is available through special environment variables:\n\n\n\n\n\n\nUnix/Linux\n\n\n$HOME\n\n\n\n\n\n\nWindows\n\n\n%USERPROFILE%\n\n\n\n\n\n\nCygwin\n\n\n$USERPROFILE\n\n\n\n\n\n\nIn order to find out where these environment variables actually point to, do the following:\n\n\n\n\n\n\non \nUnix/Linux\n, open a terminal and type the following command\n\n\necho $HOME\n\n\n\n\n\n\non \nWindows\n, open a command-prompt and type the following command\n\n\necho %USERPROFILE%\n\n\n\n\n\n\non \nCygwin\n, open a bash and type the following command\n\n\necho $USERPROFILE",
            "title": " Where is my home directory located?"
        },
        {
            "location": "/faqs/check_memory_available/",
            "text": "You can easily check, how much memory WEKA can use (this depends on the \nmaximum heap size\n the [[Java Virtual Machine]] was started with).\n\n\n\n\n\n\nSimpelCLI\n\n\n\n\nstart the SimpleCLI\n\n\n\n\nrun the following command:\n\n\njava weka.core.SystemInfo\n\n\n\n\n\n\nthe property \nmemory.max\n lists the maximum amount of memory available to WEKA\n\n\n\n\n\n\n\n\n\n\nGUIChooser\n\n\n\n\nselect \nHelp -> SystemInfo\n\n\nthe property \nmemory.max\n lists the maximum amount of memory available to WEKA\n\n\n\n\n\n\n\n\nIn case you should run into an \nOutOfMemoryException\n, you will have to increase the \nmaximum heap size\n. How much you can allocate, depends heavily on the operating system and the underlying hardware, see the [[Java Virtual Machine]] article). Also, have a look at the \nOutOfMemoryException\n section further down.",
            "title": " Can I check how much memory is available for WEKA?"
        },
        {
            "location": "/faqs/OutOfMemoryException/",
            "text": "Most Java virtual machines only allocate a certain maximum amount of memory to run Java programs. Usually, this is much less than the amount of RAM in your computer. There is some information on default heap sizes in Oracle Java virtual machines for Java 8 \nhere\n. However, you can extend the memory available for the virtual machine by setting appropriate options. With Oracle's JDK, for example, you can go\n\n\njava -Xmx2g ...\n\n\n\n\nto set the maximum Java heap size to 2GB.\n\n\nA reliable way to set the maximum heap size for Oracle Java virtual machines (and overwrite any other settings that might be provided in startup scripts, etc.) is to use the _JAVA_OPTIONS environment variable to specify the \n-Xmx\n option. There is more information \nhere\n.",
            "title": " OutOfMemoryException"
        },
        {
            "location": "/faqs/use_csv_files/",
            "text": "Yes, you can. But be aware that there is a drawback in comparison to \nARFF\n files (WEKA's default file format):\n\n\nTrain and test set may not be compatible.\n Using CSV files as train and test set can be a frustrating exercise. Since CSV files don't contain any information about the attributes, WEKA needs to determine the labels for nominal attributes itself. Not only does the order of the appearance of these labels create different nominal attributes (\"1,2,3\" vs \"1,3,2\"), but it is also not guaranteed that all the labels that appeared in the train set also appear in the test set (\"1,2,3,4\" vs \"1,3,4\") and vice versa.",
            "title": " Can I use CSV files?"
        },
        {
            "location": "/faqs/csv_file_conversion/",
            "text": "Either load the CSV file in the Explorer or use the CSV converter on the commandline as follows:\n\n\n java weka.core.converters.CSVLoader filename.csv > filename.arff\n\n\n\n\nSee also the \nConverting CSV to ARFF\n article and FAQ \nCan I use CSV files?\n.",
            "title": " CSV File Conversion"
        },
        {
            "location": "/faqs/how_do_i_divide_a_dataset_into_training_and_test_set/",
            "text": "You can use the \nRemovePercentage\n filter (package \nweka.filters.unsupervised.instance\n).\n\n\nIn the Explorer just do the following:\n\n\n\n\ntraining set:\n\n\nLoad the full dataset\n\n\nselect the \nRemovePercentage\n filter in the preprocess panel\n\n\nset the correct percentage for the split\n\n\napply the filter\n\n\nsave the generated data as a new file\n\n\n\n\n\n\ntest set:\n\n\nLoad the full dataset (or just use undo to revert the changes to the dataset)\n\n\nselect the \nRemovePercentage\n filter if not yet selected\n\n\nset the \ninvertSelection\n property to true\n\n\napply the filter\n\n\nsave the generated data as new file",
            "title": " How do I divide a dataset into training and test sets"
        },
        {
            "location": "/faqs/how_do_i_generate_compatible_train_and_test_sets_that_get_processed_with_a_filter/",
            "text": "Running a filter twice, once with the train set as input and then the second time with the test set, will create almost certainly two incompatible files. Why is that? Every time you run a filter, it will get initialized based on the input data, and, of course, training and test set will differ, hence creating incompatible output. You can avoid this by using \nbatch filtering\n. See the article on \nBatch filtering\n for more details.",
            "title": " How do I generate compatible train and test sets that get processed with a filter? "
        },
        {
            "location": "/faqs/how_do_i_perform_attribute_selection/",
            "text": "WEKA offers different approaches for performing attribute selection:\n\n\n\n\ndirectly with the attribute selection classes,\n\n\nwith a meta-classifier, and \n\n\nwith a filter.\n\n\n\n\nCheck out the \nPerforming attribute selection\n article for more details and examples.",
            "title": " How do I perform attribute selection? "
        },
        {
            "location": "/faqs/how_do_i_perform_clustering/",
            "text": "WEKA offers clustering capabilities not only as standalone schemes, but also as filters and classifiers. Check out the article about \nUsing cluster algorithms\n more detailed information.",
            "title": " How do I perform clustering?"
        },
        {
            "location": "/faqs/how_do_i_perform_text_classification/",
            "text": "The article \nText categorization with WEKA\n explains a few basics of how to deal with text documents, like importing and pre-processing.",
            "title": " How do I perform text classification?"
        },
        {
            "location": "/faqs/how_can_i_perform_multi_instance_learning_in_weka/",
            "text": "The article \nMulti-instance classification\n explains what classifiers can perform multi-instance classification and what format the data has to be in for these multi-instance classifiers.",
            "title": " How can I perform multi-instance learning in WEKA?"
        },
        {
            "location": "/faqs/how_do_i_perform_cost_sensitive_classification/",
            "text": "Cost-sensitive classification can be achieved using a \nCost-Sensitive Classifier\n. Related articles to the cost-sensitive topic include:\n\n\n\n\nCost Matrix\n\n\nMetacost\n\n\n\n\nSearching for the term \ncost-sensitive\n will also show articles related to the topic.",
            "title": " How do I perform cost-sensitive classification?"
        },
        {
            "location": "/faqs/how_do_i_make_predictions_with_a_trained_model/",
            "text": "Since WEKA allows models to be saved (as Java binary serialized objects), one can use those models again to perform predictions. Check out the article \nMaking Predictions\n for more details.",
            "title": " How do I make predictions with a trained model?"
        },
        {
            "location": "/faqs/why_am_i_missing_certain_nominal_or_string_values_from_sparse_instances/",
            "text": "Internally, WEKA stores all attribute values as double precision floating point numbers. In the case of nominal or string attributes these numbers are interpreted as indexes into the set of values for the attribute in question, with 0 corresponding to the first value, 1 the second and so forth. Because sparse data does not explicitly store zeros, any instances containing the first value (with index 0) of a nominal or string attribute does not show this value when printing out an ARFF file that is sparse format.",
            "title": " Why am I missing certain nominal or string values from sparse instances?"
        },
        {
            "location": "/faqs/can_i_use_weka_for_time_series_analysis/",
            "text": "Weka 3.7.3 has a new package that provides an environment for time series analysis. The article \nHow do I use the package manager?\n can be followed to install this package. Once installed, the package provides a plugin tab in the Explorer. Documentation on the time series environment can be found \nhere\n \n\n\nOlder versions of Weka have limited support for time series analysis and consists of only two filters, \nTimeSeriesDelta\n and \nTimeSeriesTranslate\n. There are modified (not supported by the University of Waikato) versions of WEKA out there, that offer additional functionality (\n1\n, \n2\n).",
            "title": " Can I use WEKA for time series analysis?"
        },
        {
            "location": "/faqs/does_weka_support_multi_label_classification/",
            "text": "No, WEKA only allows you to specify a single class attribute (which can be numeric or contain an arbitrary number of labels). There are other third-party frameworks available that can handle this type of data. One of them is \nMulan\n, which is built on top of WEKA.",
            "title": " Does WEKA support multi-label classification?"
        },
        {
            "location": "/faqs/how_do_i_perform_one_class_classification/",
            "text": "WEKA offers some rudimentary support for one-class classfication:\n\n\n\n\nvia the \nweka.classifiers.functions.LibSVM\n wrapper classifier (stable 3.6 and developer version). See the \nLibSVM\n article for more information.\n\n\nvia the \nweka.classifiers.meta.OneClassClassifier\n meta-classifier (developer version >3.7.0 or \nsnapshot\n later than 23/7/2009)",
            "title": " How do I perform one-class classification?"
        },
        {
            "location": "/faqs/can_i_make_a_screenshot_of_a_plot_or_graph_directly_in_weka/",
            "text": "Yes, you can. The currently supported formats are BMP, EPS, JPEG and PNG. The \nmagic button\n is \nAlt+Shift+Left-Click\n.\n\n\nFrom Weka 3.7.5 it is also possible to export various charts as PNG files non-interactively from a Knowledge Flow process. See \nExporting Charts from the Knowledge Flow\n.",
            "title": " Can I make a screenshot of a plot or graph directly in WEKA?"
        },
        {
            "location": "/packages/manager/",
            "text": "Usually, the term \"package\" is used to refer to Java's concept of organizing classes. From version 3.7.2, Weka has the concept of a package as a bundle of additional functionality, separate from that supplied in the main weka.jar file. A package consists of various jar files, documentation, meta data, and possibly source code. Many learning algorithms and tools that were present in earlier versions of Weka have become separate packages from version 3.7.2. This simplifies the core Weka system and allows users to install just what they need or are interested in. It also provides a simple mechanism for people to use when contributing to Weka. There are a number of packages available for Weka that add learning schemes or extend the functionality of the core system in some fashion. Many are provided by the Weka team and others are from third parties.\n\n\nWeka includes a facility for the management of packages and a mechanism to load them dynamically at runtime. There is both a command-line and GUI package manager. If the package manager does not start when you try to run it, take a look at \nthis\n page.\n\n\nCommand line package management\n\n\nAssuming that the weka.jar file is in the classpath, the package manager can be accessed by typing:\n\n\n java weka.core.WekaPackageManager\n\n\n\n\nSupplying no options will print the usage information:\n\n\nUsage: weka.core.WekaPackageManager [option]\nOptions:\n  -list-packages <all | installed | available>\n  -package-info <repository | installed | archive> packageName\n  -install-package <packageName | packageZip | URL> [version]\n  -uninstall-package <packageName>\n  -refresh-cache\n\n\n\n\n\n\nWeka 3.7.8 and snapshot builds of the developer version of Weka after September 24 2012 now offer a completely \"offline\" mode that involves no attempts to connect to the internet. This mode can be used to install package zip files that the user already has on the file system, and to browse already installed packages. This mode can be accessed from the command line package manager by specifying the \"-offline\" option. Alternatively, the property weka.packageManager.offline=true can be provided to the Java virtual machine on the command line or in a properties file (see the section on properties below).\n\n\n\n\nInformation (meta data) about packages is stored on a web server hosted on Sourceforge. The first time the package manager is run, for a new installation of Weka, there will be a short delay while the system downloads and stores a cache of the meta data from the server. Maintaining a cache speeds up the process of browsing the package information. From time to time you should update the local cache of package meta data in order to get the latest information on packages from the server. This can be achieved by supplying the \n-refresh-cache\n option.\n\n\nThe \n-list-packages\n option will, as the name suggests, print information (version numbers and short descriptions) about various packages. The option must be followed by one of three keywords:\n\n\n\n\nall\n will print information on all packages that the system knows about\n\n\ninstalled\n will print information on all packages that are installed locally\n\n\navailable\n will print information on all packages that are not installed\n\n\n\n\nThe following shows an example of listing all packages installed locally:\n\n\njava weka.core.WekaPackageManager -list-packages installed\n\nInstalled    Repository    Package\n=========    ==========    =======\n1.0.0        1.0.0         DTNB: Class for building and using a decision table/naive bayes hybrid classifier.\n1.0.0        1.0.0         massiveOnlineAnalysis: MOA (Massive On-line Analysis).\n1.0.0        1.0.0         multiInstanceFilters: A collection of filters for manipulating multi-instance data.\n1.0.0        1.0.0         naiveBayesTree: Class for generating a decision tree with naive Bayes classifiers at the leaves.\n1.0.0        1.0.0         scatterPlot3D: A visualization component for displaying a 3D scatter plot of the data using Java 3D.\n\n\n\n\nThe \n-package-info\n command lists information about a package given its name. The command is followed by one of three keywords and then the name of a package:\n\n\n\n\nrepository\n will print info from the repository for the named package\n\n\ninstalled\n will print info on the installed version of the named package\n\n\narchive\n will print info for a package stored in a zip archive. In this case, the \u201carchive\u201d keyword must be followed by the path to an package zip archive file rather than just the name of a package\n\n\n\n\nThe following shows an example of listing information for the \u201cisotonicRegression\u201d package from the server:\n\n\njava weka.core.WekaPackageManager -package-info repository isotonicRegression\nDescription:Learns an isotonic regression model. Picks the attribute that results in the lowest squared error. Missing values are not allowed. Can only deal with numeric attributes. Considers the monotonically increasing case as well as the monotonically decreasing case.\nVersion:1.0.0\nPackageURL:http://prdownloads.sourceforge.net/weka/isotonicRegression1.0.0.zip?download\nAuthor:Eibe Frank\nPackageName:isotonicRegression\nTitle:Learns an isotonic regression model.\nDate:2009-09-10\nURL:http://weka.sourceforge.net/doc.dev/weka/classifiers/IsotonicRegression.html\nCategory:Regression\nDepends:weka (>=3.7.1)\nLicense:GPL 2.0\nMaintainer:Weka team <wekalist@list.scms.waikato.ac.nz>\n\n\n\n\nThe \n-install-package\n command allows a package to be installed from one of three locations:\n\n\n\n\nspecifying a name of a package will install the package using the information in the package description meta data stored on the server. If no version number is given, then the latest available version of the package is installed.\n\n\nproviding a path to a zip file will attempt to unpack and install the archive as a Weka package\n\n\nproviding a URL (beginning with \nhttp://\n) to a package zip file on the web will download and attempt to install the zip file as a Weka package\n\n\n\n\nThe \nuninstall-package\n command will uninstall the named package. Of course, the named package has to be installed for this command to have any effect!\n\n\nRunning installed learning algorithms\n\n\nRunning learning algorithms that come with the main weka distribution (i.e. are contained in the weka.jar file) was covered earlier in the [[Primer]]. But what about algorithms from packages that you\u2019ve installed using the package manager? We don\u2019t want to have to add a ton of jar files to our classpath every time we wan\u2019t to run a particular algorithm. Fortunately, we don\u2019t have to. Weka has a mechanism to load installed packages dynamically at run time. This means that newly installed packages are available in Weka's GUIs immediately.\n\n\nWhat about running algorithms from packages on the command line I hear you ask? We can run a named algorithm by using the Run command:\n\n\n java weka.Run\n\n\n\n\nIf no arguments are supplied, then Run outputs the following usage information:\n\n\n Usage:\n     weka.Run [-no-scan] [-no-load] <scheme name [scheme options]>\n\n\n\n\nThe Run command supports sub-string matching, so you can run a classifier (such as J48) like so:\n\n\n java weka.Run J48\n\n\n\n\nWhen there are multiple matches on a supplied scheme name you will be presented with a list. For example:\n\n\n java weka.Run NaiveBayes\n\n Select a scheme to run, or <return> to exit:\n     1) weka.classifiers.bayes.ComplementNaiveBayes\n     2) weka.classifiers.bayes.NaiveBayes\n     3) weka.classifiers.bayes.NaiveBayesMultinomial\n     4) weka.classifiers.bayes.NaiveBayesMultinomialUpdateable\n     5) weka.classifiers.bayes.NaiveBayesSimple\n     6) weka.classifiers.bayes.NaiveBayesUpdateable\n\n Enter a number >\n\n\n\n\nYou can turn off the scanning of packages and sub-string matching by providing the \n-no-scan\n option. This is useful when using the \nRun\n command in a script. In this case, you need to specify the fully qualified name of the algorithm to use. E.g.\n\n\n java weka.Run -no-scan weka.classifiers.bayes.NaiveBayes\n\n\n\n\nTo reduce startup time you can also turn off the dynamic loading of installed packages by specifying the \n-no-load\n option. In this case, you will need to explicitly include any packaged algorithms in your classpath if you plan to use them. E.g.\n\n\njava -classpath ./weka.jar:$HOME/wekafiles/packages/DTNB/DTNB.jar rweka.Run -no-load -no-scan weka.classifiers.rules.DTNB\n\n\n\n\nGUI package manager\n\n\nAs well as a command line client, there is also a graphical interface to Weka\u2019s package management system. This is available from the \nTools\n menu in the \nGUIChooser\n. All the functionality available in the command line client to the package management system is available in the GUI version, along with the ability to install and uninstall multiple packages in one hit.\n\n\n\n\nThe package manager\u2019s window is split horizontally into two parts: at the top is a list of packages and at the bottom is a mini browser that can be used to display information on the currently selected package.\n\n\nThe package list shows the name of a package, its category, the currently installed version (if installed), the latest version available via the repository and whether the package has been loaded or not. This list may be sorted by either package name or category by clicking on the appropriate column header. A second click on the same header reverses the sort order. Three radio buttons in the upper left of the window can be used to filter what is displayed in the list. All packages (default), all available packages (i.e. those not yet installed) or only installed packages can be displayed.\n\n\nIf multiple versions of a package are available, they can be accessed by clicking on an entry in the \u201cRepository version\u201d column:\n\n\n\n\nInstalling and removing packages\n\n\nAt the very top of the window are three buttons. On the left-hand side is a button that can be used to refresh the cached copy of the package repository meta data. The first time that the package manager (GUI or command line) is used there will be a short delay as the initial cache is established. \n\n\nNOTE: Weka (3.7.2) will not automatically check for new information at the central repository, so it is a good idea to refresh the local cache regularly. From Weka 3.7.3 the package manager will notify you if there are new packages available at the central repository\n.\n\n\nThe two buttons at the top right are used to install and remove packages repspectively. Multiple packages may be installed/removed by using a shift-left-click combination to select a range and/or by using a command-left-click combination to add to the selection. Underneath the install and uninstall but- tons is a checkbox that can be enabled to ignore any dependencies required by selected packages and any conflicts that may occur. Installing packages while this checkbox is selected will '''not''' install required dependencies.\n\n\nSome packages may have additional information on how to complete the installation or special instructions that gets displayed when the package is installed:\n\n\n\n\nUsually it is not necessary to restart Weka after packages have been installed\u2014the changes should be available immediately. An exception is when upgrading a package that is already installed. If in doubt, restart Weka.\n\n\nUnofficial packages\n\n\nThe package list shows those packages that have their meta data stored in Weka\u2019s central meta data repository. These packages are \u201cofficial\u201d Weka packages and the Weka team as verified that they appear to provide what is advertised (and do not contain malicious code or malware).\n\n\nIt is also possible to install an \nunofficial\n package that has not gone through the process of become official. Unofficial packages might be provided, for exam- ple, by researchers who want to make experimental algorithms quickly available to the community for feedback. Unofficial packages can be installed by clicking the \u201cFile/url\u201d button on the top-right of the package manager window. This will bring up an \u201cUnnoficial package install\u201d dialog where the user can browse their file system for a package zip file or directly enter an URL to the package zip file. Note that no dependency checking is done for unofficial packages.\n\n\nUsing a HTTP proxy\n\n\nBoth the GUI and command line package managers can operate via a http proxy. To do so, start Weka from the command line and supply property values for the proxy host and port:\n\n\n java -Dhttp.proxyHost=some.proxy.somewhere.net -Dhttp.proxyPort=port weka.gui.GUIChooser\n\n\n\n\nIf your proxy requires authentication, then Weka will present a GUI dialog where you can enter your username and password. If you are running on a headless environment, then two more (non-standard) properties can be supplied:\n\n\n -Dhttp.proxyUser=some_user_name -Dhttp.proxyPassword=some_password\n\n\n\n\nUsing an alternative central package meta data repository\n\n\nBy default, both the command-line and GUI package managers use the central package meta data repository hosted on Sourceforge. In the unlikely event that this site is unavailable for one reason or another, it is possible to point the package management system at an alternative repository. This mechanism allows a temporary backup of the official repostory to be accessed, local mirrors to be established and alternative repositories to be set up for use etc.\n\n\nAn alternative repository can be specified by setting a Java property:\n\n\nweka.core.wekaPackageRepositoryURL=http://some.mirror.somewhere\n\n\n\n\nThis can either be set when starting Weka from the command line with the \n-D\n flag, or it can be placed into a file called \u201cPackageRepository.props\u201d in \n$WEKA_HOME/props\n. The default value of \nWEKA_HOME\n is \nuser.home/wekafiles\n, where \nuser.home\n is the user\u2019s home directory. More information on how and where Weka stores configuration information is given in the [[How are packages structured for the package management system?|how are package structured for the package management system?]] article.\n\n\nPackage manager property file\n\n\nAs mentioned in the previous section, an alternative package meta data repository can be specified by placing an entry in the PackageRepository.props file in \n$WEKA_HOME/props\n. From Weka 3.7.8 (and snapshot builds after 24 September 2012), the package manager also looks for properties placed in \n$WEKA_HOME/props/PackageManager.props\n. The current set of properties that can be set are:\n\n\nweka.core.wekaPackageRepositoryURL=http://some.mirror.somewhere\nweka.packageManager.offline=[true | false]\nweka.packageManager.loadPackages=[true | false]\nweka.pluginManager.disable=com.funky.FunkyExplorerPluginTab\n\n\n\n\nThe default for offline mode (if unspecified) is \nfalse\n and for loadPackages is \ntrue\n. The weka.pluginManager.disable property can be used to specify a comma-separated list of fully qualified class names to \"disable\" in the GUI. This can be used to make problematic components unavailable in the GUI without having to prevent the entire package that contains them from being loaded. E.g. \"funkyPackage\" might provide several classifiers and a special Explorer plugin tab for visualization. Suppose, for example, that the plugin Explorer tab has issues with certain data sets and causes annoying exceptions to be generated (or perhaps in the worst cases crashes the Explorer!). In this case we might want to use the classifiers provided by the package and just disable the Explorer plugin. Listing the fully qualified name of the Explorer plugin as a member of the comma-separated list associated with the weka.pluginManager.disable property will achieve this.",
            "title": " How do I use the package manager?"
        },
        {
            "location": "/packages/manager/#command-line-package-management",
            "text": "Assuming that the weka.jar file is in the classpath, the package manager can be accessed by typing:   java weka.core.WekaPackageManager  Supplying no options will print the usage information:  Usage: weka.core.WekaPackageManager [option]\nOptions:\n  -list-packages <all | installed | available>\n  -package-info <repository | installed | archive> packageName\n  -install-package <packageName | packageZip | URL> [version]\n  -uninstall-package <packageName>\n  -refresh-cache   Weka 3.7.8 and snapshot builds of the developer version of Weka after September 24 2012 now offer a completely \"offline\" mode that involves no attempts to connect to the internet. This mode can be used to install package zip files that the user already has on the file system, and to browse already installed packages. This mode can be accessed from the command line package manager by specifying the \"-offline\" option. Alternatively, the property weka.packageManager.offline=true can be provided to the Java virtual machine on the command line or in a properties file (see the section on properties below).   Information (meta data) about packages is stored on a web server hosted on Sourceforge. The first time the package manager is run, for a new installation of Weka, there will be a short delay while the system downloads and stores a cache of the meta data from the server. Maintaining a cache speeds up the process of browsing the package information. From time to time you should update the local cache of package meta data in order to get the latest information on packages from the server. This can be achieved by supplying the  -refresh-cache  option.  The  -list-packages  option will, as the name suggests, print information (version numbers and short descriptions) about various packages. The option must be followed by one of three keywords:   all  will print information on all packages that the system knows about  installed  will print information on all packages that are installed locally  available  will print information on all packages that are not installed   The following shows an example of listing all packages installed locally:  java weka.core.WekaPackageManager -list-packages installed\n\nInstalled    Repository    Package\n=========    ==========    =======\n1.0.0        1.0.0         DTNB: Class for building and using a decision table/naive bayes hybrid classifier.\n1.0.0        1.0.0         massiveOnlineAnalysis: MOA (Massive On-line Analysis).\n1.0.0        1.0.0         multiInstanceFilters: A collection of filters for manipulating multi-instance data.\n1.0.0        1.0.0         naiveBayesTree: Class for generating a decision tree with naive Bayes classifiers at the leaves.\n1.0.0        1.0.0         scatterPlot3D: A visualization component for displaying a 3D scatter plot of the data using Java 3D.  The  -package-info  command lists information about a package given its name. The command is followed by one of three keywords and then the name of a package:   repository  will print info from the repository for the named package  installed  will print info on the installed version of the named package  archive  will print info for a package stored in a zip archive. In this case, the \u201carchive\u201d keyword must be followed by the path to an package zip archive file rather than just the name of a package   The following shows an example of listing information for the \u201cisotonicRegression\u201d package from the server:  java weka.core.WekaPackageManager -package-info repository isotonicRegression\nDescription:Learns an isotonic regression model. Picks the attribute that results in the lowest squared error. Missing values are not allowed. Can only deal with numeric attributes. Considers the monotonically increasing case as well as the monotonically decreasing case.\nVersion:1.0.0\nPackageURL:http://prdownloads.sourceforge.net/weka/isotonicRegression1.0.0.zip?download\nAuthor:Eibe Frank\nPackageName:isotonicRegression\nTitle:Learns an isotonic regression model.\nDate:2009-09-10\nURL:http://weka.sourceforge.net/doc.dev/weka/classifiers/IsotonicRegression.html\nCategory:Regression\nDepends:weka (>=3.7.1)\nLicense:GPL 2.0\nMaintainer:Weka team <wekalist@list.scms.waikato.ac.nz>  The  -install-package  command allows a package to be installed from one of three locations:   specifying a name of a package will install the package using the information in the package description meta data stored on the server. If no version number is given, then the latest available version of the package is installed.  providing a path to a zip file will attempt to unpack and install the archive as a Weka package  providing a URL (beginning with  http:// ) to a package zip file on the web will download and attempt to install the zip file as a Weka package   The  uninstall-package  command will uninstall the named package. Of course, the named package has to be installed for this command to have any effect!",
            "title": "Command line package management"
        },
        {
            "location": "/packages/manager/#running-installed-learning-algorithms",
            "text": "Running learning algorithms that come with the main weka distribution (i.e. are contained in the weka.jar file) was covered earlier in the [[Primer]]. But what about algorithms from packages that you\u2019ve installed using the package manager? We don\u2019t want to have to add a ton of jar files to our classpath every time we wan\u2019t to run a particular algorithm. Fortunately, we don\u2019t have to. Weka has a mechanism to load installed packages dynamically at run time. This means that newly installed packages are available in Weka's GUIs immediately.  What about running algorithms from packages on the command line I hear you ask? We can run a named algorithm by using the Run command:   java weka.Run  If no arguments are supplied, then Run outputs the following usage information:   Usage:\n     weka.Run [-no-scan] [-no-load] <scheme name [scheme options]>  The Run command supports sub-string matching, so you can run a classifier (such as J48) like so:   java weka.Run J48  When there are multiple matches on a supplied scheme name you will be presented with a list. For example:   java weka.Run NaiveBayes\n\n Select a scheme to run, or <return> to exit:\n     1) weka.classifiers.bayes.ComplementNaiveBayes\n     2) weka.classifiers.bayes.NaiveBayes\n     3) weka.classifiers.bayes.NaiveBayesMultinomial\n     4) weka.classifiers.bayes.NaiveBayesMultinomialUpdateable\n     5) weka.classifiers.bayes.NaiveBayesSimple\n     6) weka.classifiers.bayes.NaiveBayesUpdateable\n\n Enter a number >  You can turn off the scanning of packages and sub-string matching by providing the  -no-scan  option. This is useful when using the  Run  command in a script. In this case, you need to specify the fully qualified name of the algorithm to use. E.g.   java weka.Run -no-scan weka.classifiers.bayes.NaiveBayes  To reduce startup time you can also turn off the dynamic loading of installed packages by specifying the  -no-load  option. In this case, you will need to explicitly include any packaged algorithms in your classpath if you plan to use them. E.g.  java -classpath ./weka.jar:$HOME/wekafiles/packages/DTNB/DTNB.jar rweka.Run -no-load -no-scan weka.classifiers.rules.DTNB",
            "title": "Running installed learning algorithms"
        },
        {
            "location": "/packages/manager/#gui-package-manager",
            "text": "As well as a command line client, there is also a graphical interface to Weka\u2019s package management system. This is available from the  Tools  menu in the  GUIChooser . All the functionality available in the command line client to the package management system is available in the GUI version, along with the ability to install and uninstall multiple packages in one hit.   The package manager\u2019s window is split horizontally into two parts: at the top is a list of packages and at the bottom is a mini browser that can be used to display information on the currently selected package.  The package list shows the name of a package, its category, the currently installed version (if installed), the latest version available via the repository and whether the package has been loaded or not. This list may be sorted by either package name or category by clicking on the appropriate column header. A second click on the same header reverses the sort order. Three radio buttons in the upper left of the window can be used to filter what is displayed in the list. All packages (default), all available packages (i.e. those not yet installed) or only installed packages can be displayed.  If multiple versions of a package are available, they can be accessed by clicking on an entry in the \u201cRepository version\u201d column:",
            "title": "GUI package manager"
        },
        {
            "location": "/packages/manager/#installing-and-removing-packages",
            "text": "At the very top of the window are three buttons. On the left-hand side is a button that can be used to refresh the cached copy of the package repository meta data. The first time that the package manager (GUI or command line) is used there will be a short delay as the initial cache is established.   NOTE: Weka (3.7.2) will not automatically check for new information at the central repository, so it is a good idea to refresh the local cache regularly. From Weka 3.7.3 the package manager will notify you if there are new packages available at the central repository .  The two buttons at the top right are used to install and remove packages repspectively. Multiple packages may be installed/removed by using a shift-left-click combination to select a range and/or by using a command-left-click combination to add to the selection. Underneath the install and uninstall but- tons is a checkbox that can be enabled to ignore any dependencies required by selected packages and any conflicts that may occur. Installing packages while this checkbox is selected will '''not''' install required dependencies.  Some packages may have additional information on how to complete the installation or special instructions that gets displayed when the package is installed:   Usually it is not necessary to restart Weka after packages have been installed\u2014the changes should be available immediately. An exception is when upgrading a package that is already installed. If in doubt, restart Weka.",
            "title": "Installing and removing packages"
        },
        {
            "location": "/packages/manager/#unofficial-packages",
            "text": "The package list shows those packages that have their meta data stored in Weka\u2019s central meta data repository. These packages are \u201cofficial\u201d Weka packages and the Weka team as verified that they appear to provide what is advertised (and do not contain malicious code or malware).  It is also possible to install an  unofficial  package that has not gone through the process of become official. Unofficial packages might be provided, for exam- ple, by researchers who want to make experimental algorithms quickly available to the community for feedback. Unofficial packages can be installed by clicking the \u201cFile/url\u201d button on the top-right of the package manager window. This will bring up an \u201cUnnoficial package install\u201d dialog where the user can browse their file system for a package zip file or directly enter an URL to the package zip file. Note that no dependency checking is done for unofficial packages.",
            "title": "Unofficial packages"
        },
        {
            "location": "/packages/manager/#using-a-http-proxy",
            "text": "Both the GUI and command line package managers can operate via a http proxy. To do so, start Weka from the command line and supply property values for the proxy host and port:   java -Dhttp.proxyHost=some.proxy.somewhere.net -Dhttp.proxyPort=port weka.gui.GUIChooser  If your proxy requires authentication, then Weka will present a GUI dialog where you can enter your username and password. If you are running on a headless environment, then two more (non-standard) properties can be supplied:   -Dhttp.proxyUser=some_user_name -Dhttp.proxyPassword=some_password",
            "title": "Using a HTTP proxy"
        },
        {
            "location": "/packages/manager/#using-an-alternative-central-package-meta-data-repository",
            "text": "By default, both the command-line and GUI package managers use the central package meta data repository hosted on Sourceforge. In the unlikely event that this site is unavailable for one reason or another, it is possible to point the package management system at an alternative repository. This mechanism allows a temporary backup of the official repostory to be accessed, local mirrors to be established and alternative repositories to be set up for use etc.  An alternative repository can be specified by setting a Java property:  weka.core.wekaPackageRepositoryURL=http://some.mirror.somewhere  This can either be set when starting Weka from the command line with the  -D  flag, or it can be placed into a file called \u201cPackageRepository.props\u201d in  $WEKA_HOME/props . The default value of  WEKA_HOME  is  user.home/wekafiles , where  user.home  is the user\u2019s home directory. More information on how and where Weka stores configuration information is given in the [[How are packages structured for the package management system?|how are package structured for the package management system?]] article.",
            "title": "Using an alternative central package meta data repository"
        },
        {
            "location": "/packages/manager/#package-manager-property-file",
            "text": "As mentioned in the previous section, an alternative package meta data repository can be specified by placing an entry in the PackageRepository.props file in  $WEKA_HOME/props . From Weka 3.7.8 (and snapshot builds after 24 September 2012), the package manager also looks for properties placed in  $WEKA_HOME/props/PackageManager.props . The current set of properties that can be set are:  weka.core.wekaPackageRepositoryURL=http://some.mirror.somewhere\nweka.packageManager.offline=[true | false]\nweka.packageManager.loadPackages=[true | false]\nweka.pluginManager.disable=com.funky.FunkyExplorerPluginTab  The default for offline mode (if unspecified) is  false  and for loadPackages is  true . The weka.pluginManager.disable property can be used to specify a comma-separated list of fully qualified class names to \"disable\" in the GUI. This can be used to make problematic components unavailable in the GUI without having to prevent the entire package that contains them from being loaded. E.g. \"funkyPackage\" might provide several classifiers and a special Explorer plugin tab for visualization. Suppose, for example, that the plugin Explorer tab has issues with certain data sets and causes annoying exceptions to be generated (or perhaps in the worst cases crashes the Explorer!). In this case we might want to use the classifiers provided by the package and just disable the Explorer plugin. Listing the fully qualified name of the Explorer plugin as a member of the comma-separated list associated with the weka.pluginManager.disable property will achieve this.",
            "title": "Package manager property file"
        },
        {
            "location": "/faqs/package_manager_doesnt_start/",
            "text": "The most likely reason for this is that your computer does not have direct access to the Internet and Java needs to be told to use a proxy server to access the web. The best way to achieve this is to configure an environment variable that provides the proxy details, e.g.,\n\n\n_JAVA_OPTIONS\n\n\n\n\nwhich is read by Oracle Java virtual machines. There is more information on this variable \nhere\n. Information on how to set environment variables in Windows is \nhere\n. For Mac users, there is a nice program to set environment variables available \nhere\n.\n\n\nSet the value of this variable to\n\n\n-Dhttp.proxyHost=some.proxy.somewhere.net -Dhttp.proxyPort=port\n\n\n\n\nwhere \nsome.proxy.somewhere.net\n needs to be replaced by the name of your proxy server and \nport\n needs to be replaced by the appropriate port number on the proxy server. Your IT department should be able to give you these details.\n\n\nThis should allow the package manager to connect to the website that hosts the package meta-information. However, if the package manager still cannot connect to the Internet, you can also force it to run in offline mode, by setting the above environment variable to\n\n\n-Dweka.packageManager.offline=true\n\n\n\n\nThen, you can download package .zip files manually via your web browser, by navigating to\n\n\nhttp://weka.sourceforge.net/packageMetaData/\n\n\nclicking on the link for the package you want to install, then clicking on \nLatest\n, and finally clicking on the URL given next to \nPackageURL\n.\n\n\nOnce you have downloaded the package .zip file, open the WEKA package manager, and click on the \nFile/URL\n button in the top-right corner of the package manager window (in the \nUnofficial\n panel). Then navigate to your package .zip file and install it.\n\n\nIf you are running Weka in \noffline\n mode, and the packages you are installing have some dependencies on one another, then there can still be some problems due to Weka not being able to verify the dependencies by checking against the central repository. This is usually a problem in the case where Weka has never been able to connect to the internet and thus has not downloaded and established a cache of the central package metadata repository. Fortunately there is a simple work-around to this, as long as you can access the internet via a web browser:\n\n\n\n\nUsing your web browser, download \nhttp://weka.sourceforge.net/packageMetaData/repo.zip\n\n\nIf it doesn't already exist, create the directory \n~/wekafiles/repCache\n\n\nCopy the downloaded \nrepo.zip\n into \n~/wekafiles/repCache\n and unzip it there\n\n\nStart Weka (use the \nweka.packageManager.offline=true\n property to speed up the startup process; see [http://weka.wikispaces.com/How+do+I+use+the+package+manager%3F#Package%20manager%20property%20file] for info)",
            "title": " What do I do if the package manager does not start?"
        },
        {
            "location": "/faqs/visualization/",
            "text": "Access to \nvisualization\n from the \nClassifier\n, \nCluster\n and \nAttribute Selection\n panel is available from a popup menu. Click the right mouse button over an entry in the Result list to bring up the menu. You will be presented with options for viewing or saving the text output and, depending on the scheme, further options for visualizing errors, clusters, trees etc.",
            "title": " Where do I find visualization of classifiers, etc.?"
        },
        {
            "location": "/faqs/how_can_i_track_instances_in_weka/",
            "text": "WEKA doesn't support internal IDs for instances, one has to use ID attributes. See \nHow do I use ID attributes?",
            "title": " How can I track instances in WEKA?"
        },
        {
            "location": "/faqs/how_do_i_use_id_attributes/",
            "text": "See the \nInstance ID\n article for more information on how to use attribute IDs in WEKA.",
            "title": " How do I use ID attributes?"
        },
        {
            "location": "/faqs/how_do_i_connect_to_a_database/",
            "text": "With a bit of effort you can easily access databases via \nJDBC\n. You need the following:\n\n\n\n\nJDBC driver for the database you want to access in your CLASSPATH.\n\n\nA customized \nDatabaseUtils.props\n file. The following example files are located in the \nweka/experiment\n directory of the \nweka.jar\n archive:\n\n\nHSQLDB - \nDatabaseUtils.props.hsql\n (>= 3.4.1/3.5.0)\n\n\nMS SQL Server 2000 - \nDatabaseUtils.props.mssqlserver\n (>= 3.4.9/3.5.4)\n\n\nMS SQL Server 2005 Express Edition - \nDatabaseUtils.props.mssqlserver2005\n (> 3.4.10/3.5.5)\n\n\nMySQL - \nDatabaseUtils.props.mysql\n (>= 3.4.9/3.5.4)\n\n\nODBC - \nDatabaseUtils.props.odbc\n (>= 3.4.9/3.5.4)\n\n\nOracle - \nDatabaseUtils.props.oracle\n (>= 3.4.9/3.5.4)\n\n\nPostgreSQL - \nDatabaseUtils.props.postgresql\n (>= 3.4.9/3.5.4)\n\n\nSqlite 3.x - \nDatabaseUtils.props.sqlite3\n (> 3.4.12, > 3.5.7)\n\n\n\n\n\n\n\n\nFor more details see the following articles:\n\n\n\n\nDatabases\n\n\nDatabaseUtils.props\n\n\nWindows databases\n (covers access via ODBC)\n\n\n\n\nThe following FAQs could be of interest as well:\n\n\n\n\nCouldn't read from database: unknown data type\n\n\nTrying to add JDBC driver: ... - Error, not in CLASSPATH?",
            "title": " How do I connect to a database?"
        },
        {
            "location": "/faqs/how_do_i_use_weka_from_the_command_line/",
            "text": "Reading the \nPrimer\n article will help you understand the usage of the command line, as well as the \nHow to run WEKA schemes from commandline\n article.",
            "title": " How do I use WEKA from the command line?"
        },
        {
            "location": "/faqs/can_i_tune_the_parameters_of_a_classifier/",
            "text": "Yes, you can do that with one of the following meta-classifiers:\n\n\n\n\nweka.classifiers.meta.CVParameterSelection\n\n\nweka.classifiers.meta.GridSearch\n (only developer version)\n\n\nweka.classifiers.meta.AutoWEKAClassifier\n (via external package)\n\n\nweka.classifiers.meta.MultiSearch\n (via \nexternal package\n)\n\n\n\n\nSee the Javadoc of the respective classifier or the \nOptimizing parameters\n article for more information.",
            "title": " Can I tune the parameters of a classifier?"
        },
        {
            "location": "/faqs/how_do_i_generate_learning_curves/",
            "text": "You can generate learning curves using the \nAdvanced\n mode of the Experimenter. See the article \nLearning curves\n for more details.",
            "title": " How do I generate Learning curves?"
        },
        {
            "location": "/faqs/where_can_i_find_information_regarding_roc_curves/",
            "text": "Just check out the articles tagged with \nROC\n, which cover the subject of ROC curves and AUC. These articles cover GUI handling as well as how to create ROC curves from code.",
            "title": " Where can I find information regarding ROC curves?"
        },
        {
            "location": "/faqs/i_have_unbalanced_data_now_what/",
            "text": "You can either perform \ncost-sensitive classification\n or resample your data to get a more balanced class distribution (see \nsupervised resample\n filter).",
            "title": " I have unbalanced data - now what?"
        },
        {
            "location": "/faqs/can_i_run_an_experiment_using_clusterers_in_the_experimenter/",
            "text": "Yes, see the article \nRunning an Experiment Using Clusterers\n.",
            "title": " Can I run an experiment using clusterers in the Experimenter?"
        },
        {
            "location": "/faqs/how_can_i_use_transactional_data_in_weka/",
            "text": "Transactional data is often stored in databases by having a table with the transaction ID as the primary key. Individual items or elements of a given transaction may be split up over multiple rows in the table (each with the same ID). Data in this format needs to be converted to one row per transaction before it can be used to learn classifiers, association rules, clusterers etc. in WEKA. From WEKA 3.7.2 there is a package called \ndenormalize\n that contains a filter that can perform this kind of \"flattening\" process. The filter requires \n1) that the data contains an ID field that uniquely identifies each separate transaction\n, and \n2) the data is already sorted in order of this ID field\n. Here is an example scenario taken from the WEKA mailing list:\n\n\nHi,\n   I have data spanning multiple rows for an instance, such as below (User 1 span across multiple rows, User 2 as well).  Is it possible to use WEKA to cluster this dataset?  If not, any suggestion on how I should organize the data so that I can use WEKA to cluster this data? \n User  ItemID  Sequence TimeSpent\n 1     1       1        5\n 1     2       2        1\n 1     5       3        8\n 1     6       4        12\n 1     8       5        2\n\n 2     1       1        7\n 2     2       2        3\n 2     3       3        3\n 2     4       4        2\n 2     5       5        7\n\n\n\n\nIn WEKA 3.7.2 there is a package called \ndenormalize\n that contains a filter for flattening transactional data. The first thing you'd have to do to your example above is to convert it into an ARFF file:\n\n\n@relation test\n@attribute User numeric\n@attribute ItemID numeric\n@attribute Sequence numeric\n@attribute TimeSpent numeric\n@data\n1, 1, 1, 5\n1, 2, 2, 1\n1, 5, 3, 8\n1, 6, 4, 12\n1, 8, 5, 2\n2, 1, 1, 7\n2, 2, 2, 3\n2, 3, 3, 3\n2, 4, 4, 2\n2, 5, 5, 7\n\n\n\n\nNext, you can run the \nNumericToNominal\n filter to convert the attributes that need to be coded as nominal (the User attribute is an ID and can stay either as numeric or nominal). Here I've converted all attributes except the ID to nominal:\n\n\njava weka.filters.unsupervised.attribute.NumericToNominal -R 2-last -i test.arff > test2.arff\n\n\nThis results in:\n\n\n@relation test-weka.filters.unsupervised.attribute.NumericToNominal-R2-last\n\n@attribute User numeric\n@attribute ItemID {1,2,3,4,5,6,8}\n@attribute Sequence {1,2,3,4,5}\n@attribute TimeSpent {1,2,3,5,7,8,12}\n\n@data\n\n1,1,1,5\n1,2,2,1\n1,5,3,8\n1,6,4,12\n1,8,5,2\n2,1,1,7\n2,2,2,3\n2,3,3,3\n2,4,4,2\n2,5,5,7\n\n\n\n\nNow, assuming that the denormalize package is installed, and (\nIMPORTANT\n) that the data is already sorted in order of the ID attribute (\"User\" in this case):\n\n\njava weka.Run Denormalize -G first -i test2.arff > final.arff\n\n\nThis results in:\n\n\n@attribute User numeric\n@attribute ItemID_1 {f,t}\n@attribute ItemID_2 {f,t}\n@attribute ItemID_3 {f,t}\n@attribute ItemID_4 {f,t}\n@attribute ItemID_5 {f,t}\n@attribute ItemID_6 {f,t}\n@attribute ItemID_8 {f,t}\n@attribute Sequence_1 {f,t}\n@attribute Sequence_2 {f,t}\n@attribute Sequence_3 {f,t}\n@attribute Sequence_4 {f,t}\n@attribute Sequence_5 {f,t}\n@attribute TimeSpent_1 {f,t}\n@attribute TimeSpent_2 {f,t}\n@attribute TimeSpent_3 {f,t}\n@attribute TimeSpent_5 {f,t}\n@attribute TimeSpent_7 {f,t}\n@attribute TimeSpent_8 {f,t}\n@attribute TimeSpent_12 {f,t}\n\n@data\n\n1,t,t,f,f,t,t,t,t,t,t,t,t,t,t,f,t,f,t,t\n2,t,t,t,t,t,f,f,t,t,t,t,t,f,t,t,f,t,f,f\n\n\n\n\nNote, that for clustering/association rules you'd want to first remove the User ID attribute.\n\n\nI've shown this as an example using the command-line interface. It can all be done from the Explorer as well of course. The Denormalize filter has options for aggregating any numeric attributes (not the ID) as well, so if you left (for example) the TimeSpent attribute as numeric, rather than converting it to nominal using \nNumericToNominal\n, then \nDenormalize\n can aggregate it (sum, average, max, min).",
            "title": " How can I use transactional data in Weka?"
        },
        {
            "location": "/faqs/how_can_i_use_weka_with_matlab_or_octave/",
            "text": "Matlab\n and \nOctave\n allow you to interface with Java applications, which allows you to use Weka from within these applications.\n\n\nSee the following presentation, section \nOctave\n (\nOctave\n is fairly compatible with Matlab), on how to use the Java integration: \n\n\n\n\nWEKA Ecosystem",
            "title": " How can I use Weka with Matlab or Octave?"
        },
        {
            "location": "/faqs/can_i_change_the_colors_background_axes_etc_of_the_plots_in_weka/",
            "text": "Sure, this information is stored in the Visualize.props properties file:\n\n\n\n\nweka.gui.visualize.Plot2D.axisColour\n defines the color of the axes\n\n\nweka.gui.visualize.Plot2D.backgroundColour\n sets the background color\n\n\n\n\nFor more information see the articles about \nproperties file\n (especially the section \nPrecedence\n will tell you where to place the \n.props\n file.) and \nVisualize.props\n itself.",
            "title": " Can I change the colors (background, axes, etc.) of the plots in WEKA?"
        },
        {
            "location": "/faqs/how_do_i_add_a_new_classifier_filter_kernel_etc/",
            "text": "As of WEKA 3.4.4, all the derived classes of superclasses that can be edited in the GenericObjectEditor, like subclasses of \nweka.classifiers.Classifier\n for instance, can be determined dynamically at runtime. Read \nhere\n for more information.\n\n\nNote:\n WEKA 3.5.8 and 3.6.0 turned the automatic discovery \noff\n by default. Starting with 3.6.1 and 3.7.0 it is turned \non\n again.",
            "title": " How do I add a new classifier, filter, kernel, etc?"
        },
        {
            "location": "/faqs/how_do_i_use_libsvm_in_weka/",
            "text": "If you run the classifier \nweka.classifiers.functions.LibSVM\n and get the \nlibsvm classes not in CLASSPATH!\n error message, you are missing the libsvm jar archive in your current classpath. The \nLibSVM\n classifier is only a wrapper and doesn't need the libsvm classes to compile (uses Reflection). Check out the \nLibSVM\n article for details about how to use this classifier.",
            "title": " How do I use libsvm in WEKA?"
        },
        {
            "location": "/faqs/the_snowball_stemmers_dont_work_what_am_i_doing_wrong/",
            "text": "When you're trying to use the Snowball stemmers in the StringToWordVector nothing happens and you get the message \nStemmer 'porter' unknown!\n in the console. If this happens, you don't have the snowball classes in your classpath. Check out the article about the \nStemmers\n for how to add the snowball stemmers to WEKA.",
            "title": " The snowball stemmers dont work, what am I doing wrong?"
        },
        {
            "location": "/faqs/where_can_i_get_wekas_source_code/",
            "text": "Every WEKA release comes with a jar archive (this is just a simple ZIP archive) that contains the complete sources. It is called \nweka-src.jar\n. Alternatively, you can get WEKA's source code also from \nSubversion\n.",
            "title": " Where can I get WEKAs source code?"
        },
        {
            "location": "/faqs/how_do_i_compile_weka/",
            "text": "You can compile the source code simply with any (Sun-compliant) java compiler, or use ant, or an IDE. Check out the article about \nCompiling WEKA\n, which contains links to further articles, covering topics about \nant\n and \nIDEs\n (e.g., \nEclipse\n or \nNetBeans\n).",
            "title": " How do I compile WEKA?"
        },
        {
            "location": "/faqs/what_is_subversion_and_what_do_i_need_to_do_to_access_it/",
            "text": "Subversion\n is the \nversion control system\n that we use nowadays for WEKA's source code. See the \nSubversion\n article from more information of how to access the repository and retrieve the source code from there.",
            "title": " What is Subversion and what do I need to do to access it?"
        },
        {
            "location": "/faqs/how_do_i_use_wekas_classes_in_my_own_code/",
            "text": "It's not that hard to use WEKA classes in your own code, the following articles give a good overview of how to do that:\n\n\n\n\nUse WEKA in your Java code\n\n\nProgrammatic Use\n\n\nIn general, the articles tagged as \"\nsource code\n\".\n\n\n\n\nFurther resources:\n\n\n\n\nCheck out the chapter \nUsing the API\n in the Weka manual (\nsnapshots\n later than 09/08/2009 and releases >3.6.1 and >3.7.0).\n\n\nThe \nWeka Examples\n collection is a an \nANT\n project that is available through \nsnapshots\n and releases later than 09/08/2009, containing a lot of example classes.\n\n\n\n\nNote:\n WEKA is open-source software under the \nGNU General Public License\n, which means that your code has to be licensed under the GPL as well.",
            "title": " How do I use WEKAs classes in my own code?"
        },
        {
            "location": "/faqs/how_do_i_write_a_new_classifier_or_filter/",
            "text": "Basically, a classifier needs to be derived from \nweka.classifiers.Classifier\n and a filter from \nweka.filters.Filter\n, but this is only part of the story. The following articles cover the development of new schemes in greater detail:\n\n\n\n\nWriting your own Classifier\n\n\nWriting your own Filter\n\n\n\n\nIf your scheme is outside the usual WEKA packages, you need to make WEKA aware of this package in order to be able to use it in the GUI as well. See \nHow do I add a new classifier, filter, kernel, etc?\n for more information about this.\n\n\nNote:\n This is also covered in chapter \nExtending WEKA\n of the WEKA manual in versions later than 3.6.1/3.7.0 or \nsnapshots\n of the stable-3.6/developer version later than 10/01/2010. Furthermore, this chapter also covers clusterers, attribute selection algorithms and associators.",
            "title": " How do I write a new classifier or filter?"
        },
        {
            "location": "/faqs/can_i_compile_weka_into_native_code/",
            "text": "Yes, you have the following options:\n\n\n\n\nExcelsior JET\n - a commercial tool for compiling Java into native code (Windows/Linux)\n\n\ngcj\n - a free, cross-platform tool for compiling Java into native code\n\n\n\n\nSee the article \nCompiling WEKA with gcj\n for more details.",
            "title": " Can I compile WEKA into native code?"
        },
        {
            "location": "/faqs/can_i_use_weka_from_c_sharp/",
            "text": "Yes, you can. Read the \nUse WEKA with the Microsoft .NET Framework\n article for more information. There is also a tutorial for IKVM \navailable\n.",
            "title": " Can I use WEKA from C#?"
        },
        {
            "location": "/faqs/can_i_use_weka_from_python/",
            "text": "Yes and no. If you're starting from scratch, you might want to consider \nJython\n, a rewrite of \nPython\n to seamlessly integrate with Java. The drawback is, that you can only use the libraries that Jython implements, not others like \nNumPy\n or \nSciPy\n. The article \nUsing WEKA from Jython\n explains how to use WEKA classes from Jython and how to implement a new classifier in Jython, with an example of ZeroR implemented in Jython.\n\n\nAn approach making use of the \njavax.script\n package (new in Java 6) is \nJepp\n, \nJava embedded Python\n. Jepp seems to have the same limitations as Jython, not being able to import Scipy or Numpy, but one can import pure Python libraries. The arcticle \nUsing WEKA via Jepp\n contains more information and examples.\n\n\nAnother solution, to access Java from within Python applications is \nJPype\n, but It's still not fully matured.\n\n\nFinally, you can use the \npython-weka-wrapper\n Python 2.7 library to access most of the non-GUI functionality of Weka (3.9.x):\n\n\n\n\npypi\n \n\n\ngithub\n \n\n\n\n\nFor Python3, use the \npython-weka-wrapper3\n Python library:\n\n\n\n\npypi\n \n\n\ngithub",
            "title": " Can I use WEKA from Python?"
        },
        {
            "location": "/faqs/can_i_use_weka_from_groovy/",
            "text": "Yes, you can. Read the \nUsing WEKA from Groovy\n article for more information. This article tells you how to setup the Groovy CLASSPATH, in order to make the WEKA classes available to Groovy, and also contains some sample code.",
            "title": " Can I use WEKA from Groovy?"
        },
        {
            "location": "/faqs/serialization_is_nice_but_what_about_generating_actual_java_code_from_weka_classes/",
            "text": "Some of WEKA's schemes support the generation of Java source code based on their internal state. See the \nGenerating source code from WEKA classes\n article for more details.",
            "title": " Serialization is nice, but what about generating actual Java code from WEKA classes?"
        },
        {
            "location": "/packages/",
            "text": "Weka 3.7.2 introduced support for packages, making it easy to extend Weka\nwithout having to recompile or patch the underlying Weka installation.\n\n\nHere are some pointers for using and developing packages:\n\n\n\n\nHow do I use the package manager?\n\n\nUnofficial packages\n\n\nHow are packages structured for the package management system?",
            "title": " How are packages structured for the package management system?"
        },
        {
            "location": "/faqs/pluggable_evaluation_metrics/",
            "text": "WEKA 3.7.8 (and nightly snapshots of the developer branch of WEKA from 15-11-2012) has a mechanism to allow new classification and regression evaluation metrics to be added as plugins. The new metrics will be output, along with WEKA's standard set of evaluation metrics, in the output generated on the command line, in the Explorer's Classify panel and by the Knowledge Flow's ClassifierPerformanceEvaluator component. Furthermore, new plugin metrics are also available for analysis in the Experimenter.\n\n\nPreviously, adding a new evaluation metric involved editing and recompiling the monolithic weka.classifiers.Evaluation class - a shudder-worthy undertaking at the best of times. With the new plugin mechanism it is easy to add a new metric and deploy it via the package management system. The \"Additional configuration files\" section of \nHow are packages structured for the package management system?\n details how to tell the PluginManager class about your new plugin evaluation metric.\n\n\nClasses and interfaces\n\n\nThe main base class for all new metrics is \nweka.classifiers.evaluation.AbstractEvaluationMetric\n. This class requires the following methods to be implemented by concrete sub classes:\n\n\n\n\nboolean appliesToNominalClass()\n - true if the stats computed by this metric apply to nominal class problems\n\n\nboolean appliesToNumericClass()\n - true if the stats computed by this metric apply to numeric class problems\n\n\nString getMetricName()\n - return the name of the metric\n\n\nString getMetricDescription()\n - return a short description of the metric\n\n\nList<String> getStatisticNames()\n - return a list of statistics that this metric computes (e.g. a \"correct\" metric might return both the number correctly classified and the percentage correct)\n\n\ndouble getStatistic(String statName)\n - get the computed value for the named statistic\n\n\n\n\nTo facilitate computing statistics, the main Evaluation object (who's class now lives in weka.classifiers.evaluation) will pass a reference to itself to all plugin metrics when it is first constructed. Therefore, a plugin metric has access to all the protected fields in the Evaluation class, and can use these when computing it's own statistic(s).\n\n\nBeyond extending \nAbstractEvaluationMetric\n, a plugin metric will also need to implement one of the following interfaces:\n\n\nweka.classifiers.evaluation.StandardEvaluationMetric\n\n\nInterface for a \"standard\" evaluation metric - i.e. one that would be part of the normal output in WEKA without having to turn on specific display options.\n\n\nIt defines the following methods\n\n\n\n\nString toSummaryString()\n - return a formatted string (suitable for displaying in the console or GUI output) that contains all the statistics that this metric computes\n\n\nvoid updateStatsForClassifier(double[] predictedDistribution, Instance instance)\n - updates the statistics about a classifiers performance for the current test instance. Gets called when the class is nominal. Implementers need only implement this method if it is not possible to compute their statistics from what is stored in the base Evaluation object.\n\n\nvoid updateStatsForPredictor(double predictedValue, Instance instance)\n -  updates the statistics about a predictors performance for the current test instance. Gets called when the class is numeric. Implementers need only implement this method if it is not possible to compute their statistics from what is stored in the base Evaluation object.\n\n\n\n\nweka.classifiers.evaluation.InformationTheoreticEvaluationMetric\n\n\nInterface for information theoretic evaluation metrics to implement. Allows the command line interface to display these metrics or not based on user-supplied options.\n\n\nIt defines the following methods\n\n\n\n\nString toSummaryString()\n - return a formatted string (suitable for displaying in the console or GUI output) that contains all the statistics that this metric computes\n\n\nvoid updateStatsForClassifier(double[] predictedDistribution, Instance instance)\n - updates the statistics about a classifiers performance for the current test instance. Gets called when the class is nominal. Implementers need only implement this method if it is not possible to compute their statistics from what is stored in the base Evaluation object.\n\n\nvoid updateStatsForPredictor(double predictedValue, Instance instance)\n -  updates the statistics about a predictors performance for the current test instance. Gets called when the class is numeric. Implementers need only implement this method if it is not possible to compute their statistics from what is stored in the base Evaluation object.\n\n\nvoid updateStatsForConditionalDensityEstimator(ConditionalDensityEstimator classifier, Instance classMissing, double classValue)\n -  updates stats for conditional density estimator based on current test instance. Gets called when the class is numeric and the classifier is a ConditionalDensityEstimators. Implementers need only implement this method if it is not possible to compute their statistics from what is stored in the base Evaluation object.\n\n\n\n\nweka.classifiers.evaluation.InformationRetrievalMetric\n\n\nAn interface for information retrieval evaluation metrics to implement. Allows the command line interface to display these metrics or not based on user-supplied options. These statistics will be displayed as new columns in the table of information retrieval statistics. As such, a toSummaryString() formatted representation is not required.\n\n\nIt defines the following methods\n\n\n\n\nvoid updateStatsForClassifier(double[] predictedDistribution, Instance instance)\n - updates the statistics about a classifiers performance for the current test instance. Gets called when the class is nominal. Implementers need only implement this method if it is not possible to compute their statistics from what is stored in the base Evaluation object.\n\n\ndouble getStatistic(String name, int classIndex)\n - get the value of the named statistic for the given class index. If the implementing class is extending AbstractEvaluationMetric then the implementation of getStatistic(String statName) should just call this method with a classIndex of 0.\n\n\ndouble getClassWeightedAverageStatistic(String statName)\n - get the weighted (by class) average for this statistic.\n\n\n\n\nweka.classifiers.evaluation.IntervalBasedEvaluationMetric\n\n\nPrimarily a marker interface for interval-based evaluation metrics to implement. Allows the command line interface to display these metrics or not based on user-supplied options.\n\n\nIt defines the following methods\n\n\n\n\nString toSummaryString()\n - return a formatted string (suitable for displaying in the console or GUI output) that contains all the statistics that this metric computes\n\n\nvoid updateStatsForIntervalEstimator(IntervalEstimator classifier, Instance instance, double classValue)\n - updates stats for interval estimator based on current test instance. Implementers need only implement this method if it is not possible to compute their statistics from what is stored in the base Evaluation object.",
            "title": " Pluggable evaluation metrics for classification/regression"
        },
        {
            "location": "/faqs/pluggable_evaluation_metrics/#classes-and-interfaces",
            "text": "The main base class for all new metrics is  weka.classifiers.evaluation.AbstractEvaluationMetric . This class requires the following methods to be implemented by concrete sub classes:   boolean appliesToNominalClass()  - true if the stats computed by this metric apply to nominal class problems  boolean appliesToNumericClass()  - true if the stats computed by this metric apply to numeric class problems  String getMetricName()  - return the name of the metric  String getMetricDescription()  - return a short description of the metric  List<String> getStatisticNames()  - return a list of statistics that this metric computes (e.g. a \"correct\" metric might return both the number correctly classified and the percentage correct)  double getStatistic(String statName)  - get the computed value for the named statistic   To facilitate computing statistics, the main Evaluation object (who's class now lives in weka.classifiers.evaluation) will pass a reference to itself to all plugin metrics when it is first constructed. Therefore, a plugin metric has access to all the protected fields in the Evaluation class, and can use these when computing it's own statistic(s).  Beyond extending  AbstractEvaluationMetric , a plugin metric will also need to implement one of the following interfaces:",
            "title": "Classes and interfaces"
        },
        {
            "location": "/faqs/pluggable_evaluation_metrics/#wekaclassifiersevaluationstandardevaluationmetric",
            "text": "Interface for a \"standard\" evaluation metric - i.e. one that would be part of the normal output in WEKA without having to turn on specific display options.  It defines the following methods   String toSummaryString()  - return a formatted string (suitable for displaying in the console or GUI output) that contains all the statistics that this metric computes  void updateStatsForClassifier(double[] predictedDistribution, Instance instance)  - updates the statistics about a classifiers performance for the current test instance. Gets called when the class is nominal. Implementers need only implement this method if it is not possible to compute their statistics from what is stored in the base Evaluation object.  void updateStatsForPredictor(double predictedValue, Instance instance)  -  updates the statistics about a predictors performance for the current test instance. Gets called when the class is numeric. Implementers need only implement this method if it is not possible to compute their statistics from what is stored in the base Evaluation object.",
            "title": "weka.classifiers.evaluation.StandardEvaluationMetric"
        },
        {
            "location": "/faqs/pluggable_evaluation_metrics/#wekaclassifiersevaluationinformationtheoreticevaluationmetric",
            "text": "Interface for information theoretic evaluation metrics to implement. Allows the command line interface to display these metrics or not based on user-supplied options.  It defines the following methods   String toSummaryString()  - return a formatted string (suitable for displaying in the console or GUI output) that contains all the statistics that this metric computes  void updateStatsForClassifier(double[] predictedDistribution, Instance instance)  - updates the statistics about a classifiers performance for the current test instance. Gets called when the class is nominal. Implementers need only implement this method if it is not possible to compute their statistics from what is stored in the base Evaluation object.  void updateStatsForPredictor(double predictedValue, Instance instance)  -  updates the statistics about a predictors performance for the current test instance. Gets called when the class is numeric. Implementers need only implement this method if it is not possible to compute their statistics from what is stored in the base Evaluation object.  void updateStatsForConditionalDensityEstimator(ConditionalDensityEstimator classifier, Instance classMissing, double classValue)  -  updates stats for conditional density estimator based on current test instance. Gets called when the class is numeric and the classifier is a ConditionalDensityEstimators. Implementers need only implement this method if it is not possible to compute their statistics from what is stored in the base Evaluation object.",
            "title": "weka.classifiers.evaluation.InformationTheoreticEvaluationMetric"
        },
        {
            "location": "/faqs/pluggable_evaluation_metrics/#wekaclassifiersevaluationinformationretrievalmetric",
            "text": "An interface for information retrieval evaluation metrics to implement. Allows the command line interface to display these metrics or not based on user-supplied options. These statistics will be displayed as new columns in the table of information retrieval statistics. As such, a toSummaryString() formatted representation is not required.  It defines the following methods   void updateStatsForClassifier(double[] predictedDistribution, Instance instance)  - updates the statistics about a classifiers performance for the current test instance. Gets called when the class is nominal. Implementers need only implement this method if it is not possible to compute their statistics from what is stored in the base Evaluation object.  double getStatistic(String name, int classIndex)  - get the value of the named statistic for the given class index. If the implementing class is extending AbstractEvaluationMetric then the implementation of getStatistic(String statName) should just call this method with a classIndex of 0.  double getClassWeightedAverageStatistic(String statName)  - get the weighted (by class) average for this statistic.",
            "title": "weka.classifiers.evaluation.InformationRetrievalMetric"
        },
        {
            "location": "/faqs/pluggable_evaluation_metrics/#wekaclassifiersevaluationintervalbasedevaluationmetric",
            "text": "Primarily a marker interface for interval-based evaluation metrics to implement. Allows the command line interface to display these metrics or not based on user-supplied options.  It defines the following methods   String toSummaryString()  - return a formatted string (suitable for displaying in the console or GUI output) that contains all the statistics that this metric computes  void updateStatsForIntervalEstimator(IntervalEstimator classifier, Instance instance, double classValue)  - updates stats for interval estimator based on current test instance. Implementers need only implement this method if it is not possible to compute their statistics from what is stored in the base Evaluation object.",
            "title": "weka.classifiers.evaluation.IntervalBasedEvaluationMetric"
        },
        {
            "location": "/faqs/contribution/",
            "text": "Information on how to contribute to WEKA can be found in the \nContributing a package\n section of the \nHow are packages structured for the package management system?\n article. The conditions for new classifiers (schemes in general) are that, firstly, they have to be published in the proceedings of a renowned conference (e.g., ICML) or as an article of respected journal (e.g., Machine Learning) and, secondly, that they outperform other standard schemes (e.g., J48/C4.5).\n\n\nBut please bear in mind, that we don't have a lot of man power, i.e., being the WEKA maintainer is \nNOT\n a full-time position.",
            "title": " How can I contribute to WEKA?"
        },
        {
            "location": "/faqs/how_do_i_modify_the_runwekabat_file/",
            "text": "Check out the \nInvocation\n section of the \nJava Virtual Machine\n article.",
            "title": " How do I modify the RunWeka.bat file?"
        },
        {
            "location": "/faqs/can_i_process_utf8_datasets_or_files/",
            "text": "Java can process UTF-8 files without any problems, it is just that Java uses a different encoding for displaying them under Windows (= \"Cp1252\"). If you change the file encoding to \"utf-8\" you'll be fine (discussed in \nthis\n WEKAlist post).\n\n\nIf you are running WEKA directly from the commandline, just add the following parameter to your commandline:\n\n\n\n\n-Dfile.encoding=utf-8\n\n\n\n\nIf you are starting WEKA from the Start menu, then edit the \nRunWEKA.ini\n file:\n\n\n\n\nIf a \nfileEncoding\n placeholder already exists, then just change the value from \"Cp1252\" to \"utf-8\" (without the quotes of course).\n\n\nIf there isn't a \nfileEncoding\n yet, just the \n-Dfile.encoding=utf-8\n parameter to all the \njava\n/\njavaw\n commands (see example \nRunWEKA.ini\n in \nthis\n post).",
            "title": " Can I process UTF-8 datasets or files?"
        },
        {
            "location": "/faqs/how_do_i_run_the_windows_weka_installer_in_silent_mode/",
            "text": "To run the Windows installer for WEKA without in \"silent\" mode (i.e. without any graphical prompts/dialogs):\n\n\n\n\nOpen a command prompt window\n\n\nNavigate to the directory where the installer executable resides\n\n\nType .\\weka-x-y-z.exe /S\n\n\n\n\nReplace x, y and z with the correct version numbers for your installer of course. Note that the /S is a capital S.",
            "title": " How do I run the Windows Weka installer in silent mode?"
        },
        {
            "location": "/faqs/weka_download_problems/",
            "text": "When you \ndownload WEKA\n, make sure that the resulting file size is the same as the WEKA webpage. Otherwise things won't work properly. Apparently some web browsers have trouble downloading WEKA.\nAlso note, that the WEKA homepage only links to the files that are hosted on \nsourceforge.net\n. This normally involves a redirect to a mirror from which you'll download the actual file.",
            "title": " I have Weka download problems - what is going wrong?"
        },
        {
            "location": "/faqs/arff_does_not_load/",
            "text": "One way to figure out why \nARFF\n files are failing to load is to give them to the \nweka.core.Instances\n class. In the SimpleCLI or in the terminal, type the following:\n\n\n java weka.core.Instances filename.arff\n\n\n\n\nwhere you substitute \nfilename\n for the actual name of your file. This should return an error if there is a problem reading the file, or show some statistics if the file is ok. The error message you get should give some indication of what is wrong.\n\n\nnominal value not declared in header, read Token[X], line Y\n\n\nIf you get this error message than you seem to have declared a nominal attribute in the ARFF header section, but WEKA came across a value (\n\"X\"\n) in the data (in line \nY\n) for this particular attribute that wasn't listed as possible value.\n\n\nAll\n nominal values that appear in the data must be declared in the header.",
            "title": " My ARFF file does not load - why?"
        },
        {
            "location": "/faqs/arff_does_not_load/#nominal-value-not-declared-in-header-read-tokenx-line-y",
            "text": "If you get this error message than you seem to have declared a nominal attribute in the ARFF header section, but WEKA came across a value ( \"X\" ) in the data (in line  Y ) for this particular attribute that wasn't listed as possible value.  All  nominal values that appear in the data must be declared in the header.",
            "title": "nominal value not declared in header, read Token[X], line Y"
        },
        {
            "location": "/faqs/arff_does_not_load/",
            "text": "One way to figure out why \nARFF\n files are failing to load is to give them to the \nweka.core.Instances\n class. In the SimpleCLI or in the terminal, type the following:\n\n\n java weka.core.Instances filename.arff\n\n\n\n\nwhere you substitute \nfilename\n for the actual name of your file. This should return an error if there is a problem reading the file, or show some statistics if the file is ok. The error message you get should give some indication of what is wrong.\n\n\nnominal value not declared in header, read Token[X], line Y\n\n\nIf you get this error message than you seem to have declared a nominal attribute in the ARFF header section, but WEKA came across a value (\n\"X\"\n) in the data (in line \nY\n) for this particular attribute that wasn't listed as possible value.\n\n\nAll\n nominal values that appear in the data must be declared in the header.",
            "title": " What does nominal value not declared in header, read Token[X], line Y mean?"
        },
        {
            "location": "/faqs/arff_does_not_load/#nominal-value-not-declared-in-header-read-tokenx-line-y",
            "text": "If you get this error message than you seem to have declared a nominal attribute in the ARFF header section, but WEKA came across a value ( \"X\" ) in the data (in line  Y ) for this particular attribute that wasn't listed as possible value.  All  nominal values that appear in the data must be declared in the header.",
            "title": "nominal value not declared in header, read Token[X], line Y"
        },
        {
            "location": "/faqs/OutOfMemoryException/",
            "text": "Most Java virtual machines only allocate a certain maximum amount of memory to run Java programs. Usually, this is much less than the amount of RAM in your computer. There is some information on default heap sizes in Oracle Java virtual machines for Java 8 \nhere\n. However, you can extend the memory available for the virtual machine by setting appropriate options. With Oracle's JDK, for example, you can go\n\n\njava -Xmx2g ...\n\n\n\n\nto set the maximum Java heap size to 2GB.\n\n\nA reliable way to set the maximum heap size for Oracle Java virtual machines (and overwrite any other settings that might be provided in startup scripts, etc.) is to use the _JAVA_OPTIONS environment variable to specify the \n-Xmx\n option. There is more information \nhere\n.",
            "title": " How do I get rid of this OutOfMemoryException?"
        },
        {
            "location": "/faqs/stack_overflow_error/",
            "text": "Try increasing the stack of your virtual machine. With Sun's JDK you can use this command to increase the stacksize:\n\n\n java -Xss512k ...\n\n\n\n\nto set the maximum Java stack size to 512KB. If still not sufficient, slowly increase it.\n\n\nFor Windows, see \nOutOfMemoryException\n for pointers on how to modify your setup.",
            "title": " How do I deal with a StackOverflowError?"
        },
        {
            "location": "/faqs/why_do_i_get_the_error_message_training_and_test_set_are_not_compatible/",
            "text": "One of WEKA's fundamental assumption is that the structure of the training and test sets are \nexactly\n the same. This does not only mean that you need the exact \nsame number\n of attributes, but also the exact \nsame type\n. In case of \nnominal\n attributes, you must ensure that the \nnumber\n of labels and the \norder\n of the labels are the same.\n\n\nThis may seem odd, as for \nmaking predictions\n with a trained classifier, you wouldn't need to include any class attribute information. This is true from a human perspective, but for speed reasons, WEKA doesn't perform any checks regarding the structure of dataset (no mapping of attribute names from training space to test space, also no mapping of labels). Internally, a single row in a dataset is represented as an array of doubles. In case of \nnumeric\n attributes, this doesn't pose a problem, but for other attribute types, like \nnominal\n ones, the doubles represent indexes in the list of available labels. A different order of the labels would result in different labels represented by the same index. Predictions cannot be trusted then.\n\n\nNow, if you want to quickly check where the problem is, a visual diff program is very helpful. There is a plethora of \napplications\n available. To name a few cross-platform open-source ones: \n\n\n\n\nkdiff3\n\n\nkompare\n\n\ndiffuse\n\n\n\n\nIf you used a filter for processing training and test set, then have a look at FAQ \nHow do I generate compatible train and test sets that get processed with a filter?",
            "title": " Why do I get the error message \"training and test set are not compatible\"?"
        },
        {
            "location": "/faqs/couldnt_read_from_database_unknown_data_type/",
            "text": "Since there is a plethora of different databases out there, each with their own data types, it is impossible to define all of them beforehand. WEKA therefore comes with setups for different databases that allow you to run experiments without any additional tuning. But if you want to read database from a different data source, then it can happen that you have to tell WEKA how to import these data types.\n\n\nHere is what to do:\n\n\n\n\nExtract the \nweka/experiment/DatabaseUtils.props\n file from either the \nweka.jar\n or \nweka-src.jar\n and place it in your home directory.\n\n\nFinally, check out the section \nMissing Datatypes\n in the \nDatabases\n article and add the missing data types accordingly.\n\n\n\n\nNotes:\n\n\n\n\njar\n files are just \nZIP files\n. Just use an archive manager that can handle ZIP files to open them. Windows users can use \n7-zip\n for instance.\n\n\nMore information on props files can be found in the \nProperties file\n article.\n\n\nIf you don't know where to find your \nhome directory\n, see FAQ \nWhere is my home directory located?\n.",
            "title": " Could not read from database: unknown data type"
        },
        {
            "location": "/faqs/trying_to_add_jdbc_driver_error_not_in_classpath/",
            "text": "WEKA's default setup for databases tries to locate some common \nJDBC\n driver classes (\"JDBC\" is the Java way of connecting to databases, like MySQL, HSQLDB, etc.) at startup time. By just adding these JDBC drivers to your \nCLASSPATH\n, WEKA will be automatically able to connect to these databases. If you are not trying to access a database, just forget about these messages. Otherwise, check out the \ndatabases\n article for more information (the database type that you are trying to connect to might not be listed by default).",
            "title": " Trying to add JDBC driver: ... - Error, not in CLASSPATH?"
        },
        {
            "location": "/faqs/i_cannot_process_large_datasets_any_ideas/",
            "text": "Since most schemes in WEKA need to have all of the data present in memory, large datasets can be a problem. The article \nClassifying large datasets\n tries to present some solutions. But make sure that you have already read how to deal with an \nOutOfMemoryException\n.",
            "title": " I cannot process large datasets - any ideas?"
        },
        {
            "location": "/command_redirection/",
            "text": "Console\n\n\nWith command redirection one can redirect \nstandard streams\n like \nstdin\n, \nstdout\n and \nstderr\n to user-specified locations. Quite often it is useful to redirect the output of a program to a text file.\n\n\n\n\n\n\nredirecting \nstdout\n to a file\n\n\nsomeProgram >/some/where/output.txt     (Linux/Unix Bash)\nsomeProgram >c:\\some\\where\\output.txt   (Windows command prompt)\n\n\n\n\n\n\nredirecting \nstderr\n to a file\n\n\nsomeProgram 2>/some/where/output.txt     (Linux/Unix Bash)\nsomeProgram 2>c:\\some\\where\\output.txt   (Windows command prompt)\n\n\n\n\n\n\nredirecting \nstdout\n and \nstderr\n to a file\n\n\nsomeProgram &>/some/where/output.txt         (Linux/Unix Bash)\nsomeProgram >c:\\some\\where\\output.txt 2>&1   (Windows command prompt)\n\n\n\n\n\n\nNote:\n under Weka quite often the output is printed to \nstderr\n, e.g., if one is using the \n-p 0\n option from the commandline to print the predicted values for a test file:\n\n\n java weka.classifiers.trees.J48 -t train.arff -T test.arff -p 0 2> j48.txt\n\n\n\n\nor if one already has a trained model:\n\n\n java weka.classifiers.trees.J48 -l j48.model -T test.arff -p 0 2> j48.txt\n\n\n\n\nSimpleCLI\n\n\nOne can perform a basic redirection also in the SimpleCLI, e.g.:\n\n\n java weka.classifiers.trees.J48 -t test.arff > j48.txt\n\n\n\n\nNote:\n the \n>\n must be preceded and followed by a \nspace\n, otherwise it is not recognized as redirection, but part of another parameter.\n\n\nLinks\n\n\n\n\n\n\nLinux\n\n\n\n\nCommand redirection under Bash\n\n\nI/O Redirection under Bash\n\n\nRedirection under Unix (WikiPedia)\n\n\n\n\n\n\n\n\nWindows\n\n\n\n\nCommand redirection under MS Windows\n\n\nCommand redirection under MS DOS",
            "title": " Command redirection"
        },
        {
            "location": "/command_redirection/#console",
            "text": "With command redirection one can redirect  standard streams  like  stdin ,  stdout  and  stderr  to user-specified locations. Quite often it is useful to redirect the output of a program to a text file.    redirecting  stdout  to a file  someProgram >/some/where/output.txt     (Linux/Unix Bash)\nsomeProgram >c:\\some\\where\\output.txt   (Windows command prompt)    redirecting  stderr  to a file  someProgram 2>/some/where/output.txt     (Linux/Unix Bash)\nsomeProgram 2>c:\\some\\where\\output.txt   (Windows command prompt)    redirecting  stdout  and  stderr  to a file  someProgram &>/some/where/output.txt         (Linux/Unix Bash)\nsomeProgram >c:\\some\\where\\output.txt 2>&1   (Windows command prompt)    Note:  under Weka quite often the output is printed to  stderr , e.g., if one is using the  -p 0  option from the commandline to print the predicted values for a test file:   java weka.classifiers.trees.J48 -t train.arff -T test.arff -p 0 2> j48.txt  or if one already has a trained model:   java weka.classifiers.trees.J48 -l j48.model -T test.arff -p 0 2> j48.txt",
            "title": "Console"
        },
        {
            "location": "/command_redirection/#simplecli",
            "text": "One can perform a basic redirection also in the SimpleCLI, e.g.:   java weka.classifiers.trees.J48 -t test.arff > j48.txt  Note:  the  >  must be preceded and followed by a  space , otherwise it is not recognized as redirection, but part of another parameter.",
            "title": "SimpleCLI"
        },
        {
            "location": "/command_redirection/#links",
            "text": "Linux   Command redirection under Bash  I/O Redirection under Bash  Redirection under Unix (WikiPedia)     Windows   Command redirection under MS Windows  Command redirection under MS DOS",
            "title": "Links"
        },
        {
            "location": "/auc/",
            "text": "AUC = the \nA\nrea \nU\nnder the ROC \nC\nurve. \n\n\nWeka uses the \nMann Whitney statistic\n to calculate the AUC via the \nweka.classifiers.evaluation.ThresholdCurve\n class.\n\n\nExplorer\n\n\nSee [[ROC curves]].\n\n\nKnowledgeFlow\n\n\nSee [[ROC curves]].\n\n\nCommandline\n\n\nClassifiers can output the AUC if the \n-i\n option is provided. The \n-i\n option provides detailed information per class. \n\n\nRunning the J48 classifier on the iris UCI [[Datasets|dataset]] with the following commandline:\n\n\n java [CLASSPATH|-classpath <your-classpath>] weka.classifiers.trees.J48 -t /some/where/iris.arff -i\n\n\n\n\nproduces this output:\n\n\n == Detailed Accuracy By Class ==\n\n TP Rate   FP Rate   Precision   Recall  F-Measure   ROC Area  Class\n   0.98      0          1         0.98      0.99       0.99     Iris-setosa\n   0.94      0.03       0.94      0.94      0.94       0.952    Iris-versicolor\n   0.96      0.03       0.941     0.96      0.95       0.961    Iris-virginica\n\n\n\n\nSee also\n\n\n\n\n[[ROC curves]]\n\n\nMann Whitney statistic\n on WikiPedia\n\n\n\n\nLinks\n\n\n\n\nUniversity of Nebraska Medical Center, Interpreting Diagnostic Tests\n\n\nweka.classifiers.evaluation.ThresholdCurve",
            "title": " Area under the curve"
        },
        {
            "location": "/auc/#explorer",
            "text": "See [[ROC curves]].",
            "title": "Explorer"
        },
        {
            "location": "/auc/#knowledgeflow",
            "text": "See [[ROC curves]].",
            "title": "KnowledgeFlow"
        },
        {
            "location": "/auc/#commandline",
            "text": "Classifiers can output the AUC if the  -i  option is provided. The  -i  option provides detailed information per class.   Running the J48 classifier on the iris UCI [[Datasets|dataset]] with the following commandline:   java [CLASSPATH|-classpath <your-classpath>] weka.classifiers.trees.J48 -t /some/where/iris.arff -i  produces this output:   == Detailed Accuracy By Class ==\n\n TP Rate   FP Rate   Precision   Recall  F-Measure   ROC Area  Class\n   0.98      0          1         0.98      0.99       0.99     Iris-setosa\n   0.94      0.03       0.94      0.94      0.94       0.952    Iris-versicolor\n   0.96      0.03       0.941     0.96      0.95       0.961    Iris-virginica",
            "title": "Commandline"
        },
        {
            "location": "/auc/#see-also",
            "text": "[[ROC curves]]  Mann Whitney statistic  on WikiPedia",
            "title": "See also"
        },
        {
            "location": "/auc/#links",
            "text": "University of Nebraska Medical Center, Interpreting Diagnostic Tests  weka.classifiers.evaluation.ThresholdCurve",
            "title": "Links"
        },
        {
            "location": "/batch_filtering/",
            "text": "Batch filtering\n is used if a second dataset, normally the test set, needs to be processed with the same statistics as the the first dataset, normally the training set.\n\n\nFor example, performing standardization with the \nStandardize\n filter on two datasets separately will most certainly create two differently standardized output files, since the mean and the standard deviation are based on the input data (and those will differ if the datasets are different). The same applies to the \nStringToWordVector\n: here the word dictionary will change, since word occurrences will differ in training and test set. The generated output will be two incompatible files.\n\n\nIn order to create compatible train and test set, batch filtering is necessary. Here, the first input/output pair (\n-i\n/\n-o\n) initializes the filter's statistics and the second input/output pair (\n-r\n/\n-s\n) gets processed according to those statistics. To enable batch filtering, one has to provide the additional parameter \n-b\n on the commandline.\n\n\nHere is an example Java call:\n\n\n java weka.filters.unsupervised.attribute.Standardize \\\n   -b \\\n   -i train.arff \\\n   -o train_std.arff \\\n   -r test.arff \\\n   -s test_std.arff\n\n\n\n\nNote:\n The commandline outlined above is for a Linux/Unix bash (the backslash tells the shell that the command isn't finished yet and continues on the next line). In case of Windows or the SimpleCLI, just remove those backslashes and put everything on one line.\n\n\nSee also\n\n\n\n\nSee section \nBatch filtering\n in the article \nUse Weka in your Java code\n, in case you need to perform batch filtering from within your own code",
            "title": " Batch filtering"
        },
        {
            "location": "/batch_filtering/#see-also",
            "text": "See section  Batch filtering  in the article  Use Weka in your Java code , in case you need to perform batch filtering from within your own code",
            "title": "See also"
        },
        {
            "location": "/roc_curves/",
            "text": "General\n\n\nWeka just varies the threshold on the class probability estimates in each case. What does that mean? In case of a classifier that does not return proper class probabilities (like SMO with the -M option, or IB1), you will end up with only two points in the curve. Using a classifier that returns proper distributions, like BayesNet, J48 or SMO with -M option for building logistic models, you will get nice curves.\n\n\nThe class used for calculating the ROC and also the \nAUC\n (= area under the curve) is \nweka.classifiers.evaluation.ThresholdCurve\n.\n\n\nCommandline\n\n\nYou can output the data for the ROC curves with the following options:\n\n\n -threshold-file <file>\n        The file to save the threshold data to.\n        The format is determined by the extensions, e.g., '.arff' for ARFF\n        format or '.csv' for CSV.\n -threshold-label <label>\n        The class label to determine the threshold data for\n        (default is the first label)\n\n\n\n\nHere's an example for using J48 on the UCI dataset \nanneal\n, generating the ROC curve file for label \nU\n from a 10-fold cross-validation:\n\n\n java weka.classifiers.trees.J48 -t /some/where/anneal.arff \\\n      -threshold-file anneal_roc_U.arff -threshold-label U\n\n\n\n\nExplorer\n\n\nGenerating\n\n\nThe Weka Explorer enables you to plot the ROC (\nReceiver operating characteristic\n) curve for a certain class label of dataset:\n\n\n\n\nrun a classifier on a dataset\n\n\nright-click in the result list on the result you want to display the curve for\n\n\nselect \nVisualize threshold curve\n and choose the class label you want the plot for\n\n\n\n\nNote:\n the \nAUC\n for this plot is also displayed, just above the actual plot.\n\n\nSaving\n\n\nYou can save the ROC curve in two ways:\n\n\n\n\nas an \nARFF\n file, containing the data points (can be displayed again)\n\n\nas an \nimage\n (using \nAlt+Shift+Left click\n to bring up a save dialog)\n\n\n\n\nLoading\n\n\nA previously saved ROC data file can be displayed in two ways:\n\n\n\n\n\n\nwithout the \nAUC\n - with the following command\n\n\njava [CLASSPATH|-classpath <your-classpath>] weka.gui.visualize.VisualizePanel <file>\n\n\n\n\n\n\nwith the \nAUC\n - needs \nthis\n source code\n\n\n\n\n\n\nKnowledgeFlow\n\n\nSee \nPlotting multiple ROC curves\n.\n\n\nSee also\n\n\n\n\nPlotting multiple ROC curves\n\n\nGenerating ROC curve\n\n\n\n\nLinks\n\n\n\n\nWikiPedia article on ROC curve\n\n\nweka.classifiers.evaluation.ThresholdCurve",
            "title": " ROC Curves"
        },
        {
            "location": "/roc_curves/#general",
            "text": "Weka just varies the threshold on the class probability estimates in each case. What does that mean? In case of a classifier that does not return proper class probabilities (like SMO with the -M option, or IB1), you will end up with only two points in the curve. Using a classifier that returns proper distributions, like BayesNet, J48 or SMO with -M option for building logistic models, you will get nice curves.  The class used for calculating the ROC and also the  AUC  (= area under the curve) is  weka.classifiers.evaluation.ThresholdCurve .",
            "title": "General"
        },
        {
            "location": "/roc_curves/#commandline",
            "text": "You can output the data for the ROC curves with the following options:   -threshold-file <file>\n        The file to save the threshold data to.\n        The format is determined by the extensions, e.g., '.arff' for ARFF\n        format or '.csv' for CSV.\n -threshold-label <label>\n        The class label to determine the threshold data for\n        (default is the first label)  Here's an example for using J48 on the UCI dataset  anneal , generating the ROC curve file for label  U  from a 10-fold cross-validation:   java weka.classifiers.trees.J48 -t /some/where/anneal.arff \\\n      -threshold-file anneal_roc_U.arff -threshold-label U",
            "title": "Commandline"
        },
        {
            "location": "/roc_curves/#explorer",
            "text": "",
            "title": "Explorer"
        },
        {
            "location": "/roc_curves/#generating",
            "text": "The Weka Explorer enables you to plot the ROC ( Receiver operating characteristic ) curve for a certain class label of dataset:   run a classifier on a dataset  right-click in the result list on the result you want to display the curve for  select  Visualize threshold curve  and choose the class label you want the plot for   Note:  the  AUC  for this plot is also displayed, just above the actual plot.",
            "title": "Generating"
        },
        {
            "location": "/roc_curves/#saving",
            "text": "You can save the ROC curve in two ways:   as an  ARFF  file, containing the data points (can be displayed again)  as an  image  (using  Alt+Shift+Left click  to bring up a save dialog)",
            "title": "Saving"
        },
        {
            "location": "/roc_curves/#loading",
            "text": "A previously saved ROC data file can be displayed in two ways:    without the  AUC  - with the following command  java [CLASSPATH|-classpath <your-classpath>] weka.gui.visualize.VisualizePanel <file>    with the  AUC  - needs  this  source code",
            "title": "Loading"
        },
        {
            "location": "/roc_curves/#knowledgeflow",
            "text": "See  Plotting multiple ROC curves .",
            "title": "KnowledgeFlow"
        },
        {
            "location": "/roc_curves/#see-also",
            "text": "Plotting multiple ROC curves  Generating ROC curve",
            "title": "See also"
        },
        {
            "location": "/roc_curves/#links",
            "text": "WikiPedia article on ROC curve  weka.classifiers.evaluation.ThresholdCurve",
            "title": "Links"
        },
        {
            "location": "/plotting_multiple_roc_curves/",
            "text": "KnowledgeFlow\n\n\nDescription\n\n\nComparing different classifiers on one dataset can also be done via \nROC curves\n, not just via Accuracy, Correlation coefficient etc. In the \nExplorer\n it is \nnot\n possible to do that for several classifiers, this is only possible in the \nKnowledgeFlow\n.\n\n\nThis is the basic setup (based on a Wekalist post):\n\n\n ArffLoader \n ---dataSet---> ClassAssigner \n ---dataSet---> ClassValuePicker              (the class label you want the plot for)\n ---dataSet---> CrossValidationFoldMaker \n ---trainingSet/testSet (i.e. BOTH connections)---> Classifier of your choice \n ---batchClassifier---> ClassifierPerformanceEvaluator \n ---thresholdData---> ModelPerformanceChart\n\n\n\n\n\n\n\n\nThis setup can be easily extended to host several classifiers, which illustrates the \nPlotting_multiple_roc.kfml\n example, containing \nJ48\n and \nRandomForest\n as classifiers.\n\n\nJava\n\n\nDescription\n\n\nThe \nVisualizeMultipleROC.java\n class lets you display several ROC curves in a single plot. The data it is using for display is from previously saved ROC curves. This example class is just a modified version of the \nVisualizeROC.java\n class, which displays only a single ROC curve (see \nVisualizing ROC curve\n article).\n\n\nSee also\n\n\n\n\nWikipedia article on ROC curve\n\n\nVisualizing ROC curve\n\n\nROC curves\n\n\n\n\nDownloads\n\n\n\n\nPlotting_multiple_roc.kfml\n - Example KnowledgeFlow layout file\n\n\nVisualizeMultipleROC.java\n (\nstable\n, \ndeveloper\n)",
            "title": " Plotting multiple ROC curves"
        },
        {
            "location": "/plotting_multiple_roc_curves/#knowledgeflow",
            "text": "",
            "title": "KnowledgeFlow"
        },
        {
            "location": "/plotting_multiple_roc_curves/#description",
            "text": "Comparing different classifiers on one dataset can also be done via  ROC curves , not just via Accuracy, Correlation coefficient etc. In the  Explorer  it is  not  possible to do that for several classifiers, this is only possible in the  KnowledgeFlow .  This is the basic setup (based on a Wekalist post):   ArffLoader \n ---dataSet---> ClassAssigner \n ---dataSet---> ClassValuePicker              (the class label you want the plot for)\n ---dataSet---> CrossValidationFoldMaker \n ---trainingSet/testSet (i.e. BOTH connections)---> Classifier of your choice \n ---batchClassifier---> ClassifierPerformanceEvaluator \n ---thresholdData---> ModelPerformanceChart    This setup can be easily extended to host several classifiers, which illustrates the  Plotting_multiple_roc.kfml  example, containing  J48  and  RandomForest  as classifiers.",
            "title": "Description"
        },
        {
            "location": "/plotting_multiple_roc_curves/#java",
            "text": "",
            "title": "Java"
        },
        {
            "location": "/plotting_multiple_roc_curves/#description_1",
            "text": "The  VisualizeMultipleROC.java  class lets you display several ROC curves in a single plot. The data it is using for display is from previously saved ROC curves. This example class is just a modified version of the  VisualizeROC.java  class, which displays only a single ROC curve (see  Visualizing ROC curve  article).",
            "title": "Description"
        },
        {
            "location": "/plotting_multiple_roc_curves/#see-also",
            "text": "Wikipedia article on ROC curve  Visualizing ROC curve  ROC curves",
            "title": "See also"
        },
        {
            "location": "/plotting_multiple_roc_curves/#downloads",
            "text": "Plotting_multiple_roc.kfml  - Example KnowledgeFlow layout file  VisualizeMultipleROC.java  ( stable ,  developer )",
            "title": "Downloads"
        },
        {
            "location": "/databases/",
            "text": "CLASSPATH\n\n\nSee the \nCLASSPATH\n article for how to set up your CLASSPATH environment variable, in order to make the JDBC driver available for Weka.\n\n\nConfiguration files\n\n\nThanks to JDBC it is easy to connect to Databases that provide a JDBC driver. Responsible for the setup is the following properties file, located in the \nweka.experiment\n package:\n\n\n DatabaseUtils.props\n\n\n\n\nYou can get this properties file from the \nweka.jar\n or \nweka-src.jar\n jar-archive, both part of a normal Weka release. If you open up one of those files, you'll find the properties file in the sub-folder \nweka/experiment\n.\n\n\nWeka comes with example files for a wide range of databases:\n\n\n\n\nDatabaseUtils.props.hsql\n - HSQLDB\n\n\nDatabaseUtils.props.msaccess\n - MS Access (see the \nWindows Databases\n article for more information)\n\n\nDatabaseUtils.props.mssqlserver\n - MS SQL Server 2000\n\n\nDatabaseUtils.props.mssqlserver2005\n - MS SQL Server 2005\n\n\nDatabaseUtils.props.mysql\n - MySQL\n\n\nDatabaseUtils.props.odbc\n - ODBC access via Sun's ODBC/JDBC bridge, e.g., for MS Sql Server (see the \nWindows Databases\n article for more information)\n\n\nDatabaseUtils.props.oracle\n - Oracle 10g\n\n\nDatabaseUtils.props.postgresql\n - PostgreSQL 7.4\n\n\nDatabaseUtils.props.sqlite3\n - sqlite 3.x\n\n\n\n\nThe easiest way is just to place the extracted properties file into your HOME directory. For more information on how property files are processed, check out [[Properties File|this]] article.\n\n\nNote:\n Weka \nonly\n looks for the \nDatabaseUtils.props\n file. If you take one of the example files listed above, you need to rename it first.\n\n\nSetup\n\n\nUnder normal circumstances you only have to edit the following two properties:\n\n\n\n\njdbcDriver\n\n\njdbcURL\n\n\n\n\nDriver\n\n\njdbcDriver\n is the classname of the JDBC driver, necessary to connect to your database, e.g.:\n\n\n\n\nHSQLDB - \norg.hsqldb.jdbcDriver\n\n\nMS SQL Server 2000 (Desktop Edition) - \ncom.microsoft.jdbc.sqlserver.SQLServerDriver\n\n\nMS SQL Server 2005 - \ncom.microsoft.sqlserver.jdbc.SQLServerDriver\n\n\nMySQL - \norg.gjt.mm.mysql.Driver\n (or \ncom.mysql.jdbc.Driver\n)\n\n\nODBC - part of Sun's JDKs/JREs, no external driver necessary - \nsun.jdbc.odbc.JdbcOdbcDriver\n\n\nOracle - \noracle.jdbc.driver.OracleDriver\n\n\nPostgreSQL - \norg.postgresql.Driver\n\n\nsqlite 3.x - \norg.sqlite.JDBC\n\n\n\n\nURL\n\n\njdbcURL\n specifies the JDBC URL pointing to your database (can be still changed in the Experimenter/Explorer), e.g. for the database \nMyDatabase\n on the server \nserver.my.domain\n:\n\n\n\n\nHSQLDB - \njdbc:hsqldb:hsql://server.my.domain/MyDatabase\n\n\n\n\nMS SQL Server 2000 (Desktop Edition) - \njdbc:microsoft:sqlserver://server.my.comain:1433\n\n\n\n\nNote: if you add \n;databasename=*db-name*\n you can connect to a different database than the default one, e.g., \nMyDatabase\n\n\n\n\n\n\n\n\nMS SQL Server 2005 - \njdbc:sqlserver://server.my.domain:1433\n\n\n\n\nMySQL - \njdbc:mysql://server.my.domain:3306/MyDatabase\n\n\nODBC - \njdbc:odbc:DSN_name (replace DSN_name with the DSN that you want to use)\n\n\n\n\nOracle (thin driver) - \njdbc:oracle:thin:@server.my.domain:1526:orcl\n\n\n\n\nNote: \n@machineName:port:SID\n\n\nfor the \nExpress Edition\n you can use: \njdbc:oracle:thin:@server.my.domain:1521:XE\n\n\n\n\n\n\n\n\nPostgreSQL - \njdbc:postgresql://server.my.domain:5432/MyDatabase\n\n\n\n\nYou can also specify user and password directly in the URL: \njdbc:postgresql://server.my.domain:5432/MyDatabase?user=<...>&password=<...>\n\n\nwhere you have to replace the \n<...>\n with the correct values\n\n\n\n\n\n\n\n\nsqlite 3.x - \njdbc:sqlite:/path/to/database.db (you can access only local files)\n\n\n\n\n\n\nMissing Datatypes\n\n\nSometimes (e.g. with MySQL) it can happen that a column type cannot be interpreted. In that case it is necessary to map the name of the column type to the Java type it should be interpreted as. E.g. the MySQL type \nTEXT\n is returned as \nBLOB\n from the JDBC driver and has to be mapped to \nString\n (\n0\n represents \nString\n - the mappings can be found in the comments of the properties file):\n\n\n BLOB=0\n\n\n\n\nThe article [[weka_experiment_DatabaseUtils.props|weka/experiment/DatabaseUtils.props]] contains more details on this topic.\n\n\nStored Procedures\n\n\nLet's say you're tired of typing the same query over and over again. A good way to shorten that, is to create a stored procedure.\n\n\nPostgreSQL 7.4.x\n\n\nThe following example creates a procedure called \nemplyoee_name\n that returns the names of all the employees in table \nemployee\n. Even though it doesn't make much sense to create a stored procedure for this query, nonetheless, it shows how to create and call stored procedures in PostgreSQL.\n\n\n\n\n\n\nCreate\n\n\nCREATE OR REPLACE FUNCTION public.employee_name()\n  RETURNS SETOF text AS 'select name from employee'\n  LANGUAGE 'sql' VOLATILE;\n\n\n\n\n\n\nSQL statement to call procedure\n\n\nSELECT * FROM employee_name()\n\n\n\n\n\n\nRetrieve data via \nInstanceQuery\n\n\njava weka.experiment.InstanceQuery -Q \"SELECT * FROM employee_name()\" -U <user> -P <password>\n\n\n\n\n\n\nTroubleshooting\n\n\n\n\nIn case you're experiencing problems connecting to your database, check out the \nmailing list\n. It is possible that somebody else encountered the same problem as you and you'll find a post containing the solution to your problem.\n\n\nSpecific [[MS SQL Server 2000 (Desktop Engine)#Troubleshooting|MS SQL Server 2000]] Troubleshooting\n\n\n\n\nMS SQL Server 2005: TCP/IP is not enabled for SQL Server, or the server or port number specified is incorrect.Verify that SQL Server is listening with TCP/IP on the specified server and port. This might be reported with an exception similar to: \"The login has failed. The TCP/IP connection to the host has failed.\" This indicates one of the following:\n\n\n\n\nSQL Server is installed but TCP/IP has not been installed as a network protocol for SQL Server by using the SQL Server Network Utility for SQL Server 2000, or the SQL Server Configuration Manager for SQL Server 2005\n\n\nTCP/IP is installed as a SQL Server protocol, but it is not listening on the port specified in the JDBC connection URL. The default port is 1433.\n\n\nThe port that is used by the server has not been opened in the firewall\n\n\n\n\n\n\n\n\nThe \nAdded driver: ...\n output on the commandline does not mean that the actual class was found, but only that Weka will \nattempt\n to load the class later on in order to establish a database connection.\n\n\n\n\n\n\nThe error message \nNo suitable driver\n can be caused by the following:\n\n\n\n\nThe JDBC driver you are attempting to load is not in the [[CLASSPATH]] (Note: using \n-jar\n in the java commandline \noverwrites\n the CLASSPATH environment variable!). Open the SimpleCLI, run the command \njava weka.core.SystemInfo\n and check whether the property \njava.class.path\n lists your database jar. If not correct your [[CLASSPATH]] or the Java call you start Weka with.\n\n\nThe JDBC driver class is misspelled in the \njdbcDriver\n property or you have multiple entries of \njdbcDriver\n ([[properties file]]s need \nunique\n keys!)\n\n\nThe \njdbcURL\n property has a spelling error and tries to use a non-existing protocol or you listed it multiple times, which doesn't work either (remember, [[properties file]]s need \nunique\n keys!)\n\n\n\n\n\n\n\n\nSee also\n\n\n\n\n[[weka_experiment_DatabaseUtils.props|weka/experiment/DatabaseUtils.props]]\n\n\n[[Properties File]]\n\n\nCLASSPATH\n\n\n\n\nLinks\n\n\n\n\n\n\nHSQLDB\n\n\n\n\nhomepage\n\n\n\n\n\n\n\n\nIBM Cloudscape\n\n\n\n\nhomepage\n\n\n\n\n\n\n\n\nMicrosoft SQL Server\n\n\n\n\nSQL Server 2000 (Desktop Engine)\n\n\nSQL Server 2000 JDBC Driver SP 3\n\n\nSQL Server 2005 JDBC Driver\n\n\n\n\n\n\n\n\nMySQL\n\n\n\n\nhomepage\n\n\nJDBC driver\n\n\n\n\n\n\n\n\nOracle \n\n\n\n\nhomepage\n\n\nJDBC driver\n\n\nJDBC FAQ\n\n\n\n\n\n\n\n\nPostgreSQL\n\n\n\n\nhomepage\n\n\nJDBC driver\n\n\n\n\n\n\n\n\nsqlite\n\n\n\n\nhomepage\n\n\nJDBC driver\n\n\n\n\n\n\n\n\nWeka Mailing list",
            "title": " Databases"
        },
        {
            "location": "/databases/#classpath",
            "text": "See the  CLASSPATH  article for how to set up your CLASSPATH environment variable, in order to make the JDBC driver available for Weka.",
            "title": "CLASSPATH"
        },
        {
            "location": "/databases/#configuration-files",
            "text": "Thanks to JDBC it is easy to connect to Databases that provide a JDBC driver. Responsible for the setup is the following properties file, located in the  weka.experiment  package:   DatabaseUtils.props  You can get this properties file from the  weka.jar  or  weka-src.jar  jar-archive, both part of a normal Weka release. If you open up one of those files, you'll find the properties file in the sub-folder  weka/experiment .  Weka comes with example files for a wide range of databases:   DatabaseUtils.props.hsql  - HSQLDB  DatabaseUtils.props.msaccess  - MS Access (see the  Windows Databases  article for more information)  DatabaseUtils.props.mssqlserver  - MS SQL Server 2000  DatabaseUtils.props.mssqlserver2005  - MS SQL Server 2005  DatabaseUtils.props.mysql  - MySQL  DatabaseUtils.props.odbc  - ODBC access via Sun's ODBC/JDBC bridge, e.g., for MS Sql Server (see the  Windows Databases  article for more information)  DatabaseUtils.props.oracle  - Oracle 10g  DatabaseUtils.props.postgresql  - PostgreSQL 7.4  DatabaseUtils.props.sqlite3  - sqlite 3.x   The easiest way is just to place the extracted properties file into your HOME directory. For more information on how property files are processed, check out [[Properties File|this]] article.  Note:  Weka  only  looks for the  DatabaseUtils.props  file. If you take one of the example files listed above, you need to rename it first.",
            "title": "Configuration files"
        },
        {
            "location": "/databases/#setup",
            "text": "Under normal circumstances you only have to edit the following two properties:   jdbcDriver  jdbcURL",
            "title": "Setup"
        },
        {
            "location": "/databases/#driver",
            "text": "jdbcDriver  is the classname of the JDBC driver, necessary to connect to your database, e.g.:   HSQLDB -  org.hsqldb.jdbcDriver  MS SQL Server 2000 (Desktop Edition) -  com.microsoft.jdbc.sqlserver.SQLServerDriver  MS SQL Server 2005 -  com.microsoft.sqlserver.jdbc.SQLServerDriver  MySQL -  org.gjt.mm.mysql.Driver  (or  com.mysql.jdbc.Driver )  ODBC - part of Sun's JDKs/JREs, no external driver necessary -  sun.jdbc.odbc.JdbcOdbcDriver  Oracle -  oracle.jdbc.driver.OracleDriver  PostgreSQL -  org.postgresql.Driver  sqlite 3.x -  org.sqlite.JDBC",
            "title": "Driver"
        },
        {
            "location": "/databases/#url",
            "text": "jdbcURL  specifies the JDBC URL pointing to your database (can be still changed in the Experimenter/Explorer), e.g. for the database  MyDatabase  on the server  server.my.domain :   HSQLDB -  jdbc:hsqldb:hsql://server.my.domain/MyDatabase   MS SQL Server 2000 (Desktop Edition) -  jdbc:microsoft:sqlserver://server.my.comain:1433   Note: if you add  ;databasename=*db-name*  you can connect to a different database than the default one, e.g.,  MyDatabase     MS SQL Server 2005 -  jdbc:sqlserver://server.my.domain:1433   MySQL -  jdbc:mysql://server.my.domain:3306/MyDatabase  ODBC -  jdbc:odbc:DSN_name (replace DSN_name with the DSN that you want to use)   Oracle (thin driver) -  jdbc:oracle:thin:@server.my.domain:1526:orcl   Note:  @machineName:port:SID  for the  Express Edition  you can use:  jdbc:oracle:thin:@server.my.domain:1521:XE     PostgreSQL -  jdbc:postgresql://server.my.domain:5432/MyDatabase   You can also specify user and password directly in the URL:  jdbc:postgresql://server.my.domain:5432/MyDatabase?user=<...>&password=<...>  where you have to replace the  <...>  with the correct values     sqlite 3.x -  jdbc:sqlite:/path/to/database.db (you can access only local files)",
            "title": "URL"
        },
        {
            "location": "/databases/#missing-datatypes",
            "text": "Sometimes (e.g. with MySQL) it can happen that a column type cannot be interpreted. In that case it is necessary to map the name of the column type to the Java type it should be interpreted as. E.g. the MySQL type  TEXT  is returned as  BLOB  from the JDBC driver and has to be mapped to  String  ( 0  represents  String  - the mappings can be found in the comments of the properties file):   BLOB=0  The article [[weka_experiment_DatabaseUtils.props|weka/experiment/DatabaseUtils.props]] contains more details on this topic.",
            "title": "Missing Datatypes"
        },
        {
            "location": "/databases/#stored-procedures",
            "text": "Let's say you're tired of typing the same query over and over again. A good way to shorten that, is to create a stored procedure.",
            "title": "Stored Procedures"
        },
        {
            "location": "/databases/#postgresql-74x",
            "text": "The following example creates a procedure called  emplyoee_name  that returns the names of all the employees in table  employee . Even though it doesn't make much sense to create a stored procedure for this query, nonetheless, it shows how to create and call stored procedures in PostgreSQL.    Create  CREATE OR REPLACE FUNCTION public.employee_name()\n  RETURNS SETOF text AS 'select name from employee'\n  LANGUAGE 'sql' VOLATILE;    SQL statement to call procedure  SELECT * FROM employee_name()    Retrieve data via  InstanceQuery  java weka.experiment.InstanceQuery -Q \"SELECT * FROM employee_name()\" -U <user> -P <password>",
            "title": "PostgreSQL 7.4.x"
        },
        {
            "location": "/databases/#troubleshooting",
            "text": "In case you're experiencing problems connecting to your database, check out the  mailing list . It is possible that somebody else encountered the same problem as you and you'll find a post containing the solution to your problem.  Specific [[MS SQL Server 2000 (Desktop Engine)#Troubleshooting|MS SQL Server 2000]] Troubleshooting   MS SQL Server 2005: TCP/IP is not enabled for SQL Server, or the server or port number specified is incorrect.Verify that SQL Server is listening with TCP/IP on the specified server and port. This might be reported with an exception similar to: \"The login has failed. The TCP/IP connection to the host has failed.\" This indicates one of the following:   SQL Server is installed but TCP/IP has not been installed as a network protocol for SQL Server by using the SQL Server Network Utility for SQL Server 2000, or the SQL Server Configuration Manager for SQL Server 2005  TCP/IP is installed as a SQL Server protocol, but it is not listening on the port specified in the JDBC connection URL. The default port is 1433.  The port that is used by the server has not been opened in the firewall     The  Added driver: ...  output on the commandline does not mean that the actual class was found, but only that Weka will  attempt  to load the class later on in order to establish a database connection.    The error message  No suitable driver  can be caused by the following:   The JDBC driver you are attempting to load is not in the [[CLASSPATH]] (Note: using  -jar  in the java commandline  overwrites  the CLASSPATH environment variable!). Open the SimpleCLI, run the command  java weka.core.SystemInfo  and check whether the property  java.class.path  lists your database jar. If not correct your [[CLASSPATH]] or the Java call you start Weka with.  The JDBC driver class is misspelled in the  jdbcDriver  property or you have multiple entries of  jdbcDriver  ([[properties file]]s need  unique  keys!)  The  jdbcURL  property has a spelling error and tries to use a non-existing protocol or you listed it multiple times, which doesn't work either (remember, [[properties file]]s need  unique  keys!)",
            "title": "Troubleshooting"
        },
        {
            "location": "/databases/#see-also",
            "text": "[[weka_experiment_DatabaseUtils.props|weka/experiment/DatabaseUtils.props]]  [[Properties File]]  CLASSPATH",
            "title": "See also"
        },
        {
            "location": "/databases/#links",
            "text": "HSQLDB   homepage     IBM Cloudscape   homepage     Microsoft SQL Server   SQL Server 2000 (Desktop Engine)  SQL Server 2000 JDBC Driver SP 3  SQL Server 2005 JDBC Driver     MySQL   homepage  JDBC driver     Oracle    homepage  JDBC driver  JDBC FAQ     PostgreSQL   homepage  JDBC driver     sqlite   homepage  JDBC driver     Weka Mailing list",
            "title": "Links"
        },
        {
            "location": "/converting_csv_to_arff/",
            "text": "For converting CSV (comma separated value) files into \nARFF\n files you need the following two \nconverters\n:\n\n\n\n\nCSVLoader\n for loading the CSV file into an \nInstances\n object\n\n\nArffSaver\n to save the \nInstances\n as an ARFF file\n\n\n\n\nIn the following you'll find some example code to show you how to use the \nconverters\n. The class takes 2 arguments:\n\n\n\n\nthe \ninput\n CSV file\n\n\nthe \noutput\n \nARFF\n file\n\n\n\n\nExample code:\n\n\nimport weka.core.Instances;\nimport weka.core.converters.ArffSaver;\nimport weka.core.converters.CSVLoader;\n\nimport java.io.File;\n\npublic class CSV2Arff {\n  /**\n   * takes 2 arguments:\n   * - CSV input file\n   * - ARFF output file\n   */\n  public static void main(String[] args) throws Exception {\n    if (args.length != 2) {\n      System.out.println(\"\\nUsage: CSV2Arff <input.csv> <output.arff>\\n\");\n      System.exit(1);\n    }\n\n    // load CSV\n    CSVLoader loader = new CSVLoader();\n    loader.setSource(new File(args[0]));\n    Instances data = loader.getDataSet();\n\n    // save ARFF\n    ArffSaver saver = new ArffSaver();\n    saver.setInstances(data);\n    saver.setFile(new File(args[1]));\n    saver.setDestination(new File(args[1]));\n    saver.writeBatch();\n  }\n}\n\n\n\n\nNote:\n with versions of Weka later than 3.5.3 the call of \nsaver.setDestination(new File(args[1]));\n is no longer necessary, it is automatically done in the \nsaver.setFile(new File(args[1]));\n method.\n\n\nSee also\n\n\nThe \nWeka Examples\n collection dedicates several example classes of loading from and saving to various file formats:\n\n\n\n\nbook\n\n\nstable-3-6\n\n\ndeveloper",
            "title": " Converting CSV to ARFF"
        },
        {
            "location": "/converting_csv_to_arff/#see-also",
            "text": "The  Weka Examples  collection dedicates several example classes of loading from and saving to various file formats:   book  stable-3-6  developer",
            "title": "See also"
        },
        {
            "location": "/use_weka_in_your_java_code/",
            "text": "The most common components you might want to use are\n\n\n\n\nInstances\n - your data\n\n\nFilter\n - for preprocessing the data\n\n\nClassifier/Clusterer\n - built on the processed data\n\n\nEvaluating\n - how good is the classifier/clusterer?\n\n\nAttribute selection\n - removing irrelevant attributes from your data\n\n\n\n\nThe following sections explain how to use them in your own code. A link to an \nexample class\n can be found at the end of this page, under the \nLinks\n section. The classifiers and filters always list their options in the Javadoc API (\nstable\n, \ndeveloper\n version) specification.\n\n\nA comprehensive source of information is the chapter \nUsing the API\n of the Weka manual.\n\n\nInstances\n\n\nDatasets\n\n\nThe \nDataSource\n class is not limited to ARFF files. It can also read CSV files and other formats (basically all file formats that Weka can import via its converters; it uses the file extension to determine the associated loader).\n\n\n import weka.core.converters.ConverterUtils.DataSource;\n ...\n DataSource source = new DataSource(\"/some/where/data.arff\");\n Instances data = source.getDataSet();\n // setting class attribute if the data format does not provide this information\n // For example, the XRFF format saves the class attribute information as well\n if (data.classIndex() == -1)\n   data.setClassIndex(data.numAttributes() - 1);\n\n\n\n\nDatabase\n\n\nReading from \nDatabases\n is slightly more complicated, but still very easy. First, you'll have to modify your \nDatabaseUtils.props\n file to reflect your database connection. Suppose you want to connect to a \nMySQL\n server that is running on the local machine on the default port \n3306\n. The MySQL JDBC driver is called \nConnector/J\n. (The driver class is \norg.gjt.mm.mysql.Driver\n.) The database where your target data resides is called \nsome_database\n. Since you're only reading, you can use the default user \nnobody\n without a password. Your props file must contain the following lines:\n\n\n jdbcDriver=org.gjt.mm.mysql.Driver\n jdbcURL=jdbc:mysql://localhost:3306/some_database\n\n\n\n\nSecondly, your Java code needs to look like this to load the data from the database:\n\n\n import weka.core.Instances;\n import weka.experiment.InstanceQuery;\n ...\n InstanceQuery query = new InstanceQuery();\n query.setUsername(\"nobody\");\n query.setPassword(\"\");\n query.setQuery(\"select * from whatsoever\");\n // You can declare that your data set is sparse\n // query.setSparseData(true);\n Instances data = query.retrieveInstances();\n\n\n\n\nNotes:\n\n\n Don't forget to add the JDBC driver to your \nCLASSPATH\n.\n\n For MS Access, you must use the JDBC-ODBC-bridge that is part of a JDK. The [[Windows databases]] article explains how to do this.\n* InstanceQuery automatically converts VARCHAR database columns to NOMINAL attributes, and long TEXT database columns to STRING attributes. So if you use InstanceQuery to do text mining against text that appears in a VARCHAR column, Weka will regard such text as nominal values. Thus it will fail to tokenize and mine that text. Use the \nNominalToString\n or \nStringToNominal\n filter (package \nweka.filters.unsupervised.attribute\n) to convert the attributes into the correct type.\n\n\nOption handling\n\n\nWeka schemes that implement the \nweka.core.OptionHandler\n interface, such as classifiers, clusterers, and filters, offer the following methods for setting and retrieving options:\n\n\n\n\nvoid setOptions(String[] options)\n\n\nString[] getOptions()\n\n\n\n\nThere are several ways of setting the options:\n\n\n\n\nManually creating a String array:\n\n\n\n\n   String[] options = new String[2];\n   options[0] = \"-R\";\n   options[1] = \"1\";\n\n\n\n\n\n\nUsing a single command-line string and using the \nsplitOptions\n method of the \nweka.core.Utils\n class to turn it into an array:\n\n\n\n\n    String[] options = weka.core.Utils.splitOptions(\"-R 1\");\n\n\n\n\n\n\nUsing the \nOptionsToCode.java\n class to automatically turn a command line into code. Especially handy if the command line contains nested classes that have their own options, such as kernels for SMO:\n\n\n\n\n  java OptionsToCode weka.classifiers.functions.SMO\n\n\n\n\nwill generate output like this:\n\n\n // create new instance of scheme\n weka.classifiers.functions.SMO scheme = new weka.classifiers.functions.SMO();\n // set options\n scheme.setOptions(weka.core.Utils.splitOptions(\"-C 1.0 -L 0.0010 -P 1.0E-12 -N 0 -V -1 -W 1 -K \\\"weka.classifiers.functions.supportVector.PolyKernel -C 250007 -E 1.0\\\"\"));\n\n\n\n\nAlso, the \nOptionTree.java\n tool allows you to view a nested options string, e.g., used at the command line, as a tree. This can help you spot nesting errors.\n\n\nFilter\n\n\nA filter has two different properties:\n\n\n\n\n\n\nsupervised\n or \nunsupervised\n\n  either takes the class attribute into account or not\n\n\n\n\n\n\nattribute\n- or \ninstance\n-based\n  e.g., removing a certain attribute or removing instances that meet a certain condition\n\n\n\n\n\n\nMost filters implement the \nOptionHandler\n interface, which means you can set the options via a String array, rather than setting them each manually via \nset\n-methods.\nFor example, if you want to remove the \nfirst\n attribute of a dataset, you need this filter\n\n\n weka.filters.unsupervised.attribute.Remove\n\n\n\n\nwith this option\n\n\n -R 1\n\n\n\n\nIf you have an \nInstances\n object, called \ndata\n, you can create and apply the filter like this:\n\n\n import weka.core.Instances;\n import weka.filters.Filter;\n import weka.filters.unsupervised.attribute.Remove;\n ...\n String[] options = new String[2];\n options[0] = \"-R\";                                    // \"range\"\n options[1] = \"1\";                                     // first attribute\n Remove remove = new Remove();                         // new instance of filter\n remove.setOptions(options);                           // set options\n remove.setInputFormat(data);                          // inform filter about dataset **AFTER** setting options\n Instances newData = Filter.useFilter(data, remove);   // apply filter\n\n\n\n\nFiltering on-the-fly\n\n\nThe \nFilteredClassifer\n meta-classifier is an easy way of filtering data on the fly. It removes the necessity of filtering the data before the classifier can be trained. Also, the data need not be passed through the trained filter again at prediction time. The following is an example of using this meta-classifier with the \nRemove\n filter and \nJ48\n for getting rid of a numeric ID attribute in the data:\n\n\n import weka.classifiers.meta.FilteredClassifier;\n import weka.classifiers.trees.J48;\n import weka.filters.unsupervised.attribute.Remove;\n ...\n Instances train = ...         // from somewhere\n Instances test = ...          // from somewhere\n // filter\n Remove rm = new Remove();\n rm.setAttributeIndices(\"1\");  // remove 1st attribute\n // classifier\n J48 j48 = new J48();\n j48.setUnpruned(true);        // using an unpruned J48\n // meta-classifier\n FilteredClassifier fc = new FilteredClassifier();\n fc.setFilter(rm);\n fc.setClassifier(j48);\n // train and make predictions\n fc.buildClassifier(train);\n for (int i = 0; i < test.numInstances(); i++) {\n   double pred = fc.classifyInstance(test.instance(i));\n   System.out.print(\"ID: \" + test.instance(i).value(0));\n   System.out.print(\", actual: \" + test.classAttribute().value((int) test.instance(i).classValue()));\n   System.out.println(\", predicted: \" + test.classAttribute().value((int) pred));\n }\n\n\n\n\nOther handy meta-schemes in Weka:\n\n\n\n\nweka.clusterers.FilteredClusterer\n\n\nweka.assocations.FilteredAssociator\n\n\n\n\nBatch filtering\n\n\nOn the command line, you can enable a second input/output pair (via \n-r\n and \n-s\n) with the \n-b\n option, in order to process the second file with the same filter setup as the first one. Necessary, if you're using attribute selection or standardization - otherwise you end up with incompatible datasets. This is done fairly easy, since one initializes the filter only once with the \nsetInputFormat(Instances)\n method, namely with the training set, and then applies the filter subsequently to the training set \nand\n the test set. The following example shows how to apply the \nStandardize\n filter to a train and a test set.\n\n\n Instances train = ...   // from somewhere\n Instances test = ...    // from somewhere\n Standardize filter = new Standardize();\n filter.setInputFormat(train);  // initializing the filter once with training set\n Instances newTrain = Filter.useFilter(train, filter);  // configures the Filter based on train instances and returns filtered instances\n Instances newTest = Filter.useFilter(test, filter);    // create new test set\n\n\n\n\nCalling conventions\n\n\nThe \nsetInputFormat(Instances)\n method \nalways\n has to be the last call before the filter is applied, e.g., with \nFilter.useFilter(Instances,Filter)\n. \nWhy?\n First, it is the convention for using filters and, secondly, lots of filters generate the header of the output format in the \nsetInputFormat(Instances)\n method with the currently set options (setting otpions \nafter\n this call doesn't have any effect any more).\n\n\nClassification\n\n\nThe necessary classes can be found in this package:\n\n\n weka.classifiers\n\n\n\n\nBuilding a Classifier\n\n\nBatch\n\n\nA Weka classifier is rather simple to train on a given dataset. E.g., we can train an unpruned C4.5 tree algorithm on a given dataset \ndata\n. The training is done via the \nbuildClassifier(Instances)\n method.\n\n\n import weka.classifiers.trees.J48;\n ...\n String[] options = new String[1];\n options[0] = \"-U\";            // unpruned tree\n J48 tree = new J48();         // new instance of tree\n tree.setOptions(options);     // set the options\n tree.buildClassifier(data);   // build classifier\n\n\n\n\nIncremental\n\n\nClassifiers implementing the \nweka.classifiers.UpdateableClassifier\n interface can be trained incrementally. This conserves memory, since the data doesn't have to be loaded into memory all at once. See the Javadoc of this interface to see what classifiers are implementing it.\n\n\nThe actual process of training an incremental classifier is fairly simple:\n\n\n\n\nCall \nbuildClassifier(Instances)\n with the structure of the dataset (may or may not contain any actual data rows).\n\n\nSubsequently call the \nupdateClassifier(Instance)\n method to feed the classifier new \nweka.core.Instance\n objects, one by one.\n\n\n\n\nHere is an example using data from a \nweka.core.converters.ArffLoader\n to train \nweka.classifiers.bayes.NaiveBayesUpdateable\n:\n\n\n // load data\n ArffLoader loader = new ArffLoader();\n loader.setFile(new File(\"/some/where/data.arff\"));\n Instances structure = loader.getStructure();\n structure.setClassIndex(structure.numAttributes() - 1);\n\n // train NaiveBayes\n NaiveBayesUpdateable nb = new NaiveBayesUpdateable();\n nb.buildClassifier(structure);\n Instance current;\n while ((current = loader.getNextInstance(structure)) != null)\n   nb.updateClassifier(current);\n\n\n\n\nA working example is \nIncrementalClassifier.java\n.\n\n\nEvaluating\n\n\nCross-validation\n\n\nIf you only have a training set and no test you might want to evaluate the classifier by using 10 times 10-fold cross-validation. This can be easily done via the \nEvaluation\n class. Here we \nseed\n the random selection of our folds for the CV with \n1\n. Check out the \nEvaluation\n class for more information about the statistics it produces.\n\n\n import weka.classifiers.Evaluation;\n import java.util.Random;\n ...\n Evaluation eval = new Evaluation(newData);\n eval.crossValidateModel(tree, newData, 10, new Random(1));\n\n\n\n\nNote:\n The classifier (in our example \ntree\n) should not be trained when handed over to the \ncrossValidateModel\n method. \nWhy?\n If the classifier does not abide to the Weka convention that a classifier must be re-initialized every time the \nbuildClassifier\n method is called (in other words: subsequent calls to the \nbuildClassifier\n method always return the same results), you will get inconsistent and worthless results. The \ncrossValidateModel\n takes care of training and evaluating the classifier. (It creates a copy of the original classifier that you hand over to the \ncrossValidateModel\n for each run of the cross-validation.)\n\n\nTrain/test set\n\n\nIn case you have a dedicated test set, you can train the classifier and then evaluate it on this test set. In the following example, a J48 is instantiated, trained and then evaluated. Some statistics are printed to \nstdout\n:\n\n\n import weka.core.Instances;\n import weka.classifiers.Evaluation;\n import weka.classifiers.trees.J48;\n ...\n Instances train = ...   // from somewhere\n Instances test = ...    // from somewhere\n // train classifier\n Classifier cls = new J48();\n cls.buildClassifier(train);\n // evaluate classifier and print some statistics\n Evaluation eval = new Evaluation(train);\n eval.evaluateModel(cls, test);\n System.out.println(eval.toSummaryString(\"\\nResults\\n======\\n\", false));\n\n\n\n\nStatistics\n\n\nSome methods for retrieving the results from the evaluation:\n\n\n\n\n\n\nnominal class\n\n\n\n\ncorrect()\n - number of correctly classified instances (see also \nincorrect()\n)\n\n\npctCorrect()\n - percentage of correctly classified instances (see also \npctIncorrect()\n)\n\n\nkappa()\n - Kappa statistics\n\n\n\n\n\n\n\n\nnumeric class\n\n\n\n\ncorrelationCoefficient()\n - correlation coefficient\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\nmeanAbsoluteError()\n - the mean absolute error\n\n\nrootMeanSquaredError()\n - the root mean squared error\n\n\nunclassified()\n - number of unclassified instances\n\n\npctUnclassified()\n - percentage of unclassified instances\n\n\n\n\n\n\n\n\nIf you want to have the exact same behavior as from the command line, use this call:\n\n\n import weka.classifiers.trees.J48;\n import weka.classifiers.Evaluation;\n ...\n String[] options = new String[2];\n options[0] = \"-t\";\n options[1] = \"/some/where/somefile.arff\";\n System.out.println(Evaluation.evaluateModel(new J48(), options));\n\n\n\n\nROC curves/AUC\n\n\nYou can also generate ROC curves/AUC with the predictions Weka recorded during testing. You can access these predictions via the \npredictions()\n method of the \nEvaluation\n class. See the [[Generating ROC curve]] article for a full example of how to generate ROC curves.\n\n\nClassifying instances\n\n\nIn case you have an unlabeled dataset that you want to classify with your newly trained classifier, you can use the following code snippet. It loads the file \n/some/where/unlabeled.arff\n, uses the previously built classifier \ntree\n to label the instances, and saves the labeled data as \n/some/where/labeled.arff\n.\n\n\n import java.io.BufferedReader;\n import java.io.BufferedWriter;\n import java.io.FileReader;\n import java.io.FileWriter;\n import weka.core.Instances;\n ...\n // load unlabeled data\n Instances unlabeled = new Instances(\n                         new BufferedReader(\n                           new FileReader(\"/some/where/unlabeled.arff\")));\n\n // set class attribute\n unlabeled.setClassIndex(unlabeled.numAttributes() - 1);\n\n // create copy\n Instances labeled = new Instances(unlabeled);\n\n // label instances\n for (int i = 0; i < unlabeled.numInstances(); i++) {\n   double clsLabel = tree.classifyInstance(unlabeled.instance(i));\n   labeled.instance(i).setClassValue(clsLabel);\n }\n // save labeled data\n BufferedWriter writer = new BufferedWriter(\n                           new FileWriter(\"/some/where/labeled.arff\"));\n writer.write(labeled.toString());\n writer.newLine();\n writer.flush();\n writer.close();\n\n\n\n\nNote on nominal classes:\n\n\n\n\nIf you're interested in the distribution over all the classes, use the method \ndistributionForInstance(Instance)\n. This method returns a double array with the probability for each class.\n\n\nThe returned double value from \nclassifyInstance\n (or the index in the array returned by \ndistributionForInstance\n) is just the index for the string values in the attribute. That is, if you want the string representation for the class label returned above \nclsLabel\n, then you can print it like this:\n\n\n\n\nSystem.out.println(clsLabel + \" -> \" + unlabeled.classAttribute().value((int) clsLabel));\n\n\n\n\nClustering\n\n\nClustering is similar to classification. The necessary classes can be found in this package:\n\n\n weka.clusterers\n\n\n\n\nBuilding a Clusterer\n\n\nBatch\n\n\nA clusterer is built in much the same way as a classifier, but the \nbuildClusterer(Instances)\n method instead of \nbuildClassifier(Instances)\n. The following code snippet shows how to build an \nEM\n clusterer with a maximum of \n100\n iterations.\n\n\n import weka.clusterers.EM;\n ...\n String[] options = new String[2];\n options[0] = \"-I\";                 // max. iterations\n options[1] = \"100\";\n EM clusterer = new EM();   // new instance of clusterer\n clusterer.setOptions(options);     // set the options\n clusterer.buildClusterer(data);    // build the clusterer\n\n\n\n\nIncremental\n\n\nClusterers implementing the \nweka.clusterers.UpdateableClusterer\n interface can be trained incrementally. This conserves memory, since the data doesn't have to be loaded into memory all at once. See the Javadoc for this interface to see which clusterers implement it.\n\n\nThe actual process of training an incremental clusterer is fairly simple:\n\n\n\n\nCall \nbuildClusterer(Instances)\n with the structure of the dataset (may or may not contain any actual data rows).\n\n\nSubsequently call the \nupdateClusterer(Instance)\n method to feed the clusterer new \nweka.core.Instance\n objects, one by one.\n\n\nCall \nupdateFinished()\n after all Instance objects have been processed, for the clusterer to perform additional computations.\n\n\n\n\nHere is an example using data from a \nweka.core.converters.ArffLoader\n to train \nweka.clusterers.Cobweb\n:\n\n\n // load data\n ArffLoader loader = new ArffLoader();\n loader.setFile(new File(\"/some/where/data.arff\"));\n Instances structure = loader.getStructure();\n\n // train Cobweb\n Cobweb cw = new Cobweb();\n cw.buildClusterer(structure);\n Instance current;\n while ((current = loader.getNextInstance(structure)) != null)\n   cw.updateClusterer(current);\n cw.updateFinished();\n\n\n\n\nA working example is \nIncrementalClusterer.java\n.\n\n\nEvaluating\n\n\nFor evaluating a clusterer, you can use the \nClusterEvaluation\n class. In this example, the number of clusters found is written to output:\n\n\n import weka.clusterers.ClusterEvaluation;\n import weka.clusterers.Clusterer;\n ...\n ClusterEvaluation eval = new ClusterEvaluation();\n Clusterer clusterer = new EM();                                 // new clusterer instance, default options\n clusterer.buildClusterer(data);                                 // build clusterer\n eval.setClusterer(clusterer);                                   // the cluster to evaluate\n eval.evaluateClusterer(newData);                                // data to evaluate the clusterer on\n System.out.println(\"# of clusters: \" + eval.getNumClusters());  // output # of clusters\n\n\n\n\nOr, in the case of \nDensityBasedClusterer\n, you can cross-validate the clusterer (Note: with \nMakeDensityBasedClusterer\n you can turn any clusterer into a density-based one):\n\n\n import weka.clusterers.ClusterEvaluation;\n import weka.clusterers.DensityBasedClusterer;\n import weka.core.Instances;\n import java.util.Random;\n ...\n Instances data = ...                                     // from somewhere\n DensityBasedClusterer clusterer = new ...                // the clusterer to evaluate\n double logLikelyhood =\n    ClusterEvaluation.crossValidateModel(                 // cross-validate\n    clusterer, data, 10,                                  // with 10 folds\n    new Random(1));                                       // and random number generator with seed 1\n\n\n\n\nOr, if you want the same behavior/print-out from command line, use this call:\n\n\n import weka.clusterers.EM;\n import weka.clusterers.ClusterEvaluation;\n ...\n String[] options = new String[2];\n options[0] = \"-t\";\n options[1] = \"/some/where/somefile.arff\";\n System.out.println(ClusterEvaluation.evaluateClusterer(new EM(), options));\n\n\n\n\nClustering instances\n\n\nThe only difference with regard to classification is the method name. Instead of \nclassifyInstance(Instance)\n, it is now \nclusterInstance(Instance)\n. The method for obtaining the distribution is still the same, i.e., \ndistributionForInstance(Instance)\n.\n\n\nClasses to clusters evaluation\n\n\nIf your data contains a class attribute and you want to check how well the generated clusters fit the classes, you can perform a so-called \nclasses to clusters\n evaluation. The Weka Explorer offers this functionality, and it's quite easy to implement. These are the necessary steps (complete source code: \nClassesToClusters.java\n):\n\n\n\n\nload the data and set the class attribute\n\n\n\n\n Instances data = new Instances(new BufferedReader(new FileReader(\"/some/where/file.arff\")));\n data.setClassIndex(data.numAttributes() - 1);\n\n\n\n\n\n\ngenerate the \nclass-less\n data to train the clusterer with\n\n\n\n\n weka.filters.unsupervised.attribute.Remove filter = new weka.filters.unsupervised.attribute.Remove();\n filter.setAttributeIndices(\"\" + (data.classIndex() + 1));\n filter.setInputFormat(data);\n Instances dataClusterer = Filter.useFilter(data, filter);\n\n\n\n\n\n\ntrain the clusterer, e.g., \nEM\n\n\n\n\n EM clusterer = new EM();\n // set further options for EM, if necessary...\n clusterer.buildClusterer(dataClusterer);\n\n\n\n\n\n\nevaluate the clusterer with the data still containing the class attribute\n\n\n\n\n ClusterEvaluation eval = new ClusterEvaluation();\n eval.setClusterer(clusterer);\n eval.evaluateClusterer(data);\n\n\n\n\n\n\nprint the results of the evaluation to \nstdout\n\n\n\n\n System.out.println(eval.clusterResultsToString());\n\n\n\n\nAttribute selection\n\n\nThere is no real need to use the attribute selection classes directly in your own code, since there are already a meta-classifier and a filter available for applying attribute selection, but the low-level approach is still listed for the sake of completeness. The following examples all use \nCfsSubsetEval\n and \nGreedyStepwise\n (backwards). The code listed below is taken from the \nAttributeSelectionTest.java\n.\n\n\nMeta-Classifier\n\n\nThe following meta-classifier performs a preprocessing step of attribute selection before the data gets presented to the base classifier (in the example here, this is \nJ48\n).\n\n\n  Instances data = ...  // from somewhere\n  AttributeSelectedClassifier classifier = new AttributeSelectedClassifier();\n  CfsSubsetEval eval = new CfsSubsetEval();\n  GreedyStepwise search = new GreedyStepwise();\n  search.setSearchBackwards(true);\n  J48 base = new J48();\n  classifier.setClassifier(base);\n  classifier.setEvaluator(eval);\n  classifier.setSearch(search);\n  // 10-fold cross-validation\n  Evaluation evaluation = new Evaluation(data);\n  evaluation.crossValidateModel(classifier, data, 10, new Random(1));\n  System.out.println(evaluation.toSummaryString());\n\n\n\n\nFilter\n\n\nThe filter approach is straightforward: after setting up the filter, one just filters the data through the filter and obtains the reduced dataset.\n\n\n  Instances data = ...  // from somewhere\n  AttributeSelection filter = new AttributeSelection();  // package weka.filters.supervised.attribute!\n  CfsSubsetEval eval = new CfsSubsetEval();\n  GreedyStepwise search = new GreedyStepwise();\n  search.setSearchBackwards(true);\n  filter.setEvaluator(eval);\n  filter.setSearch(search);\n  filter.setInputFormat(data);\n  // generate new data\n  Instances newData = Filter.useFilter(data, filter);\n  System.out.println(newData);\n\n\n\n\nLow-level\n\n\nIf neither the meta-classifier nor filter approach is suitable for your purposes, you can use the attribute selection classes themselves.\n\n\n  Instances data = ...  // from somewhere\n  AttributeSelection attsel = new AttributeSelection();  // package weka.attributeSelection!\n  CfsSubsetEval eval = new CfsSubsetEval();\n  GreedyStepwise search = new GreedyStepwise();\n  search.setSearchBackwards(true);\n  attsel.setEvaluator(eval);\n  attsel.setSearch(search);\n  attsel.SelectAttributes(data);\n  // obtain the attribute indices that were selected\n  int[] indices = attsel.selectedAttributes();\n  System.out.println(Utils.arrayToString(indices));\n\n\n\n\nNote on randomization\n\n\nMost machine learning schemes, like classifiers and clusterers, are susceptible to the ordering of the data. Using a different seed for randomizing the data will most likely produce a different result. For example, the Explorer, or a classifier/clusterer run from the command line, uses only a seeded \njava.util.Random\n number generator, whereas the \nweka.core.Instances.getRandomNumberGenerator(int)\n (which the \nWekaDemo.java\n uses) also takes the data into account for seeding. Unless one runs 10-fold cross-validation 10 times and averages the results, one will most likely get different results.\n\n\nSee also\n\n\n\n\nWeka Examples\n - pointer to collection of example classes\n\n\nDatabases\n - for more information about using databases in Weka (includes ODBC, e.g., for MS Access)\n\n\n[[weka_experiment_DatabaseUtils.props|weka/experiment/DatabaseUtils.props]] - the database setup file\n\n\nGenerating cross-validation folds (Java approach)\n - in case you want to run 10-fold cross-validation manually\n\n\n[[Generating classifier evaluation output manually]] - if you want to generate some of the evaluation statistics output manually\n\n\nCreating Instances on-the-fly\n - explains how to generate a \nweka.core.Instances\n object from scratch\n\n\n[[Save Instances to an ARFF File]] - shows how to output a dataset\n\n\n[[Using the Experiment API]]\n\n\n\n\nExamples\n\n\nThe following are a few sample classes for using various parts of the Weka API:\n\n\n\n\n\n\nWekaDemo.java\n (\nstable\n, \ndeveloper\n) - little demo class that loads data from a file, runs it through a filter and trains/evaluates a classifier\n\n\n\n\n\n\nClusteringDemo.java\n (\nstable\n, \ndeveloper\n) - a basic example for using the clusterer API\n\n\n\n\n\n\nClassesToClusters.java\n (\nstable\n, \ndeveloper\n) - performs a \nclasses to clusters\n evaluation like in the Explorer\n\n\n\n\n\n\nAttributeSelectionTest.java\n (\nstable\n, \ndeveloper\n) - example code for using the attribute selection API\n\n\n\n\n\n\nM5PExample.java\n (\nstable\n, \ndeveloper\n) - example using M5P to obtain data from database, train model, serialize it to a file, and use this serialized model to make predictions again.\n\n\n\n\n\n\nOptionsToCode.java\n (\nstable\n, \ndeveloper\n) - turns a Weka command line for a scheme with options into Java code, correctly escaping quotes and backslashes.\n\n\n\n\n\n\nOptionTree.java\n (\nstable\n, \ndeveloper\n) - displays nested Weka options as tree.\n\n\n\n\n\n\nIncrementalClassifier.java\n (\nstable\n, \ndeveloper\n) - Example class for how to train an incremental classifier (in this case, \nweka.classifiers.bayes.NaiveBayesUpdateable\n).\n\n\n\n\n\n\nIncrementalClusterer.java\n (\nstable\n, \ndeveloper\n) - Example class for how to train an incremental clusterer (in this case, \nweka.clusterers.Cobweb\n).\n\n\n\n\n\n\nLinks\n\n\n\n\nWeka API\n\n\nStable version\n\n\nDeveloper version",
            "title": " Use Weka in your Java code"
        },
        {
            "location": "/use_weka_in_your_java_code/#instances",
            "text": "",
            "title": "Instances"
        },
        {
            "location": "/use_weka_in_your_java_code/#datasets",
            "text": "The  DataSource  class is not limited to ARFF files. It can also read CSV files and other formats (basically all file formats that Weka can import via its converters; it uses the file extension to determine the associated loader).   import weka.core.converters.ConverterUtils.DataSource;\n ...\n DataSource source = new DataSource(\"/some/where/data.arff\");\n Instances data = source.getDataSet();\n // setting class attribute if the data format does not provide this information\n // For example, the XRFF format saves the class attribute information as well\n if (data.classIndex() == -1)\n   data.setClassIndex(data.numAttributes() - 1);",
            "title": "Datasets"
        },
        {
            "location": "/use_weka_in_your_java_code/#database",
            "text": "Reading from  Databases  is slightly more complicated, but still very easy. First, you'll have to modify your  DatabaseUtils.props  file to reflect your database connection. Suppose you want to connect to a  MySQL  server that is running on the local machine on the default port  3306 . The MySQL JDBC driver is called  Connector/J . (The driver class is  org.gjt.mm.mysql.Driver .) The database where your target data resides is called  some_database . Since you're only reading, you can use the default user  nobody  without a password. Your props file must contain the following lines:   jdbcDriver=org.gjt.mm.mysql.Driver\n jdbcURL=jdbc:mysql://localhost:3306/some_database  Secondly, your Java code needs to look like this to load the data from the database:   import weka.core.Instances;\n import weka.experiment.InstanceQuery;\n ...\n InstanceQuery query = new InstanceQuery();\n query.setUsername(\"nobody\");\n query.setPassword(\"\");\n query.setQuery(\"select * from whatsoever\");\n // You can declare that your data set is sparse\n // query.setSparseData(true);\n Instances data = query.retrieveInstances();  Notes:   Don't forget to add the JDBC driver to your  CLASSPATH .  For MS Access, you must use the JDBC-ODBC-bridge that is part of a JDK. The [[Windows databases]] article explains how to do this.\n* InstanceQuery automatically converts VARCHAR database columns to NOMINAL attributes, and long TEXT database columns to STRING attributes. So if you use InstanceQuery to do text mining against text that appears in a VARCHAR column, Weka will regard such text as nominal values. Thus it will fail to tokenize and mine that text. Use the  NominalToString  or  StringToNominal  filter (package  weka.filters.unsupervised.attribute ) to convert the attributes into the correct type.",
            "title": "Database"
        },
        {
            "location": "/use_weka_in_your_java_code/#option-handling",
            "text": "Weka schemes that implement the  weka.core.OptionHandler  interface, such as classifiers, clusterers, and filters, offer the following methods for setting and retrieving options:   void setOptions(String[] options)  String[] getOptions()   There are several ways of setting the options:   Manually creating a String array:      String[] options = new String[2];\n   options[0] = \"-R\";\n   options[1] = \"1\";   Using a single command-line string and using the  splitOptions  method of the  weka.core.Utils  class to turn it into an array:       String[] options = weka.core.Utils.splitOptions(\"-R 1\");   Using the  OptionsToCode.java  class to automatically turn a command line into code. Especially handy if the command line contains nested classes that have their own options, such as kernels for SMO:     java OptionsToCode weka.classifiers.functions.SMO  will generate output like this:   // create new instance of scheme\n weka.classifiers.functions.SMO scheme = new weka.classifiers.functions.SMO();\n // set options\n scheme.setOptions(weka.core.Utils.splitOptions(\"-C 1.0 -L 0.0010 -P 1.0E-12 -N 0 -V -1 -W 1 -K \\\"weka.classifiers.functions.supportVector.PolyKernel -C 250007 -E 1.0\\\"\"));  Also, the  OptionTree.java  tool allows you to view a nested options string, e.g., used at the command line, as a tree. This can help you spot nesting errors.",
            "title": "Option handling"
        },
        {
            "location": "/use_weka_in_your_java_code/#filter",
            "text": "A filter has two different properties:    supervised  or  unsupervised \n  either takes the class attribute into account or not    attribute - or  instance -based\n  e.g., removing a certain attribute or removing instances that meet a certain condition    Most filters implement the  OptionHandler  interface, which means you can set the options via a String array, rather than setting them each manually via  set -methods.\nFor example, if you want to remove the  first  attribute of a dataset, you need this filter   weka.filters.unsupervised.attribute.Remove  with this option   -R 1  If you have an  Instances  object, called  data , you can create and apply the filter like this:   import weka.core.Instances;\n import weka.filters.Filter;\n import weka.filters.unsupervised.attribute.Remove;\n ...\n String[] options = new String[2];\n options[0] = \"-R\";                                    // \"range\"\n options[1] = \"1\";                                     // first attribute\n Remove remove = new Remove();                         // new instance of filter\n remove.setOptions(options);                           // set options\n remove.setInputFormat(data);                          // inform filter about dataset **AFTER** setting options\n Instances newData = Filter.useFilter(data, remove);   // apply filter",
            "title": "Filter"
        },
        {
            "location": "/use_weka_in_your_java_code/#filtering-on-the-fly",
            "text": "The  FilteredClassifer  meta-classifier is an easy way of filtering data on the fly. It removes the necessity of filtering the data before the classifier can be trained. Also, the data need not be passed through the trained filter again at prediction time. The following is an example of using this meta-classifier with the  Remove  filter and  J48  for getting rid of a numeric ID attribute in the data:   import weka.classifiers.meta.FilteredClassifier;\n import weka.classifiers.trees.J48;\n import weka.filters.unsupervised.attribute.Remove;\n ...\n Instances train = ...         // from somewhere\n Instances test = ...          // from somewhere\n // filter\n Remove rm = new Remove();\n rm.setAttributeIndices(\"1\");  // remove 1st attribute\n // classifier\n J48 j48 = new J48();\n j48.setUnpruned(true);        // using an unpruned J48\n // meta-classifier\n FilteredClassifier fc = new FilteredClassifier();\n fc.setFilter(rm);\n fc.setClassifier(j48);\n // train and make predictions\n fc.buildClassifier(train);\n for (int i = 0; i < test.numInstances(); i++) {\n   double pred = fc.classifyInstance(test.instance(i));\n   System.out.print(\"ID: \" + test.instance(i).value(0));\n   System.out.print(\", actual: \" + test.classAttribute().value((int) test.instance(i).classValue()));\n   System.out.println(\", predicted: \" + test.classAttribute().value((int) pred));\n }  Other handy meta-schemes in Weka:   weka.clusterers.FilteredClusterer  weka.assocations.FilteredAssociator",
            "title": "Filtering on-the-fly"
        },
        {
            "location": "/use_weka_in_your_java_code/#batch-filtering",
            "text": "On the command line, you can enable a second input/output pair (via  -r  and  -s ) with the  -b  option, in order to process the second file with the same filter setup as the first one. Necessary, if you're using attribute selection or standardization - otherwise you end up with incompatible datasets. This is done fairly easy, since one initializes the filter only once with the  setInputFormat(Instances)  method, namely with the training set, and then applies the filter subsequently to the training set  and  the test set. The following example shows how to apply the  Standardize  filter to a train and a test set.   Instances train = ...   // from somewhere\n Instances test = ...    // from somewhere\n Standardize filter = new Standardize();\n filter.setInputFormat(train);  // initializing the filter once with training set\n Instances newTrain = Filter.useFilter(train, filter);  // configures the Filter based on train instances and returns filtered instances\n Instances newTest = Filter.useFilter(test, filter);    // create new test set",
            "title": "Batch filtering"
        },
        {
            "location": "/use_weka_in_your_java_code/#calling-conventions",
            "text": "The  setInputFormat(Instances)  method  always  has to be the last call before the filter is applied, e.g., with  Filter.useFilter(Instances,Filter) .  Why?  First, it is the convention for using filters and, secondly, lots of filters generate the header of the output format in the  setInputFormat(Instances)  method with the currently set options (setting otpions  after  this call doesn't have any effect any more).",
            "title": "Calling conventions"
        },
        {
            "location": "/use_weka_in_your_java_code/#classification",
            "text": "The necessary classes can be found in this package:   weka.classifiers",
            "title": "Classification"
        },
        {
            "location": "/use_weka_in_your_java_code/#building-a-classifier",
            "text": "",
            "title": "Building a Classifier"
        },
        {
            "location": "/use_weka_in_your_java_code/#batch",
            "text": "A Weka classifier is rather simple to train on a given dataset. E.g., we can train an unpruned C4.5 tree algorithm on a given dataset  data . The training is done via the  buildClassifier(Instances)  method.   import weka.classifiers.trees.J48;\n ...\n String[] options = new String[1];\n options[0] = \"-U\";            // unpruned tree\n J48 tree = new J48();         // new instance of tree\n tree.setOptions(options);     // set the options\n tree.buildClassifier(data);   // build classifier",
            "title": "Batch"
        },
        {
            "location": "/use_weka_in_your_java_code/#incremental",
            "text": "Classifiers implementing the  weka.classifiers.UpdateableClassifier  interface can be trained incrementally. This conserves memory, since the data doesn't have to be loaded into memory all at once. See the Javadoc of this interface to see what classifiers are implementing it.  The actual process of training an incremental classifier is fairly simple:   Call  buildClassifier(Instances)  with the structure of the dataset (may or may not contain any actual data rows).  Subsequently call the  updateClassifier(Instance)  method to feed the classifier new  weka.core.Instance  objects, one by one.   Here is an example using data from a  weka.core.converters.ArffLoader  to train  weka.classifiers.bayes.NaiveBayesUpdateable :   // load data\n ArffLoader loader = new ArffLoader();\n loader.setFile(new File(\"/some/where/data.arff\"));\n Instances structure = loader.getStructure();\n structure.setClassIndex(structure.numAttributes() - 1);\n\n // train NaiveBayes\n NaiveBayesUpdateable nb = new NaiveBayesUpdateable();\n nb.buildClassifier(structure);\n Instance current;\n while ((current = loader.getNextInstance(structure)) != null)\n   nb.updateClassifier(current);  A working example is  IncrementalClassifier.java .",
            "title": "Incremental"
        },
        {
            "location": "/use_weka_in_your_java_code/#evaluating",
            "text": "",
            "title": "Evaluating"
        },
        {
            "location": "/use_weka_in_your_java_code/#cross-validation",
            "text": "If you only have a training set and no test you might want to evaluate the classifier by using 10 times 10-fold cross-validation. This can be easily done via the  Evaluation  class. Here we  seed  the random selection of our folds for the CV with  1 . Check out the  Evaluation  class for more information about the statistics it produces.   import weka.classifiers.Evaluation;\n import java.util.Random;\n ...\n Evaluation eval = new Evaluation(newData);\n eval.crossValidateModel(tree, newData, 10, new Random(1));  Note:  The classifier (in our example  tree ) should not be trained when handed over to the  crossValidateModel  method.  Why?  If the classifier does not abide to the Weka convention that a classifier must be re-initialized every time the  buildClassifier  method is called (in other words: subsequent calls to the  buildClassifier  method always return the same results), you will get inconsistent and worthless results. The  crossValidateModel  takes care of training and evaluating the classifier. (It creates a copy of the original classifier that you hand over to the  crossValidateModel  for each run of the cross-validation.)",
            "title": "Cross-validation"
        },
        {
            "location": "/use_weka_in_your_java_code/#traintest-set",
            "text": "In case you have a dedicated test set, you can train the classifier and then evaluate it on this test set. In the following example, a J48 is instantiated, trained and then evaluated. Some statistics are printed to  stdout :   import weka.core.Instances;\n import weka.classifiers.Evaluation;\n import weka.classifiers.trees.J48;\n ...\n Instances train = ...   // from somewhere\n Instances test = ...    // from somewhere\n // train classifier\n Classifier cls = new J48();\n cls.buildClassifier(train);\n // evaluate classifier and print some statistics\n Evaluation eval = new Evaluation(train);\n eval.evaluateModel(cls, test);\n System.out.println(eval.toSummaryString(\"\\nResults\\n======\\n\", false));",
            "title": "Train/test set"
        },
        {
            "location": "/use_weka_in_your_java_code/#statistics",
            "text": "Some methods for retrieving the results from the evaluation:    nominal class   correct()  - number of correctly classified instances (see also  incorrect() )  pctCorrect()  - percentage of correctly classified instances (see also  pctIncorrect() )  kappa()  - Kappa statistics     numeric class   correlationCoefficient()  - correlation coefficient     general   meanAbsoluteError()  - the mean absolute error  rootMeanSquaredError()  - the root mean squared error  unclassified()  - number of unclassified instances  pctUnclassified()  - percentage of unclassified instances     If you want to have the exact same behavior as from the command line, use this call:   import weka.classifiers.trees.J48;\n import weka.classifiers.Evaluation;\n ...\n String[] options = new String[2];\n options[0] = \"-t\";\n options[1] = \"/some/where/somefile.arff\";\n System.out.println(Evaluation.evaluateModel(new J48(), options));",
            "title": "Statistics"
        },
        {
            "location": "/use_weka_in_your_java_code/#roc-curvesauc",
            "text": "You can also generate ROC curves/AUC with the predictions Weka recorded during testing. You can access these predictions via the  predictions()  method of the  Evaluation  class. See the [[Generating ROC curve]] article for a full example of how to generate ROC curves.",
            "title": "ROC curves/AUC"
        },
        {
            "location": "/use_weka_in_your_java_code/#classifying-instances",
            "text": "In case you have an unlabeled dataset that you want to classify with your newly trained classifier, you can use the following code snippet. It loads the file  /some/where/unlabeled.arff , uses the previously built classifier  tree  to label the instances, and saves the labeled data as  /some/where/labeled.arff .   import java.io.BufferedReader;\n import java.io.BufferedWriter;\n import java.io.FileReader;\n import java.io.FileWriter;\n import weka.core.Instances;\n ...\n // load unlabeled data\n Instances unlabeled = new Instances(\n                         new BufferedReader(\n                           new FileReader(\"/some/where/unlabeled.arff\")));\n\n // set class attribute\n unlabeled.setClassIndex(unlabeled.numAttributes() - 1);\n\n // create copy\n Instances labeled = new Instances(unlabeled);\n\n // label instances\n for (int i = 0; i < unlabeled.numInstances(); i++) {\n   double clsLabel = tree.classifyInstance(unlabeled.instance(i));\n   labeled.instance(i).setClassValue(clsLabel);\n }\n // save labeled data\n BufferedWriter writer = new BufferedWriter(\n                           new FileWriter(\"/some/where/labeled.arff\"));\n writer.write(labeled.toString());\n writer.newLine();\n writer.flush();\n writer.close();  Note on nominal classes:   If you're interested in the distribution over all the classes, use the method  distributionForInstance(Instance) . This method returns a double array with the probability for each class.  The returned double value from  classifyInstance  (or the index in the array returned by  distributionForInstance ) is just the index for the string values in the attribute. That is, if you want the string representation for the class label returned above  clsLabel , then you can print it like this:   System.out.println(clsLabel + \" -> \" + unlabeled.classAttribute().value((int) clsLabel));",
            "title": "Classifying instances"
        },
        {
            "location": "/use_weka_in_your_java_code/#clustering",
            "text": "Clustering is similar to classification. The necessary classes can be found in this package:   weka.clusterers",
            "title": "Clustering"
        },
        {
            "location": "/use_weka_in_your_java_code/#building-a-clusterer",
            "text": "",
            "title": "Building a Clusterer"
        },
        {
            "location": "/use_weka_in_your_java_code/#batch_1",
            "text": "A clusterer is built in much the same way as a classifier, but the  buildClusterer(Instances)  method instead of  buildClassifier(Instances) . The following code snippet shows how to build an  EM  clusterer with a maximum of  100  iterations.   import weka.clusterers.EM;\n ...\n String[] options = new String[2];\n options[0] = \"-I\";                 // max. iterations\n options[1] = \"100\";\n EM clusterer = new EM();   // new instance of clusterer\n clusterer.setOptions(options);     // set the options\n clusterer.buildClusterer(data);    // build the clusterer",
            "title": "Batch"
        },
        {
            "location": "/use_weka_in_your_java_code/#incremental_1",
            "text": "Clusterers implementing the  weka.clusterers.UpdateableClusterer  interface can be trained incrementally. This conserves memory, since the data doesn't have to be loaded into memory all at once. See the Javadoc for this interface to see which clusterers implement it.  The actual process of training an incremental clusterer is fairly simple:   Call  buildClusterer(Instances)  with the structure of the dataset (may or may not contain any actual data rows).  Subsequently call the  updateClusterer(Instance)  method to feed the clusterer new  weka.core.Instance  objects, one by one.  Call  updateFinished()  after all Instance objects have been processed, for the clusterer to perform additional computations.   Here is an example using data from a  weka.core.converters.ArffLoader  to train  weka.clusterers.Cobweb :   // load data\n ArffLoader loader = new ArffLoader();\n loader.setFile(new File(\"/some/where/data.arff\"));\n Instances structure = loader.getStructure();\n\n // train Cobweb\n Cobweb cw = new Cobweb();\n cw.buildClusterer(structure);\n Instance current;\n while ((current = loader.getNextInstance(structure)) != null)\n   cw.updateClusterer(current);\n cw.updateFinished();  A working example is  IncrementalClusterer.java .",
            "title": "Incremental"
        },
        {
            "location": "/use_weka_in_your_java_code/#evaluating_1",
            "text": "For evaluating a clusterer, you can use the  ClusterEvaluation  class. In this example, the number of clusters found is written to output:   import weka.clusterers.ClusterEvaluation;\n import weka.clusterers.Clusterer;\n ...\n ClusterEvaluation eval = new ClusterEvaluation();\n Clusterer clusterer = new EM();                                 // new clusterer instance, default options\n clusterer.buildClusterer(data);                                 // build clusterer\n eval.setClusterer(clusterer);                                   // the cluster to evaluate\n eval.evaluateClusterer(newData);                                // data to evaluate the clusterer on\n System.out.println(\"# of clusters: \" + eval.getNumClusters());  // output # of clusters  Or, in the case of  DensityBasedClusterer , you can cross-validate the clusterer (Note: with  MakeDensityBasedClusterer  you can turn any clusterer into a density-based one):   import weka.clusterers.ClusterEvaluation;\n import weka.clusterers.DensityBasedClusterer;\n import weka.core.Instances;\n import java.util.Random;\n ...\n Instances data = ...                                     // from somewhere\n DensityBasedClusterer clusterer = new ...                // the clusterer to evaluate\n double logLikelyhood =\n    ClusterEvaluation.crossValidateModel(                 // cross-validate\n    clusterer, data, 10,                                  // with 10 folds\n    new Random(1));                                       // and random number generator with seed 1  Or, if you want the same behavior/print-out from command line, use this call:   import weka.clusterers.EM;\n import weka.clusterers.ClusterEvaluation;\n ...\n String[] options = new String[2];\n options[0] = \"-t\";\n options[1] = \"/some/where/somefile.arff\";\n System.out.println(ClusterEvaluation.evaluateClusterer(new EM(), options));",
            "title": "Evaluating"
        },
        {
            "location": "/use_weka_in_your_java_code/#clustering-instances",
            "text": "The only difference with regard to classification is the method name. Instead of  classifyInstance(Instance) , it is now  clusterInstance(Instance) . The method for obtaining the distribution is still the same, i.e.,  distributionForInstance(Instance) .",
            "title": "Clustering instances"
        },
        {
            "location": "/use_weka_in_your_java_code/#classes-to-clusters-evaluation",
            "text": "If your data contains a class attribute and you want to check how well the generated clusters fit the classes, you can perform a so-called  classes to clusters  evaluation. The Weka Explorer offers this functionality, and it's quite easy to implement. These are the necessary steps (complete source code:  ClassesToClusters.java ):   load the data and set the class attribute    Instances data = new Instances(new BufferedReader(new FileReader(\"/some/where/file.arff\")));\n data.setClassIndex(data.numAttributes() - 1);   generate the  class-less  data to train the clusterer with    weka.filters.unsupervised.attribute.Remove filter = new weka.filters.unsupervised.attribute.Remove();\n filter.setAttributeIndices(\"\" + (data.classIndex() + 1));\n filter.setInputFormat(data);\n Instances dataClusterer = Filter.useFilter(data, filter);   train the clusterer, e.g.,  EM    EM clusterer = new EM();\n // set further options for EM, if necessary...\n clusterer.buildClusterer(dataClusterer);   evaluate the clusterer with the data still containing the class attribute    ClusterEvaluation eval = new ClusterEvaluation();\n eval.setClusterer(clusterer);\n eval.evaluateClusterer(data);   print the results of the evaluation to  stdout    System.out.println(eval.clusterResultsToString());",
            "title": "Classes to clusters evaluation"
        },
        {
            "location": "/use_weka_in_your_java_code/#attribute-selection",
            "text": "There is no real need to use the attribute selection classes directly in your own code, since there are already a meta-classifier and a filter available for applying attribute selection, but the low-level approach is still listed for the sake of completeness. The following examples all use  CfsSubsetEval  and  GreedyStepwise  (backwards). The code listed below is taken from the  AttributeSelectionTest.java .",
            "title": "Attribute selection"
        },
        {
            "location": "/use_weka_in_your_java_code/#meta-classifier",
            "text": "The following meta-classifier performs a preprocessing step of attribute selection before the data gets presented to the base classifier (in the example here, this is  J48 ).    Instances data = ...  // from somewhere\n  AttributeSelectedClassifier classifier = new AttributeSelectedClassifier();\n  CfsSubsetEval eval = new CfsSubsetEval();\n  GreedyStepwise search = new GreedyStepwise();\n  search.setSearchBackwards(true);\n  J48 base = new J48();\n  classifier.setClassifier(base);\n  classifier.setEvaluator(eval);\n  classifier.setSearch(search);\n  // 10-fold cross-validation\n  Evaluation evaluation = new Evaluation(data);\n  evaluation.crossValidateModel(classifier, data, 10, new Random(1));\n  System.out.println(evaluation.toSummaryString());",
            "title": "Meta-Classifier"
        },
        {
            "location": "/use_weka_in_your_java_code/#filter_1",
            "text": "The filter approach is straightforward: after setting up the filter, one just filters the data through the filter and obtains the reduced dataset.    Instances data = ...  // from somewhere\n  AttributeSelection filter = new AttributeSelection();  // package weka.filters.supervised.attribute!\n  CfsSubsetEval eval = new CfsSubsetEval();\n  GreedyStepwise search = new GreedyStepwise();\n  search.setSearchBackwards(true);\n  filter.setEvaluator(eval);\n  filter.setSearch(search);\n  filter.setInputFormat(data);\n  // generate new data\n  Instances newData = Filter.useFilter(data, filter);\n  System.out.println(newData);",
            "title": "Filter"
        },
        {
            "location": "/use_weka_in_your_java_code/#low-level",
            "text": "If neither the meta-classifier nor filter approach is suitable for your purposes, you can use the attribute selection classes themselves.    Instances data = ...  // from somewhere\n  AttributeSelection attsel = new AttributeSelection();  // package weka.attributeSelection!\n  CfsSubsetEval eval = new CfsSubsetEval();\n  GreedyStepwise search = new GreedyStepwise();\n  search.setSearchBackwards(true);\n  attsel.setEvaluator(eval);\n  attsel.setSearch(search);\n  attsel.SelectAttributes(data);\n  // obtain the attribute indices that were selected\n  int[] indices = attsel.selectedAttributes();\n  System.out.println(Utils.arrayToString(indices));",
            "title": "Low-level"
        },
        {
            "location": "/use_weka_in_your_java_code/#note-on-randomization",
            "text": "Most machine learning schemes, like classifiers and clusterers, are susceptible to the ordering of the data. Using a different seed for randomizing the data will most likely produce a different result. For example, the Explorer, or a classifier/clusterer run from the command line, uses only a seeded  java.util.Random  number generator, whereas the  weka.core.Instances.getRandomNumberGenerator(int)  (which the  WekaDemo.java  uses) also takes the data into account for seeding. Unless one runs 10-fold cross-validation 10 times and averages the results, one will most likely get different results.",
            "title": "Note on randomization"
        },
        {
            "location": "/use_weka_in_your_java_code/#see-also",
            "text": "Weka Examples  - pointer to collection of example classes  Databases  - for more information about using databases in Weka (includes ODBC, e.g., for MS Access)  [[weka_experiment_DatabaseUtils.props|weka/experiment/DatabaseUtils.props]] - the database setup file  Generating cross-validation folds (Java approach)  - in case you want to run 10-fold cross-validation manually  [[Generating classifier evaluation output manually]] - if you want to generate some of the evaluation statistics output manually  Creating Instances on-the-fly  - explains how to generate a  weka.core.Instances  object from scratch  [[Save Instances to an ARFF File]] - shows how to output a dataset  [[Using the Experiment API]]",
            "title": "See also"
        },
        {
            "location": "/use_weka_in_your_java_code/#examples",
            "text": "The following are a few sample classes for using various parts of the Weka API:    WekaDemo.java  ( stable ,  developer ) - little demo class that loads data from a file, runs it through a filter and trains/evaluates a classifier    ClusteringDemo.java  ( stable ,  developer ) - a basic example for using the clusterer API    ClassesToClusters.java  ( stable ,  developer ) - performs a  classes to clusters  evaluation like in the Explorer    AttributeSelectionTest.java  ( stable ,  developer ) - example code for using the attribute selection API    M5PExample.java  ( stable ,  developer ) - example using M5P to obtain data from database, train model, serialize it to a file, and use this serialized model to make predictions again.    OptionsToCode.java  ( stable ,  developer ) - turns a Weka command line for a scheme with options into Java code, correctly escaping quotes and backslashes.    OptionTree.java  ( stable ,  developer ) - displays nested Weka options as tree.    IncrementalClassifier.java  ( stable ,  developer ) - Example class for how to train an incremental classifier (in this case,  weka.classifiers.bayes.NaiveBayesUpdateable ).    IncrementalClusterer.java  ( stable ,  developer ) - Example class for how to train an incremental clusterer (in this case,  weka.clusterers.Cobweb ).",
            "title": "Examples"
        },
        {
            "location": "/use_weka_in_your_java_code/#links",
            "text": "Weka API  Stable version  Developer version",
            "title": "Links"
        },
        {
            "location": "/weka_examples/",
            "text": "The \nWeka Examples\n collection is a comprehensive collection of examples for the different versions of Weka in the form of an \nANT\n project. You can access these examples as follows:\n\n\nSnapshots\n\n\nThrough \nsnapshots\n or releases (they contain a separate ZIP file with the \nANT\n project)\n\n\nSubversion\n\n\nThrough \nsubversion\n\n\n\n\n\n\nstable-3.8 version (3.8.x): \n\n\nhttps://svn.cms.waikato.ac.nz/svn/weka/branches/stable-3-8/wekaexamples/\n\n\n\n\n\n\ndeveloper version (3.9.x): \n\n\nhttps://svn.cms.waikato.ac.nz/svn/weka/trunk/wekaexamples/",
            "title": " Weka Examples"
        },
        {
            "location": "/weka_examples/#snapshots",
            "text": "Through  snapshots  or releases (they contain a separate ZIP file with the  ANT  project)",
            "title": "Snapshots"
        },
        {
            "location": "/weka_examples/#subversion",
            "text": "Through  subversion    stable-3.8 version (3.8.x):   https://svn.cms.waikato.ac.nz/svn/weka/branches/stable-3-8/wekaexamples/    developer version (3.9.x):   https://svn.cms.waikato.ac.nz/svn/weka/trunk/wekaexamples/",
            "title": "Subversion"
        },
        {
            "location": "/generating_cv_folds/",
            "text": "You have two choices of generating cross-validation folds:\n\n\n\n\nFilter approach\n - uses a \nbash\n script to generate the train/test pairs beforehand\n\n\nJava approach\n - to be used from within your own Java code, creates train/test pairs on the fly",
            "title": " Generating cross-validation folds"
        },
        {
            "location": "/generating_cv_folds_filter/",
            "text": "The filter \nRemoveFolds\n (package \nweka.filters.unsupervised.instance\n) can be used to generate the train/test splits used in cross-validation (for stratified folds, use \nweka.filters.supervised.instance.StratifiedRemoveFolds\n). The filter has to be used twice for each train/test split, first to generate the train set and then to obtain the test set.\n\n\nSince this is rather cumbersome by hand, one can also put this into a \nbash\n script:\n\n\n #!/bin/bash\n #\n # expects the weka.jar as first parameter and the datasets to work on as \n # second parameter.\n #\n # FracPete, 2007-04-10\n\n if [ ! $# -eq 2 ]\n then\n   echo\n   echo \"usage: folds.sh <weka.jar> <dataset>\"\n   echo\n   exit 1\n fi\n\n JAR=$1\n DATASET=$2\n FOLDS=10\n FILTER=weka.filters.unsupervised.instance.RemoveFolds\n SEED=1\n\n for ((i = 1; i <= $FOLDS; i++))\n do\n   echo \"Generating pair $i/$FOLDS...\"\n\n   OUTFILE=`echo $DATASET | sed s/\"\\.arff\"//g`\n   # train set\n   java -cp $JAR $FILTER -V -N $FOLDS -F $i -S $SEED -i $DATASET -o \"$OUTFILE-train-$i-of-$FOLDS.arff\"\n   # test set\n   java -cp $JAR $FILTER    -N $FOLDS -F $i -S $SEED -i $DATASET -o \"$OUTFILE-test-$i-of-$FOLDS.arff\"\n done\n\n\n\n\nThe script expects two parameters:\n\n\n\n\nthe \nweka.jar\n (or the path to the Weka classes)\n\n\nthe dataset to generate the train/test pairs from \n\n\n\n\nExample:\n\n\n ./folds.sh /some/where/weka.jar /some/where/else/dataset.arff\n\n\n\n\nThis example will create the train/test splits for a 10-fold cross-validation at the same location as the original dataset, i.e., in the directory \n/some/where/else/\n.\n\n\nDownloads\n\n\n\n\nfolds.sh",
            "title": " Generating cross-validation folds (Filter approach)"
        },
        {
            "location": "/generating_cv_folds_filter/#downloads",
            "text": "folds.sh",
            "title": "Downloads"
        },
        {
            "location": "/generating_cv_folds_java/",
            "text": "This article describes how to generate train/test splits for \ncross-validation\n using the Weka API directly. \n\n\nThe following variables are given:\n\n\n Instances data =  ...;   // contains the full dataset we wann create train/test sets from\n int seed = ...;          // the seed for randomizing the data\n int folds = ...;         // the number of folds to generate, >=2\n\n\n\n\nRandomize the data\n\n\nFirst, randomize your data:\n\n\n Random rand = new Random(seed);   // create seeded number generator\n randData = new Instances(data);   // create copy of original data\n randData.randomize(rand);         // randomize data with number generator\n\n\n\n\nIn case your data has a nominal class and you wanna perform stratified cross-validation:\n\n\n randData.stratify(folds);\n\n\n\n\nGenerate the folds\n\n\nSingle run\n\n\nNext thing that we have to do is creating the train and the test set:\n\n\n for (int n = 0; n < folds; n++) {\n   Instances train = randData.trainCV(folds, n, rand);\n   Instances test = randData.testCV(folds, n);\n\n   // further processing, classification, etc.\n   ...\n }\n\n\n\n\nNote:\n\n\n\n\nthe above code is used by the \nweka.filters.supervised.instance.StratifiedRemoveFolds\n filter\n\n\nthe \nweka.classifiers.Evaluation\n class and the Explorer/Experimenter would use this method for obtaining the train set:\n[[code format=\"java\"]]\n Instances train = randData.trainCV(folds, n, rand);\n[[code]]\n\n\n\n\nMultiple runs\n\n\nThe example above only performs one run of a cross-validation. In case you want to run 10 runs of 10-fold cross-validation, use the following loop:\n\n\n Instances data = ...;  // our dataset again, obtained from somewhere\n int runs = 10;\n for (int i = 0; i < runs; i++) {\n   seed = i+1;  // every run gets a new, but defined seed value\n\n   // see: randomize the data\n   ...\n\n   // see: generate the folds\n   ...\n }\n\n\n\n\nSee also\n\n\n\n\nUse Weka in your Java code\n - for general use of the Weka API\n\n\n\n\nDownloads\n\n\n\n\nCrossValidationSingleRun.java\n (\nstable\n, \ndeveloper\n) - simulates a single run of 10-fold cross-validation\n\n\nCrossValidationSingleRunVariant.java\n (\nstable\n, \ndeveloper\n) - simulates a single run of 10-fold cross-validation, but outputs the confusion matrices for each single train/test pair as well.\n\n\nCrossValidationMultipleRuns.java\n (\nstable\n, \ndeveloper\n) - simulates 10 runs of 10-fold cross-validation\n\n\nCrossValidationAddPrediction.java\n (\nstable\n, \ndeveloper\n) - simulates a single run of 10-fold cross-validation, but also adds the classification/distribution/error flag to the test data (uses the \nAddClassification\n filter)",
            "title": " Generating cross-validation folds (Java approach)"
        },
        {
            "location": "/generating_cv_folds_java/#randomize-the-data",
            "text": "First, randomize your data:   Random rand = new Random(seed);   // create seeded number generator\n randData = new Instances(data);   // create copy of original data\n randData.randomize(rand);         // randomize data with number generator  In case your data has a nominal class and you wanna perform stratified cross-validation:   randData.stratify(folds);",
            "title": "Randomize the data"
        },
        {
            "location": "/generating_cv_folds_java/#generate-the-folds",
            "text": "",
            "title": "Generate the folds"
        },
        {
            "location": "/generating_cv_folds_java/#single-run",
            "text": "Next thing that we have to do is creating the train and the test set:   for (int n = 0; n < folds; n++) {\n   Instances train = randData.trainCV(folds, n, rand);\n   Instances test = randData.testCV(folds, n);\n\n   // further processing, classification, etc.\n   ...\n }  Note:   the above code is used by the  weka.filters.supervised.instance.StratifiedRemoveFolds  filter  the  weka.classifiers.Evaluation  class and the Explorer/Experimenter would use this method for obtaining the train set:\n[[code format=\"java\"]]\n Instances train = randData.trainCV(folds, n, rand);\n[[code]]",
            "title": "Single run"
        },
        {
            "location": "/generating_cv_folds_java/#multiple-runs",
            "text": "The example above only performs one run of a cross-validation. In case you want to run 10 runs of 10-fold cross-validation, use the following loop:   Instances data = ...;  // our dataset again, obtained from somewhere\n int runs = 10;\n for (int i = 0; i < runs; i++) {\n   seed = i+1;  // every run gets a new, but defined seed value\n\n   // see: randomize the data\n   ...\n\n   // see: generate the folds\n   ...\n }",
            "title": "Multiple runs"
        },
        {
            "location": "/generating_cv_folds_java/#see-also",
            "text": "Use Weka in your Java code  - for general use of the Weka API",
            "title": "See also"
        },
        {
            "location": "/generating_cv_folds_java/#downloads",
            "text": "CrossValidationSingleRun.java  ( stable ,  developer ) - simulates a single run of 10-fold cross-validation  CrossValidationSingleRunVariant.java  ( stable ,  developer ) - simulates a single run of 10-fold cross-validation, but outputs the confusion matrices for each single train/test pair as well.  CrossValidationMultipleRuns.java  ( stable ,  developer ) - simulates 10 runs of 10-fold cross-validation  CrossValidationAddPrediction.java  ( stable ,  developer ) - simulates a single run of 10-fold cross-validation, but also adds the classification/distribution/error flag to the test data (uses the  AddClassification  filter)",
            "title": "Downloads"
        },
        {
            "location": "/generating_cv_folds/",
            "text": "You have two choices of generating cross-validation folds:\n\n\n\n\nFilter approach\n - uses a \nbash\n script to generate the train/test pairs beforehand\n\n\nJava approach\n - to be used from within your own Java code, creates train/test pairs on the fly",
            "title": " Generating cross-validation folds"
        },
        {
            "location": "/creating_arff_file/",
            "text": "The following code generates an \nInstances\n object and outputs it to stdout as \nARFF\n file.\n\n\nIt generates the following types of attributes:\n\n\n\n\nnumeric\n\n\nnominal\n\n\nstring\n\n\ndate\n\n\nrelational\n\n\n\n\nExample class \nAttTest\n:\n\n\n import weka.core.Attribute;\n import weka.core.FastVector;\n import weka.core.Instance;\n import weka.core.Instances;\n\n /**\n  * Generates a little ARFF file with different attribute types.\n  *\n  * @author FracPete\n  */\n public class AttTest {\n   public static void main(String[] args) throws Exception {\n     FastVector      atts;\n     FastVector      attsRel;\n     FastVector      attVals;\n     FastVector      attValsRel;\n     Instances       data;\n     Instances       dataRel;\n     double[]        vals;\n     double[]        valsRel;\n     int             i;\n\n     // 1. set up attributes\n     atts = new FastVector();\n     // - numeric\n     atts.addElement(new Attribute(\"att1\"));\n     // - nominal\n     attVals = new FastVector();\n     for (i = 0; i < 5; i++)\n       attVals.addElement(\"val\" + (i+1));\n     atts.addElement(new Attribute(\"att2\", attVals));\n     // - string\n     atts.addElement(new Attribute(\"att3\", (FastVector) null));\n     // - date\n     atts.addElement(new Attribute(\"att4\", \"yyyy-MM-dd\"));\n     // - relational\n     attsRel = new FastVector();\n     // -- numeric\n     attsRel.addElement(new Attribute(\"att5.1\"));\n     // -- nominal\n     attValsRel = new FastVector();\n     for (i = 0; i < 5; i++)\n       attValsRel.addElement(\"val5.\" + (i+1));\n     attsRel.addElement(new Attribute(\"att5.2\", attValsRel));\n     dataRel = new Instances(\"att5\", attsRel, 0);\n     atts.addElement(new Attribute(\"att5\", dataRel, 0));\n\n     // 2. create Instances object\n     data = new Instances(\"MyRelation\", atts, 0);\n\n     // 3. fill with data\n     // first instance\n     vals = new double[data.numAttributes()];\n     // - numeric\n     vals[0] = Math.PI;\n     // - nominal\n     vals[1] = attVals.indexOf(\"val3\");\n     // - string\n     vals[2] = data.attribute(2).addStringValue(\"This is a string!\");\n     // - date\n     vals[3] = data.attribute(3).parseDate(\"2001-11-09\");\n     // - relational\n     dataRel = new Instances(data.attribute(4).relation(), 0);\n     // -- first instance\n     valsRel = new double[2];\n     valsRel[0] = Math.PI + 1;\n     valsRel[1] = attValsRel.indexOf(\"val5.3\");\n     dataRel.add(new Instance(1.0, valsRel));\n     // -- second instance\n     valsRel = new double[2];\n     valsRel[0] = Math.PI + 2;\n     valsRel[1] = attValsRel.indexOf(\"val5.2\");\n     dataRel.add(new Instance(1.0, valsRel));\n     vals[4] = data.attribute(4).addRelation(dataRel);\n     // add\n     data.add(new Instance(1.0, vals));\n\n     // second instance\n     vals = new double[data.numAttributes()];  // important: needs NEW array!\n     // - numeric\n     vals[0] = Math.E;\n     // - nominal\n     vals[1] = attVals.indexOf(\"val1\");\n     // - string\n     vals[2] = data.attribute(2).addStringValue(\"And another one!\");\n     // - date\n     vals[3] = data.attribute(3).parseDate(\"2000-12-01\");\n     // - relational\n     dataRel = new Instances(data.attribute(4).relation(), 0);\n     // -- first instance\n     valsRel = new double[2];\n     valsRel[0] = Math.E + 1;\n     valsRel[1] = attValsRel.indexOf(\"val5.4\");\n     dataRel.add(new Instance(1.0, valsRel));\n     // -- second instance\n     valsRel = new double[2];\n     valsRel[0] = Math.E + 2;\n     valsRel[1] = attValsRel.indexOf(\"val5.1\");\n     dataRel.add(new Instance(1.0, valsRel));\n     vals[4] = data.attribute(4).addRelation(dataRel);\n     // add\n     data.add(new Instance(1.0, vals));\n\n     // 4. output data\n     System.out.println(data);\n   }\n }\n\n\n\n\nMissing values\n\n\nBy default, a new double array will be initialized with 0s. In case you want to be a value missing at a certain position, you have to explicitly set the \nmissing value\n via the \nmissingValue()\n method of the \nweka.core.Instance\n class. In Weka > 3.7.1 Instance is an interface, so \nmissingValue()\n moved into \nweka.core.Utils\n. In case you already have an existing \nweka.core.Instance\n object, then you use its \nsetMissing(int)\n method, which sets a missing value at the given position. Here are examples, which set the \nthird\n attribute to missing:\n\n\n\n\n\n\ndouble array:\n\n\ndouble[] vals = ...  // from somewhere, e.g., from AttTest.java example\nvals[2] = Instance.missingValue(); // or ... = Utils.missingValue() for Weka > 3.7.1\n\n\n\n\n\n\nweka.core.Instance\n object:\n\n\ndouble[] vals = ... // from somewhere, e.g., from AttTest.java example\nInstance inst = new Instance(1.0, vals);\ninst.setMissing(2);\n\n\n\n\n\n\nDownloads\n\n\n\n\nAttTest.java\n (\nstable\n, \ndeveloper\n) - the above class\n\n\n\n\nSee also\n\n\n\n\nSave Instances to an ARFF File\n - if you want to save the data to a file instead of printing it to stdout\n\n\nAdding attributes to a dataset\n - shows how to add attributes to an existing dataset\n\n\nARFF\n format",
            "title": " Creating an ARFF file"
        },
        {
            "location": "/creating_arff_file/#missing-values",
            "text": "By default, a new double array will be initialized with 0s. In case you want to be a value missing at a certain position, you have to explicitly set the  missing value  via the  missingValue()  method of the  weka.core.Instance  class. In Weka > 3.7.1 Instance is an interface, so  missingValue()  moved into  weka.core.Utils . In case you already have an existing  weka.core.Instance  object, then you use its  setMissing(int)  method, which sets a missing value at the given position. Here are examples, which set the  third  attribute to missing:    double array:  double[] vals = ...  // from somewhere, e.g., from AttTest.java example\nvals[2] = Instance.missingValue(); // or ... = Utils.missingValue() for Weka > 3.7.1    weka.core.Instance  object:  double[] vals = ... // from somewhere, e.g., from AttTest.java example\nInstance inst = new Instance(1.0, vals);\ninst.setMissing(2);",
            "title": "Missing values"
        },
        {
            "location": "/creating_arff_file/#downloads",
            "text": "AttTest.java  ( stable ,  developer ) - the above class",
            "title": "Downloads"
        },
        {
            "location": "/creating_arff_file/#see-also",
            "text": "Save Instances to an ARFF File  - if you want to save the data to a file instead of printing it to stdout  Adding attributes to a dataset  - shows how to add attributes to an existing dataset  ARFF  format",
            "title": "See also"
        },
        {
            "location": "/creating_instances/",
            "text": "see \nCreating an ARFF file",
            "title": " Creating Instances on-the-fly"
        },
        {
            "location": "/binarize_attribute/",
            "text": "Sometimes one wants to binarize a nominal attribute of a certain dataset by grouping all values except the one of interest together as a negation of this value. E.g., in the {{weather}} data the outlook attribute, where \nsunny\n is of interest and the other values, \nrainy\n and \novercast\n, are grouped together as \nnot-sunny\n.\n\n\nOriginal dataset:\n\n\n @relation weather\n\n @attribute outlook {sunny, overcast, rainy}\n @attribute temperature real\n @attribute humidity real\n @attribute windy {TRUE, FALSE}\n @attribute play {yes, no}\n\n @data\n sunny,85,85,FALSE,no\n sunny,80,90,TRUE,no\n overcast,83,86,FALSE,yes\n rainy,70,96,FALSE,yes\n rainy,68,80,FALSE,yes\n rainy,65,70,TRUE,no\n overcast,64,65,TRUE,yes\n sunny,72,95,FALSE,no\n sunny,69,70,FALSE,yes\n rainy,75,80,FALSE,yes\n sunny,75,70,TRUE,yes\n overcast,72,90,TRUE,yes\n overcast,81,75,FALSE,yes\n rainy,71,91,TRUE,no\n\n\n\n\nDesired output:\n\n\n @relation weather-sunny-and-not_sunny\n\n @attribute outlook {sunny,not_sunny}\n @attribute temperature numeric\n @attribute humidity numeric\n @attribute windy {TRUE,FALSE}\n @attribute play {yes,no}\n\n @data\n sunny,85,85,FALSE,no\n sunny,80,90,TRUE,no\n not_sunny,83,86,FALSE,yes\n not_sunny,70,96,FALSE,yes\n not_sunny,68,80,FALSE,yes\n not_sunny,65,70,TRUE,no\n not_sunny,64,65,TRUE,yes\n sunny,72,95,FALSE,no\n sunny,69,70,FALSE,yes\n not_sunny,75,80,FALSE,yes\n sunny,75,70,TRUE,yes\n not_sunny,72,90,TRUE,yes\n not_sunny,81,75,FALSE,yes\n not_sunny,71,91,TRUE,no\n\n\n\n\nThe Weka filter \nNominalToBinary\n cannot be used directly, since it generates a new attribute for each value of the nominal attribute. As a postprocessing step one could delete all the attributes that are of no interest, but this is quite cumbersome.\n\n\nThe \nBinarize.java\n class on the other hand generates directly several \nARFF\n out of a given one in the desired format.\n\n\nDownload\n\n\n\n\nBinarize.java\n (\nstable\n, \ndeveloper",
            "title": " Binarize Attribute"
        },
        {
            "location": "/binarize_attribute/#download",
            "text": "Binarize.java  ( stable ,  developer",
            "title": "Download"
        },
        {
            "location": "/arff_from_text_collections/",
            "text": "The following utility generates an \nARFF\n file from text documents in a given directory (download link is at the end of this article).\n\n\nThe stable/developer version of Weka offer this tool as the \nweka.core.converters.TextDirectoryLoader\n converter. This can be used as:\n\n\njava -cp <path to weka.jar> weka.core.converters.TextDirectoryLoader -dir .\n\n\n\n\nFor help just type:\n\n\njava -cp <path to weka.jar> weka.core.converters.TextDirectoryLoader\n\n\n\n\n/*\n *    TextDirectoryToArff.java\n *    Copyright (C) 2002 Richard Kirkby\n *\n *    This program is free software; you can redistribute it and/or modify\n *    it under the terms of the GNU General Public License as published by\n *    the Free Software Foundation; either version 2 of the License, or\n *    (at your option) any later version.\n *\n *    This program is distributed in the hope that it will be useful,\n *    but WITHOUT ANY WARRANTY; without even the implied warranty of\n *    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n *    GNU General Public License for more details.\n *\n *    You should have received a copy of the GNU General Public License\n *    along with this program; if not, write to the Free Software\n *    Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA.\n */\n\nimport java.io.*;\nimport weka.core.*;\n\n/**\n * Builds an arff dataset from the documents in a given directory.\n * Assumes that the file names for the documents end with \".txt\".\n *\n * Usage:<p/>\n *\n * TextDirectoryToArff <directory path> <p/>\n *\n * @author Richard Kirkby (rkirkby at cs.waikato.ac.nz)\n * @version 1.0\n */\npublic class TextDirectoryToArff {\n\n  public Instances createDataset(String directoryPath) throws Exception {\n\n    FastVector atts = new FastVector(2);\n    atts.addElement(new Attribute(\"filename\", (FastVector) null));\n    atts.addElement(new Attribute(\"contents\", (FastVector) null));\n    Instances data = new Instances(\"text_files_in_\" + directoryPath, atts, 0);\n\n    File dir = new File(directoryPath);\n    String[] files = dir.list();\n    for (int i = 0; i < files.length; i++) {\n      if (files[i].endsWith(\".txt\")) {\n    try {\n      double[] newInst = new double[2];\n      newInst[0] = (double)data.attribute(0).addStringValue(files[i]);\n      File txt = new File(directoryPath + File.separator + files[i]);\n      InputStreamReader is;\n      is = new InputStreamReader(new FileInputStream(txt));\n      StringBuffer txtStr = new StringBuffer();\n      int c;\n      while ((c = is.read()) != -1) {\n        txtStr.append((char)c);\n      }\n      newInst[1] = (double)data.attribute(1).addStringValue(txtStr.toString());\n      data.add(new Instance(1.0, newInst));\n    } catch (Exception e) {\n      //System.err.println(\"failed to convert file: \" + directoryPath + File.separator + files[i]);\n    }\n      }\n    }\n    return data;\n  }\n\n  public static void main(String[] args) {\n\n    if (args.length == 1) {\n      TextDirectoryToArff tdta = new TextDirectoryToArff();\n      try {\n    Instances dataset = tdta.createDataset(args[0]);\n    System.out.println(dataset);\n      } catch (Exception e) {\n    System.err.println(e.getMessage());\n    e.printStackTrace();\n      }\n    } else {\n      System.out.println(\"Usage: java TextDirectoryToArff <directory name>\");\n    }\n  }\n}\n\n\n\n\nSee also\n\n\n\n\nText categorization with Weka\n\n\n\n\nDownloads\n\n\n\n\nTextDirectoryToArff.java",
            "title": " ARFF files from Text Collections"
        },
        {
            "location": "/arff_from_text_collections/#see-also",
            "text": "Text categorization with Weka",
            "title": "See also"
        },
        {
            "location": "/arff_from_text_collections/#downloads",
            "text": "TextDirectoryToArff.java",
            "title": "Downloads"
        },
        {
            "location": "/adding_attributes_to_dataset/",
            "text": "The following example class adds a \nnominal\n and a \nnumeric\n attribute to the dataset identified by the filename given as first parameter. The second parameter defines whether the data is manipulated via the \nAdd\n filter (= \nfilter\n) or through the Weka API directly (= \njava\n).\n\n\nUsage:\n\n\n AddAttribute <file.arff> <filter|java>\n\n\n\n\nSource code:\n\n\n  import weka.core.*;\n  import weka.filters.Filter;\n  import weka.filters.unsupervised.attribute.Add;\n\n  import java.io.*;\n  import java.util.*;\n\n  /**\n   * Adds a nominal and a numeric attribute to the dataset provided as first\n   * parameter (and fills it with random values) and outputs the result to\n   * stdout. It's either done via the Add filter (first option \"filter\")\n   * or manual with Java (second option \"java\").\n   *\n   * Usage: AddAttribute &lt;file.arff&gt; &lt;filter|java&gt;\n   *\n   * @author FracPete (fracpete at waikato dot ac dot nz)\n   */\n  public class AddAttribute {\n    /**\n     * adds the attributes\n     *\n     * @param args    the commandline arguments\n     */\n    public static void main(String[] args) throws Exception {\n      if (args.length != 2) {\n        System.out.println(\"\\nUsage: AddAttribute <file.arff> <filter|java>\\n\");\n        System.exit(1);\n      }\n\n      // load dataset\n      Instances data = new Instances(new BufferedReader(new FileReader(args[0])));\n      Instances newData = null;\n\n      // filter or java?\n      if (args[1].equals(\"filter\")) {\n        Add filter;\n        newData = new Instances(data);\n        // 1. nominal attribute\n        filter = new Add();\n        filter.setAttributeIndex(\"last\");\n        filter.setNominalLabels(\"A,B,C,D\");\n        filter.setAttributeName(\"NewNominal\");\n        filter.setInputFormat(newData);\n        newData = Filter.useFilter(newData, filter);\n        // 2. numeric attribute\n        filter = new Add();\n        filter.setAttributeIndex(\"last\");\n        filter.setAttributeName(\"NewNumeric\");\n        filter.setInputFormat(newData);\n        newData = Filter.useFilter(newData, filter);\n      }\n      else if (args[1].equals(\"java\")) {\n        newData = new Instances(data);\n        // add new attributes\n        // 1. nominal\n        FastVector values = new FastVector(); /* FastVector is now deprecated. Users can use any java.util.List */\n        values.addElement(\"A\");               /* implementation now */\n        values.addElement(\"B\");\n        values.addElement(\"C\");\n        values.addElement(\"D\");\n        newData.insertAttributeAt(new Attribute(\"NewNominal\", values), newData.numAttributes());\n        // 2. numeric\n        newData.insertAttributeAt(new Attribute(\"NewNumeric\"), newData.numAttributes());\n      }\n      else {\n        System.out.println(\"\\nUsage: AddAttribute <file.arff> <filter|java>\\n\");\n        System.exit(2);\n      }\n\n      // random values\n      Random rand = new Random(1);\n      for (int i = 0; i < newData.numInstances(); i++) {\n        // 1. nominal\n        // index of labels A:0,B:1,C:2,D:3\n        newData.instance(i).setValue(newData.numAttributes() - 2, rand.nextInt(4));\n        // 2. numeric\n        newData.instance(i).setValue(newData.numAttributes() - 1, rand.nextDouble());\n      }\n\n      // output on stdout\n      System.out.println(newData);\n    }\n  }\n\n\n\n\nSee also\n\n\n\n\nCreating an ARFF file\n - explains the creation of all the different attribute types\n\n\nUse Weka in your Java code\n - for general usage of the Weka API\n\n\nSave Instances to an ARFF File\n - if you want to save the output to a file instead of printing them to stdout\n\n\n\n\nDownloads\n\n\n\n\nAddAttribute.java\n (\nstable\n, \ndeveloper\n)",
            "title": " Adding attributes to a dataset"
        },
        {
            "location": "/adding_attributes_to_dataset/#see-also",
            "text": "Creating an ARFF file  - explains the creation of all the different attribute types  Use Weka in your Java code  - for general usage of the Weka API  Save Instances to an ARFF File  - if you want to save the output to a file instead of printing them to stdout",
            "title": "See also"
        },
        {
            "location": "/adding_attributes_to_dataset/#downloads",
            "text": "AddAttribute.java  ( stable ,  developer )",
            "title": "Downloads"
        },
        {
            "location": "/save_instances_to_arff/",
            "text": "DataSink\n\n\nThe easiest way to save an \nweka.core.Instances\n object to a file is by using\nthe \nweka.core.converters.ConverterUtils.DataSink\n class.\n\n\n import weka.core.converters.ConverterUtils.DataSink;\n import weka.core.Instances;\n\n Instances dataset = ...\n String outputFilename = ...\n try {\n   DataSink.write(outputFilename, dataset);\n }\n catch (Exception e) {\n   System.err.println(\"Failed to save data to: \" + outputFilename);\n   e.printStackTrace();\n }\n\n\n\n\nConverter\n\n\nYou can use the \nArffSaver\n class (\nweka.core.converters.ArffSaver\n) for saving a \nweka.core.Instances\n object to a file.\n\n\nHere is the snippet :\n\n\n Instances dataSet = ...\n ArffSaver saver = new ArffSaver();\n saver.setInstances(dataSet);\n saver.setFile(new File(\"./data/test.arff\"));\n saver.writeBatch();\n\n\n\n\nNotes:\n \n\n\n\n\nusing the converter approach, one can easily swap the \nArffSaver\n with another saver, e.g., the \nCSVSaver\n to output the data in a different format.\n\n\nThe \nWeka Examples\n collection dedicates quite a few examples to the use of converters in the \nwekaexamples.core.converters\n package (\nstable\n, \ndeveloper\n)\n\n\n\n\nJava I/O\n\n\nYou can also save the \nweka.core.Instances\n object directly using Java I/O classes:\n\n\n import java.io.BufferedWriter;\n import java.io.FileWriter;\n ...\n Instances dataSet = ...\n BufferedWriter writer = new BufferedWriter(new FileWriter(\"./data/test.arff\"));\n writer.write(dataSet.toString());\n writer.flush();\n writer.close();\n\n\n\n\nNote:\n using the \ntoString()\n of the \nweka.core.Instances\n doesn't scale very well for large datasets, since the complete string has to fit into memory. It is best to use a converter, as described in the previous section, which uses an incremental approach for writing the dataset to disk.",
            "title": " Save Instances to an ARFF File"
        },
        {
            "location": "/save_instances_to_arff/#datasink",
            "text": "The easiest way to save an  weka.core.Instances  object to a file is by using\nthe  weka.core.converters.ConverterUtils.DataSink  class.   import weka.core.converters.ConverterUtils.DataSink;\n import weka.core.Instances;\n\n Instances dataset = ...\n String outputFilename = ...\n try {\n   DataSink.write(outputFilename, dataset);\n }\n catch (Exception e) {\n   System.err.println(\"Failed to save data to: \" + outputFilename);\n   e.printStackTrace();\n }",
            "title": "DataSink"
        },
        {
            "location": "/save_instances_to_arff/#converter",
            "text": "You can use the  ArffSaver  class ( weka.core.converters.ArffSaver ) for saving a  weka.core.Instances  object to a file.  Here is the snippet :   Instances dataSet = ...\n ArffSaver saver = new ArffSaver();\n saver.setInstances(dataSet);\n saver.setFile(new File(\"./data/test.arff\"));\n saver.writeBatch();  Notes:     using the converter approach, one can easily swap the  ArffSaver  with another saver, e.g., the  CSVSaver  to output the data in a different format.  The  Weka Examples  collection dedicates quite a few examples to the use of converters in the  wekaexamples.core.converters  package ( stable ,  developer )",
            "title": "Converter"
        },
        {
            "location": "/save_instances_to_arff/#java-io",
            "text": "You can also save the  weka.core.Instances  object directly using Java I/O classes:   import java.io.BufferedWriter;\n import java.io.FileWriter;\n ...\n Instances dataSet = ...\n BufferedWriter writer = new BufferedWriter(new FileWriter(\"./data/test.arff\"));\n writer.write(dataSet.toString());\n writer.flush();\n writer.close();  Note:  using the  toString()  of the  weka.core.Instances  doesn't scale very well for large datasets, since the complete string has to fit into memory. It is best to use a converter, as described in the previous section, which uses an incremental approach for writing the dataset to disk.",
            "title": "Java I/O"
        },
        {
            "location": "/generating_roc_curve/",
            "text": "The following little Java class trains a NaiveBayes classifier with a dataset provided by the user and displays the ROC curve for the first class label.\n\n\nSource code:\n\n\n import java.awt.*;\n import java.io.*;\n import java.util.*;\n import javax.swing.*;\n import weka.core.*;\n import weka.classifiers.*;\n import weka.classifiers.bayes.NaiveBayes;\n import weka.classifiers.evaluation.Evaluation;\n import weka.classifiers.evaluation.ThresholdCurve;\n import weka.gui.visualize.*;\n\n /**\n   * Generates and displays a ROC curve from a dataset. Uses a default\n   * NaiveBayes to generate the ROC data.\n   *\n   * @author FracPete\n   */\n public class GenerateROC {\n\n   /**\n    * takes one argument: dataset in ARFF format (expects class to\n    * be last attribute)\n    */\n   public static void main(String[] args) throws Exception {\n     // load data\n     Instances data = new Instances(\n                           new BufferedReader(\n                             new FileReader(args[0])));\n     data.setClassIndex(data.numAttributes() - 1);\n\n     // train classifier\n     Classifier cl = new NaiveBayes();\n     Evaluation eval = new Evaluation(data);\n     eval.crossValidateModel(cl, data, 10, new Random(1));\n\n     // generate curve\n     ThresholdCurve tc = new ThresholdCurve();\n     int classIndex = 0;\n     Instances result = tc.getCurve(eval.predictions(), classIndex);\n\n     // plot curve\n     ThresholdVisualizePanel vmc = new ThresholdVisualizePanel();\n     vmc.setROCString(\"(Area under ROC = \" +\n         Utils.doubleToString(tc.getROCArea(result), 4) + \")\");\n     vmc.setName(result.relationName());\n     PlotData2D tempd = new PlotData2D(result);\n     tempd.setPlotName(result.relationName());\n     tempd.addInstanceNumberAttribute();\n     // specify which points are connected\n     boolean[] cp = new boolean[result.numInstances()];\n     for (int n = 1; n < cp.length; n++)\n       cp[n] = true;\n     tempd.setConnectPoints(cp);\n     // add plot\n     vmc.addPlot(tempd);\n\n     // display curve\n     String plotName = vmc.getName();\n     final javax.swing.JFrame jf =\n       new javax.swing.JFrame(\"Weka Classifier Visualize: \"+plotName);\n     jf.setSize(500,400);\n     jf.getContentPane().setLayout(new BorderLayout());\n     jf.getContentPane().add(vmc, BorderLayout.CENTER);\n     jf.addWindowListener(new java.awt.event.WindowAdapter() {\n       public void windowClosing(java.awt.event.WindowEvent e) {\n       jf.dispose();\n       }\n     });\n     jf.setVisible(true);\n   }\n }\n\n\n\n\nSee also\n\n\n\n\nROC curves\n\n\nVisualizing ROC curve\n\n\nPlotting multiple ROC curves\n\n\n\n\nDownloads\n\n\n\n\nGenerateROC.java\n (\nstable\n, \ndeveloper\n)",
            "title": " Generating ROC curve"
        },
        {
            "location": "/generating_roc_curve/#see-also",
            "text": "ROC curves  Visualizing ROC curve  Plotting multiple ROC curves",
            "title": "See also"
        },
        {
            "location": "/generating_roc_curve/#downloads",
            "text": "GenerateROC.java  ( stable ,  developer )",
            "title": "Downloads"
        },
        {
            "location": "/visualizing_roc_curve/",
            "text": "The following class lets you display a previously saved [[ROC curves|ROC curve]], which also displays the \nAUC\n.\n\n\nIf you don't need the \nAUC\n, then you can also use this command to display the curve:\n\n\n java [CLASSPATH|-classpath <your-classpath>] weka.gui.visualize.VisualizePanel <file>\n\n\n\n\nSource code:\n\n\n  import java.awt.*;\n  import java.io.*;\n  import javax.swing.*;\n  import weka.core.*;\n  import weka.classifiers.evaluation.*;\n  import weka.gui.visualize.*;\n\n  /**\n   * Visualizes a previously saved ROC curve. Code taken from the \n   * <code>weka.gui.explorer.ClassifierPanel</code> - involved methods:\n   * <ul>\n   *    <li>visualize(String,int,int)</li>\n   *    </li>visualizeClassifierErrors(VisualizePanel)</li>\n   * </ul>\n   *\n   * @author FracPete\n   */\n  public class VisualizeROC {\n\n    /**\n     * takes one argument: previously saved ROC curve data (ARFF file)\n     */\n    public static void main(String[] args) throws Exception {\n      Instances result = new Instances(\n                            new BufferedReader(\n                              new FileReader(args[0])));\n      result.setClassIndex(result.numAttributes() - 1);\n      ThresholdCurve tc = new ThresholdCurve();\n      // method visualize\n      ThresholdVisualizePanel vmc = new ThresholdVisualizePanel();\n      vmc.setROCString(\"(Area under ROC = \" + \n          Utils.doubleToString(tc.getROCArea(result), 4) + \")\");\n      vmc.setName(result.relationName());\n      PlotData2D tempd = new PlotData2D(result);\n      tempd.setPlotName(result.relationName());\n      tempd.addInstanceNumberAttribute();\n      // specify which points are connected\n      boolean[] cp = new boolean[result.numInstances()];\n      for (int n = 1; n < cp.length; n++)\n        cp[n] = true;\n      tempd.setConnectPoints(cp);\n      // add plot\n      vmc.addPlot(tempd);\n      // method visualizeClassifierErrors\n      String plotName = vmc.getName(); \n      final javax.swing.JFrame jf = \n        new javax.swing.JFrame(\"Weka Classifier Visualize: \"+plotName);\n      jf.setSize(500,400);\n      jf.getContentPane().setLayout(new BorderLayout());\n\n      jf.getContentPane().add(vmc, BorderLayout.CENTER);\n      jf.addWindowListener(new java.awt.event.WindowAdapter() {\n        public void windowClosing(java.awt.event.WindowEvent e) {\n        jf.dispose();\n        }\n      });\n\n      jf.setVisible(true);\n    }\n  }\n\n\n\n\nSee also\n\n\n\n\nROC curves\n\n\nPlotting multiple ROC curves\n - also contains a Java example of plotting multiple ROC curves in a single plot\n\n\n\n\nDownloads\n\n\n\n\nVisualizeROC.java\n (\nstable\n, \ndeveloper\n)",
            "title": " Visualizing ROC curve"
        },
        {
            "location": "/visualizing_roc_curve/#see-also",
            "text": "ROC curves  Plotting multiple ROC curves  - also contains a Java example of plotting multiple ROC curves in a single plot",
            "title": "See also"
        },
        {
            "location": "/visualizing_roc_curve/#downloads",
            "text": "VisualizeROC.java  ( stable ,  developer )",
            "title": "Downloads"
        },
        {
            "location": "/serialization/",
            "text": "Serialization\n is the process of saving an object in a persistent form, e.g., on the harddisk as a bytestream. \nDeserialization\n is the process in the opposite direction, creating an object from a persistently saved data structure.\nIn \nJava\n, an object can be serialized if it imports the \njava.io.Serializable\n interface. \n\n\nMembers of an object that are not supposed to be serialized, need to be prefixed with the keyword \ntransient\n.\n\n\nIn the following you'll find some Java code snippets for serializing and deserializing a \nJ48\n classifier. Of course, serialization is not limited to classifiers. Most schemes in Weka, like clusterers and filters, are also serializable.\n\n\nSerializing\n\n\nHere we create a J48 classifier \ncls\n, train it with a dataset \n/some/where/data.arff\n, and save the built model to a file \n/some/where/j48.model\n.\n\n\n // create J48\n Classifier cls = new J48();\n\n // train\n Instances inst = new Instances(\n                    new BufferedReader(\n                      new FileReader(\"/some/where/data.arff\")));\n inst.setClassIndex(inst.numAttributes() - 1);\n cls.buildClassifier(inst);\n\n // serialize model\n ObjectOutputStream oos = new ObjectOutputStream(\n                            new FileOutputStream(\"/some/where/j48.model\"));\n oos.writeObject(cls);\n oos.flush();\n oos.close();\n\n\n\n\nIf you use the \nSerializationHelper\n class, then this shrinks to:\n\n\n // serialize model\n weka.core.SerializationHelper.write(\"/some/where/j48.model\", cls);\n\n\n\n\nDeserializing\n\n\nHere the previously saved model is deserialized as \ncls\n and again available for classification.\n\n\n // deserialize model\n ObjectInputStream ois = new ObjectInputStream(\n                           new FileInputStream(\"/some/where/j48.model\"));\n Classifier cls = (Classifier) ois.readObject();\n ois.close();\n\n\n\n\nOr, with the \nSerializationHelper\n class:\n\n\n // deserialize model\n Classifier cls = (Classifier) weka.core.SerializationHelper.read(\"/some/where/j48.model\");\n\n\n\n\nSerialization in Weka\n\n\nThe Explorer serializes the classifier and the training header together. This makes it easy to test whether a dataset is compatible with the dataset the classifier was trained with. The commandline option \n-d \n of the \ndeveloper version\n stores the training header as well.\n\n\nIn order to read serialized models that contain the header information as well, you can use the \nreadAll\n method of the \nweka.core.SerializationHelper\n. For serializing models with their datasets, use \nwriteAll\n.\n\n\nSee also\n\n\n\n\nUse Weka in your Java code\n\n\n\n\nLinks\n\n\n\n\nJava Serialization Specifications",
            "title": " Serialization"
        },
        {
            "location": "/serialization/#serializing",
            "text": "Here we create a J48 classifier  cls , train it with a dataset  /some/where/data.arff , and save the built model to a file  /some/where/j48.model .   // create J48\n Classifier cls = new J48();\n\n // train\n Instances inst = new Instances(\n                    new BufferedReader(\n                      new FileReader(\"/some/where/data.arff\")));\n inst.setClassIndex(inst.numAttributes() - 1);\n cls.buildClassifier(inst);\n\n // serialize model\n ObjectOutputStream oos = new ObjectOutputStream(\n                            new FileOutputStream(\"/some/where/j48.model\"));\n oos.writeObject(cls);\n oos.flush();\n oos.close();  If you use the  SerializationHelper  class, then this shrinks to:   // serialize model\n weka.core.SerializationHelper.write(\"/some/where/j48.model\", cls);",
            "title": "Serializing"
        },
        {
            "location": "/serialization/#deserializing",
            "text": "Here the previously saved model is deserialized as  cls  and again available for classification.   // deserialize model\n ObjectInputStream ois = new ObjectInputStream(\n                           new FileInputStream(\"/some/where/j48.model\"));\n Classifier cls = (Classifier) ois.readObject();\n ois.close();  Or, with the  SerializationHelper  class:   // deserialize model\n Classifier cls = (Classifier) weka.core.SerializationHelper.read(\"/some/where/j48.model\");",
            "title": "Deserializing"
        },
        {
            "location": "/serialization/#serialization-in-weka",
            "text": "The Explorer serializes the classifier and the training header together. This makes it easy to test whether a dataset is compatible with the dataset the classifier was trained with. The commandline option  -d   of the  developer version  stores the training header as well.  In order to read serialized models that contain the header information as well, you can use the  readAll  method of the  weka.core.SerializationHelper . For serializing models with their datasets, use  writeAll .",
            "title": "Serialization in Weka"
        },
        {
            "location": "/serialization/#see-also",
            "text": "Use Weka in your Java code",
            "title": "See also"
        },
        {
            "location": "/serialization/#links",
            "text": "Java Serialization Specifications",
            "title": "Links"
        },
        {
            "location": "/jupyter_notebooks/",
            "text": "Jupyter notebooks\n are extremely popular in the Python world,\nsimply because it is great to combine documentation and code in a visually appealing\nway. Great tool for teaching!\n\n\nThanks to the \nIJava kernel\n and the JDK 9+\n\nJShell\n feature, it is possible to run Java within Notebooks without compiling the\ncode now as well.\n\n\nInstallation on Linux\n\n\nThe following worked on Linux Mint 18.2:\n\n\n\n\n\n\ncreate a directory called \nweka-notebooks\n\n\nmkdir weka-notebooks\n\n\n\n\n\n\nchange into the directory and create a Python virtual environment:\n\n\ncd weka-notebooks\nvirtualenv -p /usr/bin/python3.5 venv\n\n\n\n\n\n\ninstall Jupyter notebooks and its dependencies:\n\n\nvenv/bin/pip install jupyter\n\n\n\n\n\n\nthen download the latest IJava \nrelease\n (at time of writing, this was \n1.20\n) into this directory\n\n\n\n\n\n\nunzip the IJava archive:\n\n\nunzip -q ijava*.zip\n\n\n\n\n\n\ninstall the Java kernel into the virtual environment, using the IJava installer:\n\n\nvenv/bin/python install.py --sys-prefix\n\n\n\n\n\n\nafter that, fire up Jupyter using:\n\n\nvenv/bin/jupyter-notebook\n\n\n\n\n\n\nnow you can create new (Java) notebooks!\n\n\n\n\n\n\nInstallation on Windows (using anaconda)\n\n\n\n\nopen a command prompt\n\n\n\n\ncreate a new environment using anaconda (e.g., for Python 3.5)\n\n\nconda create -n py35-ijava python=3.5\n\n\n\n\n\n\nactivate environment \n\n\nactivate py35-ijava\n\n\n\n\n\n\ninstall Jupyter\n\n\npip install jupyter\n\n\n\n\n\n\ndownload the latest IJava \nrelease\n (at time of writing, this was \n1.20\n)\n\n\n\n\nunzip the IJava release (e.g., with your File browser or 7-Zip)\n\n\n\n\nchange into the directory where you extracted the release, containing the \ninstall.py\n, e.g.:\n\n\ncd C:\\Users\\fracpete\\Downloads\\ijava-1.2.0\n\n\n\n\n\n\ninstall the kernel\n\n\npython install.py --sys-prefix\n\n\n\n\n\n\nstart Jupyter\n\n\njupyter-notebook\n\n\n\n\n\n\nnow you can create new (Java) notebooks!",
            "title": " Jupyter Notebooks"
        },
        {
            "location": "/jupyter_notebooks/#installation-on-linux",
            "text": "The following worked on Linux Mint 18.2:    create a directory called  weka-notebooks  mkdir weka-notebooks    change into the directory and create a Python virtual environment:  cd weka-notebooks\nvirtualenv -p /usr/bin/python3.5 venv    install Jupyter notebooks and its dependencies:  venv/bin/pip install jupyter    then download the latest IJava  release  (at time of writing, this was  1.20 ) into this directory    unzip the IJava archive:  unzip -q ijava*.zip    install the Java kernel into the virtual environment, using the IJava installer:  venv/bin/python install.py --sys-prefix    after that, fire up Jupyter using:  venv/bin/jupyter-notebook    now you can create new (Java) notebooks!",
            "title": "Installation on Linux"
        },
        {
            "location": "/jupyter_notebooks/#installation-on-windows-using-anaconda",
            "text": "open a command prompt   create a new environment using anaconda (e.g., for Python 3.5)  conda create -n py35-ijava python=3.5    activate environment   activate py35-ijava    install Jupyter  pip install jupyter    download the latest IJava  release  (at time of writing, this was  1.20 )   unzip the IJava release (e.g., with your File browser or 7-Zip)   change into the directory where you extracted the release, containing the  install.py , e.g.:  cd C:\\Users\\fracpete\\Downloads\\ijava-1.2.0    install the kernel  python install.py --sys-prefix    start Jupyter  jupyter-notebook    now you can create new (Java) notebooks!",
            "title": "Installation on Windows (using anaconda)"
        },
        {
            "location": "/writing_classifier/",
            "text": "In case you have a flash idea for a new classifier and want to write one for Weka, this HOWTO will help you developing it. \n\n\nThe Mindmap (\nBuild_classifier.pdf\n, produced with \nFreeMind\n) helps you decide from which base classifier to start, what methods are to be implemented and general guidelines.\n\n\nThe base classifiers are all located in the following package:\n\n\n weka.classifiers\n\n\n\n\nNote:\n This is also covered in chapter \nExtending WEKA\n of the WEKA manual.\n\n\nPackages\n\n\nA few comments about the different classifier sub-packages:\n\n\n\n\nbayes\n - contains bayesian classifiers, e.g. NaiveBayes\n\n\nevaluation\n - classes related to evaluation, e.g., cost matrix\n\n\nfunctions\n - e.g., Support Vector Machines, regression algorithms, neural nets\n\n\nlazy\n - no \noffline\n learning, that is done during runtime, e.g., k-NN\n\n\nmeta\n - Meta classifiers that use a \nbase\n classifier as input, e.g., boosting or bagging\n\n\nmi\n - classifiers that handle multi-instance data\n\n\nmisc\n - various classifiers that don't fit in any another category\n\n\nrules\n - rule-based classifiers, e.g. ZeroR\n\n\ntrees\n - tree classifiers, like decision trees\n\n\n\n\nCoding\n\n\nIn the following you'll find notes about certain implementation parts listed in the Mindmap, which need a bit more explanation.\n\n\nRandom number generators\n\n\nIn order to get repeatable experiments, one is not allowed to use \nunseeded\n random number generators like \nMath.random()\n. Instead, one has to instantiate a \njava.util.Random\n object in the \nbuildClassifier(Instances)\n method with a specific seed value. The seed value can be user supplied, of course, which all the \nRandomizable...\n abstract classifiers already implement.\n\n\nCapabilities\n\n\nIn old versions of Weka (up to version 3.5.2), all classifiers could handle basically every kind of data by default, unless they were throwing an Exception (in the \nbuildClassifier(Instances)\n method). Since this behavior makes it cumbersome to introduce new attribute types, for instance (\nall\n classifiers have to be modified, which can't handle the new attribute type!), the general \nCapabilities\n were introduced.\n\n\nBase-classifier\n\n\nNormal classifiers only state what kind of attributes and what kind of classes they can handle.\n\n\nThe \ngetCapabilities()\n method of \nweka.classifiers.trees.RandomTree\n, for instance, looks like this:\n\n\n  public Capabilities getCapabilities() {\n    Capabilities result = super.getCapabilities();   // returns the object from weka.classifiers.Classifier\n\n    // attributes\n    result.enable(Capability.NOMINAL_ATTRIBUTES);\n    result.enable(Capability.NUMERIC_ATTRIBUTES);\n    result.enable(Capability.DATE_ATTRIBUTES);\n    result.enable(Capability.MISSING_VALUES);\n\n    // class\n    result.enable(Capability.NOMINAL_CLASS);\n    result.enable(Capability.MISSING_CLASS_VALUES);\n\n    return result;\n  }\n\n\n\n\nSpecial cases:\n\n\n\n\n\n\nincremental classifiers\n - By default, at least 1 instance has to be in the dataset, which does not apply for incremental classifiers. They have to lower the limit to \n0\n:\n\n\nresult.setMinimumNumberInstances(0);\n\n\n\n\n\n\nmulti-instance classifiers\n - The structure for multi-instance classifiers is always fixed to \nbagID,bag-data,class\n. To restrict the data to multi-instance data, add the following:\n\n\nresult.enable(Capability.ONLY_MULTIINSTANCE);\n\n\nMulti-instance classifiers also implement the following interface, which returns the Capabilities for the bag-data, which is just a \nrelational\n attribute (the reason why \nRELATIONAL_ATTRIBUTES\n has to be enabled):\n\n\nweka.core.MultiInstanceCapabilitiesHandler\n\n\n\n\n\n\nclusterer\n - Since clusterer don't need a class attribute like classifiers, the following Capability has to be specified to enable datasets without a class attribute (which is already done in the superclass \nweka.clusterers.Clusterer\n):\n\n\nresult.enable(Capability.NO_CLASS);\n\n\n\n\n\n\nMeta-classifier\n\n\nMeta-classifiers, by default, just return the capabilities of their base classifiers - in case of descendants of the \nweka.classifier.MultipleClassifiersCombiner\n, an \nAND\n over all the Capabilities of the base classifiers is returned.\n\n\nDue to this behavior, the Capabilities depend (normally) only on the currently configured base classifier(s). To \nsoften\n filtering for certain behavior, meta-classifiers also define so-called \nDependencies\n on a per-Capability basis. These dependencies tell the filter that even though a certain capability is not supported right now, it is possible that it will be supported with a different base classifier. By default, all Capabilities are initialized as Dependencies. \n\n\nweka.classifiers.meta.LogitBoost\n, e.g., is restricted to nominal classes. For that reason it disables the Dependencies for the class:\n\n\n    result.disableAllClasses();               // disable all class types\n    result.disableAllClassDependencies();     // no dependencies!\n    result.enable(Capability.NOMINAL_CLASS);  // only nominal classes allowed\n\n\n\n\nRelevant classes\n\n\n\n\nweka.core.Capabilities\n\n\nweka.core.CapabilitiesHandler\n\n\nweka.core.MultiInstanceCapabilitiesHandler\n (for multi-instance classifiers)\n\n\n\n\nPaper reference(s)\n\n\nIn order to make it easy to generate a bibliography of all the algorithms in Weka, the [[paper references]] located so far in the Javadoc were extracted and placed in the code.\n\n\nClasses that are based on some technical paper should implement the \nTechnicalInformationHandler\n interface and return a customized \nTechnicalInformation\n instance. The format used is based on \nBibTeX\n and the \nTechnicalInformation\n class can either return a plain text string via the \ntoString()\n method or a real \nBibTeX\n entry via the \ntoBibTex()\n method. This two methods are then used to automatically update the Javadoc (see \nJavadoc\n further down) of a class.\n\n\nRelevant classes:\n\n\n\n\nweka.core.TechnicalInformation\n\n\nweka.core.TechnicalInformationHandler\n\n\n\n\nJavadoc\n\n\nOpen-source software is only as good as its documentation. Hence, correct and up-to-date documentation is vital. So far most of the Javadoc was maintained manually, which made it hard to maintain, e.g., as soon as new options were added the Javadoc had to be changed accordingly, too. And that normally in several places:\n\n\n\n\nClass description\n\n\nsetOptions(String[])\n method\n\n\n\n\nOver the time the documentation got out of sync, which made it frustrating determining what options were really relevant and active. Since a lot of the documentation is already available in the code itself, the next logical step was to automate the Javadoc generation as much as possible. In the following you will see how to structure your Javadoc to reduce maintainance. For this purpose special comment tags are used, where the content in between will be replaced automatically by the classes listed below in the \nRelevant classes\n section.\n\n\nThe indentation of the generated Javadoc depends on the indentation of the \n&lt;\n of the starting comment tag.\n\n\nThis general layout order should be used for all classes:\n\n\n\n\n\n\nclass description\n Javadoc\n\n\n\n\nglobalinfo\n\n\nbibtex - \nif available\n\n\ncommandline options\n\n\n\n\n\n\n\n\nsetOptions\n Javadoc\n\n\n\n\ncommandline options\n\n\n\n\n\n\n\n\nGeneral\n\n\nThe general description for all classes displayed in the GenericObjectEditor was already in place, with the following method:\n\n\n globalInfo()\n\n\n\n\nThe return value can be placed in the Javadoc, surrounded by the following comment tags:\n\n\n <!-- globalinfo-start -->\n will be automatically replaced\n <!-- globalinfo-end -->\n\n\n\n\nPaper reference(s)\n\n\nIf available, the paper reference should also be listed in the Javadoc. Since the \nglobalInfo()\n method should return a short version of the reference, it is sufficient to list the full \nBibTeX\n documentation:\n\n\n <!-- technical-bibtex-start -->\n will be automatically replaced\n <!-- technical-bibtex-end -->\n\n\n\n\nIn case it is necessary to list the short, plain text version, too, one can use the following tags:\n\n\n <!-- technical-plaintext-start -->\n will be automatically replaced\n <!-- technical-plaintext-end -->\n\n\n\n\nOptions\n\n\nTo place the commandline options, use the following comment tags:\n\n\n <!-- options-start -->\n will be automatically replaced\n <!-- options-end -->\n\n\n\n\nRelevant classes\n\n\n\n\nweka.core.AllJavadoc\n - executes all Javadoc-producing classes\n\n\nweka.core.GlobalInfoJavadoc\n - updates the globalInfo tags\n\n\nweka.core.OptionHandlerJavadoc\n - updates the option tags\n\n\nweka.core.TechnicalInformationHandlerJavadoc\n - updates the technical tags (plain text and \nBibTeX\n)\n\n\n\n\nIntegration\n\n\nAfter finishing the coding stage, it's time to integrate your classifier in the Weka framework, i.e., to make it available in the Explorer, Experimenter, etc.\n\n\nThe [[GenericObjectEditor]] article shows you how to tell Weka where to find your classifier and therefore displaying it in the \nGenericObjectEditor\n.\n\n\nRevisions\n\n\nClassifiers also implement the \nweka.core.RevisionHandler\n interface. This provides the functionality of obtaining the \nSubversion\n revision from within Java. Classifiers that are not part of the official Weka distribution will have to implement the method \ngetRevision()\n as follows, which will return a dummy revision of \n1.0\n:\n\n\n  /**\n   * Returns the revision string.\n   * \n   * @return        the revision\n   */\n  public String getRevision() {\n    return RevisionUtils.extract(\"$Revision: 1.0 $\");\n  }\n\n\n\n\nTesting\n\n\nWeka provides already a test framework to ensure the basic functionality of a classifier. It is essential for the classifier to pass these tests.\n\n\nCommandline test\n\n\nGeneral\n\n\nUse the CheckClassifier class to test your classifier from commandline:\n\n\n weka.classifiers.CheckClassifier -W classname [-- additional parameters]\n\n\n\n\nOnly the following tests may have \"no\" as result, the others must have a \"no (OK error message)\" or \"yes\":\n\n\n\n\noptions\n\n\nupdateable classifier\n\n\nweighted instances classifier\n\n\nmulti-instance classifier\n\n\n\n\nOption handling\n\n\nAdditionally, check the \noption handling\n of your classifier with the following tool from commandline:\n\n\n weka.core.CheckOptionHandler -W classname [-- additional parameters]\n\n\n\n\nAll tests need to return \nyes\n.\n\n\nGenericObjectEditor\n\n\nThe \nCheckGOE\n class checks whether all the properties available in the GUI have a tooltip accompanying them and whether the \nglobalInfo()\n method is declared:\n\n\n weka.core.CheckGOE -W classname [-- additional parameters]\n\n\n\n\nAll tests, once again, need to return \nyes\n.\n\n\nSource code\n\n\nClassifiers that implement the \nweka.classifiers.Sourcable\n interface can output Java code of their model. In order to check the generated code, one should not only compile the code, but also test it with the following test class:\n\n\n weka.classifiers.CheckSource\n\n\n\n\nThis class takes the original Weka classifier, the generated code and the dataset used for generating the source code as parameters. It builds the Weka classifier on the dataset and compares the predictions, the ones from the Weka classifier and the ones from the generated source code, whether they are the same.\n\n\nHere's an example call for \nweka.classifiers.trees.Id3\n and the generated class \nweka.classifiers.WekaWrapper\n (it wraps the actual generated code in a pseudo-classifier):\n\n\n java weka.classifiers.CheckSource \\\n    -W \"weka.classifiers.trees.Id3\" \\\n    -S weka.classifiers.WekaWrapper \\\n    -t data.arff \\\n    -c last\n\n\n\n\nIt needs to return \nTests OK!\n.\n\n\nUnit tests\n\n\nIn order to make sure that your classifier applies to the Weka criteria, you should add your classifier to the \njunit\n unit test framework, i.e., by creating a Test class derived from \nAbstractClassifierTest\n. This class uses the \nCheckClassifier\n, \nCheckOptionHandler\n and \nCheckGOE\n class to run a battery of tests.\n\n\nHow to check out the unit test framework, you can find \nhere\n.\n\n\nSee also\n\n\n\n\n[[GenericObjectEditor|GenericObjectEditor/GenericPropertiesCreator]]\n\n\n[[Paper References|HOWTO extract paper references]]\n\n\n\n\nLinks\n\n\n\n\nBuild_classifier.pdf\n - MindMap for implementing a new classifier\n\n\nWeka API (\nstable\n, \ndeveloper\n)\n\n\nFreemind\n\n\njunit",
            "title": " Writing your own Classifier"
        },
        {
            "location": "/writing_classifier/#packages",
            "text": "A few comments about the different classifier sub-packages:   bayes  - contains bayesian classifiers, e.g. NaiveBayes  evaluation  - classes related to evaluation, e.g., cost matrix  functions  - e.g., Support Vector Machines, regression algorithms, neural nets  lazy  - no  offline  learning, that is done during runtime, e.g., k-NN  meta  - Meta classifiers that use a  base  classifier as input, e.g., boosting or bagging  mi  - classifiers that handle multi-instance data  misc  - various classifiers that don't fit in any another category  rules  - rule-based classifiers, e.g. ZeroR  trees  - tree classifiers, like decision trees",
            "title": "Packages"
        },
        {
            "location": "/writing_classifier/#coding",
            "text": "In the following you'll find notes about certain implementation parts listed in the Mindmap, which need a bit more explanation.",
            "title": "Coding"
        },
        {
            "location": "/writing_classifier/#random-number-generators",
            "text": "In order to get repeatable experiments, one is not allowed to use  unseeded  random number generators like  Math.random() . Instead, one has to instantiate a  java.util.Random  object in the  buildClassifier(Instances)  method with a specific seed value. The seed value can be user supplied, of course, which all the  Randomizable...  abstract classifiers already implement.",
            "title": "Random number generators"
        },
        {
            "location": "/writing_classifier/#capabilities",
            "text": "In old versions of Weka (up to version 3.5.2), all classifiers could handle basically every kind of data by default, unless they were throwing an Exception (in the  buildClassifier(Instances)  method). Since this behavior makes it cumbersome to introduce new attribute types, for instance ( all  classifiers have to be modified, which can't handle the new attribute type!), the general  Capabilities  were introduced.",
            "title": "Capabilities"
        },
        {
            "location": "/writing_classifier/#base-classifier",
            "text": "Normal classifiers only state what kind of attributes and what kind of classes they can handle.  The  getCapabilities()  method of  weka.classifiers.trees.RandomTree , for instance, looks like this:    public Capabilities getCapabilities() {\n    Capabilities result = super.getCapabilities();   // returns the object from weka.classifiers.Classifier\n\n    // attributes\n    result.enable(Capability.NOMINAL_ATTRIBUTES);\n    result.enable(Capability.NUMERIC_ATTRIBUTES);\n    result.enable(Capability.DATE_ATTRIBUTES);\n    result.enable(Capability.MISSING_VALUES);\n\n    // class\n    result.enable(Capability.NOMINAL_CLASS);\n    result.enable(Capability.MISSING_CLASS_VALUES);\n\n    return result;\n  }  Special cases:    incremental classifiers  - By default, at least 1 instance has to be in the dataset, which does not apply for incremental classifiers. They have to lower the limit to  0 :  result.setMinimumNumberInstances(0);    multi-instance classifiers  - The structure for multi-instance classifiers is always fixed to  bagID,bag-data,class . To restrict the data to multi-instance data, add the following:  result.enable(Capability.ONLY_MULTIINSTANCE);  Multi-instance classifiers also implement the following interface, which returns the Capabilities for the bag-data, which is just a  relational  attribute (the reason why  RELATIONAL_ATTRIBUTES  has to be enabled):  weka.core.MultiInstanceCapabilitiesHandler    clusterer  - Since clusterer don't need a class attribute like classifiers, the following Capability has to be specified to enable datasets without a class attribute (which is already done in the superclass  weka.clusterers.Clusterer ):  result.enable(Capability.NO_CLASS);",
            "title": "Base-classifier"
        },
        {
            "location": "/writing_classifier/#meta-classifier",
            "text": "Meta-classifiers, by default, just return the capabilities of their base classifiers - in case of descendants of the  weka.classifier.MultipleClassifiersCombiner , an  AND  over all the Capabilities of the base classifiers is returned.  Due to this behavior, the Capabilities depend (normally) only on the currently configured base classifier(s). To  soften  filtering for certain behavior, meta-classifiers also define so-called  Dependencies  on a per-Capability basis. These dependencies tell the filter that even though a certain capability is not supported right now, it is possible that it will be supported with a different base classifier. By default, all Capabilities are initialized as Dependencies.   weka.classifiers.meta.LogitBoost , e.g., is restricted to nominal classes. For that reason it disables the Dependencies for the class:      result.disableAllClasses();               // disable all class types\n    result.disableAllClassDependencies();     // no dependencies!\n    result.enable(Capability.NOMINAL_CLASS);  // only nominal classes allowed",
            "title": "Meta-classifier"
        },
        {
            "location": "/writing_classifier/#relevant-classes",
            "text": "weka.core.Capabilities  weka.core.CapabilitiesHandler  weka.core.MultiInstanceCapabilitiesHandler  (for multi-instance classifiers)",
            "title": "Relevant classes"
        },
        {
            "location": "/writing_classifier/#paper-references",
            "text": "In order to make it easy to generate a bibliography of all the algorithms in Weka, the [[paper references]] located so far in the Javadoc were extracted and placed in the code.  Classes that are based on some technical paper should implement the  TechnicalInformationHandler  interface and return a customized  TechnicalInformation  instance. The format used is based on  BibTeX  and the  TechnicalInformation  class can either return a plain text string via the  toString()  method or a real  BibTeX  entry via the  toBibTex()  method. This two methods are then used to automatically update the Javadoc (see  Javadoc  further down) of a class.  Relevant classes:   weka.core.TechnicalInformation  weka.core.TechnicalInformationHandler",
            "title": "Paper reference(s)"
        },
        {
            "location": "/writing_classifier/#javadoc",
            "text": "Open-source software is only as good as its documentation. Hence, correct and up-to-date documentation is vital. So far most of the Javadoc was maintained manually, which made it hard to maintain, e.g., as soon as new options were added the Javadoc had to be changed accordingly, too. And that normally in several places:   Class description  setOptions(String[])  method   Over the time the documentation got out of sync, which made it frustrating determining what options were really relevant and active. Since a lot of the documentation is already available in the code itself, the next logical step was to automate the Javadoc generation as much as possible. In the following you will see how to structure your Javadoc to reduce maintainance. For this purpose special comment tags are used, where the content in between will be replaced automatically by the classes listed below in the  Relevant classes  section.  The indentation of the generated Javadoc depends on the indentation of the  &lt;  of the starting comment tag.  This general layout order should be used for all classes:    class description  Javadoc   globalinfo  bibtex -  if available  commandline options     setOptions  Javadoc   commandline options",
            "title": "Javadoc"
        },
        {
            "location": "/writing_classifier/#general",
            "text": "The general description for all classes displayed in the GenericObjectEditor was already in place, with the following method:   globalInfo()  The return value can be placed in the Javadoc, surrounded by the following comment tags:   <!-- globalinfo-start -->\n will be automatically replaced\n <!-- globalinfo-end -->",
            "title": "General"
        },
        {
            "location": "/writing_classifier/#paper-references_1",
            "text": "If available, the paper reference should also be listed in the Javadoc. Since the  globalInfo()  method should return a short version of the reference, it is sufficient to list the full  BibTeX  documentation:   <!-- technical-bibtex-start -->\n will be automatically replaced\n <!-- technical-bibtex-end -->  In case it is necessary to list the short, plain text version, too, one can use the following tags:   <!-- technical-plaintext-start -->\n will be automatically replaced\n <!-- technical-plaintext-end -->",
            "title": "Paper reference(s)"
        },
        {
            "location": "/writing_classifier/#options",
            "text": "To place the commandline options, use the following comment tags:   <!-- options-start -->\n will be automatically replaced\n <!-- options-end -->",
            "title": "Options"
        },
        {
            "location": "/writing_classifier/#relevant-classes_1",
            "text": "weka.core.AllJavadoc  - executes all Javadoc-producing classes  weka.core.GlobalInfoJavadoc  - updates the globalInfo tags  weka.core.OptionHandlerJavadoc  - updates the option tags  weka.core.TechnicalInformationHandlerJavadoc  - updates the technical tags (plain text and  BibTeX )",
            "title": "Relevant classes"
        },
        {
            "location": "/writing_classifier/#integration",
            "text": "After finishing the coding stage, it's time to integrate your classifier in the Weka framework, i.e., to make it available in the Explorer, Experimenter, etc.  The [[GenericObjectEditor]] article shows you how to tell Weka where to find your classifier and therefore displaying it in the  GenericObjectEditor .",
            "title": "Integration"
        },
        {
            "location": "/writing_classifier/#revisions",
            "text": "Classifiers also implement the  weka.core.RevisionHandler  interface. This provides the functionality of obtaining the  Subversion  revision from within Java. Classifiers that are not part of the official Weka distribution will have to implement the method  getRevision()  as follows, which will return a dummy revision of  1.0 :    /**\n   * Returns the revision string.\n   * \n   * @return        the revision\n   */\n  public String getRevision() {\n    return RevisionUtils.extract(\"$Revision: 1.0 $\");\n  }",
            "title": "Revisions"
        },
        {
            "location": "/writing_classifier/#testing",
            "text": "Weka provides already a test framework to ensure the basic functionality of a classifier. It is essential for the classifier to pass these tests.",
            "title": "Testing"
        },
        {
            "location": "/writing_classifier/#commandline-test",
            "text": "",
            "title": "Commandline test"
        },
        {
            "location": "/writing_classifier/#general_1",
            "text": "Use the CheckClassifier class to test your classifier from commandline:   weka.classifiers.CheckClassifier -W classname [-- additional parameters]  Only the following tests may have \"no\" as result, the others must have a \"no (OK error message)\" or \"yes\":   options  updateable classifier  weighted instances classifier  multi-instance classifier",
            "title": "General"
        },
        {
            "location": "/writing_classifier/#option-handling",
            "text": "Additionally, check the  option handling  of your classifier with the following tool from commandline:   weka.core.CheckOptionHandler -W classname [-- additional parameters]  All tests need to return  yes .",
            "title": "Option handling"
        },
        {
            "location": "/writing_classifier/#genericobjecteditor",
            "text": "The  CheckGOE  class checks whether all the properties available in the GUI have a tooltip accompanying them and whether the  globalInfo()  method is declared:   weka.core.CheckGOE -W classname [-- additional parameters]  All tests, once again, need to return  yes .",
            "title": "GenericObjectEditor"
        },
        {
            "location": "/writing_classifier/#source-code",
            "text": "Classifiers that implement the  weka.classifiers.Sourcable  interface can output Java code of their model. In order to check the generated code, one should not only compile the code, but also test it with the following test class:   weka.classifiers.CheckSource  This class takes the original Weka classifier, the generated code and the dataset used for generating the source code as parameters. It builds the Weka classifier on the dataset and compares the predictions, the ones from the Weka classifier and the ones from the generated source code, whether they are the same.  Here's an example call for  weka.classifiers.trees.Id3  and the generated class  weka.classifiers.WekaWrapper  (it wraps the actual generated code in a pseudo-classifier):   java weka.classifiers.CheckSource \\\n    -W \"weka.classifiers.trees.Id3\" \\\n    -S weka.classifiers.WekaWrapper \\\n    -t data.arff \\\n    -c last  It needs to return  Tests OK! .",
            "title": "Source code"
        },
        {
            "location": "/writing_classifier/#unit-tests",
            "text": "In order to make sure that your classifier applies to the Weka criteria, you should add your classifier to the  junit  unit test framework, i.e., by creating a Test class derived from  AbstractClassifierTest . This class uses the  CheckClassifier ,  CheckOptionHandler  and  CheckGOE  class to run a battery of tests.  How to check out the unit test framework, you can find  here .",
            "title": "Unit tests"
        },
        {
            "location": "/writing_classifier/#see-also",
            "text": "[[GenericObjectEditor|GenericObjectEditor/GenericPropertiesCreator]]  [[Paper References|HOWTO extract paper references]]",
            "title": "See also"
        },
        {
            "location": "/writing_classifier/#links",
            "text": "Build_classifier.pdf  - MindMap for implementing a new classifier  Weka API ( stable ,  developer )  Freemind  junit",
            "title": "Links"
        },
        {
            "location": "/writing_filter/",
            "text": "Note:\n This is also covered in chapter \nExtending WEKA\n of the WEKA manual.\n\n\nPackages\n\n\nA few comments about the different filter sub-packages:\n\n\n\n\n\n\nsupervised\n - contains supervised filters, i.e., filters that take class distributions into account. Must implement the \nweka.filters.SupervisedFilter\n interface.\n\n\n\n\nattribute\n - filters that work column-wise.\n\n\ninstance\n - filters that work row-wise.\n\n\n\n\n\n\n\n\nunsupervised\n - contains unsupervised filters, i.e., they work without taking any class distributions into account. Must implement the \nweka.filters.UnsupervisedFilter\n interface.\n\n\n\n\nattribute\n - filters that work column-wise.\n\n\ninstance\n - filters that work row-wise.\n\n\n\n\n\n\n\n\nChoosing the superclass\n\n\nThe base filters and interfaces are all located in the following package:\n\n\n weka.filters\n\n\n\n\nOne can basically distinguish between two different kinds of filters:\n\n\n\n\nbatch filters\n - they need to see the whole dataset before they can start processing it, which they do in one go\n\n\nstream filters\n - they can start producing output right away and the data just passes through while being modified\n\n\n\n\nAll filters are derived from the abstract superclass \nweka.filters.Filter\n.\n\n\nTo speed up development, there are also the following abstract filters, depending on the kind of classifier you want to implement:\n\n\n\n\nweka.filters.SimpleBatchFilter\n\n\nweka.filters.SimpleStreamFilter\n\n\n\n\nThese filters simplify the rather general and complex framework introduced by the abstract superclass \nweka.filters.Filter\n. One only needs to implement a couple of abstract methods that will process the actual data and override, if necessary, a few existing methods for option handling.\n\n\nFilter\n\n\nImplementation\n\n\nThe following methods are of importance for the implementation of a filter and explained in detail further down:\n\n\n\n\ngetCapabilities()\n\n\nsetInputFormat(Instances)\n\n\ngetInputFormat()\n\n\nsetOutputFormat(Instances)\n\n\ngetOutputFormat()\n\n\ninput(Instance)\n\n\nbufferInput(Instance)\n\n\npush(Instance)\n\n\noutput()\n\n\nbatchFinished()\n\n\nflushInput()\n\n\ngetRevision()\n\n\n\n\nBut only the following ones need normally be modified:\n\n\n\n\ngetCapabilities()\n\n\nsetInputFormat(Instances)\n\n\ninput(Instance)\n\n\nbatchFinished()\n\n\ngetRevision()\n\n\n\n\ngetCapabilities()\n\n\nFilters implement the \nweka.core.CapabilitiesHandler\n interface like the classifiers. This method returns what kind of data the filter is able to process. Needs to be adapted for each individual filter.\n\n\nsetInputFormat(Instances)\n\n\nWith this format, the user tells the filter what format, i.e., attributes, the input data has. This method also tests, whether the filter can actually process this data. All older Weka versions or book branch versions need to check the data manually and throw fitting exceptions, e.g., not being able to handle String attributes.\n\n\nIf the output format of the filter, i.e., the new Instances header, can be determined based alone on this information, then the method should set the output format via \nsetOutputFormat(Instances)\n and return \ntrue\n, otherwise it has to return \nfalse\n.\n\n\ngetInputFormat()\n\n\nThis method returns an Instances object containing all currently buffered Instance objects from the input queue.\n\n\nsetOutputFormat(Instances)\n\n\nsetOutputFormat(Instances)\n defines the new Instances header for the output data. For filters that work on a row-basis, there shouldn't be any changes between the input and output format. But filters that work on attributes, e.g., removing, adding, modifying, will affect this format. This method must be called with the appropriate Instances object as parameter, since all Instance objects being processed will rely on the output format.\n\n\ngetOutputFormat()\n\n\nThis method returns the currently set Instances object that defines the output format. In case \nsetOutputFormat(Instances)\n hasn't been called yet, this method will return \nnull\n.\n\n\ninput(Instance)\n\n\nThe \ninput(Instance)\n method returns \ntrue\n if the given Instance can be processed straight away and can be collected immediately via the \noutput()\n method (after adding it to the output queue via \npush(Instance)\n, of course). This is also the case if the first batch of data has been processed and the instance belongs to the second batch. Via \nisFirstBatchDone()\n one can query whether this instance is still part of the first batch or of the second.\n\n\nIf the Instance cannot be processed immediately, e.g., the filter needs to collect all the data first before doing some calculations, then it needs to be buffered with \nbufferInput(Instance)\n until \nbatchFinished()\n is called.\n\n\nbufferInput(Instance)\n\n\nIn case an Instance cannot be processed immediately, one can use this method to buffer them in the input queue. All buffered Instance objects are available via the \ngetInputFormat()\n method.\n\n\npush(Instance)\n\n\npush(Instance)\n adds the given Instance to the output queue.\n\n\noutput()\n\n\nReturns the next Instance object from the output queue and removes it from there. In case there is no Instance available this method returns \nnull\n.\n\n\nbatchFinished()\n\n\nThe \nbatchFinished()\n method signifies the end of a dataset being pushed through the filter. In case of a filter that couldn't process the data of the first batch immediately, this is the place to determine what the output format will be (and set if via \nsetOutputFormat(Instances)\n) and process the actual data. The currently available data can be retrieved with the \ngetInputFormat()\n method. After processing the data, one needs to call \nflushInput()\n to remove all the pending input data.\n\n\nflushInput()\n\n\nflushInput()\n removes all buffered Instance objects from the input queue. This method must be called after all the Instance objects have been processed in the \nbatchFinished()\n method.\n\n\nOption handling\n\n\nIf the filter should be able to handle commandline options, then the weka.core.OptionHandler interface needs to be implemented. In addition to that, the following code should be added at the end of the \nsetOptions(String[])\n method:\n\n\n if (getInputFormat() != null)\n    setInputFormat(getInputFormat());\n\n\n\n\nThis will inform the filter about changes in the options and therefore reset it.\n\n\nExamples\n\n\nThe following examples are to illustrate the filter framework. \n\n\nNote:\n unseeded random number generators like \nMath.random()\n should never be used since they will produce different results in each run and repeatable results are essential in machine learning.\n\n\nBatchFilter\n\n\nThis simple batch filter adds a new attribute called //bla// at the end of the dataset. The rows of this attribute contain only the row's index in the data. Since the batch-filter need not see all the data before creating the output format, the \nsetInputFormat(Instances)\n sets the output format and returns \ntrue\n (indicating that the output format can be queried immediately). The \nbatchFinished()\n method performs the processing of all the data.\n\n\n  import weka.core.*;\n  import weka.core.Capabilities.*;\n\n  public class BatchFilter\n    extends Filter {\n\n    public String globalInfo() {\n      return   \"A batch filter that adds an additional attribute 'bla' at the end \"\n             + \"containing the index of the processed instance. The output format \"\n             + \"can be collected immediately.\";\n    }\n\n    public Capabilities getCapabilities() {\n      Capabilities result = super.getCapabilities();\n      result.enableAllAttributes();\n      result.enableAllClasses();\n      result.enable(Capability.NO_CLASS);  // filter doesn't need class to be set\n      return result;\n    }\n\n    public boolean setInputFormat(Instances instanceInfo) throws Exception {\n      super.setInputFormat(instanceInfo);\n\n      Instances outFormat = new Instances(instanceInfo, 0);\n      outFormat.insertAttributeAt(new Attribute(\"bla\"), outFormat.numAttributes());\n      setOutputFormat(outFormat);\n\n      return true;  // output format is immediately available\n    }\n\n    public boolean batchFinished() throws Exception {\n      if (getInputFormat() = null)\n        throw new NullPointerException(\"No input instance format defined\");\n\n      Instances inst = getInputFormat();\n      Instances outFormat = getOutputFormat();\n      for (int i = 0; i < inst.numInstances(); i++) {\n        double[] newValues = new double[outFormat.numAttributes()];\n        double[] oldValues = inst.instance(i).toDoubleArray();\n        System.arraycopy(oldValues, 0, newValues, 0, oldValues.length);\n        newValues[newValues.length - 1] = i;\n        push(new Instance(1.0, newValues));\n      }\n\n      flushInput();\n      m_NewBatch = true;\n      m_FirstBatchDone = true;\n      return (numPendingOutput() != 0);\n    }\n\n    public static void main(String[] args) {\n      runFilter(new BatchFilter(), args);\n    }\n  }\n\n\n\n\nBatchFilter2\n\n\nIn contrast to the first batch filter, this one here cannot determine the output format immediately (the number of instances in the first batch is part of the attribute name now). This is done in the \nbatchFinished()\n method.\n\n\n  import weka.core.*;\n  import weka.core.Capabilities.*;\n\n  public class BatchFilter2\n    extends Filter {\n\n    public String globalInfo() {\n      return   \"A batch filter that adds an additional attribute 'bla' at the end \"\n             + \"containing the index of the processed instance. The output format \"\n             + \"cannot be collected immediately.\";\n    }\n\n    public Capabilities getCapabilities() {\n      Capabilities result = super.getCapabilities();\n      result.enableAllAttributes();\n      result.enableAllClasses();\n      result.enable(Capability.NO_CLASS);  // filter doesn't need class to be set\n      return result;\n    }\n\n    public boolean batchFinished() throws Exception {\n      if (getInputFormat() = null)\n        throw new NullPointerException(\"No input instance format defined\");\n\n      // output format still needs to be set (depends on first batch of data)\n      if (!isFirstBatchDone()) {\n        Instances outFormat = new Instances(getInputFormat(), 0);\n        outFormat.insertAttributeAt(new Attribute(\n            \"bla-\" + getInputFormat().numInstances()), outFormat.numAttributes());\n        setOutputFormat(outFormat);\n      }\n\n      Instances inst = getInputFormat();\n      Instances outFormat = getOutputFormat();\n      for (int i = 0; i < inst.numInstances(); i++) {\n        double[] newValues = new double[outFormat.numAttributes()];\n        double[] oldValues = inst.instance(i).toDoubleArray();\n        System.arraycopy(oldValues, 0, newValues, 0, oldValues.length);\n        newValues[newValues.length - 1] = i;\n        push(new Instance(1.0, newValues));\n      }\n\n      flushInput();\n      m_NewBatch = true;\n      m_FirstBatchDone = true;\n\n      return (numPendingOutput() != 0);\n    }\n\n    public static void main(String[] args) {\n      runFilter(new BatchFilter2(), args);\n    }\n  }\n\n\n\n\nBatchFilter3\n\n\nAs soon as this batch filter's first batch is done, it can process Instance objects immediately in the \ninput(Instance)\n method. It adds a new attribute which contains just a random number, but the random number generator being used is seeded with the number of instances from the first batch.\n\n\n  import weka.core.*;\n  import weka.core.Capabilities.*;\n\n  import java.util.Random;\n\n  public class BatchFilter3\n    extends Filter {\n\n    protected int m_Seed;\n    protected Random m_Random;\n\n    public String globalInfo() {\n      return   \"A batch filter that adds an attribute 'bla' at the end \"\n             + \"containing a random number. The output format cannot be collected \"\n             + \"immediately.\";\n    }\n\n    public Capabilities getCapabilities() {\n      Capabilities result = super.getCapabilities();\n      result.enableAllAttributes();\n      result.enableAllClasses();\n      result.enable(Capability.NO_CLASS);  // filter doesn't need class to be set\n      return result;\n    }\n\n    public boolean input(Instance instance) throws Exception {\n      if (getInputFormat() = null)\n        throw new NullPointerException(\"No input instance format defined\");\n\n      if (isNewBatch()) {\n        resetQueue();\n        m_NewBatch = false;\n      }\n\n      if (isFirstBatchDone())\n        convertInstance(instance);\n      else\n        bufferInput(instance);\n\n      return isFirstBatchDone();\n    }\n\n    public boolean batchFinished() throws Exception {\n      if (getInputFormat() = null)\n        throw new NullPointerException(\"No input instance format defined\");\n\n      // output format still needs to be set (random number generator is seeded\n      // with number of instances of first batch)\n      if (!isFirstBatchDone()) {\n        m_Seed = getInputFormat().numInstances();\n        Instances outFormat = new Instances(getInputFormat(), 0);\n        outFormat.insertAttributeAt(new Attribute(\n            \"bla-\" + getInputFormat().numInstances()), outFormat.numAttributes());\n        setOutputFormat(outFormat);\n      }\n\n      Instances inst = getInputFormat();\n      for (int i = 0; i < inst.numInstances(); i++) {\n        convertInstance(inst.instance(i));\n      }\n\n      flushInput();\n      m_NewBatch = true;\n      m_FirstBatchDone = true;\n      m_Random = null;\n\n      return (numPendingOutput() != 0);\n    }\n\n    protected void convertInstance(Instance instance) {\n      if (m_Random = null)\n        m_Random = new Random(m_Seed);\n      double[] newValues = new double[instance.numAttributes() + 1];\n      double[] oldValues = instance.toDoubleArray();\n      newValues[newValues.length - 1] = m_Random.nextInt();\n      System.arraycopy(oldValues, 0, newValues, 0, oldValues.length);\n      push(new Instance(1.0, newValues));\n    }\n\n    public static void main(String[] args) {\n      runFilter(new BatchFilter3(), args);\n    }\n  }\n\n\n\n\nStreamFilter\n\n\nThis stream filter adds a random number at the end of each instance of the input data. Since this doesn't rely on having access to the full data of the first batch, the output format is accessible immediately after using \nsetInputFormat(Instances)\n. All the Instance objects are immediately processed in \ninput(Instance)\n via the \nconvertInstance(Instance)\n method, which pushes them immediately to the output queue.\n\n\n  import weka.core.*;\n  import weka.core.Capabilities.*;\n\n  import java.util.Random;\n\n  public class StreamFilter\n    extends Filter {\n\n    protected Random m_Random;\n\n    public String globalInfo() {\n      return   \"A stream filter that adds an attribute 'bla' at the end \"\n             + \"containing a random number. The output format can be collected \"\n             + \"immediately.\";\n    }\n\n    public Capabilities getCapabilities() {\n      Capabilities result = super.getCapabilities();\n      result.enableAllAttributes();\n      result.enableAllClasses();\n      result.enable(Capability.NO_CLASS);  // filter doesn't need class to be set\n      return result;\n    }\n\n    public boolean setInputFormat(Instances instanceInfo) throws Exception {\n      super.setInputFormat(instanceInfo);\n\n      Instances outFormat = new Instances(instanceInfo, 0);\n      outFormat.insertAttributeAt(new Attribute(\"bla\"), outFormat.numAttributes());\n      setOutputFormat(outFormat);\n\n      m_Random = new Random(1);\n\n      return true;  // output format is immediately available\n    }\n\n    public boolean input(Instance instance) throws Exception {\n      if (getInputFormat() = null)\n        throw new NullPointerException(\"No input instance format defined\");\n\n      if (isNewBatch()) {\n        resetQueue();\n        m_NewBatch = false;\n      }\n\n      convertInstance(instance);\n\n      return true;  // can be immediately collected via output()\n    }\n\n    protected void convertInstance(Instance instance) {\n      double[] newValues = new double[instance.numAttributes() + 1];\n      double[] oldValues = instance.toDoubleArray();\n      newValues[newValues.length - 1] = m_Random.nextInt();\n      System.arraycopy(oldValues, 0, newValues, 0, oldValues.length);\n      push(new Instance(1.0, newValues));\n    }\n\n    public static void main(String[] args) {\n      runFilter(new StreamFilter(), args);\n    }\n  }\n\n\n\n\nSimpleBatchFilter\n\n\nOnly the following abstract methods need to be implemented:\n\n\n\n\nglobalInfo()\n - returns a short description of what the filter does; will be displayed in the GUI\n\n\ndetermineOutputFormat(Instances)\n - generates the new format, based on the input data\n\n\nprocess(Instances)\n - processes the whole dataset in one go\n\n\ngetRevision()\n - returns the \nSubversion\n revision information\n\n\n\n\nIf you need access to the full input dataset in \ndetermineOutputFormat(Instances)\n, then you need to also override the method \nallowAccessToFullInputFormat()\n and make it return true. \n\n\nIf more options are necessary, then the following methods need to be overridden:\n\n\n\n\nlistOptions()\n - returns an enumeration of the available options; these are printed if one calls the filter with the \n-h\n option\n\n\nsetOptions(String[])\n - parses the given option array, that were passed from commandline\n\n\ngetOptions()\n - returns an array of options, resembling the current setup of the filter\n\n\n\n\nIn the following an \nexample implementation\n that adds an additional attribute at the end, containing the index of the processed instance:\n\n\n import weka.core.*;\n import weka.core.Capabilities.*;\n import weka.filters.*;\n\n public class SimpleBatch\n   extends SimpleBatchFilter {\n\n   public String globalInfo() {\n     return   \"A simple batch filter that adds an additional attribute 'bla' at the end \"\n            + \"containing the index of the processed instance.\";\n   }\n\n   public Capabilities getCapabilities() {\n     Capabilities result = super.getCapabilities();\n     result.enableAllAttributes();\n     result.enableAllClasses();\n     result.enable(Capability.NO_CLASS);  //// filter doesn't need class to be set//\n     return result;\n   }\n\n   protected Instances determineOutputFormat(Instances inputFormat) {\n     Instances result = new Instances(inputFormat, 0);\n     result.insertAttributeAt(new Attribute(\"bla\"), result.numAttributes());\n     return result;\n   }\n\n   protected Instances process(Instances inst) {\n     Instances result = new Instances(determineOutputFormat(inst), 0);\n     for (int i = 0; i < inst.numInstances(); i++) {\n       double[] values = new double[result.numAttributes()];\n       for (int n = 0; n < inst.numAttributes(); n++)\n         values[n] = inst.instance(i).value(n);\n       values[values.length - 1] = i;\n       result.add(new Instance(1, values));\n     }\n     return result;\n   }\n\n   public static void main(String[] args) {\n     runFilter(new SimpleBatch(), args);\n   }\n }\n\n\n\n\nSimpleStreamFilter\n\n\nOnly the following abstract methods need to be implemented:\n\n\n\n\nglobalInfo()\n - returns a short description of what the filter does; will be displayed in the GUI\n\n\ndetermineOutputFormat(Instances)\n - generates the new format, based on the input data\n\n\nprocess(Instance)\nprocesses a single instance and turns it from the old format into the new one\n\n\ngetRevision()\n - returns the \nSubversion\n revision information\n\n\n\n\nThe \nreset()\n method is only used, since the random number generator needs to be re-initialized in order to obtain repeatable results.\n\n\nIf more options are necessary, then the following methods need to be overridden:\n\n\n\n\nlistOptions()\n - returns an enumeration of the available options; these are printed if one calls the filter with the \n-h\n option\n\n\nsetOptions(String[])\n - parses the given option array, that were passed from commandline\n\n\ngetOptions()\n - returns an array of options, resembling the current setup of the filter\n\n\n\n\nIn the following an \nexample implementation\n of a stream filter that adds an extra attribute at the end, which is filled with random numbers:\n\n\n import weka.core.*;\n import weka.core.Capabilities.*;\n import weka.filters.*;\n\n import java.util.Random;\n\n public class SimpleStream\n   extends SimpleStreamFilter {\n\n   protected Random m_Random;\n\n   public String globalInfo() {\n     return   \"A simple stream filter that adds an attribute 'bla' at the end \"\n            + \"containing a random number.\";\n   }\n\n   public Capabilities getCapabilities() {\n     Capabilities result = super.getCapabilities();\n     result.enableAllAttributes();\n     result.enableAllClasses();\n     result.enable(Capability.NO_CLASS);  //// filter doesn't need class to be set//\n     return result;\n   }\n\n   protected void reset() {\n     super.reset();\n     m_Random = new Random(1);\n   }\n\n   protected Instances determineOutputFormat(Instances inputFormat) {\n     Instances result = new Instances(inputFormat, 0);\n     result.insertAttributeAt(new Attribute(\"bla\"), result.numAttributes());\n     return result;\n   }\n\n   protected Instance process(Instance inst) {\n     double[] values = new double[inst.numAttributes() + 1];\n     for (int n = 0; n < inst.numAttributes(); n++)\n       values[n] = inst.value(n);\n     values[values.length - 1] = m_Random.nextInt();\n     Instance result = new Instance(1, values);\n     return result;\n   }\n\n   public static void main(String[] args) {\n     runFilter(new SimpleStream(), args);\n   }\n }\n\n\n\n\nA \nreal-world\n implementation of a stream filter is the \nMultiFilter\n (package \nweka.filters\n), which passes the data through all the filters it contains. Depending on whether all the used filters are streamable or not, it acts either as a stream filter or as batch filter.\n\n\nInternals\n\n\nSome useful methods of the filter classes:\n\n\n\n\nisNewBatch()\n - returns \ntrue\n if an instance of the filter was just instantiated via \nnew\n or a new batch was started via the \nbatchFinished()\n method.\n\n\nisFirstBatchDone()\n - returns \ntrue\n as soon as the first batch was finished via the \nbatchFinished()\n method. Useful for \nsupervised\n filters, which should not be altered after being trained with the first batch of instances.\n\n\n\n\nRevisions\n\n\nFilters also implement the \nweka.core.RevisionHandler\n interface. This provides the functionality of obtaining the \nSubversion\n revision from within Java. Filters that are not part of the official Weka distribution will have to implement the method \ngetRevision()\n as follows, which will return a dummy revision of \n1.0\n:\n\n\n  /**\n   * Returns the revision string.\n   * \n   * @return        the revision\n   */\n  public String getRevision() {\n    return RevisionUtils.extract(\"$Revision: 1.0 $\");\n  }\n\n\n\n\nIntegration\n\n\nAfter finishing the coding stage, it's time to integrate your filter in the Weka framework, i.e., to make it available in the Explorer, Experimenter, etc.\n\n\nThe [[GenericObjectEditor]] article shows you how to tell Weka where to find your filter and therefore displaying it in the \nGenericObjectEditor\n (filters work in the same fashion as classifiers, regarding the discovery).\n\n\nTesting\n\n\nWeka provides already a test framework to ensure the basic functionality of a filter. It is essential for the filter to pass these tests.\n\n\nOption handling\n\n\nIf your filter implements \nweka.core.OptionHandler\n, check the \noption handling\n of your filter with the following tool from commandline:\n\n\n weka.core.CheckOptionHandler -W classname [-- additional parameters]\n\n\n\n\nAll tests need to return \nyes\n.\n\n\nGenericObjectEditor\n\n\nThe \nCheckGOE\n class checks whether all the properties available in the GUI have a tooltip accompanying them and whether the \nglobalInfo()\n method is declared:\n\n\n weka.core.CheckGOE -W classname [-- additional parameters]\n\n\n\n\nAll tests, once again, need to return \nyes\n.\n\n\nSource code\n\n\nFilters that implement the \nweka.filters.Sourcable\n interface can output Java code of their internal representation. In order to check the generated code, one should not only compile the code, but also test it with the following test class:\n\n\n weka.filters.CheckSource\n\n\n\n\nThis class takes the original Weka filter, the generated code and the dataset used for generating the source code (and an optional class index) as parameters. It builds the Weka filter on the dataset and compares the output, the one from the Weka filter and the one from the generated source code, whether they are the same.\n\n\nHere's an example call for \nweka.filters.unsupervised.attribute.ReplaceMissingValues\n and the generated class \nweka.filters.WekaWrapper\n (it wraps the actual generated code in a pseudo-filter):\n\n\n java weka.filters.CheckSource \\\n    -W weka.filters.unsupervised.attribute.ReplaceMissingValues \\\n    -S weka.filters.WekaWrapper \\\n    -t data.arff\n\n\n\n\nIt needs to return \nTests OK!\n.\n\n\nUnit tests\n\n\nIn order to make sure that your filter applies to the Weka criteria, you should add your filter to the \njunit\n unit test framework, i.e., by creating a Test class.\n\n\nHow to check out the unit test framework, you can find \nhere\n.\n\n\nSee also\n\n\n\n\n[[GenericObjectEditor|GenericObjectEditor/GenericPropertiesCreator]]\n\n\n\n\nLinks\n\n\n\n\njunit",
            "title": " Writing your own Filter"
        },
        {
            "location": "/writing_filter/#packages",
            "text": "A few comments about the different filter sub-packages:    supervised  - contains supervised filters, i.e., filters that take class distributions into account. Must implement the  weka.filters.SupervisedFilter  interface.   attribute  - filters that work column-wise.  instance  - filters that work row-wise.     unsupervised  - contains unsupervised filters, i.e., they work without taking any class distributions into account. Must implement the  weka.filters.UnsupervisedFilter  interface.   attribute  - filters that work column-wise.  instance  - filters that work row-wise.",
            "title": "Packages"
        },
        {
            "location": "/writing_filter/#choosing-the-superclass",
            "text": "The base filters and interfaces are all located in the following package:   weka.filters  One can basically distinguish between two different kinds of filters:   batch filters  - they need to see the whole dataset before they can start processing it, which they do in one go  stream filters  - they can start producing output right away and the data just passes through while being modified   All filters are derived from the abstract superclass  weka.filters.Filter .  To speed up development, there are also the following abstract filters, depending on the kind of classifier you want to implement:   weka.filters.SimpleBatchFilter  weka.filters.SimpleStreamFilter   These filters simplify the rather general and complex framework introduced by the abstract superclass  weka.filters.Filter . One only needs to implement a couple of abstract methods that will process the actual data and override, if necessary, a few existing methods for option handling.",
            "title": "Choosing the superclass"
        },
        {
            "location": "/writing_filter/#filter",
            "text": "",
            "title": "Filter"
        },
        {
            "location": "/writing_filter/#implementation",
            "text": "The following methods are of importance for the implementation of a filter and explained in detail further down:   getCapabilities()  setInputFormat(Instances)  getInputFormat()  setOutputFormat(Instances)  getOutputFormat()  input(Instance)  bufferInput(Instance)  push(Instance)  output()  batchFinished()  flushInput()  getRevision()   But only the following ones need normally be modified:   getCapabilities()  setInputFormat(Instances)  input(Instance)  batchFinished()  getRevision()",
            "title": "Implementation"
        },
        {
            "location": "/writing_filter/#getcapabilities",
            "text": "Filters implement the  weka.core.CapabilitiesHandler  interface like the classifiers. This method returns what kind of data the filter is able to process. Needs to be adapted for each individual filter.",
            "title": "getCapabilities()"
        },
        {
            "location": "/writing_filter/#setinputformatinstances",
            "text": "With this format, the user tells the filter what format, i.e., attributes, the input data has. This method also tests, whether the filter can actually process this data. All older Weka versions or book branch versions need to check the data manually and throw fitting exceptions, e.g., not being able to handle String attributes.  If the output format of the filter, i.e., the new Instances header, can be determined based alone on this information, then the method should set the output format via  setOutputFormat(Instances)  and return  true , otherwise it has to return  false .",
            "title": "setInputFormat(Instances)"
        },
        {
            "location": "/writing_filter/#getinputformat",
            "text": "This method returns an Instances object containing all currently buffered Instance objects from the input queue.",
            "title": "getInputFormat()"
        },
        {
            "location": "/writing_filter/#setoutputformatinstances",
            "text": "setOutputFormat(Instances)  defines the new Instances header for the output data. For filters that work on a row-basis, there shouldn't be any changes between the input and output format. But filters that work on attributes, e.g., removing, adding, modifying, will affect this format. This method must be called with the appropriate Instances object as parameter, since all Instance objects being processed will rely on the output format.",
            "title": "setOutputFormat(Instances)"
        },
        {
            "location": "/writing_filter/#getoutputformat",
            "text": "This method returns the currently set Instances object that defines the output format. In case  setOutputFormat(Instances)  hasn't been called yet, this method will return  null .",
            "title": "getOutputFormat()"
        },
        {
            "location": "/writing_filter/#inputinstance",
            "text": "The  input(Instance)  method returns  true  if the given Instance can be processed straight away and can be collected immediately via the  output()  method (after adding it to the output queue via  push(Instance) , of course). This is also the case if the first batch of data has been processed and the instance belongs to the second batch. Via  isFirstBatchDone()  one can query whether this instance is still part of the first batch or of the second.  If the Instance cannot be processed immediately, e.g., the filter needs to collect all the data first before doing some calculations, then it needs to be buffered with  bufferInput(Instance)  until  batchFinished()  is called.",
            "title": "input(Instance)"
        },
        {
            "location": "/writing_filter/#bufferinputinstance",
            "text": "In case an Instance cannot be processed immediately, one can use this method to buffer them in the input queue. All buffered Instance objects are available via the  getInputFormat()  method.",
            "title": "bufferInput(Instance)"
        },
        {
            "location": "/writing_filter/#pushinstance",
            "text": "push(Instance)  adds the given Instance to the output queue.",
            "title": "push(Instance)"
        },
        {
            "location": "/writing_filter/#output",
            "text": "Returns the next Instance object from the output queue and removes it from there. In case there is no Instance available this method returns  null .",
            "title": "output()"
        },
        {
            "location": "/writing_filter/#batchfinished",
            "text": "The  batchFinished()  method signifies the end of a dataset being pushed through the filter. In case of a filter that couldn't process the data of the first batch immediately, this is the place to determine what the output format will be (and set if via  setOutputFormat(Instances) ) and process the actual data. The currently available data can be retrieved with the  getInputFormat()  method. After processing the data, one needs to call  flushInput()  to remove all the pending input data.",
            "title": "batchFinished()"
        },
        {
            "location": "/writing_filter/#flushinput",
            "text": "flushInput()  removes all buffered Instance objects from the input queue. This method must be called after all the Instance objects have been processed in the  batchFinished()  method.",
            "title": "flushInput()"
        },
        {
            "location": "/writing_filter/#option-handling",
            "text": "If the filter should be able to handle commandline options, then the weka.core.OptionHandler interface needs to be implemented. In addition to that, the following code should be added at the end of the  setOptions(String[])  method:   if (getInputFormat() != null)\n    setInputFormat(getInputFormat());  This will inform the filter about changes in the options and therefore reset it.",
            "title": "Option handling"
        },
        {
            "location": "/writing_filter/#examples",
            "text": "The following examples are to illustrate the filter framework.   Note:  unseeded random number generators like  Math.random()  should never be used since they will produce different results in each run and repeatable results are essential in machine learning.",
            "title": "Examples"
        },
        {
            "location": "/writing_filter/#batchfilter",
            "text": "This simple batch filter adds a new attribute called //bla// at the end of the dataset. The rows of this attribute contain only the row's index in the data. Since the batch-filter need not see all the data before creating the output format, the  setInputFormat(Instances)  sets the output format and returns  true  (indicating that the output format can be queried immediately). The  batchFinished()  method performs the processing of all the data.    import weka.core.*;\n  import weka.core.Capabilities.*;\n\n  public class BatchFilter\n    extends Filter {\n\n    public String globalInfo() {\n      return   \"A batch filter that adds an additional attribute 'bla' at the end \"\n             + \"containing the index of the processed instance. The output format \"\n             + \"can be collected immediately.\";\n    }\n\n    public Capabilities getCapabilities() {\n      Capabilities result = super.getCapabilities();\n      result.enableAllAttributes();\n      result.enableAllClasses();\n      result.enable(Capability.NO_CLASS);  // filter doesn't need class to be set\n      return result;\n    }\n\n    public boolean setInputFormat(Instances instanceInfo) throws Exception {\n      super.setInputFormat(instanceInfo);\n\n      Instances outFormat = new Instances(instanceInfo, 0);\n      outFormat.insertAttributeAt(new Attribute(\"bla\"), outFormat.numAttributes());\n      setOutputFormat(outFormat);\n\n      return true;  // output format is immediately available\n    }\n\n    public boolean batchFinished() throws Exception {\n      if (getInputFormat() = null)\n        throw new NullPointerException(\"No input instance format defined\");\n\n      Instances inst = getInputFormat();\n      Instances outFormat = getOutputFormat();\n      for (int i = 0; i < inst.numInstances(); i++) {\n        double[] newValues = new double[outFormat.numAttributes()];\n        double[] oldValues = inst.instance(i).toDoubleArray();\n        System.arraycopy(oldValues, 0, newValues, 0, oldValues.length);\n        newValues[newValues.length - 1] = i;\n        push(new Instance(1.0, newValues));\n      }\n\n      flushInput();\n      m_NewBatch = true;\n      m_FirstBatchDone = true;\n      return (numPendingOutput() != 0);\n    }\n\n    public static void main(String[] args) {\n      runFilter(new BatchFilter(), args);\n    }\n  }",
            "title": "BatchFilter"
        },
        {
            "location": "/writing_filter/#batchfilter2",
            "text": "In contrast to the first batch filter, this one here cannot determine the output format immediately (the number of instances in the first batch is part of the attribute name now). This is done in the  batchFinished()  method.    import weka.core.*;\n  import weka.core.Capabilities.*;\n\n  public class BatchFilter2\n    extends Filter {\n\n    public String globalInfo() {\n      return   \"A batch filter that adds an additional attribute 'bla' at the end \"\n             + \"containing the index of the processed instance. The output format \"\n             + \"cannot be collected immediately.\";\n    }\n\n    public Capabilities getCapabilities() {\n      Capabilities result = super.getCapabilities();\n      result.enableAllAttributes();\n      result.enableAllClasses();\n      result.enable(Capability.NO_CLASS);  // filter doesn't need class to be set\n      return result;\n    }\n\n    public boolean batchFinished() throws Exception {\n      if (getInputFormat() = null)\n        throw new NullPointerException(\"No input instance format defined\");\n\n      // output format still needs to be set (depends on first batch of data)\n      if (!isFirstBatchDone()) {\n        Instances outFormat = new Instances(getInputFormat(), 0);\n        outFormat.insertAttributeAt(new Attribute(\n            \"bla-\" + getInputFormat().numInstances()), outFormat.numAttributes());\n        setOutputFormat(outFormat);\n      }\n\n      Instances inst = getInputFormat();\n      Instances outFormat = getOutputFormat();\n      for (int i = 0; i < inst.numInstances(); i++) {\n        double[] newValues = new double[outFormat.numAttributes()];\n        double[] oldValues = inst.instance(i).toDoubleArray();\n        System.arraycopy(oldValues, 0, newValues, 0, oldValues.length);\n        newValues[newValues.length - 1] = i;\n        push(new Instance(1.0, newValues));\n      }\n\n      flushInput();\n      m_NewBatch = true;\n      m_FirstBatchDone = true;\n\n      return (numPendingOutput() != 0);\n    }\n\n    public static void main(String[] args) {\n      runFilter(new BatchFilter2(), args);\n    }\n  }",
            "title": "BatchFilter2"
        },
        {
            "location": "/writing_filter/#batchfilter3",
            "text": "As soon as this batch filter's first batch is done, it can process Instance objects immediately in the  input(Instance)  method. It adds a new attribute which contains just a random number, but the random number generator being used is seeded with the number of instances from the first batch.    import weka.core.*;\n  import weka.core.Capabilities.*;\n\n  import java.util.Random;\n\n  public class BatchFilter3\n    extends Filter {\n\n    protected int m_Seed;\n    protected Random m_Random;\n\n    public String globalInfo() {\n      return   \"A batch filter that adds an attribute 'bla' at the end \"\n             + \"containing a random number. The output format cannot be collected \"\n             + \"immediately.\";\n    }\n\n    public Capabilities getCapabilities() {\n      Capabilities result = super.getCapabilities();\n      result.enableAllAttributes();\n      result.enableAllClasses();\n      result.enable(Capability.NO_CLASS);  // filter doesn't need class to be set\n      return result;\n    }\n\n    public boolean input(Instance instance) throws Exception {\n      if (getInputFormat() = null)\n        throw new NullPointerException(\"No input instance format defined\");\n\n      if (isNewBatch()) {\n        resetQueue();\n        m_NewBatch = false;\n      }\n\n      if (isFirstBatchDone())\n        convertInstance(instance);\n      else\n        bufferInput(instance);\n\n      return isFirstBatchDone();\n    }\n\n    public boolean batchFinished() throws Exception {\n      if (getInputFormat() = null)\n        throw new NullPointerException(\"No input instance format defined\");\n\n      // output format still needs to be set (random number generator is seeded\n      // with number of instances of first batch)\n      if (!isFirstBatchDone()) {\n        m_Seed = getInputFormat().numInstances();\n        Instances outFormat = new Instances(getInputFormat(), 0);\n        outFormat.insertAttributeAt(new Attribute(\n            \"bla-\" + getInputFormat().numInstances()), outFormat.numAttributes());\n        setOutputFormat(outFormat);\n      }\n\n      Instances inst = getInputFormat();\n      for (int i = 0; i < inst.numInstances(); i++) {\n        convertInstance(inst.instance(i));\n      }\n\n      flushInput();\n      m_NewBatch = true;\n      m_FirstBatchDone = true;\n      m_Random = null;\n\n      return (numPendingOutput() != 0);\n    }\n\n    protected void convertInstance(Instance instance) {\n      if (m_Random = null)\n        m_Random = new Random(m_Seed);\n      double[] newValues = new double[instance.numAttributes() + 1];\n      double[] oldValues = instance.toDoubleArray();\n      newValues[newValues.length - 1] = m_Random.nextInt();\n      System.arraycopy(oldValues, 0, newValues, 0, oldValues.length);\n      push(new Instance(1.0, newValues));\n    }\n\n    public static void main(String[] args) {\n      runFilter(new BatchFilter3(), args);\n    }\n  }",
            "title": "BatchFilter3"
        },
        {
            "location": "/writing_filter/#streamfilter",
            "text": "This stream filter adds a random number at the end of each instance of the input data. Since this doesn't rely on having access to the full data of the first batch, the output format is accessible immediately after using  setInputFormat(Instances) . All the Instance objects are immediately processed in  input(Instance)  via the  convertInstance(Instance)  method, which pushes them immediately to the output queue.    import weka.core.*;\n  import weka.core.Capabilities.*;\n\n  import java.util.Random;\n\n  public class StreamFilter\n    extends Filter {\n\n    protected Random m_Random;\n\n    public String globalInfo() {\n      return   \"A stream filter that adds an attribute 'bla' at the end \"\n             + \"containing a random number. The output format can be collected \"\n             + \"immediately.\";\n    }\n\n    public Capabilities getCapabilities() {\n      Capabilities result = super.getCapabilities();\n      result.enableAllAttributes();\n      result.enableAllClasses();\n      result.enable(Capability.NO_CLASS);  // filter doesn't need class to be set\n      return result;\n    }\n\n    public boolean setInputFormat(Instances instanceInfo) throws Exception {\n      super.setInputFormat(instanceInfo);\n\n      Instances outFormat = new Instances(instanceInfo, 0);\n      outFormat.insertAttributeAt(new Attribute(\"bla\"), outFormat.numAttributes());\n      setOutputFormat(outFormat);\n\n      m_Random = new Random(1);\n\n      return true;  // output format is immediately available\n    }\n\n    public boolean input(Instance instance) throws Exception {\n      if (getInputFormat() = null)\n        throw new NullPointerException(\"No input instance format defined\");\n\n      if (isNewBatch()) {\n        resetQueue();\n        m_NewBatch = false;\n      }\n\n      convertInstance(instance);\n\n      return true;  // can be immediately collected via output()\n    }\n\n    protected void convertInstance(Instance instance) {\n      double[] newValues = new double[instance.numAttributes() + 1];\n      double[] oldValues = instance.toDoubleArray();\n      newValues[newValues.length - 1] = m_Random.nextInt();\n      System.arraycopy(oldValues, 0, newValues, 0, oldValues.length);\n      push(new Instance(1.0, newValues));\n    }\n\n    public static void main(String[] args) {\n      runFilter(new StreamFilter(), args);\n    }\n  }",
            "title": "StreamFilter"
        },
        {
            "location": "/writing_filter/#simplebatchfilter",
            "text": "Only the following abstract methods need to be implemented:   globalInfo()  - returns a short description of what the filter does; will be displayed in the GUI  determineOutputFormat(Instances)  - generates the new format, based on the input data  process(Instances)  - processes the whole dataset in one go  getRevision()  - returns the  Subversion  revision information   If you need access to the full input dataset in  determineOutputFormat(Instances) , then you need to also override the method  allowAccessToFullInputFormat()  and make it return true.   If more options are necessary, then the following methods need to be overridden:   listOptions()  - returns an enumeration of the available options; these are printed if one calls the filter with the  -h  option  setOptions(String[])  - parses the given option array, that were passed from commandline  getOptions()  - returns an array of options, resembling the current setup of the filter   In the following an  example implementation  that adds an additional attribute at the end, containing the index of the processed instance:   import weka.core.*;\n import weka.core.Capabilities.*;\n import weka.filters.*;\n\n public class SimpleBatch\n   extends SimpleBatchFilter {\n\n   public String globalInfo() {\n     return   \"A simple batch filter that adds an additional attribute 'bla' at the end \"\n            + \"containing the index of the processed instance.\";\n   }\n\n   public Capabilities getCapabilities() {\n     Capabilities result = super.getCapabilities();\n     result.enableAllAttributes();\n     result.enableAllClasses();\n     result.enable(Capability.NO_CLASS);  //// filter doesn't need class to be set//\n     return result;\n   }\n\n   protected Instances determineOutputFormat(Instances inputFormat) {\n     Instances result = new Instances(inputFormat, 0);\n     result.insertAttributeAt(new Attribute(\"bla\"), result.numAttributes());\n     return result;\n   }\n\n   protected Instances process(Instances inst) {\n     Instances result = new Instances(determineOutputFormat(inst), 0);\n     for (int i = 0; i < inst.numInstances(); i++) {\n       double[] values = new double[result.numAttributes()];\n       for (int n = 0; n < inst.numAttributes(); n++)\n         values[n] = inst.instance(i).value(n);\n       values[values.length - 1] = i;\n       result.add(new Instance(1, values));\n     }\n     return result;\n   }\n\n   public static void main(String[] args) {\n     runFilter(new SimpleBatch(), args);\n   }\n }",
            "title": "SimpleBatchFilter"
        },
        {
            "location": "/writing_filter/#simplestreamfilter",
            "text": "Only the following abstract methods need to be implemented:   globalInfo()  - returns a short description of what the filter does; will be displayed in the GUI  determineOutputFormat(Instances)  - generates the new format, based on the input data  process(Instance) processes a single instance and turns it from the old format into the new one  getRevision()  - returns the  Subversion  revision information   The  reset()  method is only used, since the random number generator needs to be re-initialized in order to obtain repeatable results.  If more options are necessary, then the following methods need to be overridden:   listOptions()  - returns an enumeration of the available options; these are printed if one calls the filter with the  -h  option  setOptions(String[])  - parses the given option array, that were passed from commandline  getOptions()  - returns an array of options, resembling the current setup of the filter   In the following an  example implementation  of a stream filter that adds an extra attribute at the end, which is filled with random numbers:   import weka.core.*;\n import weka.core.Capabilities.*;\n import weka.filters.*;\n\n import java.util.Random;\n\n public class SimpleStream\n   extends SimpleStreamFilter {\n\n   protected Random m_Random;\n\n   public String globalInfo() {\n     return   \"A simple stream filter that adds an attribute 'bla' at the end \"\n            + \"containing a random number.\";\n   }\n\n   public Capabilities getCapabilities() {\n     Capabilities result = super.getCapabilities();\n     result.enableAllAttributes();\n     result.enableAllClasses();\n     result.enable(Capability.NO_CLASS);  //// filter doesn't need class to be set//\n     return result;\n   }\n\n   protected void reset() {\n     super.reset();\n     m_Random = new Random(1);\n   }\n\n   protected Instances determineOutputFormat(Instances inputFormat) {\n     Instances result = new Instances(inputFormat, 0);\n     result.insertAttributeAt(new Attribute(\"bla\"), result.numAttributes());\n     return result;\n   }\n\n   protected Instance process(Instance inst) {\n     double[] values = new double[inst.numAttributes() + 1];\n     for (int n = 0; n < inst.numAttributes(); n++)\n       values[n] = inst.value(n);\n     values[values.length - 1] = m_Random.nextInt();\n     Instance result = new Instance(1, values);\n     return result;\n   }\n\n   public static void main(String[] args) {\n     runFilter(new SimpleStream(), args);\n   }\n }  A  real-world  implementation of a stream filter is the  MultiFilter  (package  weka.filters ), which passes the data through all the filters it contains. Depending on whether all the used filters are streamable or not, it acts either as a stream filter or as batch filter.",
            "title": "SimpleStreamFilter"
        },
        {
            "location": "/writing_filter/#internals",
            "text": "Some useful methods of the filter classes:   isNewBatch()  - returns  true  if an instance of the filter was just instantiated via  new  or a new batch was started via the  batchFinished()  method.  isFirstBatchDone()  - returns  true  as soon as the first batch was finished via the  batchFinished()  method. Useful for  supervised  filters, which should not be altered after being trained with the first batch of instances.",
            "title": "Internals"
        },
        {
            "location": "/writing_filter/#revisions",
            "text": "Filters also implement the  weka.core.RevisionHandler  interface. This provides the functionality of obtaining the  Subversion  revision from within Java. Filters that are not part of the official Weka distribution will have to implement the method  getRevision()  as follows, which will return a dummy revision of  1.0 :    /**\n   * Returns the revision string.\n   * \n   * @return        the revision\n   */\n  public String getRevision() {\n    return RevisionUtils.extract(\"$Revision: 1.0 $\");\n  }",
            "title": "Revisions"
        },
        {
            "location": "/writing_filter/#integration",
            "text": "After finishing the coding stage, it's time to integrate your filter in the Weka framework, i.e., to make it available in the Explorer, Experimenter, etc.  The [[GenericObjectEditor]] article shows you how to tell Weka where to find your filter and therefore displaying it in the  GenericObjectEditor  (filters work in the same fashion as classifiers, regarding the discovery).",
            "title": "Integration"
        },
        {
            "location": "/writing_filter/#testing",
            "text": "Weka provides already a test framework to ensure the basic functionality of a filter. It is essential for the filter to pass these tests.",
            "title": "Testing"
        },
        {
            "location": "/writing_filter/#option-handling_1",
            "text": "If your filter implements  weka.core.OptionHandler , check the  option handling  of your filter with the following tool from commandline:   weka.core.CheckOptionHandler -W classname [-- additional parameters]  All tests need to return  yes .",
            "title": "Option handling"
        },
        {
            "location": "/writing_filter/#genericobjecteditor",
            "text": "The  CheckGOE  class checks whether all the properties available in the GUI have a tooltip accompanying them and whether the  globalInfo()  method is declared:   weka.core.CheckGOE -W classname [-- additional parameters]  All tests, once again, need to return  yes .",
            "title": "GenericObjectEditor"
        },
        {
            "location": "/writing_filter/#source-code",
            "text": "Filters that implement the  weka.filters.Sourcable  interface can output Java code of their internal representation. In order to check the generated code, one should not only compile the code, but also test it with the following test class:   weka.filters.CheckSource  This class takes the original Weka filter, the generated code and the dataset used for generating the source code (and an optional class index) as parameters. It builds the Weka filter on the dataset and compares the output, the one from the Weka filter and the one from the generated source code, whether they are the same.  Here's an example call for  weka.filters.unsupervised.attribute.ReplaceMissingValues  and the generated class  weka.filters.WekaWrapper  (it wraps the actual generated code in a pseudo-filter):   java weka.filters.CheckSource \\\n    -W weka.filters.unsupervised.attribute.ReplaceMissingValues \\\n    -S weka.filters.WekaWrapper \\\n    -t data.arff  It needs to return  Tests OK! .",
            "title": "Source code"
        },
        {
            "location": "/writing_filter/#unit-tests",
            "text": "In order to make sure that your filter applies to the Weka criteria, you should add your filter to the  junit  unit test framework, i.e., by creating a Test class.  How to check out the unit test framework, you can find  here .",
            "title": "Unit tests"
        },
        {
            "location": "/writing_filter/#see-also",
            "text": "[[GenericObjectEditor|GenericObjectEditor/GenericPropertiesCreator]]",
            "title": "See also"
        },
        {
            "location": "/writing_filter/#links",
            "text": "junit",
            "title": "Links"
        },
        {
            "location": "/add_weights_to_dataset/",
            "text": "The following examples show how to add weights to normal datasets and save them in the new XRFF data format. A version of Weka later than 3.5.3 (or the code from \nSubversion\n) is necessary for this code to work.\n\n\nAdd arbitrary weights\n\n\n import weka.core.converters.ConverterUtils.DataSource;\n import weka.core.converters.XRFFSaver;\n import weka.core.Instances;\n\n import java.io.File;\n\n /**\n  * Loads file \"args[0]\", sets class if necessary (in that case the last \n  * attribute), adds some test weights and saves it as XRFF file\n  * under \"args[1]\". E.g.: \n<br/>\n  *   AddWeights anneal.arff anneal.xrff.gz\n  *\n  * @author FracPete (fracpete at waikato dot ac dot nz)\n  */\n public class AddWeights {\n   public static void main(String[] args) throws Exception {\n     * load data\n     DataSource source = new DataSource(args[0]);\n     Instances data = source.getDataSet();\n     if (data.classIndex() == -1)\n       data.setClassIndex(data.numAttributes() - 1);\n\n     * set weights\n     double factor = 0.5  / (double) data.numInstances();\n     for (int i = 0; i < data.numInstances(); i++) {\n       data.instance(i).setWeight(0.5 + factor*i);\n     }\n\n     // save data\n     XRFFSaver saver = new XRFFSaver();\n     saver.setFile(new File(args[1]));\n     saver.setInstances(data);\n     saver.writeBatch();\n   }\n }\n\n\n\n\nAdd weights stored in an external file\n\n\n import weka.core.converters.ConverterUtils.DataSource;\n import weka.core.converters.XRFFSaver;\n import weka.core.Instances;\n\n import java.io.BufferedReader;\n import java.io.File;\n import java.io.FileReader;\n\n /**\n  * Loads file \"args[0]\" (can be ARFF, CSV, C4.5, etc.), sets class if necessary\n  * (in that case the last attribute), adds weights from \"args[1]\" (one weight\n  * per line) and saves it as XRFF file under \"args[2]\". E.g.: \n<br/>\n  *   AddWeightsFromFile anneal.arff weights.txt anneal.xrff.gz\n  *\n  * @author FracPete (fracpete at waikato dot ac dot nz)\n  */\n public class AddWeightsFromFile {\n   public static void main(String[] args) throws Exception {\n     * load data\n     DataSource source = new DataSource(args[0]);\n     Instances data = source.getDataSet();\n     if (data.classIndex() == -1)\n       data.setClassIndex(data.numAttributes() - 1);\n\n     * read and set weights\n     BufferedReader reader = new BufferedReader(new FileReader(args[1]));\n     for (int i = 0; i < data.numInstances(); i++) {\n       String line = reader.readLine();\n       double weight = Double.parseDouble(line);\n       data.instance(i).setWeight(weight);\n     }\n     reader.close();\n\n     // save data\n     XRFFSaver saver = new XRFFSaver();\n     saver.setFile(new File(args[2]));\n     saver.setInstances(data);\n     saver.writeBatch();\n   }\n }\n\n\n\n\nAdd weights stored in the attribute\n\n\n import weka.core.converters.ConverterUtils.DataSource;\n import weka.core.converters.XRFFSaver;\n import weka.core.Instances; \n\n import java.io.File;\n\n /**\n  * Loads file \"args[0]\", Adds weight given in attribute with\n  * index \"args[1]\" - 1, deletes this attribute.\n  * sets class if necessary (in that case the last \n  * attribute) and saves it as XRFF file\n  * under \"args[2]\". E.g.: \n<br/>\n  *   AddWeightsFromAtt file.arff 2 file.xrff.gz\n  *\n  * @author FracPete (fracpete at waikato dot ac dot nz)\n  * @author gabi (gs23 at waikato dot ac dot nz)\n  */\n public class AddWeightsFromAtt {\n   public static void main(String[] args) throws Exception {\n     * load data\n     DataSource source = new DataSource(args[0]);\n     Instances data = source.getDataSet(); \n\n     * get weight index\n     int wIndex = Integer.parseInt(args[1]) - 1;\n\n     * set weights\n     for (int i = 0; i < data.numInstances(); i++) {\n       double weight = data.instance(i).value(wIndex);\n       data.instance(i).setWeight(weight);\n     }\n\n     * delete weight attribute and set class index\n     data.deleteAttributeAt(wIndex);\n     if (data.classIndex() == -1)\n       data.setClassIndex(data.numAttributes() - 1);\n\n     * save data\n     XRFFSaver saver = new XRFFSaver();\n     saver.setFile(new File(args[2]));\n     saver.setInstances(data);\n     saver.writeBatch();\n   }\n }\n\n\n\n\nDownload\n\n\n\n\nAddWeights.java\n\n\nAddWeightsFromFile.java\n\n\nAddWeightsFromAtt.java\n\n\n\n\nSee also\n\n\n\n\nSubversion\n\n\nThe unofficial Weka package \ndataset-weights\n allows you to modify attribute/instance weights using filters - no coding required",
            "title": " Add weights to dataset"
        },
        {
            "location": "/add_weights_to_dataset/#add-arbitrary-weights",
            "text": "import weka.core.converters.ConverterUtils.DataSource;\n import weka.core.converters.XRFFSaver;\n import weka.core.Instances;\n\n import java.io.File;\n\n /**\n  * Loads file \"args[0]\", sets class if necessary (in that case the last \n  * attribute), adds some test weights and saves it as XRFF file\n  * under \"args[1]\". E.g.: \n<br/>\n  *   AddWeights anneal.arff anneal.xrff.gz\n  *\n  * @author FracPete (fracpete at waikato dot ac dot nz)\n  */\n public class AddWeights {\n   public static void main(String[] args) throws Exception {\n     * load data\n     DataSource source = new DataSource(args[0]);\n     Instances data = source.getDataSet();\n     if (data.classIndex() == -1)\n       data.setClassIndex(data.numAttributes() - 1);\n\n     * set weights\n     double factor = 0.5  / (double) data.numInstances();\n     for (int i = 0; i < data.numInstances(); i++) {\n       data.instance(i).setWeight(0.5 + factor*i);\n     }\n\n     // save data\n     XRFFSaver saver = new XRFFSaver();\n     saver.setFile(new File(args[1]));\n     saver.setInstances(data);\n     saver.writeBatch();\n   }\n }",
            "title": "Add arbitrary weights"
        },
        {
            "location": "/add_weights_to_dataset/#add-weights-stored-in-an-external-file",
            "text": "import weka.core.converters.ConverterUtils.DataSource;\n import weka.core.converters.XRFFSaver;\n import weka.core.Instances;\n\n import java.io.BufferedReader;\n import java.io.File;\n import java.io.FileReader;\n\n /**\n  * Loads file \"args[0]\" (can be ARFF, CSV, C4.5, etc.), sets class if necessary\n  * (in that case the last attribute), adds weights from \"args[1]\" (one weight\n  * per line) and saves it as XRFF file under \"args[2]\". E.g.: \n<br/>\n  *   AddWeightsFromFile anneal.arff weights.txt anneal.xrff.gz\n  *\n  * @author FracPete (fracpete at waikato dot ac dot nz)\n  */\n public class AddWeightsFromFile {\n   public static void main(String[] args) throws Exception {\n     * load data\n     DataSource source = new DataSource(args[0]);\n     Instances data = source.getDataSet();\n     if (data.classIndex() == -1)\n       data.setClassIndex(data.numAttributes() - 1);\n\n     * read and set weights\n     BufferedReader reader = new BufferedReader(new FileReader(args[1]));\n     for (int i = 0; i < data.numInstances(); i++) {\n       String line = reader.readLine();\n       double weight = Double.parseDouble(line);\n       data.instance(i).setWeight(weight);\n     }\n     reader.close();\n\n     // save data\n     XRFFSaver saver = new XRFFSaver();\n     saver.setFile(new File(args[2]));\n     saver.setInstances(data);\n     saver.writeBatch();\n   }\n }",
            "title": "Add weights stored in an external file"
        },
        {
            "location": "/add_weights_to_dataset/#add-weights-stored-in-the-attribute",
            "text": "import weka.core.converters.ConverterUtils.DataSource;\n import weka.core.converters.XRFFSaver;\n import weka.core.Instances; \n\n import java.io.File;\n\n /**\n  * Loads file \"args[0]\", Adds weight given in attribute with\n  * index \"args[1]\" - 1, deletes this attribute.\n  * sets class if necessary (in that case the last \n  * attribute) and saves it as XRFF file\n  * under \"args[2]\". E.g.: \n<br/>\n  *   AddWeightsFromAtt file.arff 2 file.xrff.gz\n  *\n  * @author FracPete (fracpete at waikato dot ac dot nz)\n  * @author gabi (gs23 at waikato dot ac dot nz)\n  */\n public class AddWeightsFromAtt {\n   public static void main(String[] args) throws Exception {\n     * load data\n     DataSource source = new DataSource(args[0]);\n     Instances data = source.getDataSet(); \n\n     * get weight index\n     int wIndex = Integer.parseInt(args[1]) - 1;\n\n     * set weights\n     for (int i = 0; i < data.numInstances(); i++) {\n       double weight = data.instance(i).value(wIndex);\n       data.instance(i).setWeight(weight);\n     }\n\n     * delete weight attribute and set class index\n     data.deleteAttributeAt(wIndex);\n     if (data.classIndex() == -1)\n       data.setClassIndex(data.numAttributes() - 1);\n\n     * save data\n     XRFFSaver saver = new XRFFSaver();\n     saver.setFile(new File(args[2]));\n     saver.setInstances(data);\n     saver.writeBatch();\n   }\n }",
            "title": "Add weights stored in the attribute"
        },
        {
            "location": "/add_weights_to_dataset/#download",
            "text": "AddWeights.java  AddWeightsFromFile.java  AddWeightsFromAtt.java",
            "title": "Download"
        },
        {
            "location": "/add_weights_to_dataset/#see-also",
            "text": "Subversion  The unofficial Weka package  dataset-weights  allows you to modify attribute/instance weights using filters - no coding required",
            "title": "See also"
        },
        {
            "location": "/adding_tabs_in_the_explorer/",
            "text": "Description\n\n\nThis article explains how to add extra tabs in the Explorer in order to add new functionality without the hassle of having to dig into the Explorer code oneself. With the new plugin-architecture of the Explorer it is fairly easy making your extensions available in the GUI.\n\n\nNote:\n This is also covered in chapter \nExtending WEKA\n of the WEKA manual in versions later than 3.6.1/3.7.0 or \nsnapshots\n of the stable-3.6/developer version later than 10/01/2010.\n\n\nVersion\n\n\n\n\n3.5.5 or \nSnapshot\n\n\n\n\nRequirements\n\n\nHere is roughly what is required in order to add a new tab (the examples go into more detail):\n\n\n\n\nyour class must be derived from \njavax.swing.JPanel\n\n\nyour class must implemented the interface \nweka.gui.explorer.Explorer.ExplorerPanel\n\n\noptional interfaces\n\n\nweka.gui.explorer.Explorer.LogHandler\n\n\n\n\nin case you want to take advantage of the logging in the Explorer\n\n\n\n\n\n\n\n\nweka.gui.explorer.Explorer.CapabilitiesFilterChangeListener\n\n\n\n\nin case your class needs to be notified of changes in the Capabilities, e.g., if new data is loaded into the Explorer\n\n\n\n\n\n\n\n\n\n\n\n\nadding the classname of your class to the \nTabs\n property in the \nExplorer.props\n file\n\n\n\n\nExamples\n\n\nThe following examples demonstrate the new plugin architecture (a bold term for such a simple extension mechanism). Only the necessary details are discussed, as the full source code is available for download as well.\n\n\nSQL worksheet\n\n\nPurpose\n\n\nDisplaying the \nSqlViewer\n as a tab in the Explorer instead of using it either via the \nOpen DB...\n button or as standalone application. Uses the existing components already available in Weka and just assembles them in a \nJPanel\n. Since this tab does not rely on a dataset being loaded into the Explorer, it will be used as a  \nstandalone\n one.\n\n\nUseful for people who are working a lot with databases and would like to have an SQL worksheet available all the time instead of clicking on a button every time to open up a database dialog.\n\n\nImplementation\n\n\n\n\nclass is derived from \njavax.swing.JPanel\n and implements the \nweka.gui.explorer.Explorer.ExplorerPanel\n interface (the full source code also imports the \nweka.gui.explorer.Explorer.LogHandler\n interface, but that is only additional functionality):\n\n\n\n\n public class SqlPanel\n   extends JPanel\n   implements ExplorerPanel {\n\n\n\n\n\n\nsome basic members that we need to have\n\n\n\n\n  /** the parent frame */\n  protected Explorer m_Explorer = null;\n\n  /** sends notifications when the set of working instances gets changed*/\n  protected PropertyChangeSupport m_Support = new PropertyChangeSupport(this);\n\n\n\n\n\n\nmethods we need to implement due to the used interfaces\n\n\n\n\n  /** Sets the Explorer to use as parent frame */\n  public void setExplorer(Explorer parent) {\n    m_Explorer = parent;\n  }\n\n  /** returns the parent Explorer frame */\n  public Explorer getExplorer() {\n    return m_Explorer;\n  }\n\n  /** Returns the title for the tab in the Explorer */\n  public String getTabTitle() {\n    return \"SQL\";  * what's displayed as tab-title, e.g., *Classify//\n  }\n\n  /** Returns the tooltip for the tab in the Explorer */\n  public String getTabTitleToolTip() {\n    return \"Retrieving data from databases\";  // the tooltip of the tab\n  }\n\n  /** ignored, since we *\"generate\"* data and not receive it */\n  public void setInstances(Instances inst) {\n  }\n\n  /** PropertyChangeListener who will be notified of value changes. */\n  public void addPropertyChangeListener(PropertyChangeListener l) {\n    m_Support.addPropertyChangeListener(l);\n  }\n\n  /** Removes a PropertyChangeListener. */\n  public void removePropertyChangeListener(PropertyChangeListener l) {\n    m_Support.removePropertyChangeListener(l);\n  }\n\n\n\n\n\n\nadditional GUI elements\n\n\n\n\n  /** the actual SQL worksheet */\n  protected SqlViewer m_Viewer;\n\n  /** the panel for the buttons */\n  protected JPanel m_PanelButtons;\n\n  /** the Load button - makes the data available in the Explorer */\n  protected JButton m_ButtonLoad = new JButton(\"Load data\");\n\n  /** displays the current query */\n  protected JLabel m_LabelQuery = new JLabel(\"\");\n\n\n\n\n\n\nloading the data into the Explorer by clicking on the \nLoad\n button will fire a property change:\n\n\n\n\n    m_ButtonLoad.addActionListener(new ActionListener() {\n      public void actionPerformed(ActionEvent evt){\n        m_Support.firePropertyChange(\"\", null, null);\n      }\n    });\n\n\n\n\n\n\nthe \npropertyChange\n event will perform the actual loading of the data, hence we add an anonymous property change listener to our panel:\n\n\n\n\n  addPropertyChangeListener(new PropertyChangeListener() {\n    public void propertyChange(PropertyChangeEvent e) {\n      try {\n        * load data\n        InstanceQuery query = new InstanceQuery();\n        query.setDatabaseURL(m_Viewer.getURL());\n        query.setUsername(m_Viewer.getUser());\n        query.setPassword(m_Viewer.getPassword());\n        Instances data = query.retrieveInstances(m_Viewer.getQuery());\n\n        * set data in preprocess panel (will also notify of capabilties changes)\n        getExplorer().getPreprocessPanel().setInstances(data);\n      }\n      catch (Exception ex) {\n        ex.printStackTrace();\n      }\n    }\n  });\n\n\n\n\n\n\nIn order to add our \nSqlPanel\n to the list of tabs displayed in the Explorer, we need to modify the \nExplorer.props\n file (just extract it from the \nweka.jar\n and place it in your home directory). The \nTabs\n property must look like this:\n\n\n\n\n Tabs=weka.gui.explorer.SqlPanel,\\\n      weka.gui.explorer.ClassifierPanel,\\\n      weka.gui.explorer.ClustererPanel,\\\n      weka.gui.explorer.AssociationsPanel,\\\n      weka.gui.explorer.AttributeSelectionPanel,\\\n      weka.gui.explorer.VisualizePanel\n\n\n\n\nScreenshot\n\n\n\n\nSource\n\n\n\n\nSqlPanel.java\n (\nstable-3.6\n, \ndeveloper\n)\n\n\n\n\nArtificial data generation\n\n\nPurpose\n\n\nInstead of only having a \nGenerate...\n button in the PreprocessPanel or using it from commandline, this example creates a new panel to be displayed as extra tab in the Explorer. This tab will be available regardless whether a dataset is already loaded or not (= \nstandalone\n).\n\n\nImplementation\n\n\n\n\nclass is derived from \njavax.swing.JPanel\n and implements the \nweka.gui.Explorer.ExplorerPanel\n interface (the full source code also imports the \nweka.gui.Explorer.LogHandler\n interface, but that is only additional functionality):\n\n\n\n\n public class GeneratorPanel\n   extends JPanel\n   implements ExplorerPanel {\n\n\n\n\n\n\nsome basic members that we need to have (the same as for the \nSqlPanel\n class):\n\n\n\n\n  /** the parent frame */\n protected Explorer m_Explorer = null;\n\n /** sends notifications when the set of working instances gets changed*/\n protected PropertyChangeSupport m_Support = new PropertyChangeSupport(this);\n\n\n\n\n\n\nmethods we need to implement due to the used interfaces (almost identical to \nSqlPanel\n):\n\n\n\n\n /** Sets the Explorer to use as parent frame */\n public void setExplorer(Explorer parent) {\n   m_Explorer = parent;\n }\n\n /** returns the parent Explorer frame */\n public Explorer getExplorer() {\n   return m_Explorer;\n }\n\n /** Returns the title for the tab in the Explorer */\n public String getTabTitle() {\n   return \"DataGeneration\";  // what's displayed as tab-title, e.g., Classify\n }\n\n /** Returns the tooltip for the tab in the Explorer */\n public String getTabTitleToolTip() {\n   return \"Generating artificial datasets\";  // the tooltip of the tab\n }\n\n /** ignored, since we \"generate\" data and not receive it */\n public void setInstances(Instances inst) {\n }\n\n /** PropertyChangeListener who will be notified of value changes. */\n public void addPropertyChangeListener(PropertyChangeListener l) {\n   m_Support.addPropertyChangeListener(l);\n }\n\n /** Removes a PropertyChangeListener. */\n public void removePropertyChangeListener(PropertyChangeListener l) {\n   m_Support.removePropertyChangeListener(l);\n }\n\n\n\n\n\n\nadditional GUI elements:\n\n\n\n\n  /** the GOE for the generators */\n  protected GenericObjectEditor m_GeneratorEditor = new GenericObjectEditor();\n\n  /** the text area for the output of the generated data */\n  protected JTextArea m_Output = new JTextArea();\n\n  /** the Generate button */\n  protected JButton m_ButtonGenerate = new JButton(\"Generate\");\n\n  /** the Use button */\n  protected JButton m_ButtonUse = new JButton(\"Use\");\n\n\n\n\n\n\nthe \nGenerate\n button doesn't load the generated data directly into the Explorer, but only outputs in the \nJTextArea\n (this is done with the \nUse\n button - see further down):\n\n\n\n\n  m_ButtonGenerate.addActionListener(new ActionListener(){\n    public void actionPerformed(ActionEvent evt){\n      DataGenerator generator = (DataGenerator) m_GeneratorEditor.getValue();\n      String relName = generator.getRelationName();\n\n      String cname = generator.getClass().getName().replaceAll(\".*\\\\.\", \"\");\n      String cmd = generator.getClass().getName();\n      if (generator instanceof OptionHandler)\n        cmd += \" \" + Utils.joinOptions(((OptionHandler) generator).getOptions());\n\n      try {\n        * generate data\n        StringWriter output = new StringWriter();\n        generator.setOutput(new PrintWriter(output));\n        DataGenerator.makeData(generator, generator.getOptions());\n        m_Output.setText(output.toString());\n      }\n      catch (Exception ex) {\n        ex.printStackTrace();\n        JOptionPane.showMessageDialog(\n          getExplorer(), \"Error generating data:\\n\" + ex.getMessage(), \n          \"Error\", JOptionPane.ERROR_MESSAGE);\n      }\n\n      generator.setRelationName(relName);\n    }\n  });\n\n\n\n\n\n\nthe \nUse\n button finally fires a \nproperty change\n event that will load the data into the Explorer:\n\n\n\n\n    m_ButtonUse.addActionListener(new ActionListener(){\n      public void actionPerformed(ActionEvent evt){\n        m_Support.firePropertyChange(\"\", null, null);\n      }\n    });\n\n\n\n\n\n\nthe \npropertyChange\n event will perform the actual loading of the data, hence we add an anonymous property change listener to our panel:\n\n\n\n\n  addPropertyChangeListener(new PropertyChangeListener() {\n    public void propertyChange(PropertyChangeEvent e) {\n      try {\n        Instances data = new Instances(new StringReader(m_Output.getText()));\n        * set data in preprocess panel (will also notify of capabilties changes)\n        getExplorer().getPreprocessPanel().setInstances(data);\n      }\n      catch (Exception ex) {\n        ex.printStackTrace();\n        JOptionPane.showMessageDialog(\n          getExplorer(), \"Error generating data:\\n\" + ex.getMessage(), \n          \"Error\", JOptionPane.ERROR_MESSAGE);\n      }\n    }\n  });\n\n\n\n\n\n\nIn order to add our \nGeneratorPanel\n to the list of tabs displayed in the Explorer, we need to modify the \nExplorer.props\n file (just extract it from the \nweka.jar\n and place it in your home directory). The \nTabs\n property must look like this:\n\n\n\n\n Tabs=weka.gui.explorer.GeneratorPanel:standalone,\\\n      weka.gui.explorer.ClassifierPanel,\\\n      weka.gui.explorer.ClustererPanel,\\\n      weka.gui.explorer.AssociationsPanel,\\\n      weka.gui.explorer.AttributeSelectionPanel,\\\n      weka.gui.explorer.VisualizePanel\n\n\n\n\n\n\nNote:\n the \nstandalone\n option is used to make the tab available without requiring the preprocess panel to load a dataset first.\n\n\n\n\nScreenshot\n\n\n\n\nSource\n\n\n\n\nGeneratorPanel.java\n (\nstable-3.6\n, \ndeveloper\n)\n\n\n\n\nExperimenter \"light\"\n\n\nPurpose\n\n\nBy default the \nClassify\n panel only performs 1 run of 10-fold cross-validation. Since most classifiers are rather sensitive to the order of the data being presented to them, those results can be too optimistic or pessimistic. Averaging the results over 10 runs with differently randomized train/test pairs returns more reliable results. And this is where this plugin comes in: \nit can be used to obtain statistical sound results for a specific classifier/dataset combination, without having to setup a whole experiment in the Experimenter. \n\n\nImplementation\n\n\n\n\nSince this plugin is rather bulky, we omit the implementation details, but the following can be said:\n\n\n\n\n based on the \nweka.gui.explorer.ClassifierPanel\n\n\n the \nactual\n code doing the work follows the example in \nUsing the Experiment API\n article\n\n In order to add our \nExperimentPanel\n to the list of tabs displayed in the Explorer, we need to modify the \nExplorer.props\n file (just extract it from the \nweka.jar\n and place it in your home directory). The \nTabs* property must look like this:\n\n\n Tabs=weka.gui.explorer.ClassifierPanel,\\\n      weka.gui.explorer.ExperimentPanel,\\\n      weka.gui.explorer.ClustererPanel,\\\n      weka.gui.explorer.AssociationsPanel,\\\n      weka.gui.explorer.AttributeSelectionPanel,\\\n      weka.gui.explorer.VisualizePanel\n\n\n\n\nScreenshot\n\n\n\n\nSource\n\n\n\n\nExperimentPanel.java\n (\nstable-3.6\n, \ndeveloper\n)",
            "title": " Adding tabs in the Explorer"
        },
        {
            "location": "/adding_tabs_in_the_explorer/#description",
            "text": "This article explains how to add extra tabs in the Explorer in order to add new functionality without the hassle of having to dig into the Explorer code oneself. With the new plugin-architecture of the Explorer it is fairly easy making your extensions available in the GUI.  Note:  This is also covered in chapter  Extending WEKA  of the WEKA manual in versions later than 3.6.1/3.7.0 or  snapshots  of the stable-3.6/developer version later than 10/01/2010.",
            "title": "Description"
        },
        {
            "location": "/adding_tabs_in_the_explorer/#version",
            "text": "3.5.5 or  Snapshot",
            "title": "Version"
        },
        {
            "location": "/adding_tabs_in_the_explorer/#requirements",
            "text": "Here is roughly what is required in order to add a new tab (the examples go into more detail):   your class must be derived from  javax.swing.JPanel  your class must implemented the interface  weka.gui.explorer.Explorer.ExplorerPanel  optional interfaces  weka.gui.explorer.Explorer.LogHandler   in case you want to take advantage of the logging in the Explorer     weka.gui.explorer.Explorer.CapabilitiesFilterChangeListener   in case your class needs to be notified of changes in the Capabilities, e.g., if new data is loaded into the Explorer       adding the classname of your class to the  Tabs  property in the  Explorer.props  file",
            "title": "Requirements"
        },
        {
            "location": "/adding_tabs_in_the_explorer/#examples",
            "text": "The following examples demonstrate the new plugin architecture (a bold term for such a simple extension mechanism). Only the necessary details are discussed, as the full source code is available for download as well.",
            "title": "Examples"
        },
        {
            "location": "/adding_tabs_in_the_explorer/#sql-worksheet",
            "text": "",
            "title": "SQL worksheet"
        },
        {
            "location": "/adding_tabs_in_the_explorer/#purpose",
            "text": "Displaying the  SqlViewer  as a tab in the Explorer instead of using it either via the  Open DB...  button or as standalone application. Uses the existing components already available in Weka and just assembles them in a  JPanel . Since this tab does not rely on a dataset being loaded into the Explorer, it will be used as a   standalone  one.  Useful for people who are working a lot with databases and would like to have an SQL worksheet available all the time instead of clicking on a button every time to open up a database dialog.",
            "title": "Purpose"
        },
        {
            "location": "/adding_tabs_in_the_explorer/#implementation",
            "text": "class is derived from  javax.swing.JPanel  and implements the  weka.gui.explorer.Explorer.ExplorerPanel  interface (the full source code also imports the  weka.gui.explorer.Explorer.LogHandler  interface, but that is only additional functionality):    public class SqlPanel\n   extends JPanel\n   implements ExplorerPanel {   some basic members that we need to have     /** the parent frame */\n  protected Explorer m_Explorer = null;\n\n  /** sends notifications when the set of working instances gets changed*/\n  protected PropertyChangeSupport m_Support = new PropertyChangeSupport(this);   methods we need to implement due to the used interfaces     /** Sets the Explorer to use as parent frame */\n  public void setExplorer(Explorer parent) {\n    m_Explorer = parent;\n  }\n\n  /** returns the parent Explorer frame */\n  public Explorer getExplorer() {\n    return m_Explorer;\n  }\n\n  /** Returns the title for the tab in the Explorer */\n  public String getTabTitle() {\n    return \"SQL\";  * what's displayed as tab-title, e.g., *Classify//\n  }\n\n  /** Returns the tooltip for the tab in the Explorer */\n  public String getTabTitleToolTip() {\n    return \"Retrieving data from databases\";  // the tooltip of the tab\n  }\n\n  /** ignored, since we *\"generate\"* data and not receive it */\n  public void setInstances(Instances inst) {\n  }\n\n  /** PropertyChangeListener who will be notified of value changes. */\n  public void addPropertyChangeListener(PropertyChangeListener l) {\n    m_Support.addPropertyChangeListener(l);\n  }\n\n  /** Removes a PropertyChangeListener. */\n  public void removePropertyChangeListener(PropertyChangeListener l) {\n    m_Support.removePropertyChangeListener(l);\n  }   additional GUI elements     /** the actual SQL worksheet */\n  protected SqlViewer m_Viewer;\n\n  /** the panel for the buttons */\n  protected JPanel m_PanelButtons;\n\n  /** the Load button - makes the data available in the Explorer */\n  protected JButton m_ButtonLoad = new JButton(\"Load data\");\n\n  /** displays the current query */\n  protected JLabel m_LabelQuery = new JLabel(\"\");   loading the data into the Explorer by clicking on the  Load  button will fire a property change:       m_ButtonLoad.addActionListener(new ActionListener() {\n      public void actionPerformed(ActionEvent evt){\n        m_Support.firePropertyChange(\"\", null, null);\n      }\n    });   the  propertyChange  event will perform the actual loading of the data, hence we add an anonymous property change listener to our panel:     addPropertyChangeListener(new PropertyChangeListener() {\n    public void propertyChange(PropertyChangeEvent e) {\n      try {\n        * load data\n        InstanceQuery query = new InstanceQuery();\n        query.setDatabaseURL(m_Viewer.getURL());\n        query.setUsername(m_Viewer.getUser());\n        query.setPassword(m_Viewer.getPassword());\n        Instances data = query.retrieveInstances(m_Viewer.getQuery());\n\n        * set data in preprocess panel (will also notify of capabilties changes)\n        getExplorer().getPreprocessPanel().setInstances(data);\n      }\n      catch (Exception ex) {\n        ex.printStackTrace();\n      }\n    }\n  });   In order to add our  SqlPanel  to the list of tabs displayed in the Explorer, we need to modify the  Explorer.props  file (just extract it from the  weka.jar  and place it in your home directory). The  Tabs  property must look like this:    Tabs=weka.gui.explorer.SqlPanel,\\\n      weka.gui.explorer.ClassifierPanel,\\\n      weka.gui.explorer.ClustererPanel,\\\n      weka.gui.explorer.AssociationsPanel,\\\n      weka.gui.explorer.AttributeSelectionPanel,\\\n      weka.gui.explorer.VisualizePanel",
            "title": "Implementation"
        },
        {
            "location": "/adding_tabs_in_the_explorer/#screenshot",
            "text": "",
            "title": "Screenshot"
        },
        {
            "location": "/adding_tabs_in_the_explorer/#source",
            "text": "SqlPanel.java  ( stable-3.6 ,  developer )",
            "title": "Source"
        },
        {
            "location": "/adding_tabs_in_the_explorer/#artificial-data-generation",
            "text": "",
            "title": "Artificial data generation"
        },
        {
            "location": "/adding_tabs_in_the_explorer/#purpose_1",
            "text": "Instead of only having a  Generate...  button in the PreprocessPanel or using it from commandline, this example creates a new panel to be displayed as extra tab in the Explorer. This tab will be available regardless whether a dataset is already loaded or not (=  standalone ).",
            "title": "Purpose"
        },
        {
            "location": "/adding_tabs_in_the_explorer/#implementation_1",
            "text": "class is derived from  javax.swing.JPanel  and implements the  weka.gui.Explorer.ExplorerPanel  interface (the full source code also imports the  weka.gui.Explorer.LogHandler  interface, but that is only additional functionality):    public class GeneratorPanel\n   extends JPanel\n   implements ExplorerPanel {   some basic members that we need to have (the same as for the  SqlPanel  class):     /** the parent frame */\n protected Explorer m_Explorer = null;\n\n /** sends notifications when the set of working instances gets changed*/\n protected PropertyChangeSupport m_Support = new PropertyChangeSupport(this);   methods we need to implement due to the used interfaces (almost identical to  SqlPanel ):    /** Sets the Explorer to use as parent frame */\n public void setExplorer(Explorer parent) {\n   m_Explorer = parent;\n }\n\n /** returns the parent Explorer frame */\n public Explorer getExplorer() {\n   return m_Explorer;\n }\n\n /** Returns the title for the tab in the Explorer */\n public String getTabTitle() {\n   return \"DataGeneration\";  // what's displayed as tab-title, e.g., Classify\n }\n\n /** Returns the tooltip for the tab in the Explorer */\n public String getTabTitleToolTip() {\n   return \"Generating artificial datasets\";  // the tooltip of the tab\n }\n\n /** ignored, since we \"generate\" data and not receive it */\n public void setInstances(Instances inst) {\n }\n\n /** PropertyChangeListener who will be notified of value changes. */\n public void addPropertyChangeListener(PropertyChangeListener l) {\n   m_Support.addPropertyChangeListener(l);\n }\n\n /** Removes a PropertyChangeListener. */\n public void removePropertyChangeListener(PropertyChangeListener l) {\n   m_Support.removePropertyChangeListener(l);\n }   additional GUI elements:     /** the GOE for the generators */\n  protected GenericObjectEditor m_GeneratorEditor = new GenericObjectEditor();\n\n  /** the text area for the output of the generated data */\n  protected JTextArea m_Output = new JTextArea();\n\n  /** the Generate button */\n  protected JButton m_ButtonGenerate = new JButton(\"Generate\");\n\n  /** the Use button */\n  protected JButton m_ButtonUse = new JButton(\"Use\");   the  Generate  button doesn't load the generated data directly into the Explorer, but only outputs in the  JTextArea  (this is done with the  Use  button - see further down):     m_ButtonGenerate.addActionListener(new ActionListener(){\n    public void actionPerformed(ActionEvent evt){\n      DataGenerator generator = (DataGenerator) m_GeneratorEditor.getValue();\n      String relName = generator.getRelationName();\n\n      String cname = generator.getClass().getName().replaceAll(\".*\\\\.\", \"\");\n      String cmd = generator.getClass().getName();\n      if (generator instanceof OptionHandler)\n        cmd += \" \" + Utils.joinOptions(((OptionHandler) generator).getOptions());\n\n      try {\n        * generate data\n        StringWriter output = new StringWriter();\n        generator.setOutput(new PrintWriter(output));\n        DataGenerator.makeData(generator, generator.getOptions());\n        m_Output.setText(output.toString());\n      }\n      catch (Exception ex) {\n        ex.printStackTrace();\n        JOptionPane.showMessageDialog(\n          getExplorer(), \"Error generating data:\\n\" + ex.getMessage(), \n          \"Error\", JOptionPane.ERROR_MESSAGE);\n      }\n\n      generator.setRelationName(relName);\n    }\n  });   the  Use  button finally fires a  property change  event that will load the data into the Explorer:       m_ButtonUse.addActionListener(new ActionListener(){\n      public void actionPerformed(ActionEvent evt){\n        m_Support.firePropertyChange(\"\", null, null);\n      }\n    });   the  propertyChange  event will perform the actual loading of the data, hence we add an anonymous property change listener to our panel:     addPropertyChangeListener(new PropertyChangeListener() {\n    public void propertyChange(PropertyChangeEvent e) {\n      try {\n        Instances data = new Instances(new StringReader(m_Output.getText()));\n        * set data in preprocess panel (will also notify of capabilties changes)\n        getExplorer().getPreprocessPanel().setInstances(data);\n      }\n      catch (Exception ex) {\n        ex.printStackTrace();\n        JOptionPane.showMessageDialog(\n          getExplorer(), \"Error generating data:\\n\" + ex.getMessage(), \n          \"Error\", JOptionPane.ERROR_MESSAGE);\n      }\n    }\n  });   In order to add our  GeneratorPanel  to the list of tabs displayed in the Explorer, we need to modify the  Explorer.props  file (just extract it from the  weka.jar  and place it in your home directory). The  Tabs  property must look like this:    Tabs=weka.gui.explorer.GeneratorPanel:standalone,\\\n      weka.gui.explorer.ClassifierPanel,\\\n      weka.gui.explorer.ClustererPanel,\\\n      weka.gui.explorer.AssociationsPanel,\\\n      weka.gui.explorer.AttributeSelectionPanel,\\\n      weka.gui.explorer.VisualizePanel   Note:  the  standalone  option is used to make the tab available without requiring the preprocess panel to load a dataset first.",
            "title": "Implementation"
        },
        {
            "location": "/adding_tabs_in_the_explorer/#screenshot_1",
            "text": "",
            "title": "Screenshot"
        },
        {
            "location": "/adding_tabs_in_the_explorer/#source_1",
            "text": "GeneratorPanel.java  ( stable-3.6 ,  developer )",
            "title": "Source"
        },
        {
            "location": "/adding_tabs_in_the_explorer/#experimenter-light",
            "text": "",
            "title": "Experimenter \"light\""
        },
        {
            "location": "/adding_tabs_in_the_explorer/#purpose_2",
            "text": "By default the  Classify  panel only performs 1 run of 10-fold cross-validation. Since most classifiers are rather sensitive to the order of the data being presented to them, those results can be too optimistic or pessimistic. Averaging the results over 10 runs with differently randomized train/test pairs returns more reliable results. And this is where this plugin comes in: \nit can be used to obtain statistical sound results for a specific classifier/dataset combination, without having to setup a whole experiment in the Experimenter.",
            "title": "Purpose"
        },
        {
            "location": "/adding_tabs_in_the_explorer/#implementation_2",
            "text": "Since this plugin is rather bulky, we omit the implementation details, but the following can be said:    based on the  weka.gui.explorer.ClassifierPanel   the  actual  code doing the work follows the example in  Using the Experiment API  article  In order to add our  ExperimentPanel  to the list of tabs displayed in the Explorer, we need to modify the  Explorer.props  file (just extract it from the  weka.jar  and place it in your home directory). The  Tabs* property must look like this:   Tabs=weka.gui.explorer.ClassifierPanel,\\\n      weka.gui.explorer.ExperimentPanel,\\\n      weka.gui.explorer.ClustererPanel,\\\n      weka.gui.explorer.AssociationsPanel,\\\n      weka.gui.explorer.AttributeSelectionPanel,\\\n      weka.gui.explorer.VisualizePanel",
            "title": "Implementation"
        },
        {
            "location": "/adding_tabs_in_the_explorer/#screenshot_2",
            "text": "",
            "title": "Screenshot"
        },
        {
            "location": "/adding_tabs_in_the_explorer/#source_2",
            "text": "ExperimentPanel.java  ( stable-3.6 ,  developer )",
            "title": "Source"
        },
        {
            "location": "/changing_the_plot_background/",
            "text": "The \ndefault background\n color for plots, e.g., for \nROC curves\n, is \nblack\n, which might not be convenient for screenshots.\n\n\nSince the color is stored as a value in a \nProperties file\n, you can easily change the color:\n\n\n\n\nextract the following file from the \nweka.jar\n or \nweka-src.jar\n file\n\n\n\n\n weka/gui/visualize/Visualize.props\n\n\n\n\n\n\nplace a copy of that file in your HOME directory (on *nix \n$HOME\n, on Windows \n%USERPROFILE%\n)\n\n\nedit the \nProperties file\n with any text editor\n\n\nreplace the value of following key with a color of your liking, e.g., \nwhite\n\n\n\n\n weka.gui.visualize.Plot2D.backgroundColour\n\n\n\n\n\n\nsave the file and restart Weka",
            "title": " Changing the plot background"
        },
        {
            "location": "/classifying_large_datasets/",
            "text": "Unless one has access to a 64-bit machine with lots of RAM, it can happen quite easy that one runs into an \nOutOfMemoryException\n running WEKA on large datasets. This article tries to present some solutions apart from buying new hardware.\n\n\nSampling\n\n\nThe question is, does one really need to train with \nall\n the data, or is a subset of the data already sufficient? \n\n\nWEKA offers several filters for re-sampling a dataset and generating a new dataset reduced in size:\n\n\n\n\nweka.filters.supervised.instance.Resample\n\n\nThis filter takes the class distribution into account for generating the sample, i.e., you can even adjust the distribution by adding a bias.\n\n\n\n\n\n\nweka.filters.unsupervised.instance.Resample\n\n\nThe unsupervised filter does not take the class distribution into account for generating the output.\n\n\n\n\n\n\nweka.filters.supervised.instance.SpreadSubsample\n\n\nIt allows you to specify the maximum \"spread\" between the rarest and most common class.\n\n\n\n\n\n\n\n\nSee the respective Javadoc for more information (\nbook version\n, \ndeveloper version\n).\n\n\nIncremental classifiers\n\n\nMost classifiers need to see all the data before they can be trained, e.g., J48 or SMO. But there are also schemes that can be trained in an incremental fashion, not just in batch mode. All classifiers implementing the \nweka.classifiers.UpdateableClassifier\n interface are able to process data in such a way.\n\n\nRunning such a classifier from commandline will load the dataset incrementally (NB: \nnot all data formats can be loaded incrementally; \nXRFF\n is one of them, \nARFF\n on the other hand can be read incrementally) and feed the data instance by instance to the classifier.\n\n\nCheck out the Javadoc of the \nUpdateableClassifier\n interface to see what schemes implement it (\nbook version\n, \ndeveloper version\n).\n\n\nOther tools\n\n\n\n\nMOA - Massive Online Analysis\n\n\nA framework for learning from a continuous supply of examples, a data stream. Includes tools for evaluation and a collection of machine learning algorithms. Related to the WEKA project, also written in Java, while scaling to more demanding problems.",
            "title": " Classifying large datasets"
        },
        {
            "location": "/classifying_large_datasets/#sampling",
            "text": "The question is, does one really need to train with  all  the data, or is a subset of the data already sufficient?   WEKA offers several filters for re-sampling a dataset and generating a new dataset reduced in size:   weka.filters.supervised.instance.Resample  This filter takes the class distribution into account for generating the sample, i.e., you can even adjust the distribution by adding a bias.    weka.filters.unsupervised.instance.Resample  The unsupervised filter does not take the class distribution into account for generating the output.    weka.filters.supervised.instance.SpreadSubsample  It allows you to specify the maximum \"spread\" between the rarest and most common class.     See the respective Javadoc for more information ( book version ,  developer version ).",
            "title": "Sampling"
        },
        {
            "location": "/classifying_large_datasets/#incremental-classifiers",
            "text": "Most classifiers need to see all the data before they can be trained, e.g., J48 or SMO. But there are also schemes that can be trained in an incremental fashion, not just in batch mode. All classifiers implementing the  weka.classifiers.UpdateableClassifier  interface are able to process data in such a way.  Running such a classifier from commandline will load the dataset incrementally (NB: \nnot all data formats can be loaded incrementally;  XRFF  is one of them,  ARFF  on the other hand can be read incrementally) and feed the data instance by instance to the classifier.  Check out the Javadoc of the  UpdateableClassifier  interface to see what schemes implement it ( book version ,  developer version ).",
            "title": "Incremental classifiers"
        },
        {
            "location": "/classifying_large_datasets/#other-tools",
            "text": "MOA - Massive Online Analysis  A framework for learning from a continuous supply of examples, a data stream. Includes tools for evaluation and a collection of machine learning algorithms. Related to the WEKA project, also written in Java, while scaling to more demanding problems.",
            "title": "Other tools"
        },
        {
            "location": "/classpath_problems/",
            "text": "Having problems getting Weka to run from a DOS/UNIX command prompt? Getting \njava.lang.NoClassDefFoundError\n exceptions? Most likely your \nCLASSPATH\n environment variable is not set correctly - it needs to point to the \nweka.jar\n file that you downloaded with Weka (or the parent of the Weka directory if you have extracted the jar). Under DOS this can be achieved with:\n\n\n set CLASSPATH=c:\\weka-3-4\\weka.jar;%CLASSPATH%\n\n\n\n\nUnder UNIX/Linux something like:\n\n\n export CLASSPATH=/home/weka/weka.jar:$CLASSPATH\n\n\n\n\nAn easy way to get avoid setting the variable this is to specify the CLASSPATH when calling Java. For example, if the jar file is located at c:\\weka-3-4\\weka.jar you can use:\n\n\n java -cp c:\\weka-3-4\\weka.jar weka.classifiers... \n\n\n\n\nSee also the \nCLASSPATH\n article.",
            "title": " CLASSPATH problems"
        },
        {
            "location": "/cost_matrix/",
            "text": "Format\n\n\nFormat of the cost matrices:\n\n\n\n\nregular \n\n\n\n\n % Rows Columns\n 2  2\n % Matrix elements\n 0.0    5.0\n 1.0    0.0\n\n\n\n\n\n\nMatlab\n single-line format (see also the \nMatlab Primer\n)\n\n\n\n\n [0.0 5.0; 1.0 0.0]\n\n\n\n\nTesting the format\n\n\nThe following code loads a cost matrix and prints its content to the console. Useful, if one wants to test whether the format is correct:\n\n\n import weka.classifiers.CostMatrix;\n\n import java.io.BufferedReader;\n import java.io.FileReader;\n\n /**\n  * Loads the cost matrix \"args[0]\" and prints its content to the console.\n  *\n  * @author FracPete (fracpete at waikato dot ac dot nz)\n  */\n public class CostMatrixLoader {\n   public static void main(String[] args) throws Exception {\n     CostMatrix matrix = new **CostMatrix**(\n                           new BufferedReader(new FileReader(args[0])));\n     System.out.println(matrix);\n   }\n }\n\n\n\n\nSee also\n\n\n\n\nCostSensitiveClassifier\n\n\nMetaCost\n\n\n\n\nDownloads\n\n\n\n\nCostMatrixLoader.java",
            "title": " CostMatrix"
        },
        {
            "location": "/cost_matrix/#format",
            "text": "Format of the cost matrices:   regular     % Rows Columns\n 2  2\n % Matrix elements\n 0.0    5.0\n 1.0    0.0   Matlab  single-line format (see also the  Matlab Primer )    [0.0 5.0; 1.0 0.0]",
            "title": "Format"
        },
        {
            "location": "/cost_matrix/#testing-the-format",
            "text": "The following code loads a cost matrix and prints its content to the console. Useful, if one wants to test whether the format is correct:   import weka.classifiers.CostMatrix;\n\n import java.io.BufferedReader;\n import java.io.FileReader;\n\n /**\n  * Loads the cost matrix \"args[0]\" and prints its content to the console.\n  *\n  * @author FracPete (fracpete at waikato dot ac dot nz)\n  */\n public class CostMatrixLoader {\n   public static void main(String[] args) throws Exception {\n     CostMatrix matrix = new **CostMatrix**(\n                           new BufferedReader(new FileReader(args[0])));\n     System.out.println(matrix);\n   }\n }",
            "title": "Testing the format"
        },
        {
            "location": "/cost_matrix/#see-also",
            "text": "CostSensitiveClassifier  MetaCost",
            "title": "See also"
        },
        {
            "location": "/cost_matrix/#downloads",
            "text": "CostMatrixLoader.java",
            "title": "Downloads"
        },
        {
            "location": "/cost_sensitive_classifier/",
            "text": "A meta classifier that makes its base classifier cost-sensitive. Two methods can be used to introduce cost-sensitivity: \nreweighting training instances according to the total cost assigned to each class; or predicting the class with minimum expected misclassification cost (rather than the most likely class). Performance can often be improved by using a Bagged classifier to improve the probability estimates of the base classifier.\n\n\nSince the classifier \nnormalizes\n the cost matrix before applying it, it \nmakes it hard\n coming up with a cost matrix, e.g., to balance out imbalanced data. Here's an example:\n\n\n\n\ninput cost matrix\n\n\n\n\n -3  1  1\n  1 -6  1\n  0  0  0\n\n\n\n\n\n\nnormalized cost matrix\n\n\n\n\n 0 7 1\n 4 0 1\n 3 6 0\n\n\n\n\nBut still, according to \nWeka users\n, one can set \nthe cost as to equalize the class distributions (i.e.\n\n\nachieving a 1:1 class distribution afterward)\n. But this could be limited to 2-class problems.\n\n\nThe application of the \ncost matrix\n in \nMetaCost\n is more intuitive.\n\n\nSee also\n\n\n\n\nMetaCost\n\n\nCostMatrix",
            "title": " CostSensitiveClassifier"
        },
        {
            "location": "/cost_sensitive_classifier/#see-also",
            "text": "MetaCost  CostMatrix",
            "title": "See also"
        },
        {
            "location": "/datasets/",
            "text": "Some example datasets are included in the Weka distribution.\n\n\nAvailable separately:\n\n\n\n\nA jarfile containing 37 classification problems, originally obtained from the \nUCI repository\n (\ndatasets-UCI.jar\n, 1,190,961 Bytes).\n\n\nA jarfile containing 37 regression problems, obtained from various sources (\ndatasets-numeric.jar\n, 169,344 Bytes).\n\n\nA jarfile containing 6 agricultural datasets obtained from agricultural researchers in New Zealand (\nagridatasets.jar\n, 31,200 Bytes).\n\n\nA jarfile containing 30 regression datasets collected by Luis Torgo (\nregression-datasets.jar\n, 10,090,266 Bytes).\n\n\nA gzip'ed tar containing \nUCI\n and \nUCI KDD\n datasets (\nuci-20070111.tar.gz\n, 17,952,832  Bytes)\n\n\nA gzip'ed tar containing \nStatLib\n datasets (\nstatlib-20050214.tar.gz\n, 12,785,582 Bytes)\n\n\nA gzip'ed tar containing ordinal, real-world datasets donated by Dr. Arie Ben David (Holon Inst. of Technology/Israel) (\ndatasets-arie_ben_david.tar.gz\n, 11,348 Bytes)\n\n\nA zip file containing 19 multi-class (1-of-n) text datasets donated by \nGeorge Forman/Hewlett-Packard Labs\n (\n19MclassTextWc.zip\n, 14,084,828 Bytes)\n\n\nA bzip'ed tar file containing the Reuters21578 dataset split into separate files according to the ModApte split \nreuters21578-ModApte.tar.bz2\n, 81,745,032 Bytes\n\n\nA zip file containing 41 drug design datasets formed using the Adriana.Code software - donated by Dr. M. Fatih Amasyali (Yildiz Technical Unversity) (\nDrug-datasets.zip\n, 11,376,153 Bytes)\n\n\nA zip file containing 80 artificial datasets generated from the Friedman function donated by Dr. M. Fatih Amasyali (Yildiz Technical Unversity) (\nFriedman-datasets.zip\n, 5,802,204 Bytes)\n\n\n\n\nAfter expanding into a directory using your jar utility (or an archive program that handles tar-archives/zip files in case of the gzip'ed tars/zip files), these datasets may be used with Weka.\n\n\nOther datasets in ARFF format\n\n\n\n\nProtein data sets, maintained by Shuiwang Ji, \nCS Department, Louisiana State University\n\n\nKent Ridge Biomedical Data Set Repository, maintained by Jinyan Li and Huiqing Liu, \nInstitute for Infocomm Research\n, Shanghai\n\n\n\n\nLinks\n\n\n\n\nUCI\n\n\nUCI KDD\n\n\nStatLib\n\n\nWeka on SourceForge",
            "title": " Datasets"
        },
        {
            "location": "/datasets/#other-datasets-in-arff-format",
            "text": "Protein data sets, maintained by Shuiwang Ji,  CS Department, Louisiana State University  Kent Ridge Biomedical Data Set Repository, maintained by Jinyan Li and Huiqing Liu,  Institute for Infocomm Research , Shanghai",
            "title": "Other datasets in ARFF format"
        },
        {
            "location": "/datasets/#links",
            "text": "UCI  UCI KDD  StatLib  Weka on SourceForge",
            "title": "Links"
        },
        {
            "location": "/discretizing_datasets/",
            "text": "Once in a while one has numeric data but wants to use classifier that handles only nominal values. In that case one needs to \ndiscretize\n the data, which can be done with the following filters:\n\n\n\n\nweka.filters.supervised.attribute.Discretize\n\n\nuses either Fayyad & Irani's MDL method or Kononeko's MDL criterion\n\n\n\n\n\n\nweka.filters.unsupervised.attribute.Discretize\n\n\nuses simple binning\n\n\n\n\n\n\n\n\nBut, since discretization depends on the data which presented to the discretization algorithm, one easily end up with incompatible train and test files. The following shows how to generate compatible discretized files out of a training and a test file by using the \nsupervised\n version of the filter.\n\n\nThe class takes four files as arguments:\n\n\n\n\ninput training file\n\n\ninput test file\n\n\noutput training file\n\n\noutput test file\n\n\n\n\n import java.io.*;\n\n import weka.core.*;\n import weka.filters.Filter;\n import weka.filters.supervised.attribute.Discretize;\n\n /**\n  * Shows how to generate compatible train/test sets using the Discretize\n  * filter.\n  *\n  * @author FracPete (fracpete at waikato dot ac dot nz)\n  */\n public class DiscretizeTest {\n\n   /**\n    * loads the given ARFF file and sets the class attribute as the last\n    * attribute.\n    *\n    * @param filename    the file to load\n    * @throws Exception  if somethings goes wrong\n    */\n   protected static Instances load(String filename) throws Exception {\n     Instances       result;\n     BufferedReader  reader;\n\n     reader = new BufferedReader(new FileReader(filename));\n     result = new Instances(reader);\n     result.setClassIndex(result.numAttributes() - 1);\n     reader.close();\n\n     return result;\n   }\n\n   /**\n    * saves the data to the specified file\n    *\n    * @param data        the data to save to a file\n    * @param filename    the file to save the data to\n    * @throws Exception  if something goes wrong\n    */\n   protected static void save(Instances data, String filename) throws Exception {\n     BufferedWriter  writer;\n\n     writer = new BufferedWriter(new FileWriter(filename));\n     writer.write(data.toString());\n     writer.newLine();\n     writer.flush();\n     writer.close();\n   }\n\n   /**\n    * Takes four arguments:\n\n    * <ol>\n    *   <li>input train file</li>\n    *   <li>input test file</li>\n    *   <li>output train file</li>\n    *   <li>output test file</li>\n    * </ol>\n    *\n    * @param args        the commandline arguments\n    * @throws Exception  if something goes wrong\n    */\n   public static void main(String[] args) throws Exception {\n     Instances     inputTrain;\n     Instances     inputTest;\n     Instances     outputTrain;\n     Instances     outputTest;\n     Discretize    filter;\n\n     * load data (class attribute is assumed to be last attribute)\n     inputTrain = load(args[0]);\n     inputTest  = load(args[1]);\n\n     * setup filter\n     filter = new Discretize();\n     filter.setInputFormat(inputTrain);\n\n     * apply filter\n     outputTrain = Filter.useFilter(inputTrain, filter);\n     outputTest  = Filter.useFilter(inputTest,  filter);\n\n     * save output\n     save(outputTrain, args[2]);\n     save(outputTest,  args[3]);\n   }\n }\n\n\n\n\nThe same can be achieved from the commandline with this command (\nbatch filtering\n):\n\n\n java weka.filters.supervised.attribute.Discretize -b -i <in-train> -o <out-train> -r <in-test> -s <out-test> -c <class-index>\n\n\n\n\nSee also\n\n\n\n\nManual discretization (Using the MathExpression filter)\n\n\nBatch filtering\n\n\n\n\nDownloads\n\n\n\n\nDiscretizeTest.java\n\n\n\n\nLinks\n\n\n\n\nJavadoc\n\n\nDiscretize (supervised)\n\n\nDiscretize (unsupervised)",
            "title": " Discretizing Datasets"
        },
        {
            "location": "/discretizing_datasets/#see-also",
            "text": "Manual discretization (Using the MathExpression filter)  Batch filtering",
            "title": "See also"
        },
        {
            "location": "/discretizing_datasets/#downloads",
            "text": "DiscretizeTest.java",
            "title": "Downloads"
        },
        {
            "location": "/discretizing_datasets/#links",
            "text": "Javadoc  Discretize (supervised)  Discretize (unsupervised)",
            "title": "Links"
        },
        {
            "location": "/displaying_results_of_cross_validation_folds/",
            "text": "The following KnowledgeFlow setup outputs the cross-validation models for each train/test pair:\n\n\n  ArffLoader\n    --dataSet--> CrossValidationFoldMaker\n    --trainingSet/testSet--> J48\n    --text--> TextViewer\n\n\n\n\nLinks\n\n\n\n\nCross-validation_folds_output.kfml\n - Example KnowledgeFlow layout file",
            "title": " Displaying results of cross-validation folds"
        },
        {
            "location": "/displaying_results_of_cross_validation_folds/#links",
            "text": "Cross-validation_folds_output.kfml  - Example KnowledgeFlow layout file",
            "title": "Links"
        },
        {
            "location": "/document_classification/",
            "text": "See \nText categorization with Weka",
            "title": " Document Classification"
        },
        {
            "location": "/ensemble_selection/",
            "text": "Notes\n\n\n\n\nThis bug has now been fixed. (12/2014)\n\n\nThere is a bug in the code to build a library -- trying to build any model specification with three layers (e.g., Bagging a REPTree) causes the form to freeze up and/or crash.\n\n\n\n\n\n\n\n\nThe documentation on how to run from the command line is outdated. Some corrections:\n\n\n\n\nThe \"-D\" option no longer exists.\n\n\nThe command shown for training a library from the command line:\n\n\n\n\n\n\njava weka.classifiers.meta.EnsembleSelection -no-cv -v -L path/to/your/mode/list/file.model.xml -W /path/to/your/working/directory -A library -X 5 -S 1 -O -t yourTrainingInstances.arff\n\n\nfails for me with an exception that \"Folds 1 and 5 are not equal.\" A command line that works is to set the folds to 1:\n\n\njava weka.classifiers.meta.EnsembleSelection -no-cv -v -L path/to/your/mode/list/file.model.xml -W /path/to/your/working/directory -A library -X 1 -S 1 -O -t yourTrainingInstances.arff\n\n\n\n\n\n\n\n\nLinks\n\n\n\n\nEnsemble_selection.pdf\n - Documentation on how to use Ensemble Selection in Weka\n\n\nEnsemble Selection from Libraries of Models, ICML'04",
            "title": " Ensemble Selection"
        },
        {
            "location": "/ensemble_selection/#notes",
            "text": "This bug has now been fixed. (12/2014)  There is a bug in the code to build a library -- trying to build any model specification with three layers (e.g., Bagging a REPTree) causes the form to freeze up and/or crash.     The documentation on how to run from the command line is outdated. Some corrections:   The \"-D\" option no longer exists.  The command shown for training a library from the command line:    java weka.classifiers.meta.EnsembleSelection -no-cv -v -L path/to/your/mode/list/file.model.xml -W /path/to/your/working/directory -A library -X 5 -S 1 -O -t yourTrainingInstances.arff  fails for me with an exception that \"Folds 1 and 5 are not equal.\" A command line that works is to set the folds to 1:  java weka.classifiers.meta.EnsembleSelection -no-cv -v -L path/to/your/mode/list/file.model.xml -W /path/to/your/working/directory -A library -X 1 -S 1 -O -t yourTrainingInstances.arff",
            "title": "Notes"
        },
        {
            "location": "/ensemble_selection/#links",
            "text": "Ensemble_selection.pdf  - Documentation on how to use Ensemble Selection in Weka  Ensemble Selection from Libraries of Models, ICML'04",
            "title": "Links"
        },
        {
            "location": "/explorer_error_visualization_plugins/",
            "text": "Introduction\n\n\nAs of Weka version >3.5.8 (only developer version, not stable-3.6 branch) one can easily add graph visualization plugins in the Explorer (Classify panel). This makes it easy to implement custom visualizations, if the ones Weka offers are not sufficient.\n\n\nNote:\n This is also covered in chapter \nExtending WEKA\n of the WEKA manual in versions later than 3.7.0 or \nsnapshots\n of the developer version later than 10/01/2010.\n\n\nRequirements\n\n\n\n\ncustom visualization class must implement the following \ninterface\n\n\nweka.gui.visualize.plugins.ErrorVisualizePlugin\n\n\n\n\n\n\nthe class must \neither\n reside in the following \npackage\n (visualization classes are automatically discovered during run-time)\n\n\nweka.gui.visualize.plugins\n\n\n\n\n\n\nor the class' package must be listed in the \nweka.gui.visualize.plugins.ErrorVisualizePlugin\n key of the \nweka/gui/GenericPropertiesCreator.props\n file.\n\n\n\n\nImplementation\n\n\nThe visualization interface contains the following four methods\n\n\n\n\ngetMinVersion\n\n\nThis method returns the \nminimal\n version (inclusive) of Weka that is necessary to execute the plugin, e.g., \n3.5.0\n.\n\n \ngetMaxVersion\n\nThis method returns the \nmaximal* version (exclusive) of Weka that is necessary to execute the plugin, e.g., \n3.6.0\n.\n\n\n\n\n\n\ngetDesignVersion\n\n\nReturns the actual version of Weka this plugin was designed for, e.g., \n3.5.1\n\n\n\n\n\n\ngetVisualizeMenuItem\n\n\nThe \nJMenuItem\n that is returned via this method will be added to the plugins menu in the popup in the Explorer. The ActionListener for clicking the menu item will most likely open a new frame containing the visualized data.\n\n\n\n\n\n\n\n\nExamples\n\n\nThe following screenshots were generated using \nweka.classifiers.functions.LinearRegression\n with default parameters on the UCI \ndataset\n \nbolts\n, using a percentage split of 66% for the training set and the remainder for testing.\n\n\nUsing Weka panels\n\n\nThe \nClassifierErrorsWeka.java\n example simply displays the classifier errors as the \nVisualize classifier errors\n menu item already available in Weka. It is just to demonstrate how to use existing Weka classes. \n\n\n\n\nUsing JMathtools' Boxplot\n\n\nThe \nClassifierErrorsMathtools.java\n. The relative error per prediction is displayed as vertical line.\n\n\n\n\nNote:\n This display is only available for numeric classes.\n\n\nDownloads\n\n\n\n\nClassifierErrorsWeka.java\n\n\nClassifierErrorsMathtools.java\n\n\n\n\nSee also\n\n\n\n\nUse Weka in your Java code\n - general overview of the basic Weka API\n\n\nExplorer visualization plugins\n\n\n\n\nLinks\n\n\n\n\nJMathTools homepage",
            "title": " Explorer error visualization plugins"
        },
        {
            "location": "/explorer_error_visualization_plugins/#introduction",
            "text": "As of Weka version >3.5.8 (only developer version, not stable-3.6 branch) one can easily add graph visualization plugins in the Explorer (Classify panel). This makes it easy to implement custom visualizations, if the ones Weka offers are not sufficient.  Note:  This is also covered in chapter  Extending WEKA  of the WEKA manual in versions later than 3.7.0 or  snapshots  of the developer version later than 10/01/2010.",
            "title": "Introduction"
        },
        {
            "location": "/explorer_error_visualization_plugins/#requirements",
            "text": "custom visualization class must implement the following  interface  weka.gui.visualize.plugins.ErrorVisualizePlugin    the class must  either  reside in the following  package  (visualization classes are automatically discovered during run-time)  weka.gui.visualize.plugins    or the class' package must be listed in the  weka.gui.visualize.plugins.ErrorVisualizePlugin  key of the  weka/gui/GenericPropertiesCreator.props  file.",
            "title": "Requirements"
        },
        {
            "location": "/explorer_error_visualization_plugins/#implementation",
            "text": "The visualization interface contains the following four methods   getMinVersion  This method returns the  minimal  version (inclusive) of Weka that is necessary to execute the plugin, e.g.,  3.5.0 .   getMaxVersion \nThis method returns the  maximal* version (exclusive) of Weka that is necessary to execute the plugin, e.g.,  3.6.0 .    getDesignVersion  Returns the actual version of Weka this plugin was designed for, e.g.,  3.5.1    getVisualizeMenuItem  The  JMenuItem  that is returned via this method will be added to the plugins menu in the popup in the Explorer. The ActionListener for clicking the menu item will most likely open a new frame containing the visualized data.",
            "title": "Implementation"
        },
        {
            "location": "/explorer_error_visualization_plugins/#examples",
            "text": "The following screenshots were generated using  weka.classifiers.functions.LinearRegression  with default parameters on the UCI  dataset   bolts , using a percentage split of 66% for the training set and the remainder for testing.",
            "title": "Examples"
        },
        {
            "location": "/explorer_error_visualization_plugins/#using-weka-panels",
            "text": "The  ClassifierErrorsWeka.java  example simply displays the classifier errors as the  Visualize classifier errors  menu item already available in Weka. It is just to demonstrate how to use existing Weka classes.",
            "title": "Using Weka panels"
        },
        {
            "location": "/explorer_error_visualization_plugins/#using-jmathtools-boxplot",
            "text": "The  ClassifierErrorsMathtools.java . The relative error per prediction is displayed as vertical line.   Note:  This display is only available for numeric classes.",
            "title": "Using JMathtools' Boxplot"
        },
        {
            "location": "/explorer_error_visualization_plugins/#downloads",
            "text": "ClassifierErrorsWeka.java  ClassifierErrorsMathtools.java",
            "title": "Downloads"
        },
        {
            "location": "/explorer_error_visualization_plugins/#see-also",
            "text": "Use Weka in your Java code  - general overview of the basic Weka API  Explorer visualization plugins",
            "title": "See also"
        },
        {
            "location": "/explorer_error_visualization_plugins/#links",
            "text": "JMathTools homepage",
            "title": "Links"
        },
        {
            "location": "/explorer_graph_visualization_plugins/",
            "text": "Introduction\n\n\nAs of Weka version >3.5.8 (only developer version, not stable-3.6 branch) one can easily add graph visualization plugins in the Explorer (Classify panel). This makes it easy to implement custom visualizations, if the ones Weka offers are not sufficient. \ngraph\n is referring to graphs generated, for instance, by the \nweka.classifiers.bayes.BayesNet\n classifier.\n\n\nNote:\n This is also covered in chapter \nExtending WEKA\n of the WEKA manual in versions later than 3.7.0 or \nsnapshots\n of the developer version later than 10/01/2010.\n\n\nRequirements\n\n\n\n\ncustom visualization class must implement the following \ninterface\n\n\nweka.gui.visualize.plugins.GraphVisualizePlugin\n\n\n\n\n\n\nthe class must \neither\n reside in the following \npackage\n (visualization classes are automatically discovered during run-time)\n\n\nweka.gui.visualize.plugins\n\n\n\n\n\n\nor the class' package must be listed in the \nweka.gui.visualize.plugins.GraphVisualizePlugin\n key of the \n[weka/gui/GenericPropertiesCreator.props](weka_gui_genericpropertiescreator.props.md)\n file.\n\n\n\n\nImplementation\n\n\nThe visualization interface contains the following four methods\n\n\n\n\ngetMinVersion\n\n\nThis method returns the \nminimal\n version (inclusive) of Weka that is necessary to execute the plugin, e.g., \n3.5.0\n.\n\n\n\n\n\n\ngetMaxVersion\n\n\nThis method returns the \nmaximal\n version (exclusive) of Weka that is necessary to execute the plugin, e.g., \n3.6.0\n.\n\n\n\n\n\n\ngetDesignVersion\n\n\nReturns the actual version of Weka this plugin was designed for, e.g., \n3.5.1\n\n\n\n\n\n\ngetVisualizeMenuItem\n\n\nThe \nJMenuItem\n that is returned via this method will be added to the plugins menu in the popup in the Explorer. The ActionListener for clicking the menu item will most likely open a new frame containing the visualized data.\n\n\n\n\n\n\n\n\nExamples\n\n\nPrefuse visualization toolkit\n\n\nThe \nPrefuseGraph.java\n. It is based on the \nprefuse.demos.GraphView\n demo class.\n\n\nThe following screenshot was generated using \nBayesNet\n on the UCI dataset \nanneal\n with the following parametrization:\n\n\n\n\nweka.classifiers.bayes.BayesNet -D -Q weka.classifiers.bayes.net.search.local.K2 -- -P 3 -S BAYES -E weka.classifiers.bayes.net.estimate.SimpleEstimator -- -A 0.5\n\n\n\n\n\n\nDownloads\n\n\n\n\nPrefuseGraph.java\n\n\n\n\nSee also\n\n\n\n\nUse Weka in your Java code\n - general overview of the basic Weka API\n\n\nExplorer visualization plugins\n\n\nExplorer tree visualization plugins\n\n\n\n\nLinks\n\n\n\n\nPrefuse homepage\n\n\nPAP - prefuse assistance pool",
            "title": " Explorer graph visualization plugins"
        },
        {
            "location": "/explorer_graph_visualization_plugins/#introduction",
            "text": "As of Weka version >3.5.8 (only developer version, not stable-3.6 branch) one can easily add graph visualization plugins in the Explorer (Classify panel). This makes it easy to implement custom visualizations, if the ones Weka offers are not sufficient.  graph  is referring to graphs generated, for instance, by the  weka.classifiers.bayes.BayesNet  classifier.  Note:  This is also covered in chapter  Extending WEKA  of the WEKA manual in versions later than 3.7.0 or  snapshots  of the developer version later than 10/01/2010.",
            "title": "Introduction"
        },
        {
            "location": "/explorer_graph_visualization_plugins/#requirements",
            "text": "custom visualization class must implement the following  interface  weka.gui.visualize.plugins.GraphVisualizePlugin    the class must  either  reside in the following  package  (visualization classes are automatically discovered during run-time)  weka.gui.visualize.plugins    or the class' package must be listed in the  weka.gui.visualize.plugins.GraphVisualizePlugin  key of the  [weka/gui/GenericPropertiesCreator.props](weka_gui_genericpropertiescreator.props.md)  file.",
            "title": "Requirements"
        },
        {
            "location": "/explorer_graph_visualization_plugins/#implementation",
            "text": "The visualization interface contains the following four methods   getMinVersion  This method returns the  minimal  version (inclusive) of Weka that is necessary to execute the plugin, e.g.,  3.5.0 .    getMaxVersion  This method returns the  maximal  version (exclusive) of Weka that is necessary to execute the plugin, e.g.,  3.6.0 .    getDesignVersion  Returns the actual version of Weka this plugin was designed for, e.g.,  3.5.1    getVisualizeMenuItem  The  JMenuItem  that is returned via this method will be added to the plugins menu in the popup in the Explorer. The ActionListener for clicking the menu item will most likely open a new frame containing the visualized data.",
            "title": "Implementation"
        },
        {
            "location": "/explorer_graph_visualization_plugins/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/explorer_graph_visualization_plugins/#prefuse-visualization-toolkit",
            "text": "The  PrefuseGraph.java . It is based on the  prefuse.demos.GraphView  demo class.  The following screenshot was generated using  BayesNet  on the UCI dataset  anneal  with the following parametrization:   weka.classifiers.bayes.BayesNet -D -Q weka.classifiers.bayes.net.search.local.K2 -- -P 3 -S BAYES -E weka.classifiers.bayes.net.estimate.SimpleEstimator -- -A 0.5",
            "title": "Prefuse visualization toolkit"
        },
        {
            "location": "/explorer_graph_visualization_plugins/#downloads",
            "text": "PrefuseGraph.java",
            "title": "Downloads"
        },
        {
            "location": "/explorer_graph_visualization_plugins/#see-also",
            "text": "Use Weka in your Java code  - general overview of the basic Weka API  Explorer visualization plugins  Explorer tree visualization plugins",
            "title": "See also"
        },
        {
            "location": "/explorer_graph_visualization_plugins/#links",
            "text": "Prefuse homepage  PAP - prefuse assistance pool",
            "title": "Links"
        },
        {
            "location": "/explorer_prediction_visualization_plugins/",
            "text": "Introduction\n\n\nAs of Weka version >3.5.2 one can easily add visualization plugins in the Explorer (Classify panel). This makes it easy to implement custom visualizations, if the ones Weka offers are not sufficient.\n\n\nNote:\n This is also covered in chapter \nExtending WEKA\n of the WEKA manual in versions later than 3.6.1/3.7.0 or \nsnapshots\n of the stable-3.6/developer version later than 10/01/2010.\n\n\nRequirements\n\n\n\n\ncustom visualization class must implement the following \ninterface\n\n\nweka.gui.visualize.plugins.VisualizePlugin\n\n\n\n\n\n\nthe class must \neither\n reside in the following \npackage\n (visualization classes are automatically discovered during run-time)\n\n\nweka.gui.visualize.plugins\n\n\n\n\n\n\nor the class' package must be listed in the \nweka.gui.visualize.plugins.VisualizePlugin\n key of the \n[weka/gui/GenericPropertiesCreator.props](weka_gui_genericpropertiescreator.props.md)\n file (only available in 3.5.7 or later).\n\n\n\n\nImplementation\n\n\nThe visualization interface contains the following four methods\n\n\n\n\ngetMinVersion\n\n\nThis method returns the \nminimal\n version (inclusive) of Weka that is necessary to execute the plugin, e.g., \n3.5.0\n.\n\n\n\n\n\n\ngetMaxVersion\n\n\nThis method returns the \nmaximal\n version (exclusive) of Weka that is necessary to execute the plugin, e.g., \n3.6.0\n.\n\n\n\n\n\n\ngetDesignVersion\n\n\nReturns the actual version of Weka this plugin was designed for, e.g., \n3.5.1\n\n\n\n\n\n\ngetVisualizeMenuItem\n\n\nThe \nJMenuItem\n that is returned via this method will be added to the plugins menu in the popup in the Explorer. The ActionListener for clicking the menu item will most likely open a new frame containing the visualized data.\n\n\n\n\n\n\n\n\nExamples\n\n\nTable with predictions\n\n\nThe \nPredictionTable.java\n example simply displays the \nactual\n class label and the one predicted by the classifier. In addition to that, it lists whether it was an incorrect prediction and the class probability for the correct class label.\n\n\n\n\nBar plot with probabilities\n\n\nThe \nPredictionError.java\n to display a simple bar plot of the predictions. The correct predictions are displayed in \nblue\n, the incorrect ones in \nred\n. In both cases the class probability that the classifier returned for the correct class label is displayed on the y axis. The x axis is simply the index of the prediction starting with 0.\n\n\n\n\nDownloads\n\n\n\n\nPredictionTable.java\n\n\nPredictionError.java\n\n\n\n\nSee also\n\n\n\n\nUse Weka in your Java code\n - general overview of the basic Weka API\n\n\nExplorer visualization plugins\n\n\n\n\nLinks\n\n\n\n\nJMathTools homepage",
            "title": " Explorer prediction visualization plugins"
        },
        {
            "location": "/explorer_prediction_visualization_plugins/#introduction",
            "text": "As of Weka version >3.5.2 one can easily add visualization plugins in the Explorer (Classify panel). This makes it easy to implement custom visualizations, if the ones Weka offers are not sufficient.  Note:  This is also covered in chapter  Extending WEKA  of the WEKA manual in versions later than 3.6.1/3.7.0 or  snapshots  of the stable-3.6/developer version later than 10/01/2010.",
            "title": "Introduction"
        },
        {
            "location": "/explorer_prediction_visualization_plugins/#requirements",
            "text": "custom visualization class must implement the following  interface  weka.gui.visualize.plugins.VisualizePlugin    the class must  either  reside in the following  package  (visualization classes are automatically discovered during run-time)  weka.gui.visualize.plugins    or the class' package must be listed in the  weka.gui.visualize.plugins.VisualizePlugin  key of the  [weka/gui/GenericPropertiesCreator.props](weka_gui_genericpropertiescreator.props.md)  file (only available in 3.5.7 or later).",
            "title": "Requirements"
        },
        {
            "location": "/explorer_prediction_visualization_plugins/#implementation",
            "text": "The visualization interface contains the following four methods   getMinVersion  This method returns the  minimal  version (inclusive) of Weka that is necessary to execute the plugin, e.g.,  3.5.0 .    getMaxVersion  This method returns the  maximal  version (exclusive) of Weka that is necessary to execute the plugin, e.g.,  3.6.0 .    getDesignVersion  Returns the actual version of Weka this plugin was designed for, e.g.,  3.5.1    getVisualizeMenuItem  The  JMenuItem  that is returned via this method will be added to the plugins menu in the popup in the Explorer. The ActionListener for clicking the menu item will most likely open a new frame containing the visualized data.",
            "title": "Implementation"
        },
        {
            "location": "/explorer_prediction_visualization_plugins/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/explorer_prediction_visualization_plugins/#table-with-predictions",
            "text": "The  PredictionTable.java  example simply displays the  actual  class label and the one predicted by the classifier. In addition to that, it lists whether it was an incorrect prediction and the class probability for the correct class label.",
            "title": "Table with predictions"
        },
        {
            "location": "/explorer_prediction_visualization_plugins/#bar-plot-with-probabilities",
            "text": "The  PredictionError.java  to display a simple bar plot of the predictions. The correct predictions are displayed in  blue , the incorrect ones in  red . In both cases the class probability that the classifier returned for the correct class label is displayed on the y axis. The x axis is simply the index of the prediction starting with 0.",
            "title": "Bar plot with probabilities"
        },
        {
            "location": "/explorer_prediction_visualization_plugins/#downloads",
            "text": "PredictionTable.java  PredictionError.java",
            "title": "Downloads"
        },
        {
            "location": "/explorer_prediction_visualization_plugins/#see-also",
            "text": "Use Weka in your Java code  - general overview of the basic Weka API  Explorer visualization plugins",
            "title": "See also"
        },
        {
            "location": "/explorer_prediction_visualization_plugins/#links",
            "text": "JMathTools homepage",
            "title": "Links"
        },
        {
            "location": "/explorer_tree_visualization_plugins/",
            "text": "Introduction\n\n\nAs of Weka version >3.5.8 (only developer version, not stable-3.6 branch) one can easily add tree visualization plugins in the Explorer (Classify and Cluster panel). This makes it easy to implement custom visualizations, if the ones Weka offers are not sufficient.\n\n\nNote:\n This is also covered in chapter \nExtending WEKA\n of the WEKA manual in versions later than 3.7.0 or \nsnapshots\n of the developer version later than 10/01/2010.\n\n\ntree\n is referring to trees generated, for instance, by the \nweka.classifiers.trees.J48\n classifier. To be more precise, all classes that import the \nweka.core.Drawable\n interface and which \ngraphType()\n method returns \nweka.core.Drawable.TREE\n. This means, that the trees the clusterer \nweka.clusterers.Cobweb\n generates, can be displayed as well.\n\n\nRequirements\n\n\n\n\ncustom visualization class must implement the following \ninterface\n\n\nweka.gui.visualize.plugins.TreeVisualizePlugin\n\n\n\n\n\n\nthe class must \neither\n reside in the following \npackage\n (visualization classes are automatically discovered during run-time)\n\n\nweka.gui.visualize.plugins\n\n\n\n\n\n\nor the class' package must be listed in the \nweka.gui.visualize.plugins.TreeVisualizePlugin\n key of the \n[weka/gui/GenericPropertiesCreator.props](weka_gui_genericpropertiescreator.props.md)\n file.\n\n\n\n\nImplementation\n\n\nThe visualization interface contains the following four methods\n\n\n\n\ngetMinVersion\n\n\nThis method returns the \nminimal\n version (inclusive) of Weka that is necessary to execute the plugin, e.g., \n3.5.0\n.\n\n\n\n\n\n\ngetMaxVersion\n\n\nThis method returns the \nmaximal\n version (exclusive) of Weka that is necessary to execute the plugin, e.g., \n3.6.0\n.\n\n\n\n\n\n\ngetDesignVersion\n\n\nReturns the actual version of Weka this plugin was designed for, e.g., \n3.5.1\n\n\n\n\n\n\ngetVisualizeMenuItem\n\n\nThe \nJMenuItem\n that is returned via this method will be added to the plugins menu in the popup in the Explorer. The ActionListener for clicking the menu item will most likely open a new frame containing the visualized data.\n\n\n\n\n\n\n\n\nExamples\n\n\nprefuse visualization toolkit\n\n\nThe \nPrefuseTree.java\n. It is based on the \nprefuse.demos.TreeView\n demo class.\n\n\nThe following screenshot was generated using \nJ48\n on the UCI dataset \nanneal\n with default parameters:\n\n\n\n\nAnd here is an example of \nCobweb\n on the same dataset, once again with default parameters:\n\n\n\n\nNote:\n Both trees are only partially displayed, since the prefuse tree component offers \nexploration\n of the loaded tree.\n\n\nDownloads\n\n\n\n\nPrefuseTree.java\n\n\n\n\nSee also\n\n\n\n\nUse Weka in your Java code\n - general overview of the basic Weka API\n\n\nExplorer visualization plugins\n\n\n\n\nLinks\n\n\n\n\nPrefuse homepage\n\n\nPAP - prefuse assistance pool",
            "title": " Explorer tree visualization plugins"
        },
        {
            "location": "/explorer_tree_visualization_plugins/#introduction",
            "text": "As of Weka version >3.5.8 (only developer version, not stable-3.6 branch) one can easily add tree visualization plugins in the Explorer (Classify and Cluster panel). This makes it easy to implement custom visualizations, if the ones Weka offers are not sufficient.  Note:  This is also covered in chapter  Extending WEKA  of the WEKA manual in versions later than 3.7.0 or  snapshots  of the developer version later than 10/01/2010.  tree  is referring to trees generated, for instance, by the  weka.classifiers.trees.J48  classifier. To be more precise, all classes that import the  weka.core.Drawable  interface and which  graphType()  method returns  weka.core.Drawable.TREE . This means, that the trees the clusterer  weka.clusterers.Cobweb  generates, can be displayed as well.",
            "title": "Introduction"
        },
        {
            "location": "/explorer_tree_visualization_plugins/#requirements",
            "text": "custom visualization class must implement the following  interface  weka.gui.visualize.plugins.TreeVisualizePlugin    the class must  either  reside in the following  package  (visualization classes are automatically discovered during run-time)  weka.gui.visualize.plugins    or the class' package must be listed in the  weka.gui.visualize.plugins.TreeVisualizePlugin  key of the  [weka/gui/GenericPropertiesCreator.props](weka_gui_genericpropertiescreator.props.md)  file.",
            "title": "Requirements"
        },
        {
            "location": "/explorer_tree_visualization_plugins/#implementation",
            "text": "The visualization interface contains the following four methods   getMinVersion  This method returns the  minimal  version (inclusive) of Weka that is necessary to execute the plugin, e.g.,  3.5.0 .    getMaxVersion  This method returns the  maximal  version (exclusive) of Weka that is necessary to execute the plugin, e.g.,  3.6.0 .    getDesignVersion  Returns the actual version of Weka this plugin was designed for, e.g.,  3.5.1    getVisualizeMenuItem  The  JMenuItem  that is returned via this method will be added to the plugins menu in the popup in the Explorer. The ActionListener for clicking the menu item will most likely open a new frame containing the visualized data.",
            "title": "Implementation"
        },
        {
            "location": "/explorer_tree_visualization_plugins/#examples",
            "text": "",
            "title": "Examples"
        },
        {
            "location": "/explorer_tree_visualization_plugins/#prefuse-visualization-toolkit",
            "text": "The  PrefuseTree.java . It is based on the  prefuse.demos.TreeView  demo class.  The following screenshot was generated using  J48  on the UCI dataset  anneal  with default parameters:   And here is an example of  Cobweb  on the same dataset, once again with default parameters:   Note:  Both trees are only partially displayed, since the prefuse tree component offers  exploration  of the loaded tree.",
            "title": "prefuse visualization toolkit"
        },
        {
            "location": "/explorer_tree_visualization_plugins/#downloads",
            "text": "PrefuseTree.java",
            "title": "Downloads"
        },
        {
            "location": "/explorer_tree_visualization_plugins/#see-also",
            "text": "Use Weka in your Java code  - general overview of the basic Weka API  Explorer visualization plugins",
            "title": "See also"
        },
        {
            "location": "/explorer_tree_visualization_plugins/#links",
            "text": "Prefuse homepage  PAP - prefuse assistance pool",
            "title": "Links"
        },
        {
            "location": "/explorer_visualization_plugins/",
            "text": "The Explorer offers various possibilities to add custom plugins:\n\n\n\n\npredictions\n\n\nerrors\n\n\ngraphs\n\n\ntrees\n\n\n\n\nNote:\n This is also covered in chapter \nExtending WEKA\n of the WEKA manual in versions later than 3.6.1/3.7.0 or \nsnapshots\n of the stable-3.6/developer version later than 10/01/2010.",
            "title": " Explorer visualization plugins"
        },
        {
            "location": "/exporting_charts_from_the_knowledge_flow/",
            "text": "From Weka 3.7.5 it is possible to create and save charts such as scatter plots, attribute histograms, error plots, ROC curves etc. non-interactively as part of a data mining process. Weka's built-in charts can be used or an optional package, such as \njfreechartOffscreenChartRenderer\n (to be released in conjunction with Weka 3.7.5) can be installed using the package manager in order to render pretty charts using the \nJFreeChart library\n. A new template is included in the Knowledge Flow that can be used to get you started and demonstrate the options available.\n\n\n\n\nThe above example Knowledge Flow uses the german credit data from the UCI repository and is configured to use the built-in Weka charting routines. The \"\nImageSaver\n\" components are configured to save the generated PNG charts to the user's home directory (any writable place on the filesystem can be used of course). Tool tips appear when the mouse hovers over the labels of the options for the renderer in the configuration dialog for the \"\nDataVisualizer\n\", \"\nAttributeSummarizer\n\" and \"\nModelPerformanceChart\n\" components that explain the available options. The following screenshots show the charts that are generated by the flow using using the optional \njfreechartOffscreenChartRenderer\n package:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nVersion 1.0.1 of the \njfreechartOffscreenChartRenderer\n adds the ability to plot a Pareto chart for nominal attributes. The \n-pareto\n renderer option can be used to accomplish this.",
            "title": " Exporting Charts from the Knowledge Flow"
        },
        {
            "location": "/extensions_for_wekas_main_gui/",
            "text": "Description\n\n\nThe main GUI (= \nweka.gui.Main\n) contains a plugin mechanism to add functionality to the main menu without having to modify the code of that class (the GUIChooser in the developer version as well). Thanks to the automatic class discovery, Weka will display all components that are found in packages listed in the \nGenericPropertiesCreator.props\n file.\n\n\nVersion\n\n\n\n\n3.5.5 (or \nsnapshot\n/\nSubversion\n after 25/05/2007)\n\n\n\n\nRequirements\n\n\nThe are only \ntwo\n requirements for components to be listed in the main menu (under the \nExtensions\n menu):\n\n\n\n\nthey have to implement the \nweka.gui.MainMenuExtension\n interface\n\n\nthe packages they reside in must be listed in the \nGenericPropertiesCreator.props\n under the \nweka.gui.MainMenuExtension\n entry\n\n\n\n\nExamples\n\n\nIn the following, I'll present two really simple examples of how to add \nstuff\n to the main menu. An item that gets added to the main menu either handles everything itself, i.e., creating frame and displaying it, or it needs a frame to place its GUI components in. In the first case, one only needs to let the \ngetActionListener(JFrame)\n method return an \nActionListener\n and implement the \nfillFrame(Component)\n method with an empty body. In the other case, one lets the \ngetActionListener(JFrame)\n method return \nnull\n and uses the \nfillFrame(Component)\n method to fill the frame with life.\n\n\nLaunching a browser\n\n\nLaunching a browser with the Weka homepage is a really example, since one only needs to use the \nweka.gui.BrowserHelper\n class to launch a browser with a specific URL. Since we don't need a frame for this, we don't add any functionality to the \nfillFrame(Component)\n, but only let the \ngetActionListener(JFrame)\n method return an \nActionListener\n that launches the browser with the Weka homepage. The \nStartBrowser.java\n extension will be listed in the sub-menu \nInternet\n as \nStart browser\n.\n\n\n  public String getSubmenuTitle() {\n    return \"Internet\";\n  }\n\n  public String getMenuTitle() {\n    return \"Start browser\";\n  }\n\n  public ActionListener getActionListener(JFrame owner) {\n    final JFrame finalOwner = owner;\n\n    ActionListener result = new ActionListener() {\n      public void actionPerformed(ActionEvent evt) {\n    BrowserHelper.openURL(finalOwner, \"http://www.cs.waikato.ac.nz/~ml/weka/\");\n      }\n    };\n\n    return result;\n  }\n\n  public void fillFrame(Component frame) {\n  }\n\n\n\n\nSince the class is part of the \nweka.gui.extensions\n package, we must add this package to the \nweka.gui.MainMenuExtension\n entry of the \nGenericPropertiesCreator.props\n file, e.g.:\n\n\n weka.gui.MainMenuExtension=\\\n  weka.gui.extensions\n\n\n\n\nSQL Worksheet\n\n\nThe \nSqlWorksheet.java\n mode, one needs to take care of to check for \nJFrame\n and \nJInternalFrame\n as ancestor of the frame that is being passed through. The method only instantiates an \nSqlViewer\n panel, places it in the center of the frame, resizes the frame to 800x600 and moves it into the center of the screen.\n\n\n  public String getSubmenuTitle() {\n    return null;\n  }\n\n  public String getMenuTitle() {\n    return \"SQL Worksheet\";\n  }\n\n  public ActionListener getActionListener(JFrame owner) {\n    return null;\n  }\n\n  public void fillFrame(Component frame) {\n    SqlViewer sql = new SqlViewer(null);\n\n    * add sql viewer component\n    if (frame instanceof JFrame) {\n      JFrame f = (JFrame) frame;\n      f.setLayout(new BorderLayout());\n      f.add(sql, BorderLayout.CENTER);\n      f.pack();\n    }\n    else if (frame instanceof JInternalFrame) {\n      JInternalFrame f = (JInternalFrame) frame;\n      f.setLayout(new BorderLayout());\n      f.add(sql, BorderLayout.CENTER);\n      f.pack();\n    }\n\n    * size + location (= centered)\n    frame.setSize(800, 600);\n    frame.validate();\n    int screenHeight = frame.getGraphicsConfiguration().getBounds().height;\n    int screenWidth  = frame.getGraphicsConfiguration().getBounds().width;\n    frame.setLocation(\n      (screenWidth - frame.getBounds().width) / 2,\n      (screenHeight - frame.getBounds().height) / 2);\n  }\n\n\n\n\nThis class is part of the \nweka.gui.extensions\n package and we therefore must add this package to the \nweka.gui.MainMenuExtension\n entry of the \nGenericPropertiesCreator.props\n file:\n\n\n weka.gui.MainMenuExtension=\\\n  weka.gui.extensions\n\n\n\n\nDownloads\n\n\n\n\nStartBrowser.java\n\n\nSqlWorksheet.java",
            "title": " Extensions for WEKAs main GUI"
        },
        {
            "location": "/extensions_for_wekas_main_gui/#description",
            "text": "The main GUI (=  weka.gui.Main ) contains a plugin mechanism to add functionality to the main menu without having to modify the code of that class (the GUIChooser in the developer version as well). Thanks to the automatic class discovery, Weka will display all components that are found in packages listed in the  GenericPropertiesCreator.props  file.",
            "title": "Description"
        },
        {
            "location": "/extensions_for_wekas_main_gui/#version",
            "text": "3.5.5 (or  snapshot / Subversion  after 25/05/2007)",
            "title": "Version"
        },
        {
            "location": "/extensions_for_wekas_main_gui/#requirements",
            "text": "The are only  two  requirements for components to be listed in the main menu (under the  Extensions  menu):   they have to implement the  weka.gui.MainMenuExtension  interface  the packages they reside in must be listed in the  GenericPropertiesCreator.props  under the  weka.gui.MainMenuExtension  entry",
            "title": "Requirements"
        },
        {
            "location": "/extensions_for_wekas_main_gui/#examples",
            "text": "In the following, I'll present two really simple examples of how to add  stuff  to the main menu. An item that gets added to the main menu either handles everything itself, i.e., creating frame and displaying it, or it needs a frame to place its GUI components in. In the first case, one only needs to let the  getActionListener(JFrame)  method return an  ActionListener  and implement the  fillFrame(Component)  method with an empty body. In the other case, one lets the  getActionListener(JFrame)  method return  null  and uses the  fillFrame(Component)  method to fill the frame with life.",
            "title": "Examples"
        },
        {
            "location": "/extensions_for_wekas_main_gui/#launching-a-browser",
            "text": "Launching a browser with the Weka homepage is a really example, since one only needs to use the  weka.gui.BrowserHelper  class to launch a browser with a specific URL. Since we don't need a frame for this, we don't add any functionality to the  fillFrame(Component) , but only let the  getActionListener(JFrame)  method return an  ActionListener  that launches the browser with the Weka homepage. The  StartBrowser.java  extension will be listed in the sub-menu  Internet  as  Start browser .    public String getSubmenuTitle() {\n    return \"Internet\";\n  }\n\n  public String getMenuTitle() {\n    return \"Start browser\";\n  }\n\n  public ActionListener getActionListener(JFrame owner) {\n    final JFrame finalOwner = owner;\n\n    ActionListener result = new ActionListener() {\n      public void actionPerformed(ActionEvent evt) {\n    BrowserHelper.openURL(finalOwner, \"http://www.cs.waikato.ac.nz/~ml/weka/\");\n      }\n    };\n\n    return result;\n  }\n\n  public void fillFrame(Component frame) {\n  }  Since the class is part of the  weka.gui.extensions  package, we must add this package to the  weka.gui.MainMenuExtension  entry of the  GenericPropertiesCreator.props  file, e.g.:   weka.gui.MainMenuExtension=\\\n  weka.gui.extensions",
            "title": "Launching a browser"
        },
        {
            "location": "/extensions_for_wekas_main_gui/#sql-worksheet",
            "text": "The  SqlWorksheet.java  mode, one needs to take care of to check for  JFrame  and  JInternalFrame  as ancestor of the frame that is being passed through. The method only instantiates an  SqlViewer  panel, places it in the center of the frame, resizes the frame to 800x600 and moves it into the center of the screen.    public String getSubmenuTitle() {\n    return null;\n  }\n\n  public String getMenuTitle() {\n    return \"SQL Worksheet\";\n  }\n\n  public ActionListener getActionListener(JFrame owner) {\n    return null;\n  }\n\n  public void fillFrame(Component frame) {\n    SqlViewer sql = new SqlViewer(null);\n\n    * add sql viewer component\n    if (frame instanceof JFrame) {\n      JFrame f = (JFrame) frame;\n      f.setLayout(new BorderLayout());\n      f.add(sql, BorderLayout.CENTER);\n      f.pack();\n    }\n    else if (frame instanceof JInternalFrame) {\n      JInternalFrame f = (JInternalFrame) frame;\n      f.setLayout(new BorderLayout());\n      f.add(sql, BorderLayout.CENTER);\n      f.pack();\n    }\n\n    * size + location (= centered)\n    frame.setSize(800, 600);\n    frame.validate();\n    int screenHeight = frame.getGraphicsConfiguration().getBounds().height;\n    int screenWidth  = frame.getGraphicsConfiguration().getBounds().width;\n    frame.setLocation(\n      (screenWidth - frame.getBounds().width) / 2,\n      (screenHeight - frame.getBounds().height) / 2);\n  }  This class is part of the  weka.gui.extensions  package and we therefore must add this package to the  weka.gui.MainMenuExtension  entry of the  GenericPropertiesCreator.props  file:   weka.gui.MainMenuExtension=\\\n  weka.gui.extensions",
            "title": "SQL Worksheet"
        },
        {
            "location": "/extensions_for_wekas_main_gui/#downloads",
            "text": "StartBrowser.java  SqlWorksheet.java",
            "title": "Downloads"
        },
        {
            "location": "/gui_chooser_starts_but_not_experimenter_or_explorer/",
            "text": "The GUIChooser starts, but Explorer and Experimenter don't start and output an Exception like this in the terminal:\n\n\n /usr/share/themes/Mist/gtk-2.0/gtkrc:48: Engine \"mist\" is unsupported, ignoring\n ---Registering Weka Editors---\n java.lang.NullPointerException\n        at weka.gui.explorer.PreprocessPanel.addPropertyChangeListener(PreprocessPanel.java:519)\n        at javax.swing.plaf.synth.SynthPanelUI.installListeners(SynthPanelUI.java:49)\n        at javax.swing.plaf.synth.SynthPanelUI.installUI(SynthPanelUI.java:38)\n        at javax.swing.JComponent.setUI(JComponent.java:652)\n        at javax.swing.JPanel.setUI(JPanel.java:131) \n        ...\n\n\n\n\nThis behavior happens only under Java 5/6 and Gnome/Linux, KDE doesn't produce this error. The reason for this is, that  Weka tries to look more \"native\" and therefore sets a platform-specific Swing theme. Unfortunately, this doesn't seem to be working correctly in Java 5/6 together with Gnome. A workaround for this is to set the cross-platform \nMetal\n theme. \n\n\nIn order to use another theme one only has to create the following properties file in ones home directory:\n\n\n LookAndFeel.props\n\n\n\n\nWith this content:\n\n\n Theme=javax.swing.plaf.metal.MetalLookAndFeel\n\n\n\n\nMore information can be found in \nthis Weka list post\n or \nhere\n.",
            "title": " GUIChooser starts but not Experimenter or Explorer"
        },
        {
            "location": "/instance_id/",
            "text": "People often want to \ntag\n their \ninstances with identifiers\n, so they can keep track of them and the predictions made on them.\n\n\nAdding the ID\n\n\nA new ID attribute is added real easy: one only needs to run the \nAddID\n filter over the dataset and it's done. Here's an example (at a DOS/Unix command prompt):\n\n\n java weka.filters.unsupervised.attribute.AddID\n   -i data_without_id.arff\n   -o data_with_id.arff\n\n\n\n\n(all on a single line)\n\n\nNote:\n the AddID filter adds a numeric attribute, not a String attribute to the dataset. If you want to remove this ID attribute for the classifier in a \nFilteredClassifier\n environment again, use the \nRemove\n filter instead of the \nRemoveType\n filter (same package).\n\n\nRemoving the ID\n\n\nIf you run from the command line you can use the \n-p\n option to output predictions plus any other attributes you are interested in. So it is possible to have a string attribute in your data that acts as an identifier. A problem is that most classifiers don't like String attributes, but you can get around this by using the \nRemoveType\n (this removes \nString\n attributes by default).\n\n\nHere's an example. Lets say you have a training file named \ntrain.arff\n, a testing file named \ntest.arff\n, and they have an identifier String attribute as their 5th attribute. You can get the predictions from \nJ48\n along with the identifier strings by issuing the following command (at a DOS/Unix command prompt):\n\n\n java weka.classifiers.meta.FilteredClassifier\n   -F weka.filters.unsupervised.attribute.RemoveType\n   -W weka.classifiers.trees.J48\n   -t train.arff -T test.arff -p 5\n\n\n\n\n(all on a single line)\n\n\nIf you want, you can redirect the output to a file by adding \"\n> output.txt\n\" to the end of the line.\n\n\nIn the Explorer GUI you could try a similar trick of using the String attribute identifiers here as well. Choose the \nFilteredClassifier\n, with the \nRemoveType\n as the filter, and whatever classifier you prefer. When you visualize the results you will need click through each instance to see the identifier listed for each.",
            "title": " Instance ID"
        },
        {
            "location": "/instance_id/#adding-the-id",
            "text": "A new ID attribute is added real easy: one only needs to run the  AddID  filter over the dataset and it's done. Here's an example (at a DOS/Unix command prompt):   java weka.filters.unsupervised.attribute.AddID\n   -i data_without_id.arff\n   -o data_with_id.arff  (all on a single line)  Note:  the AddID filter adds a numeric attribute, not a String attribute to the dataset. If you want to remove this ID attribute for the classifier in a  FilteredClassifier  environment again, use the  Remove  filter instead of the  RemoveType  filter (same package).",
            "title": "Adding the ID"
        },
        {
            "location": "/instance_id/#removing-the-id",
            "text": "If you run from the command line you can use the  -p  option to output predictions plus any other attributes you are interested in. So it is possible to have a string attribute in your data that acts as an identifier. A problem is that most classifiers don't like String attributes, but you can get around this by using the  RemoveType  (this removes  String  attributes by default).  Here's an example. Lets say you have a training file named  train.arff , a testing file named  test.arff , and they have an identifier String attribute as their 5th attribute. You can get the predictions from  J48  along with the identifier strings by issuing the following command (at a DOS/Unix command prompt):   java weka.classifiers.meta.FilteredClassifier\n   -F weka.filters.unsupervised.attribute.RemoveType\n   -W weka.classifiers.trees.J48\n   -t train.arff -T test.arff -p 5  (all on a single line)  If you want, you can redirect the output to a file by adding \" > output.txt \" to the end of the line.  In the Explorer GUI you could try a similar trick of using the String attribute identifiers here as well. Choose the  FilteredClassifier , with the  RemoveType  as the filter, and whatever classifier you prefer. When you visualize the results you will need click through each instance to see the identifier listed for each.",
            "title": "Removing the ID"
        },
        {
            "location": "/just_in_time_jit_compiler/",
            "text": "For maximum enjoyment, use a virtual machine that incorporates a \njust-in-time compiler\n. This can speed things up quite significantly. Note also that there can be large differences in execution time between different virtual machines. The Sun JDK/JRE all include a JIT compiler (\"hotspot\").",
            "title": " just-in-time (JIT) compiler"
        },
        {
            "location": "/knowledge_flow_toolbars_are_empty/",
            "text": "In the terminal, you will most likely see this output as well:\n\n\n Failed to instantiate: weka.gui.beans.Loader\n\n\n\n\nThis behavior can happen under Gnome with Java 5/6, see \nGUIChooser starts but not Experimenter or Explorer\n for a solution.",
            "title": " KnowledgeFlow toolbars are empty"
        },
        {
            "location": "/memory_consumption_and_garbage_collector/",
            "text": "There is the ability to print \nhow much memory\n is available in the Explorer and Experimenter and to run the garbage collector. Just right click over the Status area in the Explorer/Experimenter.",
            "title": " Memory consumption and Garbage collector"
        },
        {
            "location": "/osx_mountain_lion_weka_x_y_z_is_damaged_and_cant_be_installed_you_should_eject_the_disk_image/",
            "text": "Mac OS X 10.8 (Mountain Lion) introduced a new security feature that, by default, limits \"acceptable\" applications to only those downloaded from the Mac App store. Thankfully, you can alter this in the system preferences. Go to \"Security & Privacy\" and change the \"Allow applications downloaded from:\" to \"Anywhere\". Weka will launch successfully after this change.",
            "title": " OSX Mountain Lion - Weka x-y-z is damaged and cannot be installed. You should eject the disk image"
        },
        {
            "location": "/single_quotes_in_labels_of_arff_files/",
            "text": "Single quotes in Weka are used to surround strings with spaces or other special characters (see \nspaces in labels of ARFF files\n). If some of your labels contain single quotes, you have to escape them with a backslash.\nFor example, the name of one of Alexandre Duma's musketeers is\n\n\nD'Artagnan\n\n\n\n\nand needs to be quoted and escaped as follows\n\n\n'D\\'Artagnan'\n\n\n\n\nUsing the Weka API, you can use the \nquote(String)\n method of the \nweka.core.Utils\n class for doing this:\n\n\nimport weka.core.Utils;\n...\nString raw = \"D'Artagnan\";\nString escaped = Utils.quote(raw);",
            "title": " Single quotes in labels of ARFF files"
        },
        {
            "location": "/spaces_in_labels_of_arff_files/",
            "text": "A common problem people have with \nARFF\n files is that labels can only have spaces if they are enclosed in single quotes, i.e., a label such as:\n\n\n some value\n\n\n\n\nshould be written either\n\n\n 'some value'\n\n\n\n\nor\n\n\n some_value\n\n\n\n\nin the file.\n\n\nSee \nsingle quotes in labels of ARFF files\n for an example using the Weka API for automatically quoting such strings.",
            "title": " Spaces in labels of ARFF files"
        },
        {
            "location": "/feature_extraction_from_images/",
            "text": "ImageJ\n can be used to extract features from images. ImageJ contains a macro language with which it is easy to extract features and then dump them into an \nARFF\n file.\n\n\nLinks\n\n\n\n\nImageJ homepage",
            "title": " Feature extraction from images"
        },
        {
            "location": "/feature_extraction_from_images/#links",
            "text": "ImageJ homepage",
            "title": "Links"
        },
        {
            "location": "/filtered_classifier_updateable/",
            "text": "Description\n\n\nIncremental version of \nweka.classifiers.meta.FilteredClassifier\n, which takes only incremental base classifiers (i.e., classifiers implementing \nweka.classifiers.UpdateableClassifier\n).\n\n\nReference\n\n\n-none-\n\n\nPackage\n\n\nweka.classifiers.meta\n\n\nDownload\n\n\n\n\nSource code: \n\nFilteredClassifierUpdateable.java\n\n\nExample class: \n\nFilteredUpdateableTest.java\n\n\n\n\nAdditional Information\n\n\n-none-\n\n\nVersion\n\n\nTested with source code from \nSubversion\n (= \ntrunk/weka\n) as of 10/11/2008.",
            "title": " FilteredClassifierUpdateable"
        },
        {
            "location": "/filtered_classifier_updateable/#description",
            "text": "Incremental version of  weka.classifiers.meta.FilteredClassifier , which takes only incremental base classifiers (i.e., classifiers implementing  weka.classifiers.UpdateableClassifier ).",
            "title": "Description"
        },
        {
            "location": "/filtered_classifier_updateable/#reference",
            "text": "-none-",
            "title": "Reference"
        },
        {
            "location": "/filtered_classifier_updateable/#package",
            "text": "weka.classifiers.meta",
            "title": "Package"
        },
        {
            "location": "/filtered_classifier_updateable/#download",
            "text": "Source code:  FilteredClassifierUpdateable.java  Example class:  FilteredUpdateableTest.java",
            "title": "Download"
        },
        {
            "location": "/filtered_classifier_updateable/#additional-information",
            "text": "-none-",
            "title": "Additional Information"
        },
        {
            "location": "/filtered_classifier_updateable/#version",
            "text": "Tested with source code from  Subversion  (=  trunk/weka ) as of 10/11/2008.",
            "title": "Version"
        },
        {
            "location": "/generating_and_saving_a_precision_recall_curve/",
            "text": "The following Java class evaluates a NaiveBayes classifier using cross-validation with a dataset provided by the user and saves a precision-recall curve for the first class label as a JPEG file, based on a user-specified file name.\n\n\nSource code:\n\n\nimport java.awt.*;\nimport java.io.*;\nimport java.util.*;\nimport javax.swing.*;\nimport weka.core.*;\nimport weka.classifiers.*;\nimport weka.classifiers.bayes.NaiveBayes;\nimport weka.classifiers.evaluation.Evaluation;\nimport weka.classifiers.evaluation.ThresholdCurve;\nimport weka.gui.visualize.*;\n\n/**\n * Generates and saves a precision-recall curve. Uses a cross-validation\n * with NaiveBayes to make the curve.\n *\n * @author FracPete\n * @author Eibe Frank\n */\npublic class SavePrecisionRecallCurve {\n\n  /**\n   * takes two arguments: dataset in ARFF format (expects class to\n   * be last attribute) and name of file with output\n   */\n  public static void main(String[] args) throws Exception {\n\n    // load data\n    Instances data = new Instances(new BufferedReader(new FileReader(args[0])));\n    data.setClassIndex(data.numAttributes() - 1);\n\n    // train classifier\n    Classifier cl = new NaiveBayes();\n    Evaluation eval = new Evaluation(data);\n    eval.crossValidateModel(cl, data, 10, new Random(1));\n\n    // generate curve\n    ThresholdCurve tc = new ThresholdCurve();\n    int classIndex = 0;\n    Instances result = tc.getCurve(eval.predictions(), classIndex);\n\n    // plot curve\n    ThresholdVisualizePanel vmc = new ThresholdVisualizePanel();\n    PlotData2D tempd = new PlotData2D(result);\n\n    // specify which points are connected\n    boolean[] cp = new boolean[result.numInstances()];\n    for (int n = 1; n < cp.length; n++)\n      cp[n] = true;\n    tempd.setConnectPoints(cp);\n    // add plot\n    vmc.addPlot(tempd);\n\n    // We want a precision-recall curve\n    vmc.setXIndex(result.attribute(\"Recall\").index());\n    vmc.setYIndex(result.attribute(\"Precision\").index());\n\n    // Make window with plot but don't show it\n    JFrame jf =  new JFrame();\n    jf.setSize(500,400);\n    jf.getContentPane().add(vmc);\n    jf.pack();\n\n    // Save to file specified as second argument (can use any of\n    // BMPWriter, JPEGWriter, PNGWriter, PostscriptWriter for different formats)\n    JComponentWriter jcw = new JPEGWriter(vmc.getPlotPanel(), new File(args[1]));\n    jcw.toOutput();\n    System.exit(1);\n  }\n}\n\n\n\n\nSee also\n\n\n\n\nROC curves\n\n\nVisualizing ROC curve\n\n\nPlotting multiple ROC curves\n\n\n\n\n\n\nVersion\n\n\nNeeds the developer version >=3.5.1 or 3.6.x",
            "title": " Generating and saving a precision-recall curve"
        },
        {
            "location": "/generating_and_saving_a_precision_recall_curve/#see-also",
            "text": "ROC curves  Visualizing ROC curve  Plotting multiple ROC curves",
            "title": "See also"
        },
        {
            "location": "/generating_and_saving_a_precision_recall_curve/#version",
            "text": "Needs the developer version >=3.5.1 or 3.6.x",
            "title": "Version"
        },
        {
            "location": "/generating_classifier_evaluation_output_manually/",
            "text": "In the following some code snippets that explain how to generate the output Weka generates when one runs a classifier from the commandline. When referring to the \nEvaluation\n class, the \nweka.classifiers.Evaluation\n class is meant. This article provides only a quick overview, for more details, please see the \nJavadoc\n of the \nEvaluation\n class.\n\n\nModel\n\n\nA classifier's model, if that classifier supports the output of it, can be simply output by using the \ntoString()\n method after it got trained:\n\n\n Instances data = ... // from somewhere\n Classifier cls = new weka.classifiers.trees.J48();\n cls.buildClassifier(data);\n System.out.println(cls);\n\n\n\n\nNB:\n Weka always outputs the model based on the \nfull\n training set (provided with the option \n-t\n), no matter whether cross-validation is used or a designated test set (via \n-T\n). The 10 models generated during a 10-fold cross-validation run are never output. If you want to output these models you have to simulate the \ncrossValidateModel\n method yourself, use the KnowledgeFlow (see article \nDisplaying results of cross-validation folds\n).\n\n\nStatistics\n\n\nThe statistics, also called the summary of an evaluation, can be be generated via the \ntoSummaryString\n methods. Here is an example of the summary from a cross-validated J48:\n\n\n Classifier cls = new J48();\n Evaluation eval = new Evaluation(data);\n Random rand = new Random(1);  // using seed = 1\n int folds = 10;\n eval.crossValidateModel(cls, data, folds, rand);\n System.out.println(eval.toSummaryString());\n\n\n\n\nDetailed class statistics\n\n\nIn order to generate the detailed statistics per class (on the commandline via option \n-i\n), one can use the \ntoClassDetailsString\n methods. Once again a code snippet featuring a cross-validated J48:\n\n\n Classifier cls = new J48();\n Evaluation eval = new Evaluation(data);\n Random rand = new Random(1);  // using seed = 1\n int folds = 10;\n eval.crossValidateModel(cls, data, folds, rand);\n System.out.println(eval.toClassDetailsString());\n\n\n\n\nConfusion matrix\n\n\nThe confusion matrix is simply output with the \ntoMatrixString()\n or \ntoMatrixString(String)\n method of the \nEvaluation\n class. In the following an example of cross-validating J48 on a dataset and outputting the confusion matrix to stdout.\n\n\n Classifier cls = new J48();\n Evaluation eval = new Evaluation(data);\n Random rand = new Random(1);  // using seed = 1\n int folds = 10;\n eval.crossValidateModel(cls, data, folds, rand);\n System.out.println(eval.toMatrixString());\n\n\n\n\nSee also\n\n\n\n\nUse Weka in your Java code\n - general overview of the Weka API",
            "title": " Generating classifier evaluation output manually"
        },
        {
            "location": "/generating_classifier_evaluation_output_manually/#model",
            "text": "A classifier's model, if that classifier supports the output of it, can be simply output by using the  toString()  method after it got trained:   Instances data = ... // from somewhere\n Classifier cls = new weka.classifiers.trees.J48();\n cls.buildClassifier(data);\n System.out.println(cls);  NB:  Weka always outputs the model based on the  full  training set (provided with the option  -t ), no matter whether cross-validation is used or a designated test set (via  -T ). The 10 models generated during a 10-fold cross-validation run are never output. If you want to output these models you have to simulate the  crossValidateModel  method yourself, use the KnowledgeFlow (see article  Displaying results of cross-validation folds ).",
            "title": "Model"
        },
        {
            "location": "/generating_classifier_evaluation_output_manually/#statistics",
            "text": "The statistics, also called the summary of an evaluation, can be be generated via the  toSummaryString  methods. Here is an example of the summary from a cross-validated J48:   Classifier cls = new J48();\n Evaluation eval = new Evaluation(data);\n Random rand = new Random(1);  // using seed = 1\n int folds = 10;\n eval.crossValidateModel(cls, data, folds, rand);\n System.out.println(eval.toSummaryString());",
            "title": "Statistics"
        },
        {
            "location": "/generating_classifier_evaluation_output_manually/#detailed-class-statistics",
            "text": "In order to generate the detailed statistics per class (on the commandline via option  -i ), one can use the  toClassDetailsString  methods. Once again a code snippet featuring a cross-validated J48:   Classifier cls = new J48();\n Evaluation eval = new Evaluation(data);\n Random rand = new Random(1);  // using seed = 1\n int folds = 10;\n eval.crossValidateModel(cls, data, folds, rand);\n System.out.println(eval.toClassDetailsString());",
            "title": "Detailed class statistics"
        },
        {
            "location": "/generating_classifier_evaluation_output_manually/#confusion-matrix",
            "text": "The confusion matrix is simply output with the  toMatrixString()  or  toMatrixString(String)  method of the  Evaluation  class. In the following an example of cross-validating J48 on a dataset and outputting the confusion matrix to stdout.   Classifier cls = new J48();\n Evaluation eval = new Evaluation(data);\n Random rand = new Random(1);  // using seed = 1\n int folds = 10;\n eval.crossValidateModel(cls, data, folds, rand);\n System.out.println(eval.toMatrixString());",
            "title": "Confusion matrix"
        },
        {
            "location": "/generating_classifier_evaluation_output_manually/#see-also",
            "text": "Use Weka in your Java code  - general overview of the Weka API",
            "title": "See also"
        },
        {
            "location": "/generating_source_code_from_weka_classes/",
            "text": "Some of the schemes in Weka can generate Java source code that represents their current internal state. At the moment these are classifiers (book and developer version) and filters (\nsnapshot\n or >3.5.6). The generated code can be used within Weka as normal classifier/filter, since this code will be derived from the same superclass (\nweka.classifiers.Classifier\n or \nweka.filters.Filter\n) as the generating code.\n\n\nNote:\n The commands listed here are for a Linux/Unix bash (the backslash tells the shell that the command isn't finished yet and continues on the next line). In case of Windows or the SimpleCLI, just remove the backslashes and put everything on one line.\n\n\nClassifiers\n\n\nInstead of using a serialized filter to perform further classifications/predictions, one can also obtain source code from a trained classifier and use this instead. The advantage of this is being less dependent on version changes and incompatible serialized files. All classifiers implementing the \nweka.classifiers.Sourcable\n interface can turn their model into Java source code (check the Javadoc of this interface for all the classifiers implementing it).\n\n\nHere's an example of generating source code from a trained \nJ48\n (the source code is saved in a file called \nWekaWrapper.java\n):\n\n\n java weka.classifiers.trees.J48 \\\n   -t /some/where/data.arff \\\n   -z SourcedJ48 \\                  # name of the inner class, gets called by wrapper class WekaWrapper\n   > /else/where/WekaWrapper.java   # redirecting the output of the code into a file\n\n\n\n\nThe package of the wrapper class is by default the \nweka.classifiers\n package. Make sure that you place the source code and/or class files in the correct location. The generated classifier can be used from the commandline or GUI like any other classifier within Weka, you only need to make sure that your \nGenericObjectEditor\n lists the package you place the classifier in (\nweka.classifiers\n is \nnot\n listed by default).\n\n\nThe following command calls the generated classifier with a training set (training has no effect, of course) and outputs the predictions for this dataset to \nstdout\n:\n\n\n java weka.classifiers.WekaWrapper \\\n   -t /some/file.arff \\\n   -p 0                  # output predictions for training set\n\n\n\n\nNote:\n the Explorer can output source code as well, you only have to check the \nOutput source code\n option in the \nMore options\n dialog.\n\n\nFilters\n\n\nWith versions of Weka later than 3.5.6 or a \nsnapshot\n of the developer version, one can now also turn filters into source code. The process is basically the same as with classifiers outlined above. All filters that implement the \nweka.filters.Sourcable\n interface can be turned into Java code (again, check out the Javadoc for this interface, to see the filters implementing it).\n\n\nThe following command turns an initialized ReplaceMissingValues filter into source code:\n\n\n java weka.filters.unsupervised.attribute.ReplaceMissingValues \\\n   -i /somewhere1/input.arff \\\n   -o /somewhere2/output.arff \\\n   -z SourcedRMV \\                   # name of the inner class, gets called by wrapper class WekaWrapper\n   > /some/place/WekaWrapper.java    # redirecting the output of the code into a file\n\n\n\n\nThe package of the wrapper class is by default the \nweka.filters\n package. Make sure that you place the source code and/or class files in the correct location. The generated filter can be used from the commandline or GUI like any other filter within Weka, you only need to make sure that your \nGenericObjectEditor\n lists the package you place the filter in.\n\n\nAnd again a little demonstration of how to call the generated source code:\n\n\n java weka.filters.WekaWrapper \\\n   -i /some/where/input.arff \\     # must have the same structure as **/somewhere1/input.arff**, of course\n   -o /other/place/output.arff \n\n\n\n\nSee also\n\n\n\n\nSerialization\n - can be used for all classifiers and filters to save them in a persistent state.",
            "title": " Generating source code from Weka classes"
        },
        {
            "location": "/generating_source_code_from_weka_classes/#classifiers",
            "text": "Instead of using a serialized filter to perform further classifications/predictions, one can also obtain source code from a trained classifier and use this instead. The advantage of this is being less dependent on version changes and incompatible serialized files. All classifiers implementing the  weka.classifiers.Sourcable  interface can turn their model into Java source code (check the Javadoc of this interface for all the classifiers implementing it).  Here's an example of generating source code from a trained  J48  (the source code is saved in a file called  WekaWrapper.java ):   java weka.classifiers.trees.J48 \\\n   -t /some/where/data.arff \\\n   -z SourcedJ48 \\                  # name of the inner class, gets called by wrapper class WekaWrapper\n   > /else/where/WekaWrapper.java   # redirecting the output of the code into a file  The package of the wrapper class is by default the  weka.classifiers  package. Make sure that you place the source code and/or class files in the correct location. The generated classifier can be used from the commandline or GUI like any other classifier within Weka, you only need to make sure that your  GenericObjectEditor  lists the package you place the classifier in ( weka.classifiers  is  not  listed by default).  The following command calls the generated classifier with a training set (training has no effect, of course) and outputs the predictions for this dataset to  stdout :   java weka.classifiers.WekaWrapper \\\n   -t /some/file.arff \\\n   -p 0                  # output predictions for training set  Note:  the Explorer can output source code as well, you only have to check the  Output source code  option in the  More options  dialog.",
            "title": "Classifiers"
        },
        {
            "location": "/generating_source_code_from_weka_classes/#filters",
            "text": "With versions of Weka later than 3.5.6 or a  snapshot  of the developer version, one can now also turn filters into source code. The process is basically the same as with classifiers outlined above. All filters that implement the  weka.filters.Sourcable  interface can be turned into Java code (again, check out the Javadoc for this interface, to see the filters implementing it).  The following command turns an initialized ReplaceMissingValues filter into source code:   java weka.filters.unsupervised.attribute.ReplaceMissingValues \\\n   -i /somewhere1/input.arff \\\n   -o /somewhere2/output.arff \\\n   -z SourcedRMV \\                   # name of the inner class, gets called by wrapper class WekaWrapper\n   > /some/place/WekaWrapper.java    # redirecting the output of the code into a file  The package of the wrapper class is by default the  weka.filters  package. Make sure that you place the source code and/or class files in the correct location. The generated filter can be used from the commandline or GUI like any other filter within Weka, you only need to make sure that your  GenericObjectEditor  lists the package you place the filter in.  And again a little demonstration of how to call the generated source code:   java weka.filters.WekaWrapper \\\n   -i /some/where/input.arff \\     # must have the same structure as **/somewhere1/input.arff**, of course\n   -o /other/place/output.arff",
            "title": "Filters"
        },
        {
            "location": "/generating_source_code_from_weka_classes/#see-also",
            "text": "Serialization  - can be used for all classifiers and filters to save them in a persistent state.",
            "title": "See also"
        },
        {
            "location": "/generic_object_editor/",
            "text": "The GenericObjectEditor is the core component in Weka for modifying schemes, like classifiers and filters in the GUI. It has to be configured correctly in order to show default and additional schemes. See the following articles for more details:\n\n\n\n\nGenericObjectEditor (book version)\n\n\nGenericObjectEditor (developer version)",
            "title": " GenericObjectEditor"
        },
        {
            "location": "/generic_object_editor_book_version/",
            "text": "Introduction\n\n\nAs of version 3.4.4 it is possible for WEKA to \ndynamically discover classes at runtime\n (rather than using only those specified in the \nGenericObjectEditor.props\n (GOE) file).\n\n\nIf dynamic class discovery is too slow, e.g., due to an enormous CLASSPATH, you can generate a new \nGenericObjectEditor.props\n file and then turn dynamic class discovery off. It is assumed that you already placed the \nGenericPropertiesCreator.props\n (GPC) file in your home directory (this file is located in directory \nweka/gui\n of either the \nweka.jar\n or \nweka-src.jar\n ZIP archive) and that the \nweka.jar\n jar archive with the WEKA classes is in your CLASSPATH (otherwise you have to add it to the \njava\n call using the \n-classpath\n option).\n\n\nFor generating the GOE file, execute the following steps:\n\n\n\n\n\n\ngenerate a new \nGenericObjectEditor.props\n file using the following command:\n\n\n\n\n\n\nLinux/Unix\n\n\n\n\njava weka.gui.GenericPropertiesCreator \\\n   $HOME/GenericPropertiesCreator.props \\\n   $HOME/GenericObjectEditor.props\n\n\n\n\n\n\n\n\nWindows (command must be in one line)\n\n\n\n\njava weka.gui.GenericPropertiesCreator \n   %USERPROFILE%\\GenericPropertiesCreator.props\n   %USERPROFILE%\\GenericObjectEditor.props\n\n\n\n\n\n\n\n\n\n\n\n\nedit the \nGenericPropertiesCreator.props\n file in your home directory and set \nUseDynamic\n to \nfalse\n.\n\n\n\n\n\n\nFor disabling dynamic class discovery, you need to set the boolean constant \nUSE_DYNAMIC\n of the \nweka.gui.GenericObjectEditor\n class to \nfalse\n. See article \nCompiling WEKA\n for more information on how to compile a modified version of WEKA.\n\n\nA limitation of the GOE prior to 3.4.4 was, that additional classifiers, filters, etc., had to fit into the same package structure as the already existing ones, i.e., all had to be located below \nweka\n. WEKA can now display multiple class hierarchies in the GUI, which makes adding new functionality quite easy as we will see later in an example (it is not restricted to classifiers only, but also works with all the other entries in the GPC file).\n\n\nFile Structure\n\n\nThe structure of the GOE so far was a key-value-pair, separated by an \nequals\n-sign. The \nvalue\n is a comma separated list of classes that are all derived from the superclass/superinterface \nkey\n. The GPC is slightly different, instead of declaring all the classes/interfaces one need only to specify all the packages descendants are located in (only non-abstract ones are then listed). E.g., the \nweka.classifiers.Classifier\n entry in the GOE file looks like this:\n\n\n weka.classifiers.Classifier=\\\n  weka.classifiers.bayes.AODE,\\\n  weka.classifiers.bayes.BayesNet,\\\n  weka.classifiers.bayes.ComplementNaiveBayes,\\\n  weka.classifiers.bayes.NaiveBayes,\\\n  weka.classifiers.bayes.NaiveBayesMultinomial,\\\n  weka.classifiers.bayes.NaiveBayesSimple,\\\n  weka.classifiers.bayes.NaiveBayesUpdateable,\\\n  weka.classifiers.functions.LeastMedSq,\\\n  weka.classifiers.functions.LinearRegression,\\\n  weka.classifiers.functions.Logistic,\\\n  ...\n\n\n\n\nThe entry producing the same output for the classifiers in the GPC looks like this (7 lines instead of over 70!):\n\n\n weka.classifiers.Classifier=\\\n  weka.classifiers.bayes,\\\n  weka.classifiers.functions,\\\n  weka.classifiers.lazy,\\\n  weka.classifiers.meta,\\\n  weka.classifiers.trees,\\\n  weka.classifiers.rules\n\n\n\n\nClass Discovery\n\n\nUnlike the \nClass.forName(String)\n method that grabs the first class it can find in the \nCLASSPATH\n, and therefore fixes the location of the package it found the class in, the dynamic discovery examines the complete \nCLASSPATH\n you're starting the \nJava Virtual Machine\n (JVM) with. This means that you can have several parallel directories with the same WEKA package structure, e.g. the standard release of WEKA in one directory ( \n/distribution/weka.jar\n) and another one with your own classes (\n/development/weka/...\n), and display all of the classifiers in the GUI. In case of a name conflict, i.e. two directories contain the same class, the first one that can be found is used. In a nutshell, your \njava\n call of the GUIChooser could look like this:\n\n\n java -classpath \"/development:/distribution/weka.jar\" weka.gui.GUIChooser\n\n\n\n\nNote:\n Windows users have to replace the \":\" with \";\" and the forward slashes with backslashes.\n\n\nMultiple Class Hierarchies\n\n\nIn case you're developing your own framework, but still want to use your classifiers within WEKA that wasn't possible so far. With the release \n3.4.4\n it is possible to have multiple class hierarchies being displayed in the GUI. If you've developed a modified version of J48, let's call it \nMyJ48\n and it's located in the package \ndummy.classifiers\n then you'll have to add this package to the classifiers list in the GPC file like this:\n\n\n weka.classifiers.Classifier=\\\n  weka.classifiers.bayes,\\\n  weka.classifiers.functions,\\\n  weka.classifiers.lazy,\\\n  weka.classifiers.meta,\\\n  weka.classifiers.trees,\\\n  weka.classifiers.rules,\\\n  dummy.classifiers\n\n\n\n\nYour \njava\n call for the GUIChooser might look like this:\n\n\n java -classpath \"weka.jar:dummy.jar\" weka.gui.GUIChooser\n\n\n\n\nNote:\n Windows users have to replace the \":\" with \";\" and the forward slashes with backslashes.\n\n\nStarting up the GUI you'll now have another root node in the tree view of the classifiers, called \nroot\n, and below it the \nweka\n and the \ndummy\n package hierarchy as you can see here:\n\n\nLinks\n\n\n\n\nGenericObjectEditor (developer version)\n\n\nCLASSPATH\n\n\nProperties file\n\n\nGenericPropertiesCreator.props",
            "title": " GenericObjectEditor (book version)"
        },
        {
            "location": "/generic_object_editor_book_version/#introduction",
            "text": "As of version 3.4.4 it is possible for WEKA to  dynamically discover classes at runtime  (rather than using only those specified in the  GenericObjectEditor.props  (GOE) file).  If dynamic class discovery is too slow, e.g., due to an enormous CLASSPATH, you can generate a new  GenericObjectEditor.props  file and then turn dynamic class discovery off. It is assumed that you already placed the  GenericPropertiesCreator.props  (GPC) file in your home directory (this file is located in directory  weka/gui  of either the  weka.jar  or  weka-src.jar  ZIP archive) and that the  weka.jar  jar archive with the WEKA classes is in your CLASSPATH (otherwise you have to add it to the  java  call using the  -classpath  option).  For generating the GOE file, execute the following steps:    generate a new  GenericObjectEditor.props  file using the following command:    Linux/Unix   java weka.gui.GenericPropertiesCreator \\\n   $HOME/GenericPropertiesCreator.props \\\n   $HOME/GenericObjectEditor.props     Windows (command must be in one line)   java weka.gui.GenericPropertiesCreator \n   %USERPROFILE%\\GenericPropertiesCreator.props\n   %USERPROFILE%\\GenericObjectEditor.props       edit the  GenericPropertiesCreator.props  file in your home directory and set  UseDynamic  to  false .    For disabling dynamic class discovery, you need to set the boolean constant  USE_DYNAMIC  of the  weka.gui.GenericObjectEditor  class to  false . See article  Compiling WEKA  for more information on how to compile a modified version of WEKA.  A limitation of the GOE prior to 3.4.4 was, that additional classifiers, filters, etc., had to fit into the same package structure as the already existing ones, i.e., all had to be located below  weka . WEKA can now display multiple class hierarchies in the GUI, which makes adding new functionality quite easy as we will see later in an example (it is not restricted to classifiers only, but also works with all the other entries in the GPC file).",
            "title": "Introduction"
        },
        {
            "location": "/generic_object_editor_book_version/#file-structure",
            "text": "The structure of the GOE so far was a key-value-pair, separated by an  equals -sign. The  value  is a comma separated list of classes that are all derived from the superclass/superinterface  key . The GPC is slightly different, instead of declaring all the classes/interfaces one need only to specify all the packages descendants are located in (only non-abstract ones are then listed). E.g., the  weka.classifiers.Classifier  entry in the GOE file looks like this:   weka.classifiers.Classifier=\\\n  weka.classifiers.bayes.AODE,\\\n  weka.classifiers.bayes.BayesNet,\\\n  weka.classifiers.bayes.ComplementNaiveBayes,\\\n  weka.classifiers.bayes.NaiveBayes,\\\n  weka.classifiers.bayes.NaiveBayesMultinomial,\\\n  weka.classifiers.bayes.NaiveBayesSimple,\\\n  weka.classifiers.bayes.NaiveBayesUpdateable,\\\n  weka.classifiers.functions.LeastMedSq,\\\n  weka.classifiers.functions.LinearRegression,\\\n  weka.classifiers.functions.Logistic,\\\n  ...  The entry producing the same output for the classifiers in the GPC looks like this (7 lines instead of over 70!):   weka.classifiers.Classifier=\\\n  weka.classifiers.bayes,\\\n  weka.classifiers.functions,\\\n  weka.classifiers.lazy,\\\n  weka.classifiers.meta,\\\n  weka.classifiers.trees,\\\n  weka.classifiers.rules",
            "title": "File Structure"
        },
        {
            "location": "/generic_object_editor_book_version/#class-discovery",
            "text": "Unlike the  Class.forName(String)  method that grabs the first class it can find in the  CLASSPATH , and therefore fixes the location of the package it found the class in, the dynamic discovery examines the complete  CLASSPATH  you're starting the  Java Virtual Machine  (JVM) with. This means that you can have several parallel directories with the same WEKA package structure, e.g. the standard release of WEKA in one directory (  /distribution/weka.jar ) and another one with your own classes ( /development/weka/... ), and display all of the classifiers in the GUI. In case of a name conflict, i.e. two directories contain the same class, the first one that can be found is used. In a nutshell, your  java  call of the GUIChooser could look like this:   java -classpath \"/development:/distribution/weka.jar\" weka.gui.GUIChooser  Note:  Windows users have to replace the \":\" with \";\" and the forward slashes with backslashes.",
            "title": "Class Discovery"
        },
        {
            "location": "/generic_object_editor_book_version/#multiple-class-hierarchies",
            "text": "In case you're developing your own framework, but still want to use your classifiers within WEKA that wasn't possible so far. With the release  3.4.4  it is possible to have multiple class hierarchies being displayed in the GUI. If you've developed a modified version of J48, let's call it  MyJ48  and it's located in the package  dummy.classifiers  then you'll have to add this package to the classifiers list in the GPC file like this:   weka.classifiers.Classifier=\\\n  weka.classifiers.bayes,\\\n  weka.classifiers.functions,\\\n  weka.classifiers.lazy,\\\n  weka.classifiers.meta,\\\n  weka.classifiers.trees,\\\n  weka.classifiers.rules,\\\n  dummy.classifiers  Your  java  call for the GUIChooser might look like this:   java -classpath \"weka.jar:dummy.jar\" weka.gui.GUIChooser  Note:  Windows users have to replace the \":\" with \";\" and the forward slashes with backslashes.  Starting up the GUI you'll now have another root node in the tree view of the classifiers, called  root , and below it the  weka  and the  dummy  package hierarchy as you can see here:",
            "title": "Multiple Class Hierarchies"
        },
        {
            "location": "/generic_object_editor_book_version/#links",
            "text": "GenericObjectEditor (developer version)  CLASSPATH  Properties file  GenericPropertiesCreator.props",
            "title": "Links"
        },
        {
            "location": "/generic_object_editor_developer_version/",
            "text": "Introduction\n\n\nAs of version 3.4.4 it is possible for WEKA to \ndynamically discover classes at runtime\n (rather than using only those specified in the \nGenericObjectEditor.props\n (GOE) file). In some versions (3.5.8, 3.6.0) this facility was not enabled by default as it is a bit slower than the GOE file approach, and, furthermore, does not function in environments that do not have a CLASSPATH (e.g., application servers). Later versions (3.6.1, 3.7.0) enabled the dynamic discovery again, as WEKA can now distinguish between being a standalone Java application or being run in a non-CLASSPATH environment.\n\n\nIf you wish to enable or disable dynamic class discovery, the relevant file to edit is \nGenericPropertiesCreator.props\n (GPC). You can obtain this file either from the \nweka.jar\n or \nweka-src.jar\n archive. Open one of these files with an archive manager that can handle ZIP files (for Windows users, you can use \n7-Zip\n for this) and navigate to the \nweka/gui\n directory, where the GPC file is located. All that is required, is to change the \nUseDynamic\n property in this file from \nfalse\n to \ntrue\n (for enabling it) or the other way round (for disabling it). After changing the file, you just place it in your home directory. In order to find out the location of your home directory, do the following:\n\n\n\n\nLinux/Unix\n\n\nOpen a terminal\n\n\nrun the following command:\n\n\n\n\n\n\n\n\n\n\n\n\necho $HOME\n\n\n\n\n\n\n\n\nWindows\n\n\nOpen a command-primpt\n\n\nrun the following command:\n\n\n\n\n\n\n\n\n\n\n\n\necho %USERPROFILE%\n\n\n\n\n\n\nIf dynamic class discovery is too slow, e.g., due to an enormous CLASSPATH, you can generate a new \nGenericObjectEditor.props\n file and then turn dynamic class discovery off again. It is assumed that you already place the GPC file in your home directory (see steps above) and that the \nweka.jar\n jar archive with the WEKA classes is in your CLASSPATH (otherwise you have to add it to the \njava\n call using the \n-classpath\n option).\n\n\nFor generating the GOE file, execute the following steps:\n\n\n\n\n\n\ngenerate a new \nGenericObjectEditor.props\n file using the following command:\n\n\n\n\n\n\nLinux/Unix\n\n\n\n\njava weka.gui.GenericPropertiesCreator \\\n   $HOME/GenericPropertiesCreator.props \\\n   $HOME/GenericObjectEditor.props\n\n\n\n\n\n\n\n\nWindows (command must be in one line)\n\n\n\n\njava weka.gui.GenericPropertiesCreator \n   %USERPROFILE%\\GenericPropertiesCreator.props\n   %USERPROFILE%\\GenericObjectEditor.props\n\n\n\n\n\n\n\n\n\n\n\n\nedit the \nGenericPropertiesCreator.props\n file in your home directory and set \nUseDynamic\n to \nfalse\n.\n\n\n\n\n\n\nA limitation of the GOE prior to 3.4.4 was, that additional classifiers, filters, etc., had to fit into the same package structure as the already existing ones, i.e., all had to be located below \nweka\n. WEKA can now display multiple class hierarchies in the GUI, which makes adding new functionality quite easy as we will see later in an example (it is not restricted to classifiers only, but also works with all the other entries in the GPC file).\n\n\nFile Structure\n\n\nThe structure of the GOE so far was a key-value-pair, separated by an \nequals\n-sign. The \nvalue\n is a comma separated list of classes that are all derived from the superclass/superinterface \nkey\n. The GPC is slightly different, instead of declaring all the classes/interfaces one need only to specify all the packages descendants are located in (only non-abstract ones are then listed). E.g., the \nweka.classifiers.Classifier\n entry in the GOE file looks like this:\n\n\n weka.classifiers.Classifier=\\\n  weka.classifiers.bayes.AODE,\\\n  weka.classifiers.bayes.BayesNet,\\\n  weka.classifiers.bayes.ComplementNaiveBayes,\\\n  weka.classifiers.bayes.NaiveBayes,\\\n  weka.classifiers.bayes.NaiveBayesMultinomial,\\\n  weka.classifiers.bayes.NaiveBayesSimple,\\\n  weka.classifiers.bayes.NaiveBayesUpdateable,\\\n  weka.classifiers.functions.LeastMedSq,\\\n  weka.classifiers.functions.LinearRegression,\\\n  weka.classifiers.functions.Logistic,\\\n  ...\n\n\n\n\nThe entry producing the same output for the classifiers in the GPC looks like this (7 lines instead of over 70 in WEKA 3.4.4!):\n\n\n weka.classifiers.Classifier=\\\n  weka.classifiers.bayes,\\\n  weka.classifiers.functions,\\\n  weka.classifiers.lazy,\\\n  weka.classifiers.meta,\\\n  weka.classifiers.trees,\\\n  weka.classifiers.rules\n\n\n\n\nExclusion\n\n\nIt may not always be desired to list all the classes that can be found along the \nCLASSPATH\n. Sometimes, classes cannot be declared \nabstract\n but still shouldn't be listed in the GOE. For that reason one can list classes, interfaces, superclasses for certain packages to be excluded from display. This exclusion is done with the following file:\n\n\n weka/gui/GenericPropertiesCreator.excludes\n\n\n\n\nThe format of this \nproperties file\n is fairly easy:\n\n\n <key>=<prefix>:<class>[,<prefix>:<class>]\n\n\n\n\nWhere the \n<key>\n corresponds to a key in the \nGenericPropertiesCreator.props\n file and the \n<prefix>\n can be one of the following:\n\n\n\n\nS\n - \nSuperclass\n\n\nany class class derived from this will be excluded\n\n\n\n\n\n\nI\n - \nInterface\n\n\nany class implementing this interface will be excluded\n\n\n\n\n\n\nC\n - \nClass\n\n\nexactly this class will be excluded\n\n\n\n\n\n\n\n\nHere are a few examples:\n\n\n # exclude all ResultListeners that also implement the ResultProducer interface\n # (all ResultProducers do that!)\n weka.experiment.ResultListener=\\\n   I:weka.experiment.ResultProducer\n\n # exclude J48 and all SingleClassifierEnhancers\n weka.classifiers.Classifier=\\\n   C:weka.classifiers.trees.J48,\\\n   S:weka.classifiers.SingleClassifierEnhancer\n\n\n\n\nClass Discovery\n\n\nUnlike the \nClass.forName(String)\n method that grabs the first class it can find in the \nCLASSPATH\n, and therefore fixes the location of the package it found the class in, the dynamic discovery examines the complete \nCLASSPATH\n you're starting the \nJava Virtual Machine\n (JVM) with. This means that you can have several parallel directories with the same WEKA package structure, e.g. the standard release of WEKA in one directory (\n/distribution/weka.jar\n) and another one with your own classes (\n/development/weka/...\n), and display all of the classifiers in the GUI. In case of a name conflict, i.e. two directories contain the same class, the first one that can be found is used. In a nutshell, your \njava\n call of the GUIChooser could look like this:\n\n\n java -classpath \"/development:/distribution/weka.jar\" weka.gui.GUIChooser\n\n\n\n\nNote:\n Windows users have to replace the \":\" with \";\" and the forward slashes with backslashes.\n\n\nMultiple Class Hierarchies\n\n\nIn case you're developing your own framework, but still want to use your classifiers within WEKA that wasn't possible so far. With the release \n3.4.4\n it is possible to have multiple class hierarchies being displayed in the GUI. If you've developed a modified version of J48, let's call it \nMyJ48\n and it's located in the package \ndummy.classifiers\n then you'll have to add this package to the classifiers list in the GPC file like this:\n\n\n weka.classifiers.Classifier=\\\n  weka.classifiers.bayes,\\\n  weka.classifiers.functions,\\\n  weka.classifiers.lazy,\\\n  weka.classifiers.meta,\\\n  weka.classifiers.trees,\\\n  weka.classifiers.rules,\\\n  dummy.classifiers\n\n\n\n\nYour \njava\n call for the GUIChooser might look like this:\n\n\n java -classpath \"weka.jar:dummy.jar\" weka.gui.GUIChooser\n\n\n\n\nNote:\n Windows users have to replace the \":\" with \";\" and the forward slashes with backslashes.\n\n\nStarting up the GUI you'll now have another root node in the tree view of the classifiers, called \nroot\n, and below it the \nweka\n and the \ndummy\n package hierarchy as you can see here:\n\n\nCapabilities\n\n\nVersion \n3.5.3\n of Weka introduces the notion of \nCapabilities\n. Capabilities basically list what kind of data a certain object can handle, e.g., one classifier can handle numeric classes, but another cannot. In case a class supports capabilities the additional buttons \nFilter...\n and \nRemove filter\n will be available in the GOE. The \nFilter...\n button pops up a dialog which lists all available Capabilities:\n\n\nOne can then choose those capabilities an object, e.g., a classifier, should have. If one is looking for classification problem, then the \nNominal class\n Capability can be selected. On the other hand, if one needs a regression scheme, then the Capability \nNumeric class\n can be selected. This filtering mechanism makes the search for an appropriate learning scheme easier. After applying that filter, the tree with the objects will be displayed again and lists all objects that can handle all the selected Capabilities \nblack\n, the ones that cannot \nred\n (starting with 3.5.8: \n\nsilver\n) and the ones that might be able to handle them \nblue\n (e.g., meta classifiers which depend on their base classifier(s)).\n\n\nLinks\n\n\n\n\nGenericObjectEditor (book version)\n\n\nCLASSPATH\n\n\nProperties file\n\n\nGenericPropertiesCreator.props\n\n\nGenericPropertiesCreator.excludes",
            "title": " GenericObjectEditor (developer version)"
        },
        {
            "location": "/generic_object_editor_developer_version/#introduction",
            "text": "As of version 3.4.4 it is possible for WEKA to  dynamically discover classes at runtime  (rather than using only those specified in the  GenericObjectEditor.props  (GOE) file). In some versions (3.5.8, 3.6.0) this facility was not enabled by default as it is a bit slower than the GOE file approach, and, furthermore, does not function in environments that do not have a CLASSPATH (e.g., application servers). Later versions (3.6.1, 3.7.0) enabled the dynamic discovery again, as WEKA can now distinguish between being a standalone Java application or being run in a non-CLASSPATH environment.  If you wish to enable or disable dynamic class discovery, the relevant file to edit is  GenericPropertiesCreator.props  (GPC). You can obtain this file either from the  weka.jar  or  weka-src.jar  archive. Open one of these files with an archive manager that can handle ZIP files (for Windows users, you can use  7-Zip  for this) and navigate to the  weka/gui  directory, where the GPC file is located. All that is required, is to change the  UseDynamic  property in this file from  false  to  true  (for enabling it) or the other way round (for disabling it). After changing the file, you just place it in your home directory. In order to find out the location of your home directory, do the following:   Linux/Unix  Open a terminal  run the following command:       echo $HOME     Windows  Open a command-primpt  run the following command:       echo %USERPROFILE%    If dynamic class discovery is too slow, e.g., due to an enormous CLASSPATH, you can generate a new  GenericObjectEditor.props  file and then turn dynamic class discovery off again. It is assumed that you already place the GPC file in your home directory (see steps above) and that the  weka.jar  jar archive with the WEKA classes is in your CLASSPATH (otherwise you have to add it to the  java  call using the  -classpath  option).  For generating the GOE file, execute the following steps:    generate a new  GenericObjectEditor.props  file using the following command:    Linux/Unix   java weka.gui.GenericPropertiesCreator \\\n   $HOME/GenericPropertiesCreator.props \\\n   $HOME/GenericObjectEditor.props     Windows (command must be in one line)   java weka.gui.GenericPropertiesCreator \n   %USERPROFILE%\\GenericPropertiesCreator.props\n   %USERPROFILE%\\GenericObjectEditor.props       edit the  GenericPropertiesCreator.props  file in your home directory and set  UseDynamic  to  false .    A limitation of the GOE prior to 3.4.4 was, that additional classifiers, filters, etc., had to fit into the same package structure as the already existing ones, i.e., all had to be located below  weka . WEKA can now display multiple class hierarchies in the GUI, which makes adding new functionality quite easy as we will see later in an example (it is not restricted to classifiers only, but also works with all the other entries in the GPC file).",
            "title": "Introduction"
        },
        {
            "location": "/generic_object_editor_developer_version/#file-structure",
            "text": "The structure of the GOE so far was a key-value-pair, separated by an  equals -sign. The  value  is a comma separated list of classes that are all derived from the superclass/superinterface  key . The GPC is slightly different, instead of declaring all the classes/interfaces one need only to specify all the packages descendants are located in (only non-abstract ones are then listed). E.g., the  weka.classifiers.Classifier  entry in the GOE file looks like this:   weka.classifiers.Classifier=\\\n  weka.classifiers.bayes.AODE,\\\n  weka.classifiers.bayes.BayesNet,\\\n  weka.classifiers.bayes.ComplementNaiveBayes,\\\n  weka.classifiers.bayes.NaiveBayes,\\\n  weka.classifiers.bayes.NaiveBayesMultinomial,\\\n  weka.classifiers.bayes.NaiveBayesSimple,\\\n  weka.classifiers.bayes.NaiveBayesUpdateable,\\\n  weka.classifiers.functions.LeastMedSq,\\\n  weka.classifiers.functions.LinearRegression,\\\n  weka.classifiers.functions.Logistic,\\\n  ...  The entry producing the same output for the classifiers in the GPC looks like this (7 lines instead of over 70 in WEKA 3.4.4!):   weka.classifiers.Classifier=\\\n  weka.classifiers.bayes,\\\n  weka.classifiers.functions,\\\n  weka.classifiers.lazy,\\\n  weka.classifiers.meta,\\\n  weka.classifiers.trees,\\\n  weka.classifiers.rules",
            "title": "File Structure"
        },
        {
            "location": "/generic_object_editor_developer_version/#exclusion",
            "text": "It may not always be desired to list all the classes that can be found along the  CLASSPATH . Sometimes, classes cannot be declared  abstract  but still shouldn't be listed in the GOE. For that reason one can list classes, interfaces, superclasses for certain packages to be excluded from display. This exclusion is done with the following file:   weka/gui/GenericPropertiesCreator.excludes  The format of this  properties file  is fairly easy:   <key>=<prefix>:<class>[,<prefix>:<class>]  Where the  <key>  corresponds to a key in the  GenericPropertiesCreator.props  file and the  <prefix>  can be one of the following:   S  -  Superclass  any class class derived from this will be excluded    I  -  Interface  any class implementing this interface will be excluded    C  -  Class  exactly this class will be excluded     Here are a few examples:   # exclude all ResultListeners that also implement the ResultProducer interface\n # (all ResultProducers do that!)\n weka.experiment.ResultListener=\\\n   I:weka.experiment.ResultProducer\n\n # exclude J48 and all SingleClassifierEnhancers\n weka.classifiers.Classifier=\\\n   C:weka.classifiers.trees.J48,\\\n   S:weka.classifiers.SingleClassifierEnhancer",
            "title": "Exclusion"
        },
        {
            "location": "/generic_object_editor_developer_version/#class-discovery",
            "text": "Unlike the  Class.forName(String)  method that grabs the first class it can find in the  CLASSPATH , and therefore fixes the location of the package it found the class in, the dynamic discovery examines the complete  CLASSPATH  you're starting the  Java Virtual Machine  (JVM) with. This means that you can have several parallel directories with the same WEKA package structure, e.g. the standard release of WEKA in one directory ( /distribution/weka.jar ) and another one with your own classes ( /development/weka/... ), and display all of the classifiers in the GUI. In case of a name conflict, i.e. two directories contain the same class, the first one that can be found is used. In a nutshell, your  java  call of the GUIChooser could look like this:   java -classpath \"/development:/distribution/weka.jar\" weka.gui.GUIChooser  Note:  Windows users have to replace the \":\" with \";\" and the forward slashes with backslashes.",
            "title": "Class Discovery"
        },
        {
            "location": "/generic_object_editor_developer_version/#multiple-class-hierarchies",
            "text": "In case you're developing your own framework, but still want to use your classifiers within WEKA that wasn't possible so far. With the release  3.4.4  it is possible to have multiple class hierarchies being displayed in the GUI. If you've developed a modified version of J48, let's call it  MyJ48  and it's located in the package  dummy.classifiers  then you'll have to add this package to the classifiers list in the GPC file like this:   weka.classifiers.Classifier=\\\n  weka.classifiers.bayes,\\\n  weka.classifiers.functions,\\\n  weka.classifiers.lazy,\\\n  weka.classifiers.meta,\\\n  weka.classifiers.trees,\\\n  weka.classifiers.rules,\\\n  dummy.classifiers  Your  java  call for the GUIChooser might look like this:   java -classpath \"weka.jar:dummy.jar\" weka.gui.GUIChooser  Note:  Windows users have to replace the \":\" with \";\" and the forward slashes with backslashes.  Starting up the GUI you'll now have another root node in the tree view of the classifiers, called  root , and below it the  weka  and the  dummy  package hierarchy as you can see here:",
            "title": "Multiple Class Hierarchies"
        },
        {
            "location": "/generic_object_editor_developer_version/#capabilities",
            "text": "Version  3.5.3  of Weka introduces the notion of  Capabilities . Capabilities basically list what kind of data a certain object can handle, e.g., one classifier can handle numeric classes, but another cannot. In case a class supports capabilities the additional buttons  Filter...  and  Remove filter  will be available in the GOE. The  Filter...  button pops up a dialog which lists all available Capabilities:  One can then choose those capabilities an object, e.g., a classifier, should have. If one is looking for classification problem, then the  Nominal class  Capability can be selected. On the other hand, if one needs a regression scheme, then the Capability  Numeric class  can be selected. This filtering mechanism makes the search for an appropriate learning scheme easier. After applying that filter, the tree with the objects will be displayed again and lists all objects that can handle all the selected Capabilities  black , the ones that cannot  red  (starting with 3.5.8:  silver ) and the ones that might be able to handle them  blue  (e.g., meta classifiers which depend on their base classifier(s)).",
            "title": "Capabilities"
        },
        {
            "location": "/generic_object_editor_developer_version/#links",
            "text": "GenericObjectEditor (book version)  CLASSPATH  Properties file  GenericPropertiesCreator.props  GenericPropertiesCreator.excludes",
            "title": "Links"
        },
        {
            "location": "/gpgpu/",
            "text": "See :\n\n\n\n\nthis post\n\n\n\n\nI am looking for input from WEKA users. Please leave a comment on the website and I'll respond back. The input/help I need from WEKA users is as follows:\n\n\n\n\nI need to know what algorithms would be desired to have optimized first. For now, I'm working on Bayes (for starters).\n\n\nI need willing volunteers to use the revised code that I create (I am not making any changes to the algorithms just diverting the mathematical calculations from the CPU to the GPU to increase speed) and let me know of any performance changes observed.",
            "title": " GPGPU"
        },
        {
            "location": "/how_do_i_modify_the_classpath/",
            "text": "See the article \nCLASSPATH\n and check out \nthis section\n for changing the environment variable. This article explains how to add a MySQL jar to the variable.\n\n\nWith version 3.5.4 or later you can also just use the \nRunWEKA.ini\n file to modify your CLASSPATH.",
            "title": " How do I modify the CLASSPATH?"
        },
        {
            "location": "/how_do_i_use_the_associator_generalized_sequential_patterns/",
            "text": "The article \nGeneralizedSequentialPatterns\n contains more information on this associator.",
            "title": " How do I use the associator GeneralizedSequentialPatterns?"
        },
        {
            "location": "/how_to_run_weka_schemes_from_commandline/",
            "text": "It is quite often the case that one has to run a classifier, filter, attribute selection, etc. from commandline, leaving the comfort of the GUI (most likely the Explorer). Due to the vast amount of options the Weka schemes offer, it can be quite tedious setting up a scheme on the commandline.\n\n\nIn the following, a few different approaches are listed that can be used for running a scheme from the commandline:\n\n\n\n\n\n\nHardcore approach\n (works for all versions of Weka)\n\n\n\n\none just uses the \n-h\n option to display the commandline help with all available options and chooses the ones that apply, e.g.:\n\n\n\n\njava weka.classifiers.functions.SMO -h\n\nThe drawback of this method is, that one has to take care of escaping nested quotes oneself. As soon as one has to use meta-classifiers, this gets real messy. An introduction to the commandline use can be found in the \nPrimer\n.\n\n\n\n\n\n\n\n\n\n\ncopy/paste approach\n\n\n\n\nWith this approach, one doesn't have to worry about correct nesting, since Weka takes care of that, returning correctly nested and escaped options.\n\n\n\n\n\n\nSince version 3.5.3, one can right-click (or \n<Alt>+<Shift>\n left-click for Mac users) any \nGenericObjectEditor\n panel and select the \nCopy configuration to clipboard\n option to copy the currently shown configuration to the clipboard and then just paste it into the commandline. One only needs to add the appropriate \njava\n call and other general options, like datasets, class index, etc.\n\n\nAnother copy/paste approach is copying the configurations from the \nExplorer\n log, which is available since version 3.5.4. Every action in the Explorer, like applying a filter, running a classifier, attribute selection, etc. outputs the command to the log as well. This makes is fairly easy copying it to the clipboard and using it in the console, only the \njava\n call and other general options need to be added.\n\n\n\n\nSee also\n\n\n\n\nPrimer\n - introduction to Weka from the commandline\n\n\nCLASSPATH\n - how to load all necessary libraries or welcome to the \nJAR hell\n\n\nCommand redirection\n - shows how to redirect output in files",
            "title": " How to run Weka schemes from commandline"
        },
        {
            "location": "/how_to_run_weka_schemes_from_commandline/#see-also",
            "text": "Primer  - introduction to Weka from the commandline  CLASSPATH  - how to load all necessary libraries or welcome to the  JAR hell  Command redirection  - shows how to redirect output in files",
            "title": "See also"
        },
        {
            "location": "/ikvm_with_weka_tutorial/",
            "text": "This tutorial walks you through the creation of a \nMicrosoft C#\n program that uses \nWeka\n, and some \nJava\n API classes, via \nIKVM\n. The process will be similar for other \n.NET\n languages.\n\n\nSet up / Installation\n\n\nYou will first need to install IKVM, which can be found \nhere\n. You will also need a C# compiler/VM - \nMono\n is an excellent open source solution for both linux and windows, or you could just use Microsoft Visual Studio .NET.\n\n\nConversion from Java to a .NET dll\n\n\nWith that out of the way, the first thing you will want to do is to convert the Weka .jar file into a .NET dll. To do this, we will use \nikvmc\n, which is the IKVM static compiler.\nOn the console, go to the directory which contains weka.jar, and type:\n\n\n> ikvmc -target:library weka.jar\n\n\n\n\nThe \n-target:library\n call causes \nikvmc\n to create a .dll library instead of an executable.\n\n\nNote that the IKVM tutorial tells you that you should add \n-reference:/usr/lib/IKVM.GNU.Classpath.dll\n(or appropriate path) to the above command, it tells IKVM where to find the GNU Classpath library. \nHowever, \nIKVM.GNU.Classpath.dll\n Is no longer included in the download package, and is from very old versions of IKVM. When Sun open sources Java, it got replaced by the IKVM.OpenJDK.*.dll files.\n\n\nYou should now have a file called \"weka.dll\", which is a .NET version of the entire weka API. That's exactly what we want!\n\n\nUse the dll in a .NET application\n\n\nTo try it out, lets use a small C# program that I wrote. The program simply runs the J48 classifier on the Iris dataset with a 66% test/data split, and prints out the correctness percentage. It also uses a few Java classes, and is already about 95% legal Java code.\n\n\nThe code is here:\n\n\n//start of file Main.cs\n using System;\n\n class MainClass\n {\n     public static void Main(string[] args)\n     {\n         Console.WriteLine(\"Hello Java, from C#!\");\n         classifyTest();\n      }\n\n      const int percentSplit = 66;\n      public static void classifyTest()\n      {\n         try\n         {\n             weka.core.Instances insts = new weka.core.Instances(new java.io.FileReader(\"iris.arff\"));\n             insts.setClassIndex(insts.numAttributes() - 1);\n\n             weka.classifiers.Classifier cl = new weka.classifiers.trees.J48();\n             Console.WriteLine(\"Performing \" + percentSplit + \"% split evaluation.\");\n\n             //randomize the order of the instances in the dataset.\n                         weka.filters.Filter myRandom = new weka.filters.unsupervised.instance.Randomize();\n             myRandom.setInputFormat(insts);\n                         insts = weka.filters.Filter.useFilter(insts, myRandom);\n\n             int trainSize = insts.numInstances() * percentSplit / 100;\n             int testSize = insts.numInstances() - trainSize;\n             weka.core.Instances train = new weka.core.Instances(insts, 0, trainSize);\n\n             cl.buildClassifier(train);\n             int numCorrect = 0;\n             for (int i = trainSize; i < insts.numInstances(); i++)\n             {\n                 weka.core.Instance currentInst = insts.instance(i);\n                 double predictedClass = cl.classifyInstance(currentInst);\n                 if (predictedClass == insts.instance(i).classValue())\n                     numCorrect++;\n             }\n             Console.WriteLine(numCorrect + \" out of \" + testSize + \" correct (\" +\n                        (double)((double)numCorrect / (double)testSize * 100.0) + \"%)\");\n         }\n         catch (java.lang.Exception ex)\n         {\n             ex.printStackTrace();\n         }\n     }\n\n }\n //end of file Main.cs\n\n\n\n\nCompile and run it\n\n\nNow we just need to compile it. If you are using MonoDevelop or Visual Studio, you will need to add references to weka.dll, and all of the IKVM.OpenJDK.*.dll files, and lastly IKVM.Runtime.dll into your project. Otherwise, on the command line, you can type:\n\n\nNOTE: \nreplace \nIKVM.OpenJDK.\n.dll with the remaining IKVM.openJDK files.\n\n\n>mcs Main.cs -r:weka.dll,IKVM.Runtime.dll,IKVM.OpenJDK.core.dll, IKVM.OpenJDK.*.dll\n\n\n\n\nto run the Mono C# compiler with references to the appropriate dlls (according to the Mono documentation, the command line arguments for Visual Studio are the same).\n\n\nAnd there you go! Now you can run the program. But make sure that the Iris.arff dataset is in the same directory first.\n\n\nFor mono:\n\n\n>mono Main.exe\n\n\n\n\nor if you are using visual studio, just:\n\n\n>Main.exe\n\n\n\n\nHopefully you will get as output:\n\n\n Hello Java, from C#!\n Performing 66% split evaluation.\n 49 out of 51 correct (96.078431372549%)\n\n\n\n\nAnd there you have it. Now we have a working program that uses Weka classes, and some classes from the standard Java API, in a C# program for the .NET framework.\n\n\nLinks\n\n\n\n\nAn Introduction to IKVM\n\n\nIKVM.NET\n\n\nMono\n\n\nThe official IKVM tutorial\n\n\nUse Weka with the Microsoft .NET Framework",
            "title": " IKVM with Weka tutorial"
        },
        {
            "location": "/ikvm_with_weka_tutorial/#set-up-installation",
            "text": "You will first need to install IKVM, which can be found  here . You will also need a C# compiler/VM -  Mono  is an excellent open source solution for both linux and windows, or you could just use Microsoft Visual Studio .NET.",
            "title": "Set up / Installation"
        },
        {
            "location": "/ikvm_with_weka_tutorial/#conversion-from-java-to-a-net-dll",
            "text": "With that out of the way, the first thing you will want to do is to convert the Weka .jar file into a .NET dll. To do this, we will use  ikvmc , which is the IKVM static compiler.\nOn the console, go to the directory which contains weka.jar, and type:  > ikvmc -target:library weka.jar  The  -target:library  call causes  ikvmc  to create a .dll library instead of an executable.  Note that the IKVM tutorial tells you that you should add  -reference:/usr/lib/IKVM.GNU.Classpath.dll (or appropriate path) to the above command, it tells IKVM where to find the GNU Classpath library.  However,  IKVM.GNU.Classpath.dll  Is no longer included in the download package, and is from very old versions of IKVM. When Sun open sources Java, it got replaced by the IKVM.OpenJDK.*.dll files.  You should now have a file called \"weka.dll\", which is a .NET version of the entire weka API. That's exactly what we want!",
            "title": "Conversion from Java to a .NET dll"
        },
        {
            "location": "/ikvm_with_weka_tutorial/#use-the-dll-in-a-net-application",
            "text": "To try it out, lets use a small C# program that I wrote. The program simply runs the J48 classifier on the Iris dataset with a 66% test/data split, and prints out the correctness percentage. It also uses a few Java classes, and is already about 95% legal Java code.  The code is here:  //start of file Main.cs\n using System;\n\n class MainClass\n {\n     public static void Main(string[] args)\n     {\n         Console.WriteLine(\"Hello Java, from C#!\");\n         classifyTest();\n      }\n\n      const int percentSplit = 66;\n      public static void classifyTest()\n      {\n         try\n         {\n             weka.core.Instances insts = new weka.core.Instances(new java.io.FileReader(\"iris.arff\"));\n             insts.setClassIndex(insts.numAttributes() - 1);\n\n             weka.classifiers.Classifier cl = new weka.classifiers.trees.J48();\n             Console.WriteLine(\"Performing \" + percentSplit + \"% split evaluation.\");\n\n             //randomize the order of the instances in the dataset.\n                         weka.filters.Filter myRandom = new weka.filters.unsupervised.instance.Randomize();\n             myRandom.setInputFormat(insts);\n                         insts = weka.filters.Filter.useFilter(insts, myRandom);\n\n             int trainSize = insts.numInstances() * percentSplit / 100;\n             int testSize = insts.numInstances() - trainSize;\n             weka.core.Instances train = new weka.core.Instances(insts, 0, trainSize);\n\n             cl.buildClassifier(train);\n             int numCorrect = 0;\n             for (int i = trainSize; i < insts.numInstances(); i++)\n             {\n                 weka.core.Instance currentInst = insts.instance(i);\n                 double predictedClass = cl.classifyInstance(currentInst);\n                 if (predictedClass == insts.instance(i).classValue())\n                     numCorrect++;\n             }\n             Console.WriteLine(numCorrect + \" out of \" + testSize + \" correct (\" +\n                        (double)((double)numCorrect / (double)testSize * 100.0) + \"%)\");\n         }\n         catch (java.lang.Exception ex)\n         {\n             ex.printStackTrace();\n         }\n     }\n\n }\n //end of file Main.cs",
            "title": "Use the dll in a .NET application"
        },
        {
            "location": "/ikvm_with_weka_tutorial/#compile-and-run-it",
            "text": "Now we just need to compile it. If you are using MonoDevelop or Visual Studio, you will need to add references to weka.dll, and all of the IKVM.OpenJDK.*.dll files, and lastly IKVM.Runtime.dll into your project. Otherwise, on the command line, you can type:  NOTE: \nreplace  IKVM.OpenJDK. .dll with the remaining IKVM.openJDK files.  >mcs Main.cs -r:weka.dll,IKVM.Runtime.dll,IKVM.OpenJDK.core.dll, IKVM.OpenJDK.*.dll  to run the Mono C# compiler with references to the appropriate dlls (according to the Mono documentation, the command line arguments for Visual Studio are the same).  And there you go! Now you can run the program. But make sure that the Iris.arff dataset is in the same directory first.  For mono:  >mono Main.exe  or if you are using visual studio, just:  >Main.exe  Hopefully you will get as output:   Hello Java, from C#!\n Performing 66% split evaluation.\n 49 out of 51 correct (96.078431372549%)  And there you have it. Now we have a working program that uses Weka classes, and some classes from the standard Java API, in a C# program for the .NET framework.",
            "title": "Compile and run it"
        },
        {
            "location": "/ikvm_with_weka_tutorial/#links",
            "text": "An Introduction to IKVM  IKVM.NET  Mono  The official IKVM tutorial  Use Weka with the Microsoft .NET Framework",
            "title": "Links"
        },
        {
            "location": "/j48_weighter_patch/",
            "text": "Description\n\n\nJ48-Weighter patch: \nModification of J48 for Weighted Data.\n\n\nReference\n\n\n-none-\n\n\nPackage\n\n\nPatches to:\n\n\n\n\nweka.classifiers.trees.j48\n\n\nweka.core\n\n\nweka.filters.unsupervised.attribute\n\n\n\n\nDownload\n\n\nPatch for Weka 3.4.5: \n\nj48-weighter.patch\n\n\nAdditional Information\n\n\nThis patch addresses two separate but related issues:\n\n\n\n\nThe proposed filter \"Weighter\" allows one to specify a numeric attribute to be used as an instance weight.\n\n\nAs mentioned on Wekalist, tests using weighted sample-survey data indicated possible problems in the J48 decision tree algorithm: \n\nwekalist/2004-December/003135\n\n\n\n\nThe Weighter filter\n\n\nWeighter is a general-purpose filter independent of J48 or other\nclassifiers, but to preserve the weight assignment it initially had to\nbe run under FilteredClassifier.  To make weights persistent via .arff\nfiles, some changes were made in Instances and Instance, while retaining\ncompatibility with the existing \nARFF\n format.\n\n\nBriefly, if Weighter is applied to an attribute, e.g. \"fnlwgt\" in the\n\"adult\" dataset from the UCI repository, that attribute is removed and\nits value is used as instance weight.  Upon Save, the weight is appended\nto each instance under the attribute name \"::weight::fnlwgt\"; reading\nthe .arff file inverts the Save process, transparent to the user.\n\n\nRepeated application of Weighter multiplies the weight and extends its\nname.  The special case of invoking Weighter without an attribute\nargument restores the unweighted dataset, with an appended attribute\nnamed as above.\n\n\nNote:\n the \nXRFF\n format, introduced in 3.5.4, stores instance and attribute weights, as opposed to the default \nARFF\n format.\n\n\nJ48 with instance weights\n\n\nThe simple rescaling inserted in weka.classifiers.trees.j48.Stats is\nintended to:\n\n\n\n\nuse the correct sample size in the normal approximation to the binomial,\n\n\nmake the scale of the .5 continuity correction consistent with the data,\n\n\nbase the minimum-leaf-count option (-M) on unweighted counts.\n\n\n\n\nThese changes make pruning more effective with weighted data, and help\nto reduce apparent overfitting.  This should be the case whether the\nweights reflect missing value imputation (as is common in Weka), or\nsurvey-sampling probabilities (e.g. \"fnlwgt\" in the UCI \"adult\"\nsample).\n\n\nThe modification to j48.Stats would not have worked on its own.  In\nparticular, j48.Distribution had been written to maintain one set of\ncounts only.  To work on weighted data statistical algorithms often\nrequire both weighted and unweighted counts.\n\n\nA few other minor modifications were introduced to change the way \"-M\"\nworks.  One effect is that, for this purpose, instances with missing\nx-values are no longer counted; they are considered missing.",
            "title": " J48-Weighter patch"
        },
        {
            "location": "/j48_weighter_patch/#description",
            "text": "J48-Weighter patch: \nModification of J48 for Weighted Data.",
            "title": "Description"
        },
        {
            "location": "/j48_weighter_patch/#reference",
            "text": "-none-",
            "title": "Reference"
        },
        {
            "location": "/j48_weighter_patch/#package",
            "text": "Patches to:   weka.classifiers.trees.j48  weka.core  weka.filters.unsupervised.attribute",
            "title": "Package"
        },
        {
            "location": "/j48_weighter_patch/#download",
            "text": "Patch for Weka 3.4.5:  j48-weighter.patch",
            "title": "Download"
        },
        {
            "location": "/j48_weighter_patch/#additional-information",
            "text": "This patch addresses two separate but related issues:   The proposed filter \"Weighter\" allows one to specify a numeric attribute to be used as an instance weight.  As mentioned on Wekalist, tests using weighted sample-survey data indicated possible problems in the J48 decision tree algorithm:  wekalist/2004-December/003135",
            "title": "Additional Information"
        },
        {
            "location": "/j48_weighter_patch/#the-weighter-filter",
            "text": "Weighter is a general-purpose filter independent of J48 or other\nclassifiers, but to preserve the weight assignment it initially had to\nbe run under FilteredClassifier.  To make weights persistent via .arff\nfiles, some changes were made in Instances and Instance, while retaining\ncompatibility with the existing  ARFF  format.  Briefly, if Weighter is applied to an attribute, e.g. \"fnlwgt\" in the\n\"adult\" dataset from the UCI repository, that attribute is removed and\nits value is used as instance weight.  Upon Save, the weight is appended\nto each instance under the attribute name \"::weight::fnlwgt\"; reading\nthe .arff file inverts the Save process, transparent to the user.  Repeated application of Weighter multiplies the weight and extends its\nname.  The special case of invoking Weighter without an attribute\nargument restores the unweighted dataset, with an appended attribute\nnamed as above.  Note:  the  XRFF  format, introduced in 3.5.4, stores instance and attribute weights, as opposed to the default  ARFF  format.",
            "title": "The Weighter filter"
        },
        {
            "location": "/j48_weighter_patch/#j48-with-instance-weights",
            "text": "The simple rescaling inserted in weka.classifiers.trees.j48.Stats is\nintended to:   use the correct sample size in the normal approximation to the binomial,  make the scale of the .5 continuity correction consistent with the data,  base the minimum-leaf-count option (-M) on unweighted counts.   These changes make pruning more effective with weighted data, and help\nto reduce apparent overfitting.  This should be the case whether the\nweights reflect missing value imputation (as is common in Weka), or\nsurvey-sampling probabilities (e.g. \"fnlwgt\" in the UCI \"adult\"\nsample).  The modification to j48.Stats would not have worked on its own.  In\nparticular, j48.Distribution had been written to maintain one set of\ncounts only.  To work on weighted data statistical algorithms often\nrequire both weighted and unweighted counts.  A few other minor modifications were introduced to change the way \"-M\"\nworks.  One effect is that, for this purpose, instances with missing\nx-values are no longer counted; they are considered missing.",
            "title": "J48 with instance weights"
        },
        {
            "location": "/java_virtual_machine/",
            "text": "The \nJava virtual machine\n (JVM) is the platform dependent \ninterpreter\n of the \nJava\n \nbytecode\n (i.e., the \nclasses\n). It translates the bytecode into machine specific instructions.\n\n\nAmount of available memory\n\n\nIf you start the virtual machine without any parameters it takes default values for stack and heap. In case you run into \nOutOfMemory\n exceptions, try to start your JVM with a bigger maximum heap size. (However, there's a limit, depending on your OS. See the \n32-Bit\n and \n64-Bit\n sections.)\n\n\n32-bit\n\n\nWith a 32-Bit machine you can address at most 4GB of \nvirtual memory\n. Different operating systems divide up the memory further into //system/kernel\n and \nuser space*.\n\n\nFrom experience, you can achieve the following maximum sizes for the \nheap\n on Windows and Linux:\n\n\n\n\nWindows: \n1.4GB\n\n\nLinux: \n1.7GB\n\n\n\n\n64-bit\n\n\nLarger heap sizes are available when using 64-bit Java in a conjunction with a 64-bit operating system.\n\n\nThere is more information available \nhere\n.",
            "title": " Java Virtual Machine"
        },
        {
            "location": "/java_virtual_machine/#amount-of-available-memory",
            "text": "If you start the virtual machine without any parameters it takes default values for stack and heap. In case you run into  OutOfMemory  exceptions, try to start your JVM with a bigger maximum heap size. (However, there's a limit, depending on your OS. See the  32-Bit  and  64-Bit  sections.)",
            "title": "Amount of available memory"
        },
        {
            "location": "/java_virtual_machine/#32-bit",
            "text": "With a 32-Bit machine you can address at most 4GB of  virtual memory . Different operating systems divide up the memory further into //system/kernel  and  user space*.  From experience, you can achieve the following maximum sizes for the  heap  on Windows and Linux:   Windows: \n1.4GB  Linux: \n1.7GB",
            "title": "32-bit"
        },
        {
            "location": "/java_virtual_machine/#64-bit",
            "text": "Larger heap sizes are available when using 64-bit Java in a conjunction with a 64-bit operating system.  There is more information available  here .",
            "title": "64-bit"
        },
        {
            "location": "/jvm/",
            "text": "see \nJava Virtual Machine",
            "title": " JVM"
        },
        {
            "location": "/learning_curves/",
            "text": "The \nAdvanced mode\n of the Experimenter can be used to generated learning curves for classifiers. These approaches can be setup in the \nSimple mode\n as well, but it is more cumbersome than in the advanced mode.\n\n\nNumber of instances\n\n\nFor varying the number of instances a classifier is trained on, we use the \nFilteredClassifier\n classifier (package \nweka.classifiers.meta\n) in conjunction with the \nRemovePercentage\n filter (package \nweka.filters.unsupervised.instance\n) and J48 as base classifier (package \nweka.classifiers.trees\n):\n\n\n\n\nstart the Experimenter (class \nweka.gui.experiment.Experimenter\n)\n\n\nselect the configuration mode \nAdvanced\n in the \nSetup\n panel\n\n\nchoose as \nDestination\n either an ARFF file (= \nInstancesResultListener\n) or a \ndatabase\n (= \nDatabaseResultListener\n) and configure the listener to your needs\n\n\nchoose as \nResult generator\n the \nCrossValidationResultProducer\n (or leave the \nRandomSplitResultProducer\n)\n\n\nopen the options dialog of the \nCrossValidationResultProducer\n by left-clicking on the edit field\n\n\nin case of regression datasets, choose the \nRegressionSplitEvaluator\n instead of the \nClassifierSplitEvaluator\n (the latter is used for classification problems)\n\n\nopen the options dialog for the \nsplitEvaluator\n by left-clicking on the edit field\n\n\nchoose\n the classifier that you want to analyze and setup it's parameters, in our case this is \nFilteredClassifier\n with \nJ48\n as base classifier and \nRemovePercentage\n as filter\n\n\nclose all dialogs again (accepting them with OK)\n\n\nset the \nGenerator properties\n to \nenabled\n\n\nchoose as property \npercentage\n and click on \nSelect\n:\n\n\n\n\n splitEvaluator -> classifier -> filter -> percentage\n\n\n\n\n\n\nnow you can add all the percentages that you want to test, e.g. (NB: this is the percentage being \nremoved\n!):\n\n\n\n\n 90, 80, 70, 60, 50, 40, 30, 20, 10\n\n\n\n\n\n\nadd the datasets you want to generate the learning curve for\n\n\nsave the experiment\n\n\ngo to the \nRun\n panel and start the experiment\n\n\nafter the experiment has finished, select the \nAnalyse\n panel and perform your analysis on the results\n\n\n\n\nClassifier parameter\n\n\nThis example shows how to generate a learning curve that does not vary on the number of instances, but on a specific classifier parameter, e.g., the \nconfidenceFactor\n (= commandline option \n-C\n) of \nJ48\n. \n\n\n\n\nstart the Experimenter (class \nweka.gui.experiment.Experimenter\n)\n\n\nselect the configuration mode \nAdvanced\n in the \nSetup\n panel\n\n\nchoose as \nDestination\n either an ARFF file (= \nInstancesResultListener\n) or a \ndatabase\n (= \nDatabaseResultListener\n) and configure the listener to your needs\n\n\nchoose as \nResult generator\n the \nCrossValidationResultProducer\n (or leave the \nRandomSplitResultProducer\n)\n\n\nopen the options dialog of the \nCrossValidationResultProducer\n by left-clicking on the edit field\n\n\nin case of regression datasets, choose the \nRegressionSplitEvaluator\n instead of the \nClassifierSplitEvaluator\n (the latter is used for classification problems)\n\n\nopen the options dialog for the \nsplitEvaluator\n by left-clicking on the edit field\n\n\nchoose\n the classifier that you want to analyze and setup it's parameters, in our case this is \nJ48\n\n\nclose all dialogs again (accepting them with OK)\n\n\nset the \nGenerator properties\n to \nenabled\n\n\nchoose as property \npercentage\n and click on \nSelect\n:\n\n\n\n\n splitEvaluator -> classifier -> confidenceFactor\n\n\n\n\n\n\nnow you can add all the factors that you want to test, e.g.:\n\n\n\n\n 0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50\n\n\n\n\n\n\nadd the datasets you want to generate the learning curve for\n\n\nsave the experiment\n\n\ngo to the \nRun\n panel and start the experiment\n\n\nafter the experiment has finished, select the \nAnalyse\n panel and perform your analysis on the results\n\n\n\n\nSee also\n\n\n\n\nDatabases",
            "title": " Learning curves"
        },
        {
            "location": "/learning_curves/#number-of-instances",
            "text": "For varying the number of instances a classifier is trained on, we use the  FilteredClassifier  classifier (package  weka.classifiers.meta ) in conjunction with the  RemovePercentage  filter (package  weka.filters.unsupervised.instance ) and J48 as base classifier (package  weka.classifiers.trees ):   start the Experimenter (class  weka.gui.experiment.Experimenter )  select the configuration mode  Advanced  in the  Setup  panel  choose as  Destination  either an ARFF file (=  InstancesResultListener ) or a  database  (=  DatabaseResultListener ) and configure the listener to your needs  choose as  Result generator  the  CrossValidationResultProducer  (or leave the  RandomSplitResultProducer )  open the options dialog of the  CrossValidationResultProducer  by left-clicking on the edit field  in case of regression datasets, choose the  RegressionSplitEvaluator  instead of the  ClassifierSplitEvaluator  (the latter is used for classification problems)  open the options dialog for the  splitEvaluator  by left-clicking on the edit field  choose  the classifier that you want to analyze and setup it's parameters, in our case this is  FilteredClassifier  with  J48  as base classifier and  RemovePercentage  as filter  close all dialogs again (accepting them with OK)  set the  Generator properties  to  enabled  choose as property  percentage  and click on  Select :    splitEvaluator -> classifier -> filter -> percentage   now you can add all the percentages that you want to test, e.g. (NB: this is the percentage being  removed !):    90, 80, 70, 60, 50, 40, 30, 20, 10   add the datasets you want to generate the learning curve for  save the experiment  go to the  Run  panel and start the experiment  after the experiment has finished, select the  Analyse  panel and perform your analysis on the results",
            "title": "Number of instances"
        },
        {
            "location": "/learning_curves/#classifier-parameter",
            "text": "This example shows how to generate a learning curve that does not vary on the number of instances, but on a specific classifier parameter, e.g., the  confidenceFactor  (= commandline option  -C ) of  J48 .    start the Experimenter (class  weka.gui.experiment.Experimenter )  select the configuration mode  Advanced  in the  Setup  panel  choose as  Destination  either an ARFF file (=  InstancesResultListener ) or a  database  (=  DatabaseResultListener ) and configure the listener to your needs  choose as  Result generator  the  CrossValidationResultProducer  (or leave the  RandomSplitResultProducer )  open the options dialog of the  CrossValidationResultProducer  by left-clicking on the edit field  in case of regression datasets, choose the  RegressionSplitEvaluator  instead of the  ClassifierSplitEvaluator  (the latter is used for classification problems)  open the options dialog for the  splitEvaluator  by left-clicking on the edit field  choose  the classifier that you want to analyze and setup it's parameters, in our case this is  J48  close all dialogs again (accepting them with OK)  set the  Generator properties  to  enabled  choose as property  percentage  and click on  Select :    splitEvaluator -> classifier -> confidenceFactor   now you can add all the factors that you want to test, e.g.:    0.10, 0.15, 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50   add the datasets you want to generate the learning curve for  save the experiment  go to the  Run  panel and start the experiment  after the experiment has finished, select the  Analyse  panel and perform your analysis on the results",
            "title": "Classifier parameter"
        },
        {
            "location": "/learning_curves/#see-also",
            "text": "Databases",
            "title": "See also"
        },
        {
            "location": "/lib_svm/",
            "text": "Description\n\n\nWrapper class for the \nLibSVM\n library by Chih-Chung Chang and Chih-Jen Lin. The original wrapper, named WLSVM, was developed by Yasser EL-Manzalawy. The current version is complete rewrite of the wrapper, using \nReflection\n in order to avoid compilation errors, in case the \nlibsvm.jar\n is not in the \nCLASSPATH\n.\n\n\nImportant note:\n\nFrom WEKA >= 3.7.2 installation and use of LibSVM in WEKA has been simplified by the creation of a \nLibSVM\n package that can be installed using either the graphical or command line \npackage manager\n.\n\n\nReference (Weka <=\n \n3.6.8)\n\n\n\n\nLibSVM\n\n\nWLSVM\n\n\n\n\nPackage\n\n\nweka.classifiers.functions\n\n\nDownload\n\n\nThe wrapper class is part of WEKA since version 3.5.2. But \nLibSVM\n, as a third-party-tool needs to be downloaded separately. It is recommended to upgrade to a post-3.5.3 version (or \nSubversion\n) for bug-fixes and extensions (contains now the \ndistributionForInstance\n method).\n\n\nCLASSPATH\n\n\nAdd the \nlibsvm.jar\n from the LibSVM distribution to your \nCLASSPATH\n to make it available.\n\n\nNote:\n Do NOT start WEKA then with \njava -jar weka.jar\n. The \n-jar\n option \noverwrites\n the \nCLASSPATH\n, not augments it (a very common trap to fall into). Instead use something like this on Linux:\n\n\n java -classpath $CLASSPATH:weka.jar:libsvm.jar weka.gui.GUIChooser\n\n\n\n\nor this on Win32 (if you're starting it from commandline):\n\n\n java -classpath \"%CLASSPATH%;weka.jar;libsvm.jar\" weka.gui.GUIChooser\n\n\n\n\nIf you're starting WEKA from the Start Menu on Windows, you'll have to add the \nlibsvm.jar\n to your \nCLASSPATH\n environment variable. The following steps are for \nWindows XP\n (unfortunately, the GUI changes among the different Windows versions):\n\n\n\n\nright-click on \nMy Computer\n and select \nProperties\n from the menu\n\n\nchoose the \nAdvanced\n tab and click on \nEnvironment variables\n at the bottom\n\n\neither add or modify a variable called \nCLASSPATH\n and add the \nlibsvm.jar\n with full path to it\n\n\n\n\nTroubleshooting\n\n\n\n\nLibSVM classes not in CLASSPATH!\n\n\nCheck whether the \nlibsvm.jar\n is really in your CLASSPATH. Execute the following command in the \nSimpleCLI\n:\n\n\n\n\n\n\n\n\n\n\n\n\njava weka.core.SystemInfo\n\n\nThe property \njava.class.path\n must list the \nlibsvm.jar\n. If it is listed, check whether the path is correct.\n\n\nIf you're on Windows and you find \n%CLASSPATH%\n there, see next bullet point to fix this.\n\n\n\n\n\n\n\n\nOn Windows, if you added the \nlibsvm.jar\n to your CLASSPATH environment variable, it can still happen that WEKA pops up the error message that the LibSVM classes are not in your CLASSPATH. This can happen where the \n%CLASSPATH%\n does not get expanded\n to its actual value in starting up WEKA. You can inspect your current CLASSPATH with which WEKA got started up with the \nSimpleCLI\n (see previous bullet point). If \n%CLASSPATH%\n is listed there, your system has the same problem. \nThis\n Wekalist post explains how to explicitly add the \nmysql.jar\n to \nRunWeka.ini\n (works the same for \nlibsvm.jar\n).\n\n\n\n\nNote:\n \nbackslashes have to be escaped\n, not only once, but twice (they get interpreted by Java twice!). In other words, instead of \none\n you have to use \nfour\n: \n\nC:\\some\\where\n then turns into \nC:\\\\\\\\some\\\\\\\\where\n.\n\n\n\n\n\n\n\n\n\n\nIssues with libsvm.jar\n\n\nThis section is based on \nthis\n Wekalist post.\n\n\nThe following changes were not incorporated in WEKA, since it also means modifying the LibSVM Java code, which (I think) is autogenerated from the C code. The authors of LibSVM might have to consider that update. It's left to the reader to incorporate these changes.\n\n\nlibsvm.svm uses Math.random\n\n\nlibsvm.svm calls Math.random so the model it returns is usually different for the same training set and svm parameters over time.\n\n\nObviously, if you call libsvm.svm from weka.classifiers.functions.libsvm, and you call it again from libsvm.svm_train, the results are also different.\n\n\nYou can use libsvm.svm_save_model to record the svms into files, and then compare the model file from WEKA LibSVM with the model file from libsvm.svm_predict. Then you can see that ProbA values use to be different.\n\n\nWEKA experimenter is based on using always the same random sequences in order to repeat experiments with the same results. So, I'm afraid some important design changes are required on libsvm.jar and weka.classifiers.functions.libsvm.class to keep such behaviour. We made a quick fix adding an static Random attribute to libsvm.svm class:\n\n\n static java.util.Random ranGen = new Random(0);\n\n\n\n\nWe have changed all Math.random() invokations to ranGen.nextdouble(). Then we have obtained the same svm from weka LibSVM than from LibSVM train_svm.\n\n\nHowever, WEKA accuracy results on primary_tumor data were still worse, so there's something wrong when weka uses the svm model at testing step.\n\n\nClasses without instances\n\n\nARFF format provides some meta-information (i.e. attributes name and type, set of possible values for nominal attributes), but LibSVM format doesn't. So if there are classes in the dataset with zero occurrences through all the instances, LibSVM thinks that these classes don't exist whereas WEKA knows they exist.\n\n\nFor example, there is a class in primary tumor dataset that never appears. When WEKA experimenter makes testing, it calls to:\n\n\n public static double svm_predict_probability(svm_model model, svm_node[] x, double[] prob_estimates)\n\n\n\n\npassing the array prob_estimates plenty of zeros (array cells are initialized to zero). The size of the array is equal to the number of classes (= 22). On the other hand, if this method is invoked from libsvm.svm_predict, the class that never appears is ignored, so the array dimension is now equal to 21.\n\n\nSo accuracy results are different depending on origin of svm_predict_probability method invocation. I think that better results are obtained if classes without instances are ignored, but I don't know if it is very fair. In fact, accuracies from weka.libsvm and from libsvm.predict_svm seem to be the same if the class that never appears is removed from ARFF file.\n\n\nNote that this problem only appears when testing, because the training code uses always the svm_group_classes method to compute the number of classes, so Instances.numClasses() value is never used for training. Moreover, maybe the mismatch between the training number of classes and the testing number of classes is the reason behind worse accuracy results when svm_predict_probability invocation is made from WEKA, but I haven't proved it yet.\n\n\nNote that this problem does also happen when you have a class with less examples than the number of folds. For some folds, the class will not have training examples.\n\n\nWe also made a quick fix for this problem:\n\n\n\n\nAdd this public method to libsvm.svm_model class\n\n\npublic int getNr_class(){return nr_class;}\n\n\n\n\n\n\nMake the following changes into \ndistributionforInstance\n Method at \nweka.classifiers.functions.LibSVM\n\n\nFirst line of the method:\n\n\n\n\n\n\n\n\n int[] labels = new int[instance.numClasses()];\n\n\n\n\n\n\ncould be changed to\n\n\n\n\n int[] labels = new int[((svm_model) m_Model).getNr_class()];\n\n\n\n\n\n\nLast line in \"if(m_ProbablityEstimates)\" block:\n\n\n\n\n prob_estimates = new double[instance.numClasses()];\n\n\n\n\n\n\ncould be changed to\n\n\n\n\n prob_estimates = new double[((svm_model) m_Model).getNr_class()];",
            "title": " LibSVM"
        },
        {
            "location": "/lib_svm/#description",
            "text": "Wrapper class for the  LibSVM  library by Chih-Chung Chang and Chih-Jen Lin. The original wrapper, named WLSVM, was developed by Yasser EL-Manzalawy. The current version is complete rewrite of the wrapper, using  Reflection  in order to avoid compilation errors, in case the  libsvm.jar  is not in the  CLASSPATH .  Important note: \nFrom WEKA >= 3.7.2 installation and use of LibSVM in WEKA has been simplified by the creation of a  LibSVM  package that can be installed using either the graphical or command line  package manager .",
            "title": "Description"
        },
        {
            "location": "/lib_svm/#reference-weka-368",
            "text": "LibSVM  WLSVM",
            "title": "Reference (Weka &lt;= 3.6.8)"
        },
        {
            "location": "/lib_svm/#package",
            "text": "weka.classifiers.functions",
            "title": "Package"
        },
        {
            "location": "/lib_svm/#download",
            "text": "The wrapper class is part of WEKA since version 3.5.2. But  LibSVM , as a third-party-tool needs to be downloaded separately. It is recommended to upgrade to a post-3.5.3 version (or  Subversion ) for bug-fixes and extensions (contains now the  distributionForInstance  method).",
            "title": "Download"
        },
        {
            "location": "/lib_svm/#classpath",
            "text": "Add the  libsvm.jar  from the LibSVM distribution to your  CLASSPATH  to make it available.  Note:  Do NOT start WEKA then with  java -jar weka.jar . The  -jar  option  overwrites  the  CLASSPATH , not augments it (a very common trap to fall into). Instead use something like this on Linux:   java -classpath $CLASSPATH:weka.jar:libsvm.jar weka.gui.GUIChooser  or this on Win32 (if you're starting it from commandline):   java -classpath \"%CLASSPATH%;weka.jar;libsvm.jar\" weka.gui.GUIChooser  If you're starting WEKA from the Start Menu on Windows, you'll have to add the  libsvm.jar  to your  CLASSPATH  environment variable. The following steps are for  Windows XP  (unfortunately, the GUI changes among the different Windows versions):   right-click on  My Computer  and select  Properties  from the menu  choose the  Advanced  tab and click on  Environment variables  at the bottom  either add or modify a variable called  CLASSPATH  and add the  libsvm.jar  with full path to it",
            "title": "CLASSPATH"
        },
        {
            "location": "/lib_svm/#troubleshooting",
            "text": "LibSVM classes not in CLASSPATH!  Check whether the  libsvm.jar  is really in your CLASSPATH. Execute the following command in the  SimpleCLI :       java weka.core.SystemInfo  The property  java.class.path  must list the  libsvm.jar . If it is listed, check whether the path is correct.  If you're on Windows and you find  %CLASSPATH%  there, see next bullet point to fix this.     On Windows, if you added the  libsvm.jar  to your CLASSPATH environment variable, it can still happen that WEKA pops up the error message that the LibSVM classes are not in your CLASSPATH. This can happen where the  %CLASSPATH%  does not get expanded  to its actual value in starting up WEKA. You can inspect your current CLASSPATH with which WEKA got started up with the  SimpleCLI  (see previous bullet point). If  %CLASSPATH%  is listed there, your system has the same problem.  This  Wekalist post explains how to explicitly add the  mysql.jar  to  RunWeka.ini  (works the same for  libsvm.jar ).   Note:   backslashes have to be escaped , not only once, but twice (they get interpreted by Java twice!). In other words, instead of  one  you have to use  four :  C:\\some\\where  then turns into  C:\\\\\\\\some\\\\\\\\where .",
            "title": "Troubleshooting"
        },
        {
            "location": "/lib_svm/#issues-with-libsvmjar",
            "text": "This section is based on  this  Wekalist post.  The following changes were not incorporated in WEKA, since it also means modifying the LibSVM Java code, which (I think) is autogenerated from the C code. The authors of LibSVM might have to consider that update. It's left to the reader to incorporate these changes.",
            "title": "Issues with libsvm.jar"
        },
        {
            "location": "/lib_svm/#libsvmsvm-uses-mathrandom",
            "text": "libsvm.svm calls Math.random so the model it returns is usually different for the same training set and svm parameters over time.  Obviously, if you call libsvm.svm from weka.classifiers.functions.libsvm, and you call it again from libsvm.svm_train, the results are also different.  You can use libsvm.svm_save_model to record the svms into files, and then compare the model file from WEKA LibSVM with the model file from libsvm.svm_predict. Then you can see that ProbA values use to be different.  WEKA experimenter is based on using always the same random sequences in order to repeat experiments with the same results. So, I'm afraid some important design changes are required on libsvm.jar and weka.classifiers.functions.libsvm.class to keep such behaviour. We made a quick fix adding an static Random attribute to libsvm.svm class:   static java.util.Random ranGen = new Random(0);  We have changed all Math.random() invokations to ranGen.nextdouble(). Then we have obtained the same svm from weka LibSVM than from LibSVM train_svm.  However, WEKA accuracy results on primary_tumor data were still worse, so there's something wrong when weka uses the svm model at testing step.",
            "title": "libsvm.svm uses Math.random"
        },
        {
            "location": "/lib_svm/#classes-without-instances",
            "text": "ARFF format provides some meta-information (i.e. attributes name and type, set of possible values for nominal attributes), but LibSVM format doesn't. So if there are classes in the dataset with zero occurrences through all the instances, LibSVM thinks that these classes don't exist whereas WEKA knows they exist.  For example, there is a class in primary tumor dataset that never appears. When WEKA experimenter makes testing, it calls to:   public static double svm_predict_probability(svm_model model, svm_node[] x, double[] prob_estimates)  passing the array prob_estimates plenty of zeros (array cells are initialized to zero). The size of the array is equal to the number of classes (= 22). On the other hand, if this method is invoked from libsvm.svm_predict, the class that never appears is ignored, so the array dimension is now equal to 21.  So accuracy results are different depending on origin of svm_predict_probability method invocation. I think that better results are obtained if classes without instances are ignored, but I don't know if it is very fair. In fact, accuracies from weka.libsvm and from libsvm.predict_svm seem to be the same if the class that never appears is removed from ARFF file.  Note that this problem only appears when testing, because the training code uses always the svm_group_classes method to compute the number of classes, so Instances.numClasses() value is never used for training. Moreover, maybe the mismatch between the training number of classes and the testing number of classes is the reason behind worse accuracy results when svm_predict_probability invocation is made from WEKA, but I haven't proved it yet.  Note that this problem does also happen when you have a class with less examples than the number of folds. For some folds, the class will not have training examples.  We also made a quick fix for this problem:   Add this public method to libsvm.svm_model class  public int getNr_class(){return nr_class;}    Make the following changes into  distributionforInstance  Method at  weka.classifiers.functions.LibSVM  First line of the method:      int[] labels = new int[instance.numClasses()];   could be changed to    int[] labels = new int[((svm_model) m_Model).getNr_class()];   Last line in \"if(m_ProbablityEstimates)\" block:    prob_estimates = new double[instance.numClasses()];   could be changed to    prob_estimates = new double[((svm_model) m_Model).getNr_class()];",
            "title": "Classes without instances"
        },
        {
            "location": "/load_an_xml_bif_file/",
            "text": "You should use the BIFReader class (\nweka.classifiers.bayes.net.BIFReader\n).\n\n\nHere is the snippet :\n\n\n import weka.classifiers.bayes.BayesNet;\n import weka.classifiers.bayes.net.BIFReader;\n\n public class wekaTest {\n  public static void main(String[] args) throws Exception {\n   BayesNet network = new BayesNet();\n   BIFReader reader = new BIFReader();  \n   network = reader.processFile(\"rb_on_min_attr.xml\");\n  }\n }\n\n\n\n\nhttp://weka.sourceforge.net/doc.dev/weka/classifiers/evaluation/ThresholdCurve.html\n\n\nDownloads\n\n\n\n\nLoadBif.java\n (\nbook\n, \nstable-3.8\n, \ndeveloper\n)",
            "title": " Load an XML BIF file"
        },
        {
            "location": "/load_an_xml_bif_file/#downloads",
            "text": "LoadBif.java  ( book ,  stable-3.8 ,  developer )",
            "title": "Downloads"
        },
        {
            "location": "/making_predictions/",
            "text": "Command line\n\n\nThe following sections show how to obtain predictions/classifications without writing your own Java code via the command line.\n\n\nClassifiers\n\n\nAfter a \nmodel has been saved\n, one can make predictions for a test set, whether that set contains valid class values or not. The output will contain both the actual and predicted class. (Note that if the test class contains simply '?' for the class label for each instance, the \"actual\" class label for each instance will not contain useful information, but the predicted class label will.) The \n-T <test_set>\n command-line switch specifies the dataset of instances whose classes are to be predicted, while the \n-p <attribute_range>\n switch allows the user to write out a range of attributes (examples: \"1-2\" for the first and second attributes, or \"0\" for no attributes). Sample command line:\n\n\n java weka.classifiers.trees.J48 -T unclassified.arff -l j48.model -p 0\n\n\n\n\nThe format of the output is as follows:\n\n\n<test_instance_index> <actual_class_index>:<actual_class_val> <pred_class_index>:<pred_class_val> [+| ] <prob_of_pred_class_val>\n\n\n\n\nwhere \"+\" occurs only for those items that were mispredicted. Note that if the actual class label is always \"?\" (i.e., the dataset does not include known class labels), the error column will always be empty.\n\n\nSample output:\n\n\n inst#     actual  predicted error prediction\n     1        1:?        1:0       0.757 \n     2        1:?        1:0       0.824 \n     3        1:?        1:0       0.807 \n     4        1:?        1:0       0.807 \n     5        1:?        1:0       0.79 \n     6        1:?        2:1       0.661 \n   ...\n\n\n\n\nIn this case, taken directly from a test dataset where all class attributes were marked by \"?\", the \"actual\" column, which can be ignored, simply states that each class belongs to an unknown class. The \"predicted\" column shows that instances 1 through 5 are predicted to be of class 1, whose value is 0, and instance 6 is predicted to be of class 2, whose value is 1. The error field is empty; if predictions were being performed on a labeled test set, each instance where the prediction failed to match the label would contain a \"+\". The probability that instance 1 actually belongs to class 0 is estimated at 0.757.\n\n\nNotes:\n\n\n\n\nSince Weka 3.5.4 you can also output the complete class distribution, not just the prediction, by using the parameter \n-distribution\n in conjunction with the -p option. In this case, \"*\" is placed beside the probability in the distribution that corresponds to the predicted class value.\n\n\nIf you have an ID attribute in your dataset as first attribute (you can always add one with the \nAddID\n filter), you could output it with \n-p 1\n instead of using \n-p 0\n. This works only for explicit train/test sets, but you can use the Explorer for cross-validation.\n\n\n\n\nFilters\n\n\nThe \n[AddClassification](http://weka.sourceforge.net/doc.dev/weka/filters/supervised/attribute/addclassification.html)\n filter (package \nweka.filters.supervised.attribute\n) can either train a classifier on the input data and transform this or load a serialized model to transform the input data (even though the filter was introduced in 3.5.4, due to a bug in the commandline option handling, it is recommended to download a version >3.5.5 or a snapshot from the Weka homepage).\nThis filter can add the classification, class distribution and the error per row as extra attributes to the dataset.\n\n\n\n\ntraining the classifier, e.g., J48, on the input data and replacing the class values with the ones of the trained classifier:\n\n\n\n\n   java \\\n     weka.filters.supervised.attribute.AddClassification \\\n     -W \"weka.classifiers.trees.J48\" \\\n     -classification \\\n     -remove-old-class \\\n     -i train.arff \\\n     -o train_classified.arff \\\n     -c last\n\n\n\n\n\n\nusing a serialized model, e.g., a J48 model, to replace the class values with the ones predicted by the serialized model:\n\n\n\n\n   java \\\n     weka.filters.supervised.attribute.AddClassification \\\n     -serialized /some/where/j48.model \\\n     -classification \\\n     -remove-old-class \\\n     -i train.arff \\\n     -o train_classified.arff \\\n     -c last\n\n\n\n\nGUI\n\n\nThe Weka GUI allows you as well to output predictions based on a previously saved model.\n\n\nExplorer\n\n\nSee the \nExplorer\n section of the \nSaving and loading models\n article to setup the Explorer. Additionally, you need to check the \nOutput predictions\n options in the \nMore options\n dialog. Righ-clicking on the respective results history item and selecting \nRe-evaluate model on current test set\n will output then the predictions as well (the statistics will be useless due to missing class values in the test set, so just ignore them). The output is similar to the one produced by the commandline.\n\n\nExample output for the \nanneal\n UCI dataset:\n\n\n == Predictions on test set ==\n inst#,    actual, predicted, error, probability distribution\n     1          ?        3:3      +   0      0     *1      0      0      0    \n     2          ?        3:3      +   0      0     *1      0      0      0    \n     3          ?        3:3      +   0      0     *1      0      0      0    \n    ...\n    17          ?        6:U      +   0      0      0      0      0     *1    \n    18          ?        6:U      +   0      0      0      0      0     *1    \n    19          ?        3:3      +   0      0     *1      0      0      0    \n    20          ?        3:3      +   0      0     *1      0      0      0    \n    ...\n\n\n\n\nNote:\n The developer version (>3.5.6 or \nsnapshot\n) can also output additional attributes like the commandline with the \n-p\n option. In the \nMore options...\n dialog you can specify those attribute indices with \nOutput additional attributes\n, e.g., \nfirst\n or \n1-7\n. In contrast to the commandline, this output also works for cross-validation.\n\n\nKnowledgeFlow\n\n\nUsing the PredictionAppender\n\n\nWith the \nPredictionAppender\n (from the \nEvaluation\n toolbar) you cannot use an already saved model, but you can train a classifier on a dataset and output an \nARFF\n file with the predictions appended as additional attribute. Here's an example setup:\n\n\n               /---dataSet--> TrainingSetMaker ---trainingSet--\\\n ArffLoader --<                                                 >--> J48...\n               \\---dataSet--> TestSetMaker -------testSet------/\n\n ...J48 --batchClassifier--> PredictionAppender --testSet--> ArffSaver\n\n\n\n\nUsing the AddClassification filter\n\n\nThe AddClassification filter can be used in the KnowledgeFlow as well, either for training a model, or for using a serialized model to perform the predictions. An example setup could look like this:\n\n\n ArffLoader --dataSet--> ClassAssigner --dataSet--> AddClassification --dataSet--> ArffSaver\n\n\n\n\nJava\n\n\nIf you want to perform the classification within your own code, see the \nclassifying instances\n section of \nthis article\n, explaining the Weka API in general.\n\n\nSee also\n\n\n\n\nSaving and loading models\n\n\nUse Weka in your Java code\n - general information about using the Weka API\n\n\nUsing ID attributes\n\n\n\n\nLinks\n\n\n\n\nAddClassification\n on the Wekalist\n\n\n\n\nVersion\n\n\nThe developer version shortly before the release of 3.5.6 was used as basis for this article.",
            "title": " Making predictions"
        },
        {
            "location": "/making_predictions/#command-line",
            "text": "The following sections show how to obtain predictions/classifications without writing your own Java code via the command line.",
            "title": "Command line"
        },
        {
            "location": "/making_predictions/#classifiers",
            "text": "After a  model has been saved , one can make predictions for a test set, whether that set contains valid class values or not. The output will contain both the actual and predicted class. (Note that if the test class contains simply '?' for the class label for each instance, the \"actual\" class label for each instance will not contain useful information, but the predicted class label will.) The  -T <test_set>  command-line switch specifies the dataset of instances whose classes are to be predicted, while the  -p <attribute_range>  switch allows the user to write out a range of attributes (examples: \"1-2\" for the first and second attributes, or \"0\" for no attributes). Sample command line:   java weka.classifiers.trees.J48 -T unclassified.arff -l j48.model -p 0  The format of the output is as follows:  <test_instance_index> <actual_class_index>:<actual_class_val> <pred_class_index>:<pred_class_val> [+| ] <prob_of_pred_class_val>  where \"+\" occurs only for those items that were mispredicted. Note that if the actual class label is always \"?\" (i.e., the dataset does not include known class labels), the error column will always be empty.  Sample output:   inst#     actual  predicted error prediction\n     1        1:?        1:0       0.757 \n     2        1:?        1:0       0.824 \n     3        1:?        1:0       0.807 \n     4        1:?        1:0       0.807 \n     5        1:?        1:0       0.79 \n     6        1:?        2:1       0.661 \n   ...  In this case, taken directly from a test dataset where all class attributes were marked by \"?\", the \"actual\" column, which can be ignored, simply states that each class belongs to an unknown class. The \"predicted\" column shows that instances 1 through 5 are predicted to be of class 1, whose value is 0, and instance 6 is predicted to be of class 2, whose value is 1. The error field is empty; if predictions were being performed on a labeled test set, each instance where the prediction failed to match the label would contain a \"+\". The probability that instance 1 actually belongs to class 0 is estimated at 0.757.  Notes:   Since Weka 3.5.4 you can also output the complete class distribution, not just the prediction, by using the parameter  -distribution  in conjunction with the -p option. In this case, \"*\" is placed beside the probability in the distribution that corresponds to the predicted class value.  If you have an ID attribute in your dataset as first attribute (you can always add one with the  AddID  filter), you could output it with  -p 1  instead of using  -p 0 . This works only for explicit train/test sets, but you can use the Explorer for cross-validation.",
            "title": "Classifiers"
        },
        {
            "location": "/making_predictions/#filters",
            "text": "The  [AddClassification](http://weka.sourceforge.net/doc.dev/weka/filters/supervised/attribute/addclassification.html)  filter (package  weka.filters.supervised.attribute ) can either train a classifier on the input data and transform this or load a serialized model to transform the input data (even though the filter was introduced in 3.5.4, due to a bug in the commandline option handling, it is recommended to download a version >3.5.5 or a snapshot from the Weka homepage).\nThis filter can add the classification, class distribution and the error per row as extra attributes to the dataset.   training the classifier, e.g., J48, on the input data and replacing the class values with the ones of the trained classifier:      java \\\n     weka.filters.supervised.attribute.AddClassification \\\n     -W \"weka.classifiers.trees.J48\" \\\n     -classification \\\n     -remove-old-class \\\n     -i train.arff \\\n     -o train_classified.arff \\\n     -c last   using a serialized model, e.g., a J48 model, to replace the class values with the ones predicted by the serialized model:      java \\\n     weka.filters.supervised.attribute.AddClassification \\\n     -serialized /some/where/j48.model \\\n     -classification \\\n     -remove-old-class \\\n     -i train.arff \\\n     -o train_classified.arff \\\n     -c last",
            "title": "Filters"
        },
        {
            "location": "/making_predictions/#gui",
            "text": "The Weka GUI allows you as well to output predictions based on a previously saved model.",
            "title": "GUI"
        },
        {
            "location": "/making_predictions/#explorer",
            "text": "See the  Explorer  section of the  Saving and loading models  article to setup the Explorer. Additionally, you need to check the  Output predictions  options in the  More options  dialog. Righ-clicking on the respective results history item and selecting  Re-evaluate model on current test set  will output then the predictions as well (the statistics will be useless due to missing class values in the test set, so just ignore them). The output is similar to the one produced by the commandline.  Example output for the  anneal  UCI dataset:   == Predictions on test set ==\n inst#,    actual, predicted, error, probability distribution\n     1          ?        3:3      +   0      0     *1      0      0      0    \n     2          ?        3:3      +   0      0     *1      0      0      0    \n     3          ?        3:3      +   0      0     *1      0      0      0    \n    ...\n    17          ?        6:U      +   0      0      0      0      0     *1    \n    18          ?        6:U      +   0      0      0      0      0     *1    \n    19          ?        3:3      +   0      0     *1      0      0      0    \n    20          ?        3:3      +   0      0     *1      0      0      0    \n    ...  Note:  The developer version (>3.5.6 or  snapshot ) can also output additional attributes like the commandline with the  -p  option. In the  More options...  dialog you can specify those attribute indices with  Output additional attributes , e.g.,  first  or  1-7 . In contrast to the commandline, this output also works for cross-validation.",
            "title": "Explorer"
        },
        {
            "location": "/making_predictions/#knowledgeflow",
            "text": "",
            "title": "KnowledgeFlow"
        },
        {
            "location": "/making_predictions/#using-the-predictionappender",
            "text": "With the  PredictionAppender  (from the  Evaluation  toolbar) you cannot use an already saved model, but you can train a classifier on a dataset and output an  ARFF  file with the predictions appended as additional attribute. Here's an example setup:                 /---dataSet--> TrainingSetMaker ---trainingSet--\\\n ArffLoader --<                                                 >--> J48...\n               \\---dataSet--> TestSetMaker -------testSet------/\n\n ...J48 --batchClassifier--> PredictionAppender --testSet--> ArffSaver",
            "title": "Using the PredictionAppender"
        },
        {
            "location": "/making_predictions/#using-the-addclassification-filter",
            "text": "The AddClassification filter can be used in the KnowledgeFlow as well, either for training a model, or for using a serialized model to perform the predictions. An example setup could look like this:   ArffLoader --dataSet--> ClassAssigner --dataSet--> AddClassification --dataSet--> ArffSaver",
            "title": "Using the AddClassification filter"
        },
        {
            "location": "/making_predictions/#java",
            "text": "If you want to perform the classification within your own code, see the  classifying instances  section of  this article , explaining the Weka API in general.",
            "title": "Java"
        },
        {
            "location": "/making_predictions/#see-also",
            "text": "Saving and loading models  Use Weka in your Java code  - general information about using the Weka API  Using ID attributes",
            "title": "See also"
        },
        {
            "location": "/making_predictions/#links",
            "text": "AddClassification  on the Wekalist",
            "title": "Links"
        },
        {
            "location": "/making_predictions/#version",
            "text": "The developer version shortly before the release of 3.5.6 was used as basis for this article.",
            "title": "Version"
        },
        {
            "location": "/mathematical_functions/",
            "text": "Mathematical functions implemented on dataset instances, like tan, cos, exp, log, and so on can be achived using one of the following filters:\n\n\n\n\nAddExpression\n (Stable version)\n\n\nMathExpression\n (Stable version)",
            "title": " Mathematical functions"
        },
        {
            "location": "/message_classifier/",
            "text": "In the following you'll find some information about the MessageClassifier from the \n2nd\n edition of the \nData Mining\n book by Witten and Frank.\n\n\nSource code\n\n\nDepending on the version of the book, download the corresponding version (this article is based on the \n2nd\n edition):\n\n\n\n\n1st Edition: \n\nMessageClassifier\n\n\n2nd Edition: \n\nMessageClassifier\n (\nbook\n, \nstable-3.6\n, \ndeveloper\n)\n\n\n\n\nCompiling\n\n\n\n\ncompile the source code like this, if the \nweka.jar\n is already in your \nCLASSPATH\n environment variable:\n\n\n\n\n javac MessageClassifier.java\n\n\n\n\n\n\notherwise, use this command line (of course, replace \n/path/to/\n with the correct path on your system):\n\n\n\n\n javac -classpath /path/to/weka.jar MessageClassifier.java\n\n\n\n\nNote:\n The classpath handling is omitted from here on.\n\n\nTraining\n\n\nIf you run the \nMessageClassifier\n for the first time, you need to provide labeled examples to build a classifier from, i.e., messages (\"\n-m\n\") and the corresponding classes (\"\n-c\n\"). Since the data and the model are kept for future use, one has to specify a filename, where the \nMessageClassifier\n is serialized to (\"\n-t\n\").\n\n\nHere's an example, that labels the message \nemail1.txt\n as \nmiss\n:\n\n\n java MessageClassifier -m email1.txt -c miss -t messageclassifier.model\n\n\n\n\nRepeat this for all the messages you want to have classified.\n\n\nClassifying\n\n\nClassifying an unseen message is quite straight-forward, one just omits the class option (\"\n-c\n\"). The following call\n\n\n java MessageClassifier -m email1023.txt -t messageclassifier.model\n\n\n\n\nwill produce something like this:\n\n\n Message classified as : \nmiss",
            "title": " MessageClassifier"
        },
        {
            "location": "/message_classifier/#source-code",
            "text": "Depending on the version of the book, download the corresponding version (this article is based on the  2nd  edition):   1st Edition:  MessageClassifier  2nd Edition:  MessageClassifier  ( book ,  stable-3.6 ,  developer )",
            "title": "Source code"
        },
        {
            "location": "/message_classifier/#compiling",
            "text": "compile the source code like this, if the  weka.jar  is already in your  CLASSPATH  environment variable:    javac MessageClassifier.java   otherwise, use this command line (of course, replace  /path/to/  with the correct path on your system):    javac -classpath /path/to/weka.jar MessageClassifier.java  Note:  The classpath handling is omitted from here on.",
            "title": "Compiling"
        },
        {
            "location": "/message_classifier/#training",
            "text": "If you run the  MessageClassifier  for the first time, you need to provide labeled examples to build a classifier from, i.e., messages (\" -m \") and the corresponding classes (\" -c \"). Since the data and the model are kept for future use, one has to specify a filename, where the  MessageClassifier  is serialized to (\" -t \").  Here's an example, that labels the message  email1.txt  as  miss :   java MessageClassifier -m email1.txt -c miss -t messageclassifier.model  Repeat this for all the messages you want to have classified.",
            "title": "Training"
        },
        {
            "location": "/message_classifier/#classifying",
            "text": "Classifying an unseen message is quite straight-forward, one just omits the class option (\" -c \"). The following call   java MessageClassifier -m email1023.txt -t messageclassifier.model  will produce something like this:   Message classified as : \nmiss",
            "title": "Classifying"
        },
        {
            "location": "/metacost/",
            "text": "This metaclassifier makes its base classifier cost-sensitive using the method specified in:\n\n\nPedro Domingos: MetaCost: A general method for making classifiers cost-sensitive. In: \nFifth International Conference on Knowledge Discovery and Data Mining, 155-164, 1999.\n\n\nThis classifier should produce similar results to one created by passing the base learner to Bagging, which is in turn passed to a CostSensitiveClassifier operating on minimum expected cost. The difference is that MetaCost produces a single cost-sensitive classifier of the base learner, giving the benefits of fast classification and interpretable output (if the base learner itself is interpretable). This implementation uses all bagging iterations when reclassifying training data (the MetaCost paper reports a marginal improvement when only those iterations containing each training instance are used in reclassifying that instance).\n\n\nExamples\n\n\nThe following cost matrix is used for a 3-class problem:\n\n\n -3  1  1\n  1 -6  1\n  0  0  0\n\n\n\n\nMetaCost will compute the costs (\nCosts\n) based on the class distribution the bagged base learner returns (\nClass probs\n) and select the class with the lowest cost (\nChosen class\n):\n\n\n+---------------+-----------------+--------------+\n| Class probs   | Costs           | Chosen class |\n+---------------+-----------------+--------------+\n| 1.0, 0.0, 0.0 | -3.0,  1.0, 1.0 |      1       |\n| 0.0, 1.0, 0.0 |  1.0, -6.0, 1.0 |      2       |\n| 0.0, 0.0, 1.0 |  0.0,  0.0, 0.0 |      1 *     |\n| 0.7, 0.1, 0.2 | -2.0,  0.1, 0.8 |      1       |\n| 0.2, 0.7, 0.1 |  0.1, -4.0. 0.9 |      2       |\n| 0.1, 0.2, 0.7 | -0.1, -1.1, 0.3 |      2       |\n+---------------+-----------------+--------------+\n\n\n\n\n*\n in case of a tie, the first one will be picked.\n\n\nSee also\n\n\n\n\nCostSensitiveClassifier\n\n\nCostMatrix\n\n\n\n\nLinks\n\n\n\n\nPublication on CiteSeer",
            "title": " MetaCost"
        },
        {
            "location": "/metacost/#examples",
            "text": "The following cost matrix is used for a 3-class problem:   -3  1  1\n  1 -6  1\n  0  0  0  MetaCost will compute the costs ( Costs ) based on the class distribution the bagged base learner returns ( Class probs ) and select the class with the lowest cost ( Chosen class ):  +---------------+-----------------+--------------+\n| Class probs   | Costs           | Chosen class |\n+---------------+-----------------+--------------+\n| 1.0, 0.0, 0.0 | -3.0,  1.0, 1.0 |      1       |\n| 0.0, 1.0, 0.0 |  1.0, -6.0, 1.0 |      2       |\n| 0.0, 0.0, 1.0 |  0.0,  0.0, 0.0 |      1 *     |\n| 0.7, 0.1, 0.2 | -2.0,  0.1, 0.8 |      1       |\n| 0.2, 0.7, 0.1 |  0.1, -4.0. 0.9 |      2       |\n| 0.1, 0.2, 0.7 | -0.1, -1.1, 0.3 |      2       |\n+---------------+-----------------+--------------+  *  in case of a tie, the first one will be picked.",
            "title": "Examples"
        },
        {
            "location": "/metacost/#see-also",
            "text": "CostSensitiveClassifier  CostMatrix",
            "title": "See also"
        },
        {
            "location": "/metacost/#links",
            "text": "Publication on CiteSeer",
            "title": "Links"
        },
        {
            "location": "/ms_sql_server_2000_desktop_engine/",
            "text": "Installation\n\n\n\n\nDownload the Desktop Engine (see \nLinks\n)\n\n\nExtract the files by running the downloaded executable\n\n\nEdit the \nsetup.ini\n file and add a strong password for the \nsa\n account:\n\n\n\n\n SAPWD=*password*\n\n\n\n\n\n\nNote: \nthe default password is empty, which can prevent the setup from continuing the installation\n\n\n\n\n\n\nRun the setup\n\n\n\n\nTesting\n\n\n\n\nThis\n article lists Java code for testing the connection\n\n\n\n\nTroubleshooting\n\n\n\n\nError Establishing Socket with JDBC Driver\n\n\nAdd TCP/IP to the list of protocols as stated in \nthis\n article\n\n\n\n\n\n\nLogin failed for user 'sa'. Reason: \nNot associated with a trusted SQL Server connection.\n\n\nFor changing the authentication to mixed mode see \nthis\n article\n\n\n\n\n\n\n\n\nLinks\n\n\n\n\nMicrosoft SQL Server 2000 (Desktop Engine)\n\n\nMicrosoft SQL Server 2000 JDBC Driver SP 3",
            "title": " MS SQL Server 2000 (Desktop Engine)"
        },
        {
            "location": "/ms_sql_server_2000_desktop_engine/#installation",
            "text": "Download the Desktop Engine (see  Links )  Extract the files by running the downloaded executable  Edit the  setup.ini  file and add a strong password for the  sa  account:    SAPWD=*password*   Note: \nthe default password is empty, which can prevent the setup from continuing the installation    Run the setup",
            "title": "Installation"
        },
        {
            "location": "/ms_sql_server_2000_desktop_engine/#testing",
            "text": "This  article lists Java code for testing the connection",
            "title": "Testing"
        },
        {
            "location": "/ms_sql_server_2000_desktop_engine/#troubleshooting",
            "text": "Error Establishing Socket with JDBC Driver  Add TCP/IP to the list of protocols as stated in  this  article    Login failed for user 'sa'. Reason: \nNot associated with a trusted SQL Server connection.  For changing the authentication to mixed mode see  this  article",
            "title": "Troubleshooting"
        },
        {
            "location": "/ms_sql_server_2000_desktop_engine/#links",
            "text": "Microsoft SQL Server 2000 (Desktop Engine)  Microsoft SQL Server 2000 JDBC Driver SP 3",
            "title": "Links"
        },
        {
            "location": "/multi_instance_classification/",
            "text": "Multi-instance (MI) classification is a supervised learning technique, but differs from \nnormal\n supervised learning:\n\n\n\n\nit has multiple instances in an example\n\n\nonly one class label is observable for all the instances in an example\n\n\n\n\nClassifiers\n\n\nMulti-instance classifiers were originally available through a separate software package, \nMulti-Instance Learning Kit\n (= MILK). But due to the introduction of the relational attribute in the \nARFF\n format, they became part of Weka in version 3.5.3 (developer version only). These classifiers can now be found in the following package:\n\n\n weka.classifiers.mi\n\n\n\n\nData format\n\n\nThe data format for multi-instance classifiers is fairly simple:\n\n\n\n\nbag-id\n - nominal attribute; unique identifier for each bag\n\n\nbag\n - relational attribute; contains the instances of an example\n\n\nclass\n - the class label for the examples\n\n\n\n\nWeka offers two filters to convert from flat file format (or propositional format), which is normally used in supervised classification, to multi-instance format and vice versa:\n\n\n\n\nweka.filters.unsupervised.attribute.PropositionalToMultiInstance\n\n\nweka.filters.unsupervised.attribute.MultiInstanceToPropositional\n\n\n\n\nHere is an example of the \nmusk1\n UCI dataset, used quite often in publications covering MI learning (Note: \n...\n denotes omission):\n\n\n\n\npropositional format:\n\n\nThis \nARFF\n file lists all the attributes, \nmolecule_name\n (which is the bag-id), \nf1\n to \nf166\n (containing the actual data of the instances) and the \nclass\n attribute.\n\n\n\n\n\n\n\n\n @relation musk1\n\n @attribute molecule_name {MUSK-jf78,MUSK-jf67,MUSK-jf59,...,NON-MUSK-199}\n @attribute f1 numeric\n @attribute f2 numeric\n @attribute f3 numeric\n @attribute f4 numeric\n @attribute f5 numeric\n ...\n @attribute f166 numeric\n @attribute class {0,1}\n\n @data\n MUSK-188,42,-198,-109,-75,-117,11,23,-88,-28,-27,...,48,-37,6,30,1\n MUSK-188,42,-191,-142,-65,-117,55,49,-170,-45,5,...,48,-37,5,30,1\n ...\n\n\n\n\n\n\nmulti-instance format:\n\n\nUsing the relational attribute, one only has \nthree\n attributes on the first level: \n\nmolecule_name\n, \nbag\n and \nclass\n. The relational attribute contains the instances for each example, consisting of the attributes \nf1\n to \nf166\n. The data of the relational attribute is surrounded by quotes and the single instances inside the bag are separated by line-feeds (= \n\\n\n).\n\n\n\n\n\n\n\n\n @relation musk1\n\n @attribute molecule_name {MUSK-jf78,MUSK-jf67,MUSK-jf59,...,NON-MUSK-199}\n @attribute bag relational\n   @attribute f1 numeric\n   @attribute f2 numeric\n   @attribute f3 numeric\n   @attribute f4 numeric\n   @attribute f5 numeric\n   ...\n   @attribute f166 numeric\n @end bag\n @attribute class {0,1}\n\n @data\n MUSK-188,\"42,-198,-109,-75,-117,11,23,-88,-28,-27,...,48,-37,6,30\\n42,-191,-142,-65,-117,55,49,-170,-45,5,...,48,-37,5,30\\n...\",1\n ...\n\n\n\n\nSee also\n\n\n\n\nUse Weka in your Java code\n - general article about using the Weka API\n\n\nCreating an ARFF file\n - explains how to create an ARFF file from within Java, incl. relational attributes\n\n\n\n\nLinks\n\n\n\n\nXin Xu. \nStatistical learning in multiple instance problem.\n Master's thesis, University of Waikato, Hamilton, NZ, 2003. 0657.594. \nDownload\n\n\nMILK homepage",
            "title": " Multi-instance classification"
        },
        {
            "location": "/multi_instance_classification/#classifiers",
            "text": "Multi-instance classifiers were originally available through a separate software package,  Multi-Instance Learning Kit  (= MILK). But due to the introduction of the relational attribute in the  ARFF  format, they became part of Weka in version 3.5.3 (developer version only). These classifiers can now be found in the following package:   weka.classifiers.mi",
            "title": "Classifiers"
        },
        {
            "location": "/multi_instance_classification/#data-format",
            "text": "The data format for multi-instance classifiers is fairly simple:   bag-id  - nominal attribute; unique identifier for each bag  bag  - relational attribute; contains the instances of an example  class  - the class label for the examples   Weka offers two filters to convert from flat file format (or propositional format), which is normally used in supervised classification, to multi-instance format and vice versa:   weka.filters.unsupervised.attribute.PropositionalToMultiInstance  weka.filters.unsupervised.attribute.MultiInstanceToPropositional   Here is an example of the  musk1  UCI dataset, used quite often in publications covering MI learning (Note:  ...  denotes omission):   propositional format:  This  ARFF  file lists all the attributes,  molecule_name  (which is the bag-id),  f1  to  f166  (containing the actual data of the instances) and the  class  attribute.      @relation musk1\n\n @attribute molecule_name {MUSK-jf78,MUSK-jf67,MUSK-jf59,...,NON-MUSK-199}\n @attribute f1 numeric\n @attribute f2 numeric\n @attribute f3 numeric\n @attribute f4 numeric\n @attribute f5 numeric\n ...\n @attribute f166 numeric\n @attribute class {0,1}\n\n @data\n MUSK-188,42,-198,-109,-75,-117,11,23,-88,-28,-27,...,48,-37,6,30,1\n MUSK-188,42,-191,-142,-65,-117,55,49,-170,-45,5,...,48,-37,5,30,1\n ...   multi-instance format:  Using the relational attribute, one only has  three  attributes on the first level:  molecule_name ,  bag  and  class . The relational attribute contains the instances for each example, consisting of the attributes  f1  to  f166 . The data of the relational attribute is surrounded by quotes and the single instances inside the bag are separated by line-feeds (=  \\n ).      @relation musk1\n\n @attribute molecule_name {MUSK-jf78,MUSK-jf67,MUSK-jf59,...,NON-MUSK-199}\n @attribute bag relational\n   @attribute f1 numeric\n   @attribute f2 numeric\n   @attribute f3 numeric\n   @attribute f4 numeric\n   @attribute f5 numeric\n   ...\n   @attribute f166 numeric\n @end bag\n @attribute class {0,1}\n\n @data\n MUSK-188,\"42,-198,-109,-75,-117,11,23,-88,-28,-27,...,48,-37,6,30\\n42,-191,-142,-65,-117,55,49,-170,-45,5,...,48,-37,5,30\\n...\",1\n ...",
            "title": "Data format"
        },
        {
            "location": "/multi_instance_classification/#see-also",
            "text": "Use Weka in your Java code  - general article about using the Weka API  Creating an ARFF file  - explains how to create an ARFF file from within Java, incl. relational attributes",
            "title": "See also"
        },
        {
            "location": "/multi_instance_classification/#links",
            "text": "Xin Xu.  Statistical learning in multiple instance problem.  Master's thesis, University of Waikato, Hamilton, NZ, 2003. 0657.594.  Download  MILK homepage",
            "title": "Links"
        },
        {
            "location": "/optimizing_parameters/",
            "text": "Since finding the optimal parameters for a classifier can be a rather tedious process, Weka offers some ways of automating this process a bit. The following meta-classifiers allow you to optimize some parameters of your base classifier:\n\n\n\n\nweka.classifiers.meta.CVParameterSelection\n\n\nweka.classifiers.meta.GridSearch\n (only developer version)\n\n\nweka.classifiers.meta.MultiSearch\n (\nexternal package\n for 3.7.11+)\n\n\nAuto-WEKA (\nexternal package\n package for 3.7.13+)\n\n\n\n\nAfter finding the best possible setup, the meta-classifiers then train an instance of the base classifier with these parameters and use it for subsequent predictions.\n\n\nCVParameterSelection\n\n\nThis meta-classifier can optimize over an arbitrary number of parameters, with only one drawback (apart from the obvious explosion of possible parameter combinations): \none cannot optimize on nested options, only direct options of the base classifier. What does that mean? It means, that you can optimize the \nC\n parameter of \nweka.classifiers.functions.SMO\n, but not the \nC\n of an \nweka.classifiers.functions.SMO\n within a \nweka.classifiers.meta.FilteredClassifier\n.\n\n\nHere are a few examples:\n\n\n\n\n\n\nJ48 and it's confidence interval (\"-C\")\n\n\n\n\nload your dataset in the Explorer\n\n\nchoose \nweka.classifiers.meta.CVParameterSelection\n as classifier\n\n\nselect \nweka.classifiers.trees.J48\n as base classifier within \nCVParameterSelection\n\n\n\n\nopen the ArrayEditor for \nCVParameters\n and enter the following string (and click on \nAdd\n): \nC 0.1 0.5 5\n - This will test the confidence parameter from 0.1 to 0.5 with step size 0.1 (= 5 steps)\n\n\n\n\n\n\nclose dialogs and start the classifier\n\n\n\n\nyou will get output similar to this one, with the best parameters found in bold:\n\n\n\n\n\n\n\n\n Cross-validated Parameter selection.\n Classifier: \nweka.classifiers.trees.J48\n Cross-validation Parameter: \n'-C' ranged from 0.1 to 0.5 with 5.0 steps\n Classifier Options: \n**-C 0.1** -M 2\n\n\n\n\n\n\nSMO and it's complexity parameter (\"-C\")\n\n\nload your dataset in the Explorer\n\n\nchoose \nweka.classifiers.meta.CVParameterSelection\n as classifier\n\n\nselect \nweka.classifiers.functions.SMO\n as base classifier within \nCVParameterSelection\n and modify its setup if necessary, e.g., RBF kernel\n\n\nopen the ArrayEditor for \nCVParameters\n and enter the following string (and click on \nAdd\n):\n\n\n\n\n\n\n\n\n\n\nC 2 8 4\n\nThis will test the complexity parameters 2, 4, 6 and 8 (= 4 steps)\n    * close dialogs and start the classifier\n    * you will get output similar to this one, with the best parameters found in bold:\n\n\n\n\n Cross-validated Parameter selection.\n Classifier: \nweka.classifiers.functions.SMO\n Cross-validation Parameter: \n'-C' ranged from 2.0 to 8.0 with 4.0 steps\n Classifier Options: \n**-C 8** -L 0.0010 -P 1.0E-12 -N 0 -V -1 -W 1 -K \"weka.classifiers.functions.supportVector.RBFKernel -C 250007 -G 0.01\"\n\n\n\n\n\n\nLibSVM\n and the gamma parameter of the RBF kernel (\"-G\")\n\n\nload your dataset in the Explorer\n\n\nchoose \nweka.classifiers.meta.CVParameterSelection\n as classifier\n\n\nselect \n[weka.classifiers.functions.LibSVM](lib_svm.md)\n as base classifier within \nCVParameterSelection\n and modify its setup if necessary, e.g., RBF kernel\n\n\nopen the ArrayEditor for \nCVParameters\n and enter the following string (and click on \nAdd\n):\n\n\n\n\n\n\n\n\n\n\nG 0.01 0.1 10\n\nThis will iterate over the gamma parameter, using values from 0.01 to 0.1 (= 10 steps)\n    * close dialogs and start the classifier\n    * you will get output similar to this one, with the best parameters found in bold:\n\n\n\n\n Cross-validated Parameter selection.\n Classifier: \nweka.classifiers.functions.LibSVM\n Cross-validation Parameter: \n'-G' ranged from 0.01 to 0.1 with 10.0 steps\n Classifier Options: \n**-G 0.09** -S 0 -K 2 -D 3 -R 0.0 -N 0.5 -M 40.0 -C 1.0 -E 0.0010 -P 0.1\n\n\n\n\nGridSearch\n\n\nweka.classifiers.meta.GridSearch\n is a meta-classifier for exploring 2 parameters, hence the \ngrid\n in the name. If one turns the log on, the classifier will create output suitable for \ngnuplot\n, i.e., sections of the log will contain script and data sections. Instead of just using a classifier, one can specify a base classifier \nand\n a filter, which both of them can be optimized (one parameter each). In contrast to \nCVParameterSelection\n, \nGridSearch\n is not limited to first-level parameters of the base classifier, since it's using \nJava Beans\n \nIntrospection\n and one can specify \npaths\n to the properties one wants to optimize. A \nproperty\n here is the string of the parameter displayed in the GenericObjectEditor (generated though Introspection), e.g., \nbagSizePercent\n or \nclassifier\n of \nweka.classifiers.meta.Bagging\n.\n\n\nDue to some important bugfixes, one should obtain a version of Weka >3.5.6 or a \nsnapshot\n later than 11 Sept 2007.\n\n\nFor each of the two axes, X and Y, one can specify the following parameters:\n\n\n\n\nproperty\n\n\nThe dot-separated path pointing to the property to be optimized. In order to distinguish between paths for the filter or the classifier, one needs to prefix the path either with \nfilter.\n or \nclassifier.\n for filter or classifier path respectively.\n\n\n\n\n\n\nexpression\n\n\nThe mathematical expression to generate the value for the property, processed with the \nweka.core.MathematicalExpression\n class, which supports the following functions: \nabs\n, \nsqrt\n, \nlog\n, \nexp\n, \nsin\n, \ncos\n, \ntan\n, \nrint\n, \nfloor\n, \npow\n, \nceil\n. These variables are available in the expression: \n\nBASE\n, \nFROM\n, \nTO\n, \nSTEP\n, \nI\n; with \nI\n ranging from \nFROM\n to \nTO\n.\n\n\n\n\n\n\nmin\n\n\nThe minimum value to start from.\n\n\n\n\n\n\nmax\n\n\nThe maximum value.\n\n\n\n\n\n\nstep\n\n\nThe step size used to get from \nmin\n to \nmax\n.\n\n\n\n\n\n\nbase\n\n\nUsed in \npow()\n calculations.\n\n\n\n\n\n\n\n\nGridSearch\n can also optimized based on the following measures:\n\n\n\n\nCorrelation coefficient (= CC)\n\n\nRoot mean squared error (= RMSE)\n\n\nRoot relative squared error (= RRSE)\n\n\nMean absolute error (= MAE)\n\n\nRoot absolute error (= RAE)\n\n\nCombined: \n(1-abs(CC)) + RRSE + RAE\n\n\nAccuracy (= ACC)\n\n\nKappa (= KAP) [only when using Weka packages]\n\n\n\n\nNote: \n\nCorrelation coefficient\n is only available for numeric classes and \nAccuracy\n only for nominal ones.\n\n\nHere are a some examples (taken from the Javadoc of the classifier):\n\n\n\n\nOptimizing SMO with RBFKernel (C and gamma)\n\n\nStart the Explorer and load your dataset with nominal class.\n\n\nSet the evaluation to \nAccuracy\n.\n\n\nSet the filter to \nweka.filters.AllFilter\n since we don't need any special data processing and we don't optimize the filter in this case (data gets always passed through filter!).\n\n\nSet \nweka.classifiers.functions.SMO\n as classifier with \nweka.classifiers.functions.supportVector.RBFKernel\n as kernel.\n\n\nSet the\nXProperty\n to \"classifier.c\", \nXMin\n to \"1\", \nXMax\n to \"16\", \nXStep\n to \"1\" and the \nXExpression\n to \"I\". This will test the \"C\" parameter of \nSMO\n for the values from 1 to 16.\n\n\nSet the \nYProperty\n to \"classifier.kernel.gamma\", \nYMin\n to \"-5\", \nYMax\n to \"2\", \nYStep\n to \"1\", \nYBase\n to \"10\" and \nYExpression\n to \"pow(BASE,I)\". This will test the gamma of the RBFKernel with the values 10\n-5\n, 10\n-4\n,..,10\n2\n.\n\n\nOutput will be similar to this one here:\n\n\n\n\n\n\n\n\n Filter: \nweka.filters.AllFilter\n Classifier: \nweka.classifiers.functions.SMO -C 2.0 -L 0.0010 -P 1.0E-12 -N 0 -V -1 -W 1 -K \"weka.classifiers.functions.supportVector.RBFKernel -C 250007 -G 0.0\"\n\n X property: \nclassifier.c\n Y property: \nclassifier.kernel.gamma\n\n Evaluation: \nAccuracy\n Coordinates: \n[2.0, 0.0]\n Values: \n**2.0** (X coordinate), **1.0** (Y coordinate)\n\n\n\n\n\n\nOptimizing PLSFilter with LinearRegression (# of components and ridge)\n - default setup\n\n\nStart the Explorer and load your dataset with numeric class.\n\n\nSet the evaluation to Correlation coefficient.\n\n\nSet the filter to \nweka.filters.supervised.attribute.PLSFilter\n.\n\n\nSet \nweka.classifiers.functions.LinearRegression\n as classifier and use \nno attribute selection\n and \nno elimination of colinear attributes\n (speeds up \nLinearRegression\n significantly!).\n\n\nSet the \nXProperty\n to \"filter.numComponents\", \nXMin\n to \"5\", \nXMax\n to \"20\" (this depends heavily on your dataset, should be no more than the number of attributes!), \nXStep\n to \"1\" and \nXExpression\n to \"I\". This will test the number of components the \nPLSFilter\n will produce from 5 to 20.\n\n\nSet the \nYProperty\n to \"classifier.ridge\", \nXMin\n to \"-10\", \nXMax\n to \"5\", \nYStep\n to \"1\" and \nYExpression\n to \"pow(BASE,I)\". This will try ridge parameters from 10\n-10\n to 10\n5\n.\n\n\nOutput will be similar to this one:\n\n\n\n\n\n\n\n\n Filter: \nweka.filters.supervised.attribute.PLSFilter -C 5 -M -A PLS1 -P center\n Classifier: \nweka.classifiers.functions.LinearRegression -S 1 -C -R 5.0\n\n X property: \nfilter.numComponents\n Y property: \nclassifier.ridge\n\n Evaluation: \nCorrelation coefficient\n Coordinates: \n[5.0, 5.0]\n Values: \n**5.0** (X coordinate), **100000.0** (Y coordinate)\n\n\n\n\nNotes:\n\n\n\n\na property for the classifier starts with \nclassifier.\n\n\na property for the filter starts with \nfilter.\n\n\nArrays of objects are addressed with \n[\n]\n, with the \nindex\n being 0-based. E.g., using a \nweka.filters.MultiFilter\n in \nGridSearch\n consisting of a \nReplaceMissingValues\n and a \nPLSFilter\n filter one can address the \nnumComponents\n property of the \nPLSFilter\n with \nfilter.filter[1].numComponents\n\n\n\n\nMultiSearch\n\n\nweka.classifiers.meta.MultiSearch\n is available through \nthis\n Weka package (requires Weka 3.7.11 or later; for downloads see the \nReleases\n section). MultiSearch is similar to GridSearch, more general and simpler at the same time. More general, because it allows the optimization of an arbitrary number of parameters, not just two. Simpler, because it does not offer any search space expansions or \ngnuplot\n output and less options.\n\n\nFor each parameter to optimize, the user has to define a \nsearch parameter\n. There are two types of parameters available:\n\n\n\n\nMathParameter\n - basically what \nGridSearch\n uses, with an expression to calculate the actual value using the min, max and step parameters\n\n\nListParameter\n - the blank-separated list of values is used as input for the optimization (useful, if values cannot be described by a mathematical function)\n\n\n\n\nHere is a setup for finding the best ridge parameter (property \nclassifier.ridge\n) using the \nMathParameter\n search parameter using values from 10^-10 to 10^5:\n\n\nweka.classifiers.meta.MultiSearch \\\n  -E CC \\\n  -search \"weka.core.setupgenerator.MathParameter -property classifier.ridge -min -10.0 -max 5.0 -step 1.0 -base 10.0 -expression pow(BASE,I)\" \\\n  -sample-size 100.0 -initial-folds 2 -subsequent-folds 10 -num-slots 1 -S 1 \\\n  -W weka.classifiers.functions.LinearRegression -- -S 1 -C -R 1.0E-8\n\n\n\n\nAnd here using the \nListParameter\n search parameter for evaluating values 0.001, 0.05, 0.1, 0.5, 0.75 and 1.0 for the ridge parameter (property \nclassifier.ridge\n):\n\n\nweka.classifiers.meta.MultiSearch \\\n  -E CC \\\n  -search \"weka.core.setupgenerator.ListParameter -property classifier.ridge -list \\\"0.001 0.05 0.1 0.5 0.75 1.0\\\"\" \\\n  -sample-size 100.0 -initial-folds 2 -subsequent-folds 10 -num-slots 1 -S 1 \\\n  -W weka.classifiers.functions.LinearRegression -- -S 1 -C -R 1.0E-8\n\n\n\n\nMultiSearch\n can be optimized based on the following measures:\n\n\n\n\nCorrelation coefficient (= CC)\n\n\nRoot mean squared error (= RMSE)\n\n\nRoot relative squared error (= RRSE)\n\n\nMean absolute error (= MAE)\n\n\nRoot absolute error (= RAE)\n\n\nCombined: \n(1-abs(CC)) + RRSE + RAE\n\n\nAccuracy (= ACC)\n\n\nKappa (= KAP)\n\n\n\n\nAuto-WEKA\n\n\nAuto-WEKA\n is available as a package through the WEKA package manager. It provides the class \nweka.classifiers.meta.AutoWEKAClassifier\n and optimizes all parameters of all learners. It also automatically determines the best learner to use and the best attribute selection method for a given dataset. More information is available on the project website and the \nmanual\n.\n\n\nDownloads\n\n\n\n\nCVParam.java\n - optimizes J48's \n-C\n parameter\n\n\n\n\nSee also\n\n\n\n\nLibSVM\n - you need additional jars in your \nCLASSPATH\n to be able to use LibSVM\n\n\n\n\nLinks\n\n\n\n\ngnuplot homepage\n\n\nJava Beans\n\n\nIntrospection",
            "title": " Optimizing parameters"
        },
        {
            "location": "/optimizing_parameters/#cvparameterselection",
            "text": "This meta-classifier can optimize over an arbitrary number of parameters, with only one drawback (apart from the obvious explosion of possible parameter combinations): \none cannot optimize on nested options, only direct options of the base classifier. What does that mean? It means, that you can optimize the  C  parameter of  weka.classifiers.functions.SMO , but not the  C  of an  weka.classifiers.functions.SMO  within a  weka.classifiers.meta.FilteredClassifier .  Here are a few examples:    J48 and it's confidence interval (\"-C\")   load your dataset in the Explorer  choose  weka.classifiers.meta.CVParameterSelection  as classifier  select  weka.classifiers.trees.J48  as base classifier within  CVParameterSelection   open the ArrayEditor for  CVParameters  and enter the following string (and click on  Add ):  C 0.1 0.5 5  - This will test the confidence parameter from 0.1 to 0.5 with step size 0.1 (= 5 steps)    close dialogs and start the classifier   you will get output similar to this one, with the best parameters found in bold:      Cross-validated Parameter selection.\n Classifier: \nweka.classifiers.trees.J48\n Cross-validation Parameter: \n'-C' ranged from 0.1 to 0.5 with 5.0 steps\n Classifier Options: \n**-C 0.1** -M 2   SMO and it's complexity parameter (\"-C\")  load your dataset in the Explorer  choose  weka.classifiers.meta.CVParameterSelection  as classifier  select  weka.classifiers.functions.SMO  as base classifier within  CVParameterSelection  and modify its setup if necessary, e.g., RBF kernel  open the ArrayEditor for  CVParameters  and enter the following string (and click on  Add ):      C 2 8 4 \nThis will test the complexity parameters 2, 4, 6 and 8 (= 4 steps)\n    * close dialogs and start the classifier\n    * you will get output similar to this one, with the best parameters found in bold:    Cross-validated Parameter selection.\n Classifier: \nweka.classifiers.functions.SMO\n Cross-validation Parameter: \n'-C' ranged from 2.0 to 8.0 with 4.0 steps\n Classifier Options: \n**-C 8** -L 0.0010 -P 1.0E-12 -N 0 -V -1 -W 1 -K \"weka.classifiers.functions.supportVector.RBFKernel -C 250007 -G 0.01\"   LibSVM  and the gamma parameter of the RBF kernel (\"-G\")  load your dataset in the Explorer  choose  weka.classifiers.meta.CVParameterSelection  as classifier  select  [weka.classifiers.functions.LibSVM](lib_svm.md)  as base classifier within  CVParameterSelection  and modify its setup if necessary, e.g., RBF kernel  open the ArrayEditor for  CVParameters  and enter the following string (and click on  Add ):      G 0.01 0.1 10 \nThis will iterate over the gamma parameter, using values from 0.01 to 0.1 (= 10 steps)\n    * close dialogs and start the classifier\n    * you will get output similar to this one, with the best parameters found in bold:    Cross-validated Parameter selection.\n Classifier: \nweka.classifiers.functions.LibSVM\n Cross-validation Parameter: \n'-G' ranged from 0.01 to 0.1 with 10.0 steps\n Classifier Options: \n**-G 0.09** -S 0 -K 2 -D 3 -R 0.0 -N 0.5 -M 40.0 -C 1.0 -E 0.0010 -P 0.1",
            "title": "CVParameterSelection"
        },
        {
            "location": "/optimizing_parameters/#gridsearch",
            "text": "weka.classifiers.meta.GridSearch  is a meta-classifier for exploring 2 parameters, hence the  grid  in the name. If one turns the log on, the classifier will create output suitable for  gnuplot , i.e., sections of the log will contain script and data sections. Instead of just using a classifier, one can specify a base classifier  and  a filter, which both of them can be optimized (one parameter each). In contrast to  CVParameterSelection ,  GridSearch  is not limited to first-level parameters of the base classifier, since it's using  Java Beans   Introspection  and one can specify  paths  to the properties one wants to optimize. A  property  here is the string of the parameter displayed in the GenericObjectEditor (generated though Introspection), e.g.,  bagSizePercent  or  classifier  of  weka.classifiers.meta.Bagging .  Due to some important bugfixes, one should obtain a version of Weka >3.5.6 or a  snapshot  later than 11 Sept 2007.  For each of the two axes, X and Y, one can specify the following parameters:   property  The dot-separated path pointing to the property to be optimized. In order to distinguish between paths for the filter or the classifier, one needs to prefix the path either with  filter.  or  classifier.  for filter or classifier path respectively.    expression  The mathematical expression to generate the value for the property, processed with the  weka.core.MathematicalExpression  class, which supports the following functions:  abs ,  sqrt ,  log ,  exp ,  sin ,  cos ,  tan ,  rint ,  floor ,  pow ,  ceil . These variables are available in the expression:  BASE ,  FROM ,  TO ,  STEP ,  I ; with  I  ranging from  FROM  to  TO .    min  The minimum value to start from.    max  The maximum value.    step  The step size used to get from  min  to  max .    base  Used in  pow()  calculations.     GridSearch  can also optimized based on the following measures:   Correlation coefficient (= CC)  Root mean squared error (= RMSE)  Root relative squared error (= RRSE)  Mean absolute error (= MAE)  Root absolute error (= RAE)  Combined: \n(1-abs(CC)) + RRSE + RAE  Accuracy (= ACC)  Kappa (= KAP) [only when using Weka packages]   Note:  Correlation coefficient  is only available for numeric classes and  Accuracy  only for nominal ones.  Here are a some examples (taken from the Javadoc of the classifier):   Optimizing SMO with RBFKernel (C and gamma)  Start the Explorer and load your dataset with nominal class.  Set the evaluation to  Accuracy .  Set the filter to  weka.filters.AllFilter  since we don't need any special data processing and we don't optimize the filter in this case (data gets always passed through filter!).  Set  weka.classifiers.functions.SMO  as classifier with  weka.classifiers.functions.supportVector.RBFKernel  as kernel.  Set the XProperty  to \"classifier.c\",  XMin  to \"1\",  XMax  to \"16\",  XStep  to \"1\" and the  XExpression  to \"I\". This will test the \"C\" parameter of  SMO  for the values from 1 to 16.  Set the  YProperty  to \"classifier.kernel.gamma\",  YMin  to \"-5\",  YMax  to \"2\",  YStep  to \"1\",  YBase  to \"10\" and  YExpression  to \"pow(BASE,I)\". This will test the gamma of the RBFKernel with the values 10 -5 , 10 -4 ,..,10 2 .  Output will be similar to this one here:      Filter: \nweka.filters.AllFilter\n Classifier: \nweka.classifiers.functions.SMO -C 2.0 -L 0.0010 -P 1.0E-12 -N 0 -V -1 -W 1 -K \"weka.classifiers.functions.supportVector.RBFKernel -C 250007 -G 0.0\"\n\n X property: \nclassifier.c\n Y property: \nclassifier.kernel.gamma\n\n Evaluation: \nAccuracy\n Coordinates: \n[2.0, 0.0]\n Values: \n**2.0** (X coordinate), **1.0** (Y coordinate)   Optimizing PLSFilter with LinearRegression (# of components and ridge)  - default setup  Start the Explorer and load your dataset with numeric class.  Set the evaluation to Correlation coefficient.  Set the filter to  weka.filters.supervised.attribute.PLSFilter .  Set  weka.classifiers.functions.LinearRegression  as classifier and use  no attribute selection  and  no elimination of colinear attributes  (speeds up  LinearRegression  significantly!).  Set the  XProperty  to \"filter.numComponents\",  XMin  to \"5\",  XMax  to \"20\" (this depends heavily on your dataset, should be no more than the number of attributes!),  XStep  to \"1\" and  XExpression  to \"I\". This will test the number of components the  PLSFilter  will produce from 5 to 20.  Set the  YProperty  to \"classifier.ridge\",  XMin  to \"-10\",  XMax  to \"5\",  YStep  to \"1\" and  YExpression  to \"pow(BASE,I)\". This will try ridge parameters from 10 -10  to 10 5 .  Output will be similar to this one:      Filter: \nweka.filters.supervised.attribute.PLSFilter -C 5 -M -A PLS1 -P center\n Classifier: \nweka.classifiers.functions.LinearRegression -S 1 -C -R 5.0\n\n X property: \nfilter.numComponents\n Y property: \nclassifier.ridge\n\n Evaluation: \nCorrelation coefficient\n Coordinates: \n[5.0, 5.0]\n Values: \n**5.0** (X coordinate), **100000.0** (Y coordinate)  Notes:   a property for the classifier starts with  classifier.  a property for the filter starts with  filter.  Arrays of objects are addressed with  [ ] , with the  index  being 0-based. E.g., using a  weka.filters.MultiFilter  in  GridSearch  consisting of a  ReplaceMissingValues  and a  PLSFilter  filter one can address the  numComponents  property of the  PLSFilter  with  filter.filter[1].numComponents",
            "title": "GridSearch"
        },
        {
            "location": "/optimizing_parameters/#multisearch",
            "text": "weka.classifiers.meta.MultiSearch  is available through  this  Weka package (requires Weka 3.7.11 or later; for downloads see the  Releases  section). MultiSearch is similar to GridSearch, more general and simpler at the same time. More general, because it allows the optimization of an arbitrary number of parameters, not just two. Simpler, because it does not offer any search space expansions or  gnuplot  output and less options.  For each parameter to optimize, the user has to define a  search parameter . There are two types of parameters available:   MathParameter  - basically what  GridSearch  uses, with an expression to calculate the actual value using the min, max and step parameters  ListParameter  - the blank-separated list of values is used as input for the optimization (useful, if values cannot be described by a mathematical function)   Here is a setup for finding the best ridge parameter (property  classifier.ridge ) using the  MathParameter  search parameter using values from 10^-10 to 10^5:  weka.classifiers.meta.MultiSearch \\\n  -E CC \\\n  -search \"weka.core.setupgenerator.MathParameter -property classifier.ridge -min -10.0 -max 5.0 -step 1.0 -base 10.0 -expression pow(BASE,I)\" \\\n  -sample-size 100.0 -initial-folds 2 -subsequent-folds 10 -num-slots 1 -S 1 \\\n  -W weka.classifiers.functions.LinearRegression -- -S 1 -C -R 1.0E-8  And here using the  ListParameter  search parameter for evaluating values 0.001, 0.05, 0.1, 0.5, 0.75 and 1.0 for the ridge parameter (property  classifier.ridge ):  weka.classifiers.meta.MultiSearch \\\n  -E CC \\\n  -search \"weka.core.setupgenerator.ListParameter -property classifier.ridge -list \\\"0.001 0.05 0.1 0.5 0.75 1.0\\\"\" \\\n  -sample-size 100.0 -initial-folds 2 -subsequent-folds 10 -num-slots 1 -S 1 \\\n  -W weka.classifiers.functions.LinearRegression -- -S 1 -C -R 1.0E-8  MultiSearch  can be optimized based on the following measures:   Correlation coefficient (= CC)  Root mean squared error (= RMSE)  Root relative squared error (= RRSE)  Mean absolute error (= MAE)  Root absolute error (= RAE)  Combined: \n(1-abs(CC)) + RRSE + RAE  Accuracy (= ACC)  Kappa (= KAP)",
            "title": "MultiSearch"
        },
        {
            "location": "/optimizing_parameters/#auto-weka",
            "text": "Auto-WEKA  is available as a package through the WEKA package manager. It provides the class  weka.classifiers.meta.AutoWEKAClassifier  and optimizes all parameters of all learners. It also automatically determines the best learner to use and the best attribute selection method for a given dataset. More information is available on the project website and the  manual .",
            "title": "Auto-WEKA"
        },
        {
            "location": "/optimizing_parameters/#downloads",
            "text": "CVParam.java  - optimizes J48's  -C  parameter",
            "title": "Downloads"
        },
        {
            "location": "/optimizing_parameters/#see-also",
            "text": "LibSVM  - you need additional jars in your  CLASSPATH  to be able to use LibSVM",
            "title": "See also"
        },
        {
            "location": "/optimizing_parameters/#links",
            "text": "gnuplot homepage  Java Beans  Introspection",
            "title": "Links"
        },
        {
            "location": "/paper_references/",
            "text": "Due to the introduction of the \nweka.core.TechnicalInformationHandler\n interface it is now easy to extract all the paper references via \nweka.core.ClassDiscovery\n and \nweka.core.TechnicalInformation\n.\n\n\nThe \nget_wekatechinfo.sh\n.\n\n\nTypical use (after an \nant exejar\n) for \nBibTeX\n:\n\n\n get_wekatechinfo.sh -d ../ -w ../dist/weka.jar -b > ../tech.txt\n\n\n\n\n(command is issued from the same directory the Weka \nbuild.xml\n is located in)\n\n\nLinks\n\n\n\n\nget_wekatechinfo.sh\n\n\nWriting a Classifier",
            "title": " Paper References"
        },
        {
            "location": "/paper_references/#links",
            "text": "get_wekatechinfo.sh  Writing a Classifier",
            "title": "Links"
        },
        {
            "location": "/performing_attribute_selection/",
            "text": "In Weka, you have three options of performing attribute selection from commandline (not everything is possible from the GUI):\n\n\n\n\nthe \nnative\n approach, using the attribute selection classes directly\n\n\nusing a \nmeta-classifier\n\n\nthe \nfilter\n approach\n\n\n\n\nNotes:\n\n\n\n\nThe commandlines outlined in this article are for a Linux/Unix bash (the backslash tells the shell that the command isn't finished yet and continues on the next line). In case of Windows or the SimpleCLI, just remove those backslashes and put everything on one line.\n\n\nThe Explorer in the developer version (>= 3.5.4) also outputs the commandline setups to its log. Just click on the \nLog\n button to display the log and copy/paste the commandlines (you will need to add the appropriate \njava\n call and dataset files, of course).\n\n\n\n\nNative\n\n\nUsing the attribute selection classes directly outputs some additional useful information, like number of subsets evaluated/best merit (for subset evaluators), ranked output with merit per attribute (for ranking based setups). The attribute selection classes are located in the following package:\n\n\n weka.attributeSelection\n\n\n\n\nExample using \nCfsSubsetEval\n and \nBestFirst\n:\n\n\n java weka.attributeSelection.CfsSubsetEval \\\n    -M \\\n    -s \"weka.attributeSelection.BestFirst -D 1 -N 5\" \\\n    -i <file.arff>\n\n\n\n\nMeta-classifier\n\n\nWeka also offers a meta-classifier that takes a search algorithm and evaluator next to the base classifier. This makes the attribute selection process completely transparent and the base classifier receives only the reduced dataset. This is the full classname of the meta-classifier:\n\n\n weka.classifiers.meta.AttributeSelectedClassifier\n\n\n\n\nExample using \nCfsSubsetEval\n and \nBestFirst\n:\n\n\n java weka.classifiers.meta.AttributeSelectedClassifier \\\n   -t <training.arff> \\\n   -E \"weka.attributeSelection.CfsSubsetEval -M\" \\\n   -S \"weka.attributeSelection.BestFirst -D 1 -N 5\" \\\n   -W weka.classifiers.trees.J48 \\\n   -- \\\n   -C 0.25 -M 2\n\n\n\n\nFilter\n\n\nIn case you want to obtain the reduced/ranked data and not just output the selected/ranked attributes or using it internally in a classifier, you can use the filter approach. The following filter offers attribute selection:\n\n\n  weka.filters.supervised.attribute.AttributeSelection\n\n\n\n\nExample using \nCfsSubsetEval\n and \nBestFirst\n in \nbatch mode\n:\n\n\n java weka.filters.supervised.attribute.AttributeSelection \\\n    -E \"weka.attributeSelection.CfsSubsetEval -M\" \\\n    -S \"weka.attributeSelection.BestFirst -D 1 -N 5\" \\\n    -b \\\n    -i <input1.arff> \\\n    -o <output1.arff> \\\n    -r <input2.arff> \\\n    -s <output2.arff>\n\n\n\n\nNote:\n batch mode is not available from the Explorer.\n\n\nSee also\n\n\n\n\nBatch filtering\n - general information about batch filtering\n\n\nUse Weka in your Java code\n, section \nAttribute selection\n - if you want to use attribute selection from your own code.",
            "title": " Performing attribute selection"
        },
        {
            "location": "/performing_attribute_selection/#native",
            "text": "Using the attribute selection classes directly outputs some additional useful information, like number of subsets evaluated/best merit (for subset evaluators), ranked output with merit per attribute (for ranking based setups). The attribute selection classes are located in the following package:   weka.attributeSelection  Example using  CfsSubsetEval  and  BestFirst :   java weka.attributeSelection.CfsSubsetEval \\\n    -M \\\n    -s \"weka.attributeSelection.BestFirst -D 1 -N 5\" \\\n    -i <file.arff>",
            "title": "Native"
        },
        {
            "location": "/performing_attribute_selection/#meta-classifier",
            "text": "Weka also offers a meta-classifier that takes a search algorithm and evaluator next to the base classifier. This makes the attribute selection process completely transparent and the base classifier receives only the reduced dataset. This is the full classname of the meta-classifier:   weka.classifiers.meta.AttributeSelectedClassifier  Example using  CfsSubsetEval  and  BestFirst :   java weka.classifiers.meta.AttributeSelectedClassifier \\\n   -t <training.arff> \\\n   -E \"weka.attributeSelection.CfsSubsetEval -M\" \\\n   -S \"weka.attributeSelection.BestFirst -D 1 -N 5\" \\\n   -W weka.classifiers.trees.J48 \\\n   -- \\\n   -C 0.25 -M 2",
            "title": "Meta-classifier"
        },
        {
            "location": "/performing_attribute_selection/#filter",
            "text": "In case you want to obtain the reduced/ranked data and not just output the selected/ranked attributes or using it internally in a classifier, you can use the filter approach. The following filter offers attribute selection:    weka.filters.supervised.attribute.AttributeSelection  Example using  CfsSubsetEval  and  BestFirst  in  batch mode :   java weka.filters.supervised.attribute.AttributeSelection \\\n    -E \"weka.attributeSelection.CfsSubsetEval -M\" \\\n    -S \"weka.attributeSelection.BestFirst -D 1 -N 5\" \\\n    -b \\\n    -i <input1.arff> \\\n    -o <output1.arff> \\\n    -r <input2.arff> \\\n    -s <output2.arff>  Note:  batch mode is not available from the Explorer.",
            "title": "Filter"
        },
        {
            "location": "/performing_attribute_selection/#see-also",
            "text": "Batch filtering  - general information about batch filtering  Use Weka in your Java code , section  Attribute selection  - if you want to use attribute selection from your own code.",
            "title": "See also"
        },
        {
            "location": "/piqle/",
            "text": "PIQLE\n is a set of Java classes for quickly experimenting reinforcement learning schemes. For understanding what the algorithms have learned, one can use Weka classification/clustering algorithms : \nthe (s,a,Q(s,a)) values can be formatted in \nARFF\n files.\n\n\nExternal links\n\n\n\n\nPIQLE homepage",
            "title": " Piqle"
        },
        {
            "location": "/piqle/#external-links",
            "text": "PIQLE homepage",
            "title": "External links"
        },
        {
            "location": "/plotting_error_rate_for_incremental_classifier/",
            "text": "The KnowledgeFlow enables one to plot the error rate (= RMSE, root mean squared error) and the accuracy of an incremental classifier. An \nincremental classifier\n is a classifier that does not need to see the whole data at once, but can be trained instance by instance. All classifiers implementing the interface \n[weka.classifiers.UpdateableClassifier](http://weka.sourceforge.net/doc.dev/weka/classifiers/updateableclassifier.html)\n are incremental ones.\n\n\nSetup\n\n\nThe most basic setup for an incremental classifier is show below, using the classifier \nNaiveBayesUpdateable\n:\n\n\n ArffLoader\n   --instance--> NaiveBayesUpdateable\n   --incrementalClassifier--> IncrementalClassifierEvaluator\n   --chart--> StripChart\n\n\n\n\nHere is a screenshot of the setup:\n\n\n\n\nYou can also download \nKnowledgeFlow-incremental_classifier.kfml\n, an XML version of this setup.\n\n\nDisplaying the chart\n\n\n\n\nselect a dataset that you want to train the classifier with, via \nConfigure...\n from the \nArffLoader's\n context menu, e.g., the UCI dataset \nanneal\n.\n\n\nselect \nShow chart\n from the \nStripChart's\n context menu (the chart will \nonly\n be updated if visible!)\n\n\nselect \nStart loading\n from the \nArffLoader's\n context menu.\n\n\nbring the chart back to front and you should get a graph similar to this one (click to enlarge):\n\n\n\n\n\n\nExporting the chart\n\n\nAs of 08/07/2008 (or Weka >3.5.7), the chart can be exported to several file formats using the developer version of Weka. The \nmagic\n key for bringing up the export dialog is \n<Ctrl+Alt+Shift><Left-Click>\n.\n\n\nSince the default black background is not very practical if one wants to embed the chart in a document, one can change the background color via the following property of the \nBeans.props\n \nproperties file\n (and set it to \nwhite\n):\n\n\n weka.gui.beans.StripChart.backgroundColour\n\n\n\n\nThe text color of the legend can be modified via the following property (and set it to \nblack\n):\n\n\n weka.gui.beans.StripChart$LegendPanel.borderColour\n\n\n\n\nNote:\n Due to the design of the StripChart (nested JPanels), the EPS export does not work properly. But one can always export it as PNG and then convert it under Linux via the \npngtopnm/pnmtops/ps2epsi\n chain. See commandline help of those tools for more details.\n\n\nSee also\n\n\n\n\nProperties File\n\n\nweka/gui/beans/Beans.props\n\n\n\n\nLinks\n\n\n\n\nKnowledgeFlow-incremental_classifier.kfml",
            "title": " Plotting error rate for incremental classifier"
        },
        {
            "location": "/plotting_error_rate_for_incremental_classifier/#setup",
            "text": "The most basic setup for an incremental classifier is show below, using the classifier  NaiveBayesUpdateable :   ArffLoader\n   --instance--> NaiveBayesUpdateable\n   --incrementalClassifier--> IncrementalClassifierEvaluator\n   --chart--> StripChart  Here is a screenshot of the setup:   You can also download  KnowledgeFlow-incremental_classifier.kfml , an XML version of this setup.",
            "title": "Setup"
        },
        {
            "location": "/plotting_error_rate_for_incremental_classifier/#displaying-the-chart",
            "text": "select a dataset that you want to train the classifier with, via  Configure...  from the  ArffLoader's  context menu, e.g., the UCI dataset  anneal .  select  Show chart  from the  StripChart's  context menu (the chart will  only  be updated if visible!)  select  Start loading  from the  ArffLoader's  context menu.  bring the chart back to front and you should get a graph similar to this one (click to enlarge):",
            "title": "Displaying the chart"
        },
        {
            "location": "/plotting_error_rate_for_incremental_classifier/#exporting-the-chart",
            "text": "As of 08/07/2008 (or Weka >3.5.7), the chart can be exported to several file formats using the developer version of Weka. The  magic  key for bringing up the export dialog is  <Ctrl+Alt+Shift><Left-Click> .  Since the default black background is not very practical if one wants to embed the chart in a document, one can change the background color via the following property of the  Beans.props   properties file  (and set it to  white ):   weka.gui.beans.StripChart.backgroundColour  The text color of the legend can be modified via the following property (and set it to  black ):   weka.gui.beans.StripChart$LegendPanel.borderColour  Note:  Due to the design of the StripChart (nested JPanels), the EPS export does not work properly. But one can always export it as PNG and then convert it under Linux via the  pngtopnm/pnmtops/ps2epsi  chain. See commandline help of those tools for more details.",
            "title": "Exporting the chart"
        },
        {
            "location": "/plotting_error_rate_for_incremental_classifier/#see-also",
            "text": "Properties File  weka/gui/beans/Beans.props",
            "title": "See also"
        },
        {
            "location": "/plotting_error_rate_for_incremental_classifier/#links",
            "text": "KnowledgeFlow-incremental_classifier.kfml",
            "title": "Links"
        },
        {
            "location": "/primer/",
            "text": "WEKA\n is a comprehensive workbench for machine learning and data mining. Its main strengths lie in the classification area, where many of the main machine learning approaches have been implemented within a clean, object-oriented Java class hierarchy. Regression, association rule mining, time series prediction, and clustering algorithms have also been implemented.\n\n\nThis document serves as a brief introduction to using WEKA from the command line interface. We will begin by describing \nbasic concepts\n and ideas. Then, we will describe the \nweka.filters\n package, which is used to transform input data, e.g., for preprocessing, transformation, feature generation and so on. Following that, we will consider some \nmachine learning algorithms\n that generate classification models. Afterwards, some practical examples are given.\n\n\nNote that, in the doc directory of the WEKA installation directory, you can find documentation of all Java classes in WEKA. Prepare to use it since this introduction is not intended to be complete. If you want to know exactly what is going on, take a look at the source code, which can be found in weka-src.jar and can be extracted via the jar utility from the Java Development Kit.\n\n\nBasic concepts\n\n\nDataset\n\n\nA set of data items, the dataset, is a very basic concept of machine learning. A dataset is roughly equivalent to a two-dimensional spreadsheet or database table. In WEKA, it is implemented by the \nInstances\n class. A dataset is a collection of examples, each one of class \nInstance\n. Each Instance consists of a number of attributes, any of which can be nominal (= one of a predefined list of values), numeric (= a real or integer number) or a string (= an arbitrary long list of characters, enclosed in \"double quotes\"). WEKA also supports date attributes and relational attributes. The external representation of an Instances class is an ARFF file, which consists of a header describing the attribute types and the data as comma-separated list. Here is a short, commented example. A complete description of the ARFF file format can be found \nhere\n.\n\n\n% This is a toy example, the UCI weather dataset.\n% Any relation to real weather is purely coincidental.\n\n\n\n\nComment lines at the beginning of the dataset should give an indication of its source, context and meaning.\n\n\n@relation golfWeatherMichigan_1988/02/10_14days\n\n\n\n\nHere we state the internal name of the dataset. Try to be as descriptive as possible.\n\n\n@attribute outlook {sunny, overcast rainy}\n@attribute windy {TRUE, FALSE}\n\n\n\n\nHere we define two nominal attributes, \noutlook\n and \nwindy\n. The former has three values: \nsunny\n, \novercast\n and \nrainy\n; the latter two: \n\nTRUE\n and \nFALSE\n. Nominal values with special characters, commas or spaces are enclosed in 'single quotes'.\n\n\n@attribute temperature numeric\n@attribute humidity numeric\n\n\n\n\nThese lines define two numeric attributes.\n\n\n@attribute play {yes, no}\n\n\n\n\nThe last attribute is the default target or class variable used for prediction. In our case it is a nominal attribute with two values, making this a binary classification problem.\n\n\n@data\nsunny,FALSE,85,85,no\nsunny,TRUE,80,90,no\novercast,FALSE,83,86,yes\nrainy,FALSE,70,96,yes\nrainy,FALSE,68,80,yes\n\n\n\n\nThe rest of the dataset consists of the token @data, followed by comma-separated values for the attributes -- one line per example. In our case there are five examples.\n\n\nSome basic statistics and validation of given ARFF files can be obtained via the main() routine of \nweka.core.Instances\n:\n\n\n java weka.core.Instances data/soybean.arff\n\n\n\n\nweka.core\n offers some other useful routines, e.g., \nconverters.C45Loader\n and \nconverters.CSVLoader\n, which can be used to convert C45 datasets and comma/tab-separated datasets respectively, e.g.:\n\n\n java weka.core.converters.CSVLoader data.csv > data.arff\n java weka.core.converters.C45Loader c45_filestem > data.arff\n\n\n\n\nClassifier\n\n\nAny classification or regression algorithm in WEKA is derived from the abstract \nClassifier\n class. Surprisingly little is needed for a basic classifier: \na routine which generates a classifier model from a training dataset (= \nbuildClassifier\n) and another routine which produces a classification for a given instance (= \nclassifyInstance\n), or generates a probability distribution for all classes of the instance (= \ndistributionForInstance\n).\n\n\nA classifier model is an arbitrary complex mapping from predictor attributes to the class attribute. The specific form and creation of this mapping, or model, differs from classifier to classifier. For example, \nZeroR's\n model just consists of a single value: \nthe most common class in the case of classification problems, or the median of all numeric values in case of predicting a numeric value (= regression learning). ZeroR is a trivial classifier, but it gives a lower bound on the performance of a given dataset that should be significantly improved by more complex classifiers. As such it is a reasonable test of how well the class can be predicted without considering the other attributes.\n\n\nLater\n, we will explain how to interpret the output from classifiers in detail -- for now just focus on the \nCorrectly Classified Instances\n in the section \nStratified cross-validation\n and notice how it improves from ZeroR to J48 when we use the soybean data:\n\n\n java weka.classifiers.rules.ZeroR -t soybean.arff\n java weka.classifiers.trees.J48 -t soybean.arff\n\n\n\n\nThere are various approaches to determine the performance of classifiers. It can most simply be measured by counting the proportion of correctly predicted examples in a test dataset. This value is the \nclassification\n \naccuracy\n, which is also \n1-ErrorRate\n. Both terms are used in literature.\n\n\nThe simplest case for evaluation is when we use a training set and a test set which are mutually independent. This is referred to as hold-out estimate. To estimate variance in these performance estimates, hold-out estimates may be computed by repeatedly by resampling the same dataset -- i.e., randomly shuffling it and then splitting it into training and test sets with a specific proportion of the examples, collecting all estimates on the test sets and computing average and standard deviation of accuracy.\n\n\nA more elaborate method is \nk\n-fold cross-validation. Here, a number of folds \nk\n is specified. The dataset is randomly shuffled and then split into \nk\n folds of equal size. In each iteration, one fold is used for testing and the other \nk-1\n folds are used for training the classifier. The test results are collected and pooled (or averaged) over all folds. This gives the cross-validation estimate of accuracy. The folds can be purely random or slightly modified to create the same class distributions in each fold as in the complete dataset. In the latter case the cross-validation is called \nstratified\n. Leave-one-out (loo) cross-validation signifies that \nk\n is equal to the number of examples. Out of necessity, loo cv has to be non-stratified, i.e., the class distributions in the test sets are not the same as those in the training data. Therefore loo CV can produce misleading results in rare cases. However it is still quite useful in dealing with small datasets since it utilizes the greatest amount of training data from the dataset.\n\n\nweka filters\n\n\nThe \nweka.filters\n package contains Java classes that transform datasets -- by removing or adding attributes, resampling the dataset, removing examples and so on. This package offers useful support for data preprocessing, which is an important step in machine learning.\n\n\nAll filters offer the command-line option \n-i\n for specifying the input dataset, and the option \n-o\n for specifying the output dataset. If any of these parameters is not given, this specifies standard input resp. output for use within pipes. Other parameters are specific to each filter and can be found out via -\nh\n, as with any other class. The weka.filters package is organized into supervised and unsupervised filtering, both of which are again subdivided into instance and attribute filtering. We will discuss each of the four subsection separately.\n\n\nweka.filters.supervised\n\n\nClasses below weka.filters.supervised in WEKA's Java class hierarchy are for supervised filtering, i.e., taking advantage of the class information. For those filters, a class must be assigned by providing the index of the class attribute via \n-c\n.\n\n\nattribute\n\n\nDiscretize\n is used to discretize numeric attributes into nominal ones, based on the class information, via Fayyad & Irani's MDL method, or optionally with Kononeko's MDL method. Some learning schemes or classifiers can only process nominal data, e.g., \nrules.Prism\n; and in some cases discretization may also reduce learning time and help combat overfitting.\n\n\n java weka.filters.supervised.attribute.Discretize -i data/iris.arff -o iris-nom.arff -c last\n java weka.filters.supervised.attribute.Discretize -i data/cpu.arff -o cpu-classvendor-nom.arff -c first\n\n\n\n\nNominalToBinary\n encodes all nominal attributes into binary (two-valued) attributes, which can be used to transform the dataset into a purely numeric representation, e.g., for visualization via multi-dimensional scaling.\n\n\n java weka.filters.supervised.attribute.NominalToBinary -i data/contact-lenses.arff -o contact-lenses-bin.arff -c last\n\n\n\n\nNote that most classifiers in WEKA utilize transformation filters internally, e.g., Logistic and SMO, so you may not have to use these filters explicity.\n\n\ninstance\n\n\nResample\n creates a stratified subsample of the given dataset. This means that overall class distributions are approximately retained within the sample. A bias towards uniform class distribution can be specified via -\nB\n.\n\n\n java weka.filters.supervised.instance.Resample -i data/soybean.arff -o soybean-5%.arff -c last -Z 5\n java weka.filters.supervised.instance.Resample -i data/soybean.arff -o soybean-uniform-5%.arff -c last -Z 5 -B 1\n\n\n\n\nStratifiedRemoveFolds\n creates stratified cross-validation folds of the given dataset. This means that per default the class distributions are approximately retained within each fold. The following example splits soybean.arff into stratified training and test datasets, the latter consisting of 25% (=1/4) of the data.\n\n\n java weka.filters.supervised.instance.StratifiedRemoveFolds -i data/soybean.arff -o soybean-train.arff \\\n   -c last -N 4 -F 1 -V\n java weka.filters.supervised.instance.StratifiedRemoveFolds -i data/soybean.arff -o soybean-test.arff \\\n   -c last -N 4 -F 1\n\n\n\n\nweka.filters.unsupervised\n\n\nClasses below \nweka.filters.unsupervised\n in WEKA's Java class hierarchy are for unsupervised filtering, e.g., the non-stratified version of Resample. A class should not be assigned here.\n\n\nattribute\n\n\nStringToWordVector\n transforms string attributes into a word vectors, e.g., creating one attribute for each word that either encodes presence or word count (\n-C\n) within the string. \n-W\n can be used to set an approximate limit on the number of words. When a class is assigned, the limit applies to each class separately. This filter is useful for text mining.\n\n\nObfuscate\n renames the dataset name, all attribute names and nominal attribute values. This is intended for exchanging sensitive datasets without giving away restricted information.\n\n\nRemove\n is intended for explicit deletion of attributes from a dataset, e.g. for removing attributes of the iris dataset:\n\n\n java weka.filters.unsupervised.attribute.Remove -R 1-2 -i data/iris.arff -o iris-simplified.arff\n java weka.filters.unsupervised.attribute.Remove -V -R 3-last -i data/iris.arff -o iris-simplified.arff\n\n\n\n\ninstance\n\n\nResample\n creates a non-stratified subsample of the given dataset. It performs random sampling without regard to the class information. Otherwise it is equivalent to its supervised variant.\n\n\n java weka.filters.unsupervised.instance.Resample -i data/soybean.arff -o soybean-5%.arff -Z 5\n\n\n\n\nRemoveFolds\n creates cross-validation folds of the given dataset. The class distributions are not retained. The following example splits soybean.arff into training and test datasets, the latter consisting of 25% (=1/4) of the data.\n\n\n java weka.filters.unsupervised.instance.RemoveFolds -i data/soybean.arff -o soybean-train.arff -c last -N 4 -F 1 -V\n java weka.filters.unsupervised.instance.RemoveFolds -i data/soybean.arff -o soybean-test.arff -c last -N 4 -F 1\n\n\n\n\nRemoveWithValues\n filters instances according to the value of an attribute.\n\n\n java weka.filters.unsupervised.instance.RemoveWithValues -i data/soybean.arff \\\n   -o soybean-without_herbicide_injury.arff -V -C last -L 19\n\n\n\n\nweka.classifiers\n\n\nClassifiers are at the core of WEKA. There are a lot of common options for classifiers, most of which are related to evaluation purposes. We will focus on the most important ones. All others including classifier-specific parameters can be found via -\nh\n, as usual.\n\n\n\n\n\n\n\n\nParameter\n\n\nDescription\n\n\n\n\n\n\n\n\n\n\n-t\n\n\nspecifies the training file (ARFF format)\n\n\n\n\n\n\n-T\n\n\nspecifies the test file in (ARFF format). If this parameter is missing, a crossvalidation will be performed (default: 10-fold cv)\n\n\n\n\n\n\n-x\n\n\nThis parameter determines the number of folds for the cross-validation. A cv will only be performed if -T is missing.\n\n\n\n\n\n\n-c\n\n\nAs we already know from the weka.filters section, this parameter sets the class variable with a one-based index.\n\n\n\n\n\n\n-d\n\n\nThe model after training can be saved via this parameter. Each classifier has a different binary format for the model, so it can only be read back by the ct same classifier on a compatible dataset. Only the model on the training set is saved, not the multiple models generated via cross-validation.\n\n\n\n\n\n\n-l\n\n\nLoads a previously saved model, usually for testing on new, previously unseen data. In that case, a compatible test file should be specified, i.e. the same ributes in the same order.\n\n\n\n\n\n\n-p \n\n\nIf a test file is specified, this parameter shows you the predictions and one attribute (0 for none) for all test instances.\n\n\n\n\n\n\n-o\n\n\nThis parameter switches the human-readable output of the model description off. In case of support vector machines or NaiveBayes, this makes some sense unless you want to parse and visualize a lot of information.\n\n\n\n\n\n\n\n\nWe now give a short list of selected classifiers in WEKA:\n\n\n\n\ntrees.J48\n A clone of the C4.5 decision tree learner\n\n\nbayes.NaiveBayes\n A Naive Bayesian learner. \n-K\n switches on kernel density estimation for numerical attributes which often improves performance.\n\n\nmeta.ClassificationViaRegression\n -W \nfunctions.LinearRegression\n Multi-response linear regression.\n\n\nfunctions.Logistic\n Logistic Regression.\n\n\nfunctions.SMO\n Support Vector Machine (linear, polynomial and RBF kernel) with Seuential Minimal Optimization Algorithm due to [Platt, 1998]. Defaults to SVM with linear kernel, \n-E 5 -C 10\n gives an SVM with polynomial kernel of degree 5 and lambda=10.\n\n\nlazy.KStar\n Instance-Based learner. \n-E\n sets the blend entropy automatically, which is usa`lly preferable.\n\n\nlazy.IBk\n Instance-Based learner with fixed neighborhood. \n-K\n sets the number of neighbors tou`se. \nIB1\n is equivalent to \nIBk -K 1\n\n\nrules.JRip\n A clone of the RIPPER rule learner.\n\n\n\n\nBased on a simple example, we will now explain the output of a typical classifier, \nweka.classifiers.trees.J48\n. Consider the following call from the command line, or start the WEKA explorer and train J48 on weather.numeric.arff:\n\n\n java weka.classifiers.trees.J48 -t data/weather.numeric.arff\n\n\n\n\nJ48 pruned tree\n------------------\n\noutlook = sunny\n|   humidity <= 75: \nyes (2.0)\n|   humidity > 75: \nno (3.0)\noutlook = overcast: \nyes (4.0)\noutlook = rainy\n|   windy = TRUE: \nno (2.0)\n|   windy = FALSE: \nyes (3.0)\n\nNumber of Leaves  : \n 5\n\nSize of the tree : \n 8\n\n\n\n\nThe first part, unless you specify \n-o\n, is a human-readable form of the training set model. In this case, it is a decision tree. \noutlook\n is at the root of the tree and determines the first decision. In case it is overcast, we'll always play golf. The numbers in (parentheses) at the end of each leaf tell us the number of examples in this leaf. If one or more leaves were not pure (= all of the same class), the number of misclassified examples would also be given, after a /slash/\n\n\nTime taken to build model: \n0.05 seconds\nTime taken to test model on training data: \n0 seconds\n\n\n\n\nAs you can see, a decision tree learns quite fast and is evaluated even faster.\n\n\n== Error on training data ==\n\nCorrectly Classified Instance      14              100      %\nIncorrectly Classified Instances    0                0      %\nKappa statistic                     1\nMean absolute error                 0\nRoot mean squared error             0\nRelative absolute error             0      %\nRoot relative squared error         0      %\nTotal Number of Instances          14\n\n== Detailed Accuracy By Class ==\n\nTP Rate   FP Rate   Precision   Recall  F-Measure   Class\n  1         0          1         1         1        yes\n  1         0          1         1         1        no\n\n== Confusion Matrix ==\n\n a b   <-- classified as\n 9 0 | a = yes\n 0 5 | b = no\n\n\n\n\nThis is quite boring: \nour classifier is perfect, at least on the training data -- all instances were classified correctly and all errors are zero. As is usually the case, the training set accuracy is too optimistic. The detailed accuracy by class and the confusion matrix is similarily trivial.\n\n\n== Stratified cross-validation ==\n\nCorrectly Classified Instances      9               64.2857 %\nIncorrectly Classified Instances    5               35.7143 %\nKappa statistic                     0.186\nMean absolute error                 0.2857\nRoot mean squared error             0.4818\nRelative absolute error            60      %\nRoot relative squared error        97.6586 %\nTotal Number of Instances          14\n\n\n== Detailed Accuracy By Class ==\n\nTP Rate   FP Rate   Precision   Recall  F-Measure   Class\n  0.778     0.6        0.7       0.778     0.737    yes\n  0.4       0.222      0.5       0.4       0.444    no\n\n\n== Confusion Matrix ==\n\n a b   <-- classified as\n 7 2 | a = yes\n 3 2 | b = no\n\n\n\n\nThe stratified cross-validation paints a more realistic picture. The accuracy is around 64%. The kappa statistic measures the agreement of prediction with the true class -- 1.0 signifies complete agreement. The error values that are shown, e.g., the root of the mean squared error, indicate the accuracy of the probability estimates that are generated by the classification model.\n\n\nThe confusion matrix is more commonly named \ncontingency table\n. In our case we have two classes, and therefore a 2x2 confusion matrix, the matrix could be arbitrarily large. The number of correctly classified instances is the sum of diagonals in the matrix; all others are incorrectly classified (class \"a\" gets misclassified as \"b\" exactly twice, and class \"b\" gets misclassified as \"a\" three times).\n\n\nThe \nTrue Positive (TP)\n rate is the proportion of examples which were classified as class \nx\n, among all examples which truly have class \nx\n, i.e., how much of the class was captured correctly. It is equivalent to \nRecall\n. In the confusion matrix, this is the diagonal element divided by the sum over the relevant row, i.e., 7/(7+2)=0.778 for class \nyes\n and 2/(3+2)=0.4 for class \nno\n in our example.\n\n\nThe \nFalse Positive (FP)\n rate is the proportion of examples which were classified as class \nx\n, but belong to a different class, among all examples which are not of class \nx\n. In the matrix, this is the column sum of class \nx\n minus the diagonal element, divided by the row sums of all other classes; i.e. 3/5=0.6 for class \nyes\n and 2/9=0.222 for class \nno\n.\n\n\nThe \nPrecision\n is the proportion of the examples which truly have class \nx\n among all those which were classified as class \nx\n. In the matrix, this is the diagonal element divided by the sum over the relevant column, i.e. 7/(7+3)=0.7 for class \nyes\n and 2/(2+2)=0.5 for class \nno\n.\n\n\nThe \nF-Measure\n is simply 2\nPrecision\nRecall/(Precision+Recall), a combined measure for precision and recall.\n\n\nThese measures are useful for comparing classifiers. However, if more detailed information about the classifier's predictions are necessary, \n-p #\n outputs just the predictions for each test instance, along with a range of one-based attribute ids (0 for none). Let's look at the following example. We shall assume soybean-train.arff and soybean-test.arff have been constructed via weka.filters.supervised.instance.StratifiedRemoveFolds as in a previous example.\n\n\n java weka.classifiers.bayes.NaiveBayes -K -t soybean-train.arff -T soybean-test.arff -p 0\n\n\n\n\n0 diaporthe-stem-canker 0.9999672587892333 diaporthe-stem-canker\n1 diaporthe-stem-canker 0.9999992614503429 diaporthe-stem-canker\n2 diaporthe-stem-canker 0.999998948559035 diaporthe-stem-canker\n3 diaporthe-stem-canker 0.9999998441238833 diaporthe-stem-canker\n4 diaporthe-stem-canker 0.9999989997681132 diaporthe-stem-canker\n5 rhizoctonia-root-rot 0.9999999395928124 rhizoctonia-root-rot\n6 rhizoctonia-root-rot 0.999998912860593 rhizoctonia-root-rot\n7 rhizoctonia-root-rot 0.9999994386283236 rhizoctonia-root-rot\n...\n\n\n\n\nThe values in each line are separated by a single space. The fields are the zero-based test instance id, followed by the predicted class value, the confidence for the prediction (estimated probability of predicted class), and the true class. All these are correctly classified, so let's look at a few erroneous ones.\n\n\n32 phyllosticta-leaf-spot 0.7789710144361445 brown-spot\n...\n39 alternarialeaf-spot 0.6403333824349896 brown-spot\n...\n44 phyllosticta-leaf-spot 0.893568420641914 brown-spot\n...\n46 alternarialeaf-spot 0.5788190397739439 brown-spot\n...\n73 brown-spot 0.4943768155314637 alternarialeaf-spot\n...\n\n\n\n\nIn each of these cases, a misclassification occurred, mostly between classes \nalternarialeaf-spot\n and \nbrown-spot\n. The confidences seem to be lower than for correct classification, so for a real-life application it may make sense to output \ndon't know\n below a certain threshold. WEKA also outputs a trailing newline.\n\n\nIf we had chosen a range of attributes via \n-p\n, e.g., \n-p first-last\n, the mentioned attributes would have been output afterwards as comma-separated values, in parantheses. However, the zero-based instance id in the first column offers a safer way to determine the test instances.\n\n\nUsually, if you evaluate a classifier for a longer experiment, you will do something like this (for csh):\n\n\n java -Xmx1024m weka.classifiers.trees.J48 -t data.arff -k -d J48-data.model >&! J48-data.out &\n\n\n\n\nThe -Xmx1024m parameter for maximum heap size enables the Java heap, where Java stores objects, to grow to a maximum size of 1024 Megabytes. There is no overhead involved, it just leaves more room for the heap to grow. The -\nk\n flag gives you some additional performance statistics. In case your model performs well, it makes sense to save it via \n-d\n - you can always delete it later! The implicit cross-validation gives a more reasonable estimate of the expected accuracy on unseen data than the training set accuracy. The output both of standard error and output should be redirected, so you get both errors and the normal output of your classifier. The last & starts the task in the background. Keep an eye on your task via \ntop\n and if you notice the hard disk works hard all the time (for linux), this probably means your task needs too much memory and will not finish in time for the exam. ;-) In that case, switch to a faster classifier or use \nfilters\n, e.g., for \nResample\n to reduce the size of your dataset or \nStratifiedRemoveFolds\n to create training and test sets - for most classifiers, training takes more time than testing.\n\n\nSo, now you have run a lot of experiments -- which classifier is best? Try\n\n\n cat *.out | grep -A 3 \"Stratified\" | grep \"^Correctly\"\n\n\n\n\n...this should give you all cross-validated accuracies. If the cross-validated accuracy is roughly the same as the training set accuracy, this indicates that your classifiers is presumably not overfitting the training set.\n\n\nAssume you have found the best classifier. To apply it on a new dataset, use something like\n\n\n java weka.classifiers.trees.J48 -l J48-data.model -T new-data.arff\n\n\n\n\nYou will have to use the same classifier to load the model, but you need not set any options. Just add the new test file via \n-T\n. If you want, \n-p first-last\n will output all test instances with classifications and confidence scores, followed by all attribute values, so you can look at each error separately.\n\n\nThe following more complex csh script creates datasets for learning curves, creating a 75% training set and 25% test set from a given dataset, then successively reducing the test set by factor 1.2 (83%), until it is also 25% in size. All this is repeated thirty times, with different random reorderings (-\nS\n) and the results are written to different directories. The Experimenter GUI in WEKA can be used to design and run similar experiments.\n\n\n#!/bin/csh\nforeach f ($*)\n  set run=1\n  while ( $run <= 30 )\n    mkdir $run >&! /dev/null\n    java weka.filters.supervised.instance.StratifiedRemoveFolds -N 4 -F 1 -S $run -c last -i ../$f -o $run/t_$f\n    java weka.filters.supervised.instance.StratifiedRemoveFolds -N 4 -F 1 -S $run -V -c last -i ../$f -o $run/t0$f\n    foreach nr (0 1 2 3 4 5)\n      set nrp1=$nr\n      @ nrp1++\n      java weka.filters.supervised.instance.Resample -S 0 -Z 83 -c last -i $run/t$nr$f -o $run/t$nrp1$f\n    end\n\n    echo Run $run of $f done.\n    @ run++\n  end\nend\n\n\n\n\nIf meta classifiers are used, i.e. classifiers whose options include classifier specifications - for example, \nStackingC\n or \nClassificationViaRegression\n, care must be taken not to mix the parameters. For example,\n\n\n java weka.classifiers.meta.ClassificationViaRegression -W weka.classifiers.functions.LinearRegression -S 1 \\\n   -t data/iris.arff -x 2\n\n\n\n\ngives us an illegal options exception for \n-S 1\n. This parameter is meant for LinearRegression, not for ClassificationViaRegression, but WEKA does not know this by itself. One way to clarify this situation is to enclose the classifier specification, including all parameters, in \"double\" quotes, like this:\n\n\n java weka.classifiers.meta.ClassificationViaRegression -W \"weka.classifiers.functions.LinearRegression -S 1\" \\\n   -t data/iris.arff -x 2\n\n\n\n\nHowever this does not always work, depending on how the option handling was implemented in the top-level classifier. While for Stacking this approach would work quite well, for ClassificationViaRegression it does not. We get the dubious error message that the class \nweka.classifiers.functions.LinearRegression -S 1\n cannot be found.\nFortunately, there is another approach: \nAll parameters given after -- are processed by the first sub-classifier; another -- lets us specify parameters for the second sub-classifier and so on.\n\n\n java weka.classifiers.meta.ClassificationViaRegression -W weka.classifiers.functions.LinearRegression \\\n   -t data/iris.arff -x 2 -- -S 1\n\n\n\n\nIn some cases, both approaches have to be mixed, for example:\n\n\n java weka.classifiers.meta.Stacking -B \"weka.classifiers.lazy.IBk -K 10\" \\\n   -M \"weka.classifiers.meta.ClassificationViaRegression -W weka.classifiers.functions.LinearRegression -- -S 1\" \\\n   -t data/iris.arff -x 2\n\n\n\n\nNotice that while ClassificationViaRegression honors the -- parameter, Stacking itself does not.",
            "title": " Primer"
        },
        {
            "location": "/primer/#basic-concepts",
            "text": "",
            "title": "Basic concepts"
        },
        {
            "location": "/primer/#dataset",
            "text": "A set of data items, the dataset, is a very basic concept of machine learning. A dataset is roughly equivalent to a two-dimensional spreadsheet or database table. In WEKA, it is implemented by the  Instances  class. A dataset is a collection of examples, each one of class  Instance . Each Instance consists of a number of attributes, any of which can be nominal (= one of a predefined list of values), numeric (= a real or integer number) or a string (= an arbitrary long list of characters, enclosed in \"double quotes\"). WEKA also supports date attributes and relational attributes. The external representation of an Instances class is an ARFF file, which consists of a header describing the attribute types and the data as comma-separated list. Here is a short, commented example. A complete description of the ARFF file format can be found  here .  % This is a toy example, the UCI weather dataset.\n% Any relation to real weather is purely coincidental.  Comment lines at the beginning of the dataset should give an indication of its source, context and meaning.  @relation golfWeatherMichigan_1988/02/10_14days  Here we state the internal name of the dataset. Try to be as descriptive as possible.  @attribute outlook {sunny, overcast rainy}\n@attribute windy {TRUE, FALSE}  Here we define two nominal attributes,  outlook  and  windy . The former has three values:  sunny ,  overcast  and  rainy ; the latter two:  TRUE  and  FALSE . Nominal values with special characters, commas or spaces are enclosed in 'single quotes'.  @attribute temperature numeric\n@attribute humidity numeric  These lines define two numeric attributes.  @attribute play {yes, no}  The last attribute is the default target or class variable used for prediction. In our case it is a nominal attribute with two values, making this a binary classification problem.  @data\nsunny,FALSE,85,85,no\nsunny,TRUE,80,90,no\novercast,FALSE,83,86,yes\nrainy,FALSE,70,96,yes\nrainy,FALSE,68,80,yes  The rest of the dataset consists of the token @data, followed by comma-separated values for the attributes -- one line per example. In our case there are five examples.  Some basic statistics and validation of given ARFF files can be obtained via the main() routine of  weka.core.Instances :   java weka.core.Instances data/soybean.arff  weka.core  offers some other useful routines, e.g.,  converters.C45Loader  and  converters.CSVLoader , which can be used to convert C45 datasets and comma/tab-separated datasets respectively, e.g.:   java weka.core.converters.CSVLoader data.csv > data.arff\n java weka.core.converters.C45Loader c45_filestem > data.arff",
            "title": "Dataset"
        },
        {
            "location": "/primer/#classifier",
            "text": "Any classification or regression algorithm in WEKA is derived from the abstract  Classifier  class. Surprisingly little is needed for a basic classifier: \na routine which generates a classifier model from a training dataset (=  buildClassifier ) and another routine which produces a classification for a given instance (=  classifyInstance ), or generates a probability distribution for all classes of the instance (=  distributionForInstance ).  A classifier model is an arbitrary complex mapping from predictor attributes to the class attribute. The specific form and creation of this mapping, or model, differs from classifier to classifier. For example,  ZeroR's  model just consists of a single value: \nthe most common class in the case of classification problems, or the median of all numeric values in case of predicting a numeric value (= regression learning). ZeroR is a trivial classifier, but it gives a lower bound on the performance of a given dataset that should be significantly improved by more complex classifiers. As such it is a reasonable test of how well the class can be predicted without considering the other attributes.  Later , we will explain how to interpret the output from classifiers in detail -- for now just focus on the  Correctly Classified Instances  in the section  Stratified cross-validation  and notice how it improves from ZeroR to J48 when we use the soybean data:   java weka.classifiers.rules.ZeroR -t soybean.arff\n java weka.classifiers.trees.J48 -t soybean.arff  There are various approaches to determine the performance of classifiers. It can most simply be measured by counting the proportion of correctly predicted examples in a test dataset. This value is the  classification   accuracy , which is also  1-ErrorRate . Both terms are used in literature.  The simplest case for evaluation is when we use a training set and a test set which are mutually independent. This is referred to as hold-out estimate. To estimate variance in these performance estimates, hold-out estimates may be computed by repeatedly by resampling the same dataset -- i.e., randomly shuffling it and then splitting it into training and test sets with a specific proportion of the examples, collecting all estimates on the test sets and computing average and standard deviation of accuracy.  A more elaborate method is  k -fold cross-validation. Here, a number of folds  k  is specified. The dataset is randomly shuffled and then split into  k  folds of equal size. In each iteration, one fold is used for testing and the other  k-1  folds are used for training the classifier. The test results are collected and pooled (or averaged) over all folds. This gives the cross-validation estimate of accuracy. The folds can be purely random or slightly modified to create the same class distributions in each fold as in the complete dataset. In the latter case the cross-validation is called  stratified . Leave-one-out (loo) cross-validation signifies that  k  is equal to the number of examples. Out of necessity, loo cv has to be non-stratified, i.e., the class distributions in the test sets are not the same as those in the training data. Therefore loo CV can produce misleading results in rare cases. However it is still quite useful in dealing with small datasets since it utilizes the greatest amount of training data from the dataset.",
            "title": "Classifier"
        },
        {
            "location": "/primer/#weka-filters",
            "text": "The  weka.filters  package contains Java classes that transform datasets -- by removing or adding attributes, resampling the dataset, removing examples and so on. This package offers useful support for data preprocessing, which is an important step in machine learning.  All filters offer the command-line option  -i  for specifying the input dataset, and the option  -o  for specifying the output dataset. If any of these parameters is not given, this specifies standard input resp. output for use within pipes. Other parameters are specific to each filter and can be found out via - h , as with any other class. The weka.filters package is organized into supervised and unsupervised filtering, both of which are again subdivided into instance and attribute filtering. We will discuss each of the four subsection separately.",
            "title": "weka filters"
        },
        {
            "location": "/primer/#wekafilterssupervised",
            "text": "Classes below weka.filters.supervised in WEKA's Java class hierarchy are for supervised filtering, i.e., taking advantage of the class information. For those filters, a class must be assigned by providing the index of the class attribute via  -c .",
            "title": "weka.filters.supervised"
        },
        {
            "location": "/primer/#attribute",
            "text": "Discretize  is used to discretize numeric attributes into nominal ones, based on the class information, via Fayyad & Irani's MDL method, or optionally with Kononeko's MDL method. Some learning schemes or classifiers can only process nominal data, e.g.,  rules.Prism ; and in some cases discretization may also reduce learning time and help combat overfitting.   java weka.filters.supervised.attribute.Discretize -i data/iris.arff -o iris-nom.arff -c last\n java weka.filters.supervised.attribute.Discretize -i data/cpu.arff -o cpu-classvendor-nom.arff -c first  NominalToBinary  encodes all nominal attributes into binary (two-valued) attributes, which can be used to transform the dataset into a purely numeric representation, e.g., for visualization via multi-dimensional scaling.   java weka.filters.supervised.attribute.NominalToBinary -i data/contact-lenses.arff -o contact-lenses-bin.arff -c last  Note that most classifiers in WEKA utilize transformation filters internally, e.g., Logistic and SMO, so you may not have to use these filters explicity.",
            "title": "attribute"
        },
        {
            "location": "/primer/#instance",
            "text": "Resample  creates a stratified subsample of the given dataset. This means that overall class distributions are approximately retained within the sample. A bias towards uniform class distribution can be specified via - B .   java weka.filters.supervised.instance.Resample -i data/soybean.arff -o soybean-5%.arff -c last -Z 5\n java weka.filters.supervised.instance.Resample -i data/soybean.arff -o soybean-uniform-5%.arff -c last -Z 5 -B 1  StratifiedRemoveFolds  creates stratified cross-validation folds of the given dataset. This means that per default the class distributions are approximately retained within each fold. The following example splits soybean.arff into stratified training and test datasets, the latter consisting of 25% (=1/4) of the data.   java weka.filters.supervised.instance.StratifiedRemoveFolds -i data/soybean.arff -o soybean-train.arff \\\n   -c last -N 4 -F 1 -V\n java weka.filters.supervised.instance.StratifiedRemoveFolds -i data/soybean.arff -o soybean-test.arff \\\n   -c last -N 4 -F 1",
            "title": "instance"
        },
        {
            "location": "/primer/#wekafiltersunsupervised",
            "text": "Classes below  weka.filters.unsupervised  in WEKA's Java class hierarchy are for unsupervised filtering, e.g., the non-stratified version of Resample. A class should not be assigned here.",
            "title": "weka.filters.unsupervised"
        },
        {
            "location": "/primer/#attribute_1",
            "text": "StringToWordVector  transforms string attributes into a word vectors, e.g., creating one attribute for each word that either encodes presence or word count ( -C ) within the string.  -W  can be used to set an approximate limit on the number of words. When a class is assigned, the limit applies to each class separately. This filter is useful for text mining.  Obfuscate  renames the dataset name, all attribute names and nominal attribute values. This is intended for exchanging sensitive datasets without giving away restricted information.  Remove  is intended for explicit deletion of attributes from a dataset, e.g. for removing attributes of the iris dataset:   java weka.filters.unsupervised.attribute.Remove -R 1-2 -i data/iris.arff -o iris-simplified.arff\n java weka.filters.unsupervised.attribute.Remove -V -R 3-last -i data/iris.arff -o iris-simplified.arff",
            "title": "attribute"
        },
        {
            "location": "/primer/#instance_1",
            "text": "Resample  creates a non-stratified subsample of the given dataset. It performs random sampling without regard to the class information. Otherwise it is equivalent to its supervised variant.   java weka.filters.unsupervised.instance.Resample -i data/soybean.arff -o soybean-5%.arff -Z 5  RemoveFolds  creates cross-validation folds of the given dataset. The class distributions are not retained. The following example splits soybean.arff into training and test datasets, the latter consisting of 25% (=1/4) of the data.   java weka.filters.unsupervised.instance.RemoveFolds -i data/soybean.arff -o soybean-train.arff -c last -N 4 -F 1 -V\n java weka.filters.unsupervised.instance.RemoveFolds -i data/soybean.arff -o soybean-test.arff -c last -N 4 -F 1  RemoveWithValues  filters instances according to the value of an attribute.   java weka.filters.unsupervised.instance.RemoveWithValues -i data/soybean.arff \\\n   -o soybean-without_herbicide_injury.arff -V -C last -L 19",
            "title": "instance"
        },
        {
            "location": "/primer/#wekaclassifiers",
            "text": "Classifiers are at the core of WEKA. There are a lot of common options for classifiers, most of which are related to evaluation purposes. We will focus on the most important ones. All others including classifier-specific parameters can be found via - h , as usual.     Parameter  Description      -t  specifies the training file (ARFF format)    -T  specifies the test file in (ARFF format). If this parameter is missing, a crossvalidation will be performed (default: 10-fold cv)    -x  This parameter determines the number of folds for the cross-validation. A cv will only be performed if -T is missing.    -c  As we already know from the weka.filters section, this parameter sets the class variable with a one-based index.    -d  The model after training can be saved via this parameter. Each classifier has a different binary format for the model, so it can only be read back by the ct same classifier on a compatible dataset. Only the model on the training set is saved, not the multiple models generated via cross-validation.    -l  Loads a previously saved model, usually for testing on new, previously unseen data. In that case, a compatible test file should be specified, i.e. the same ributes in the same order.    -p   If a test file is specified, this parameter shows you the predictions and one attribute (0 for none) for all test instances.    -o  This parameter switches the human-readable output of the model description off. In case of support vector machines or NaiveBayes, this makes some sense unless you want to parse and visualize a lot of information.     We now give a short list of selected classifiers in WEKA:   trees.J48  A clone of the C4.5 decision tree learner  bayes.NaiveBayes  A Naive Bayesian learner.  -K  switches on kernel density estimation for numerical attributes which often improves performance.  meta.ClassificationViaRegression  -W  functions.LinearRegression  Multi-response linear regression.  functions.Logistic  Logistic Regression.  functions.SMO  Support Vector Machine (linear, polynomial and RBF kernel) with Seuential Minimal Optimization Algorithm due to [Platt, 1998]. Defaults to SVM with linear kernel,  -E 5 -C 10  gives an SVM with polynomial kernel of degree 5 and lambda=10.  lazy.KStar  Instance-Based learner.  -E  sets the blend entropy automatically, which is usa`lly preferable.  lazy.IBk  Instance-Based learner with fixed neighborhood.  -K  sets the number of neighbors tou`se.  IB1  is equivalent to  IBk -K 1  rules.JRip  A clone of the RIPPER rule learner.   Based on a simple example, we will now explain the output of a typical classifier,  weka.classifiers.trees.J48 . Consider the following call from the command line, or start the WEKA explorer and train J48 on weather.numeric.arff:   java weka.classifiers.trees.J48 -t data/weather.numeric.arff  J48 pruned tree\n------------------\n\noutlook = sunny\n|   humidity <= 75: \nyes (2.0)\n|   humidity > 75: \nno (3.0)\noutlook = overcast: \nyes (4.0)\noutlook = rainy\n|   windy = TRUE: \nno (2.0)\n|   windy = FALSE: \nyes (3.0)\n\nNumber of Leaves  : \n 5\n\nSize of the tree : \n 8  The first part, unless you specify  -o , is a human-readable form of the training set model. In this case, it is a decision tree.  outlook  is at the root of the tree and determines the first decision. In case it is overcast, we'll always play golf. The numbers in (parentheses) at the end of each leaf tell us the number of examples in this leaf. If one or more leaves were not pure (= all of the same class), the number of misclassified examples would also be given, after a /slash/  Time taken to build model: \n0.05 seconds\nTime taken to test model on training data: \n0 seconds  As you can see, a decision tree learns quite fast and is evaluated even faster.  == Error on training data ==\n\nCorrectly Classified Instance      14              100      %\nIncorrectly Classified Instances    0                0      %\nKappa statistic                     1\nMean absolute error                 0\nRoot mean squared error             0\nRelative absolute error             0      %\nRoot relative squared error         0      %\nTotal Number of Instances          14\n\n== Detailed Accuracy By Class ==\n\nTP Rate   FP Rate   Precision   Recall  F-Measure   Class\n  1         0          1         1         1        yes\n  1         0          1         1         1        no\n\n== Confusion Matrix ==\n\n a b   <-- classified as\n 9 0 | a = yes\n 0 5 | b = no  This is quite boring: \nour classifier is perfect, at least on the training data -- all instances were classified correctly and all errors are zero. As is usually the case, the training set accuracy is too optimistic. The detailed accuracy by class and the confusion matrix is similarily trivial.  == Stratified cross-validation ==\n\nCorrectly Classified Instances      9               64.2857 %\nIncorrectly Classified Instances    5               35.7143 %\nKappa statistic                     0.186\nMean absolute error                 0.2857\nRoot mean squared error             0.4818\nRelative absolute error            60      %\nRoot relative squared error        97.6586 %\nTotal Number of Instances          14\n\n\n== Detailed Accuracy By Class ==\n\nTP Rate   FP Rate   Precision   Recall  F-Measure   Class\n  0.778     0.6        0.7       0.778     0.737    yes\n  0.4       0.222      0.5       0.4       0.444    no\n\n\n== Confusion Matrix ==\n\n a b   <-- classified as\n 7 2 | a = yes\n 3 2 | b = no  The stratified cross-validation paints a more realistic picture. The accuracy is around 64%. The kappa statistic measures the agreement of prediction with the true class -- 1.0 signifies complete agreement. The error values that are shown, e.g., the root of the mean squared error, indicate the accuracy of the probability estimates that are generated by the classification model.  The confusion matrix is more commonly named  contingency table . In our case we have two classes, and therefore a 2x2 confusion matrix, the matrix could be arbitrarily large. The number of correctly classified instances is the sum of diagonals in the matrix; all others are incorrectly classified (class \"a\" gets misclassified as \"b\" exactly twice, and class \"b\" gets misclassified as \"a\" three times).  The  True Positive (TP)  rate is the proportion of examples which were classified as class  x , among all examples which truly have class  x , i.e., how much of the class was captured correctly. It is equivalent to  Recall . In the confusion matrix, this is the diagonal element divided by the sum over the relevant row, i.e., 7/(7+2)=0.778 for class  yes  and 2/(3+2)=0.4 for class  no  in our example.  The  False Positive (FP)  rate is the proportion of examples which were classified as class  x , but belong to a different class, among all examples which are not of class  x . In the matrix, this is the column sum of class  x  minus the diagonal element, divided by the row sums of all other classes; i.e. 3/5=0.6 for class  yes  and 2/9=0.222 for class  no .  The  Precision  is the proportion of the examples which truly have class  x  among all those which were classified as class  x . In the matrix, this is the diagonal element divided by the sum over the relevant column, i.e. 7/(7+3)=0.7 for class  yes  and 2/(2+2)=0.5 for class  no .  The  F-Measure  is simply 2 Precision Recall/(Precision+Recall), a combined measure for precision and recall.  These measures are useful for comparing classifiers. However, if more detailed information about the classifier's predictions are necessary,  -p #  outputs just the predictions for each test instance, along with a range of one-based attribute ids (0 for none). Let's look at the following example. We shall assume soybean-train.arff and soybean-test.arff have been constructed via weka.filters.supervised.instance.StratifiedRemoveFolds as in a previous example.   java weka.classifiers.bayes.NaiveBayes -K -t soybean-train.arff -T soybean-test.arff -p 0  0 diaporthe-stem-canker 0.9999672587892333 diaporthe-stem-canker\n1 diaporthe-stem-canker 0.9999992614503429 diaporthe-stem-canker\n2 diaporthe-stem-canker 0.999998948559035 diaporthe-stem-canker\n3 diaporthe-stem-canker 0.9999998441238833 diaporthe-stem-canker\n4 diaporthe-stem-canker 0.9999989997681132 diaporthe-stem-canker\n5 rhizoctonia-root-rot 0.9999999395928124 rhizoctonia-root-rot\n6 rhizoctonia-root-rot 0.999998912860593 rhizoctonia-root-rot\n7 rhizoctonia-root-rot 0.9999994386283236 rhizoctonia-root-rot\n...  The values in each line are separated by a single space. The fields are the zero-based test instance id, followed by the predicted class value, the confidence for the prediction (estimated probability of predicted class), and the true class. All these are correctly classified, so let's look at a few erroneous ones.  32 phyllosticta-leaf-spot 0.7789710144361445 brown-spot\n...\n39 alternarialeaf-spot 0.6403333824349896 brown-spot\n...\n44 phyllosticta-leaf-spot 0.893568420641914 brown-spot\n...\n46 alternarialeaf-spot 0.5788190397739439 brown-spot\n...\n73 brown-spot 0.4943768155314637 alternarialeaf-spot\n...  In each of these cases, a misclassification occurred, mostly between classes  alternarialeaf-spot  and  brown-spot . The confidences seem to be lower than for correct classification, so for a real-life application it may make sense to output  don't know  below a certain threshold. WEKA also outputs a trailing newline.  If we had chosen a range of attributes via  -p , e.g.,  -p first-last , the mentioned attributes would have been output afterwards as comma-separated values, in parantheses. However, the zero-based instance id in the first column offers a safer way to determine the test instances.  Usually, if you evaluate a classifier for a longer experiment, you will do something like this (for csh):   java -Xmx1024m weka.classifiers.trees.J48 -t data.arff -k -d J48-data.model >&! J48-data.out &  The -Xmx1024m parameter for maximum heap size enables the Java heap, where Java stores objects, to grow to a maximum size of 1024 Megabytes. There is no overhead involved, it just leaves more room for the heap to grow. The - k  flag gives you some additional performance statistics. In case your model performs well, it makes sense to save it via  -d  - you can always delete it later! The implicit cross-validation gives a more reasonable estimate of the expected accuracy on unseen data than the training set accuracy. The output both of standard error and output should be redirected, so you get both errors and the normal output of your classifier. The last & starts the task in the background. Keep an eye on your task via  top  and if you notice the hard disk works hard all the time (for linux), this probably means your task needs too much memory and will not finish in time for the exam. ;-) In that case, switch to a faster classifier or use  filters , e.g., for  Resample  to reduce the size of your dataset or  StratifiedRemoveFolds  to create training and test sets - for most classifiers, training takes more time than testing.  So, now you have run a lot of experiments -- which classifier is best? Try   cat *.out | grep -A 3 \"Stratified\" | grep \"^Correctly\"  ...this should give you all cross-validated accuracies. If the cross-validated accuracy is roughly the same as the training set accuracy, this indicates that your classifiers is presumably not overfitting the training set.  Assume you have found the best classifier. To apply it on a new dataset, use something like   java weka.classifiers.trees.J48 -l J48-data.model -T new-data.arff  You will have to use the same classifier to load the model, but you need not set any options. Just add the new test file via  -T . If you want,  -p first-last  will output all test instances with classifications and confidence scores, followed by all attribute values, so you can look at each error separately.  The following more complex csh script creates datasets for learning curves, creating a 75% training set and 25% test set from a given dataset, then successively reducing the test set by factor 1.2 (83%), until it is also 25% in size. All this is repeated thirty times, with different random reorderings (- S ) and the results are written to different directories. The Experimenter GUI in WEKA can be used to design and run similar experiments.  #!/bin/csh\nforeach f ($*)\n  set run=1\n  while ( $run <= 30 )\n    mkdir $run >&! /dev/null\n    java weka.filters.supervised.instance.StratifiedRemoveFolds -N 4 -F 1 -S $run -c last -i ../$f -o $run/t_$f\n    java weka.filters.supervised.instance.StratifiedRemoveFolds -N 4 -F 1 -S $run -V -c last -i ../$f -o $run/t0$f\n    foreach nr (0 1 2 3 4 5)\n      set nrp1=$nr\n      @ nrp1++\n      java weka.filters.supervised.instance.Resample -S 0 -Z 83 -c last -i $run/t$nr$f -o $run/t$nrp1$f\n    end\n\n    echo Run $run of $f done.\n    @ run++\n  end\nend  If meta classifiers are used, i.e. classifiers whose options include classifier specifications - for example,  StackingC  or  ClassificationViaRegression , care must be taken not to mix the parameters. For example,   java weka.classifiers.meta.ClassificationViaRegression -W weka.classifiers.functions.LinearRegression -S 1 \\\n   -t data/iris.arff -x 2  gives us an illegal options exception for  -S 1 . This parameter is meant for LinearRegression, not for ClassificationViaRegression, but WEKA does not know this by itself. One way to clarify this situation is to enclose the classifier specification, including all parameters, in \"double\" quotes, like this:   java weka.classifiers.meta.ClassificationViaRegression -W \"weka.classifiers.functions.LinearRegression -S 1\" \\\n   -t data/iris.arff -x 2  However this does not always work, depending on how the option handling was implemented in the top-level classifier. While for Stacking this approach would work quite well, for ClassificationViaRegression it does not. We get the dubious error message that the class  weka.classifiers.functions.LinearRegression -S 1  cannot be found.\nFortunately, there is another approach: \nAll parameters given after -- are processed by the first sub-classifier; another -- lets us specify parameters for the second sub-classifier and so on.   java weka.classifiers.meta.ClassificationViaRegression -W weka.classifiers.functions.LinearRegression \\\n   -t data/iris.arff -x 2 -- -S 1  In some cases, both approaches have to be mixed, for example:   java weka.classifiers.meta.Stacking -B \"weka.classifiers.lazy.IBk -K 10\" \\\n   -M \"weka.classifiers.meta.ClassificationViaRegression -W weka.classifiers.functions.LinearRegression -- -S 1\" \\\n   -t data/iris.arff -x 2  Notice that while ClassificationViaRegression honors the -- parameter, Stacking itself does not.",
            "title": "weka.classifiers"
        },
        {
            "location": "/prompt/",
            "text": "PROMPT\n is a open source platform independent system for retrieval, analysis, mapping and comparison of protein sets. PROMPT's focus lies on statistical testing; Data in WEKA's \nARFF\n format can be imported and other data formats can be exported to WEKA. It thus complements the machine learning techniques of the WEKA workbench with statistical significance tests.\n\n\nExternal links\n\n\n\n\nPROMPT Website",
            "title": " PROMPT"
        },
        {
            "location": "/prompt/#external-links",
            "text": "PROMPT Website",
            "title": "External links"
        },
        {
            "location": "/properties_file/",
            "text": "General\n\n\nA properties file is a simple text file with this structure:\n\n\n\n\n<key>=<value>\n\n\n\n\nNotes:\n\n\n\n\nComments start with the hash sign \n#\n.\n\n\nBackslashes within values need to be doubled (the backslashes get interpreted already when a property is read).\n\n\n\n\nTo make a rather long property line more readable, one can use a backslash to continue on the next line. The Filter property, e.g., looks like this:\n\n\nweka.filters.Filter= \\\n> weka.filters.supervised.attribute, \\\n> weka.filters.supervised.instance, \\\n> weka.filters.unsupervised.attribute, \\\n> weka.filters.unsupervised.instance\n\n\n\n\nPrecedence\n\n\nThe Weka property files (extension \n.props\n) are searched for in the following order:\n\n\n\n\ncurrent directory\n\n\n(< Weka 3.7.2) the user's home directory (see FAQ \nWhere is my home directory located?\n for more information)\n\n\n(>= Weka 3.7.2) $WEKA_HOME/props (the default value for WEKA_HOME is user's home directory/wekafiles).\n\n\nthe class path (normally the \nweka.jar\n file) \n\n\n\n\nIf WEKA encounters those files it only supplements the properties, never overrides them. In other words, a property in the property file of the current directory has a higher precedence than the one in the user's home directory.\n\n\nNote:\n Under \nCywgin\n, the home directory is still the Windows one, since the java installation will be still one for Windows.\n\n\nHow to modify a .props file?\n\n\nIt is quite possible, that the default setup of WEKA is not to your liking and that you want to tweak it a little bit. The use of \n.props\n files instead of hard-coding makes it quite easy to modify WEKA's behavior. As example, we are modifying the background color of the 2D plots in the Explorer, changing it to \ndark gray\n. The responsible \n.props\n file is \nweka/gui/visualize/Visualize.props\n.\nThese are the necessary steps:\n\n\n\n\nclose WEKA\n\n\nextract the \n.props\n file from the \nweka.jar\n, using an archive manager that can handle ZIP files (e.g., \n7-Zip\n under Windows)\n\n\nplace this \n.props\n file in your home directory (see FAQ \nWhere is my home directory located?\n on how to determine your home directory), or for Weka 3.7.2 or higher place this \n.props\n file in $WEKA_HOME/props (the default value of WEKA_HOME is user's home directory/wekafiles)\n\n\nopen this \n.props\n with a text editor (\nNB:\n Notepad under Windows might not handle the Unix line-endings correctly!)\n\n\nnavigate to the property \nweka.gui.visualize.Plot2D.backgroundColour\n and change the color after the equal sign (\"=\") to \ndarkGray\n (the article about \nweka/gui/visualize/Visualize.props\n lists all possible colors)\n\n\nsave the file and restart WEKA\n\n\n\n\nNotes\n\n\n\n\nEscaping\n\n\nBackslashes in values need to be escaped (i.e., doubled), otherwise they get interpreted as character sequence.\nE.g., \"is\\this\" will be interpreted as \"is\nhis\". Correctly escaped, this would read as \"is\\this\".\n\n\n\n\n\n\n\n\nSee also\n\n\nFurther information about specific props files:\n\n\n\n\nweka/core/Capabilities.props\n\n\nweka/core/logging/Logging.props\n\n\nweka/experiment/DatabaseUtils.props\n\n\nweka/gui/GenericObjectEditor.props\n\n\nweka/gui/GUIEditors.props\n\n\nweka/gui/GenericPropertiesCreator.props\n\n\nweka/gui/GenericPropertiesCreator.excludes\n\n\nweka/gui/LookAndFeel.props\n\n\nweka/gui/MemoryUsage.props\n\n\nweka/gui/SimpleCLI.props\n\n\nweka/gui/beans/Beans.props\n\n\nweka/gui/experiment/Experimenter.props\n\n\nweka/gui/explorer/Explorer.props\n\n\nweka/gui/scripting/Groovy.props\n\n\nweka/gui/scripting/Jython.props\n\n\nweka/gui/treevisualizer/TreeVisualizer.props\n\n\nweka/gui/visualize/Visualize.props",
            "title": " Properties file"
        },
        {
            "location": "/properties_file/#general",
            "text": "A properties file is a simple text file with this structure:   <key>=<value>   Notes:   Comments start with the hash sign  # .  Backslashes within values need to be doubled (the backslashes get interpreted already when a property is read).   To make a rather long property line more readable, one can use a backslash to continue on the next line. The Filter property, e.g., looks like this:  weka.filters.Filter= \\\n> weka.filters.supervised.attribute, \\\n> weka.filters.supervised.instance, \\\n> weka.filters.unsupervised.attribute, \\\n> weka.filters.unsupervised.instance",
            "title": "General"
        },
        {
            "location": "/properties_file/#precedence",
            "text": "The Weka property files (extension  .props ) are searched for in the following order:   current directory  (< Weka 3.7.2) the user's home directory (see FAQ  Where is my home directory located?  for more information)  (>= Weka 3.7.2) $WEKA_HOME/props (the default value for WEKA_HOME is user's home directory/wekafiles).  the class path (normally the  weka.jar  file)    If WEKA encounters those files it only supplements the properties, never overrides them. In other words, a property in the property file of the current directory has a higher precedence than the one in the user's home directory.  Note:  Under  Cywgin , the home directory is still the Windows one, since the java installation will be still one for Windows.",
            "title": "Precedence"
        },
        {
            "location": "/properties_file/#how-to-modify-a-props-file",
            "text": "It is quite possible, that the default setup of WEKA is not to your liking and that you want to tweak it a little bit. The use of  .props  files instead of hard-coding makes it quite easy to modify WEKA's behavior. As example, we are modifying the background color of the 2D plots in the Explorer, changing it to  dark gray . The responsible  .props  file is  weka/gui/visualize/Visualize.props .\nThese are the necessary steps:   close WEKA  extract the  .props  file from the  weka.jar , using an archive manager that can handle ZIP files (e.g.,  7-Zip  under Windows)  place this  .props  file in your home directory (see FAQ  Where is my home directory located?  on how to determine your home directory), or for Weka 3.7.2 or higher place this  .props  file in $WEKA_HOME/props (the default value of WEKA_HOME is user's home directory/wekafiles)  open this  .props  with a text editor ( NB:  Notepad under Windows might not handle the Unix line-endings correctly!)  navigate to the property  weka.gui.visualize.Plot2D.backgroundColour  and change the color after the equal sign (\"=\") to  darkGray  (the article about  weka/gui/visualize/Visualize.props  lists all possible colors)  save the file and restart WEKA",
            "title": "How to modify a .props file?"
        },
        {
            "location": "/properties_file/#notes",
            "text": "Escaping  Backslashes in values need to be escaped (i.e., doubled), otherwise they get interpreted as character sequence.\nE.g., \"is\\this\" will be interpreted as \"is his\". Correctly escaped, this would read as \"is\\this\".",
            "title": "Notes"
        },
        {
            "location": "/properties_file/#see-also",
            "text": "Further information about specific props files:   weka/core/Capabilities.props  weka/core/logging/Logging.props  weka/experiment/DatabaseUtils.props  weka/gui/GenericObjectEditor.props  weka/gui/GUIEditors.props  weka/gui/GenericPropertiesCreator.props  weka/gui/GenericPropertiesCreator.excludes  weka/gui/LookAndFeel.props  weka/gui/MemoryUsage.props  weka/gui/SimpleCLI.props  weka/gui/beans/Beans.props  weka/gui/experiment/Experimenter.props  weka/gui/explorer/Explorer.props  weka/gui/scripting/Groovy.props  weka/gui/scripting/Jython.props  weka/gui/treevisualizer/TreeVisualizer.props  weka/gui/visualize/Visualize.props",
            "title": "See also"
        },
        {
            "location": "/props_file/",
            "text": "see \nProperties file",
            "title": " Props file"
        },
        {
            "location": "/related_projects/",
            "text": "There are many software projects that are related to Weka because they use it in some form. An incomplete list can be found below. Perhaps particularly noteworthy are RWeka, which provides an interface to Weka from R, python-weka-wrapper, which provides a wrapper for using Weka from Python, and ADAMS, which provides a workflow environment integrating Weka.\n\n\nYou may also want to take a look at the (incomplete) list of \n\"unofficial\" packages for WEKA 3.7\n.\n\n\nActive links\n\n\n\n\nADAMS\n - \nA\ndvanced \nD\nata mining \nA\nnd \nM\nachine learning \nS\nystem offers all of WEKA's functionality (and lots more) in its workflow engine.\n\n\nAgent Academy\n - Java integrated development framework for creating Intelligent Agents and Multi Agent Systems\n\n\nAuto-WEKA\n - Automatic parameter tuning and algorithm selection for Weka.\n\n\nBalie*\n - BAseLine Information Extraction.\n\n\nBayesian Network Classifiers\n - with bindings for Weka.\n\n\nBioWeka\n - Knowledge discovery and data analysis for biologists.\n\n\nC4.5Rule-PANE\n and \nNeC4.5\n\n\nCahit Arf\n - a data extraction utility for Weka.\n\n\nContrast Mining\n - Mining the interesting differences between pre-defined data groups.\n\n\nCost-sensitive classifiers\n - Adaboost extensions for cost-sensitive classification.\n\n\nDataconda\n - Builds a flat table (and an .arff file) from a relational database.\n\n\nDebellor\n - Data mining platform for data streams.\n\n\nDecisionTemplate\n - Combining classifiers using Decision Templates.\n\n\ndistributedWekaSpark\n - A proof of concept for running Weka in Apache Spark.\n\n\nELKI\n - Similar project, with focus on clustering and outlier detection, as well as on using database index structures for acceleration.\n\n\nFuzzyweka\n - Classifier for fuzzy classification based on fuzzy if-then rules.\n\n\nGATE\n - NLP workbench with Weka interface.\n\n\nGeneticProgramming\n - Genetic Programming Classifier for Weka\n\n\nGraph RAT\n - A framework for combining graph and non-graph algorithms.\n\n\nGroovyLab\n - Groovy based Matlab-like interface to Weka's algorithms.\n\n\nhttp://sourceforge.net/projects/wekainterfacetranslator/\n - A translation tool to facilitate the creation of message.properties files for Weka 3.6. Courtesy of the University of Jordan.\n\n\nJava Framework to crate ARFF from JPA Entity\n - Use the JPA Entities to create automatically you ARFF file.\n\n\nKea\n - automatic keyphrase extraction.\n\n\nKeplerWeka\n - Weka module for the \nKepler\n workflow environment.\n\n\nLearning Vector Quanization\n - and more with Weka.\n\n\nMatlab Weka Interface\n - A module for using Weka from Matlab.\n\n\nMayday\n - Machine Learning for Microarrays - plugin for the WEKA machine Learning Library.\n\n\nMeka\n - A multi-label extension for Weka.\n\n\nMilk\n - a workbench for multi-instance learning.\n\n\nMOA\n - Massive online analysis for data streams.\n\n\nModified version\n of Weka, including time series mining and visualization tools.\n\n\nMulan\n - Multi-label classification.\n\n\nPattern Miner\n - Integrated Pattern Management (extraction, storage, retrieval and comparison of data mining patterns)\n\n\npHMM4weka\n - Profile Hidden Markov Models for Weka.\n\n\nPROMPT\n - Statistical comparison and mapping of protein sets. Import/Export of WEKA arff data files.\n\n\npython-arff\n - Pure Python library for parsing and writing ARFF files.\n\n\npython-weka-wrapper\n - Wrapper for conveniently using Weka from Python.\n\n\nRWeka\n - an R interface to Weka.\n\n\nScalaLab\n - Scala based Matlab-like interface to Weka's algorithms.\n\n\nSemi-Supervised and Collective Classification\n using Weka.\n\n\nSpectral clustering\n by Luigi Dragone.\n\n\nStarSystem\n command-line tool implementing best practices in supervised classification, including \"agnostic\" feature selection.\n\n\nTClass\n - classifying multivariate time series.\n\n\nTertius\n - a system for rule discovery.\n\n\nTUBE\n - Tree-based Density Estimation Algorithms.\n\n\nTunedIT\n - Automated tests of machine-learning algorithms. Repository of datasets, algorithms and benchmarks.\n\n\nWeka for Computational Genetics\n - Multifactor Dimensionality Reduction (MDR) added to the Weka package.\n\n\nWeka Proper\n- Database propositionalization for Weka.\n\n\nweka4WS\n - distributed data mining.\n\n\nWeka-GDPM and Weka-STPM\n - Weka for geographic data processing and Moving Object Data Analysis and Mining.\n\n\nWekaMetal\n - a meta-learning extension to Weka.\n\n\nWeka-Parallel\n - parallel processing for Weka.\n\n\nWord sense disambiguation\n by Ted Pedersen.\n\n\nx2arff\n - A simple VB application to convert data stored in excel files into Attribute-Relation File Format.\n\n\nYALE\n - Yet Another Learning Environment.\n\n\n\n\nDead links (information potentially accessible through Internet Archive)\n\n\n\n\nBenchMarking Via Weka\n - Programming-language agnostic experimental framework.\n\n\nCMAC\n - The Cerebellar Model Articulation Controller\n\n\nCVS to ARFF converter\n - an online tool for the conversion from CSV files to ARFF files\n\n\nEpitopes Toolkit (EpiT)\n - A platform for developing epitope prediction tools.\n\n\nFAEHIM\n - Data Mining Web services.\n\n\nFastKMeans\n - a faster version of k-means clustering (.zip file).\n\n\nFuzzy algorithms\n - for clustering and classification.\n\n\nGRB Tool Shed\n - a tool to aid gamma ray burst research.\n\n\nGrid Weka\n - grid computing with Weka.\n\n\nInstance-based Classifiers\n - a collection of Instance-based Classifiers.\n\n\nJudge\n - software for document classification and clustering.\n\n\nKDDML-MQL\n - support for KDD process.\n\n\nLocBoost\n classification demo applet.\n\n\nMARFF\n - extension of ARFF for Multi-Relational Applications.\n\n\nMathematica interface\n for Weka.\n\n\nmaxdView\n visualisation tool for microarray data.\n\n\nMobile Data Mining\n (in portuguese) - more in \nthis post\n.\n\n\nOlexSuite\n - Text classification methods.\n\n\nOpenSubspace\n - Subspace-clustering for Weka.\n\n\nRarff\n - A Ruby library for manipulating ARFF files.\n\n\nRSW\n - sequential classification with Weka.\n\n\nTagHelper Tools\n - a tool for analysis of conversational data.\n\n\nWeka on Text\n - software for text mining.\n\n\nWeka Visualization tools\n - using PMML, VisWiz, and ROCOn.\n\n\nWeka-GDPM\n - extended version of Weka 3.4 to support automatic geographic data preprocessing for spatial data mining.\n\n\nFaster \nimplementation\n of a NeuralNet with backpropagation (only nominal classes).",
            "title": " Related Projects"
        },
        {
            "location": "/related_projects/#active-links",
            "text": "ADAMS  -  A dvanced  D ata mining  A nd  M achine learning  S ystem offers all of WEKA's functionality (and lots more) in its workflow engine.  Agent Academy  - Java integrated development framework for creating Intelligent Agents and Multi Agent Systems  Auto-WEKA  - Automatic parameter tuning and algorithm selection for Weka.  Balie*  - BAseLine Information Extraction.  Bayesian Network Classifiers  - with bindings for Weka.  BioWeka  - Knowledge discovery and data analysis for biologists.  C4.5Rule-PANE  and  NeC4.5  Cahit Arf  - a data extraction utility for Weka.  Contrast Mining  - Mining the interesting differences between pre-defined data groups.  Cost-sensitive classifiers  - Adaboost extensions for cost-sensitive classification.  Dataconda  - Builds a flat table (and an .arff file) from a relational database.  Debellor  - Data mining platform for data streams.  DecisionTemplate  - Combining classifiers using Decision Templates.  distributedWekaSpark  - A proof of concept for running Weka in Apache Spark.  ELKI  - Similar project, with focus on clustering and outlier detection, as well as on using database index structures for acceleration.  Fuzzyweka  - Classifier for fuzzy classification based on fuzzy if-then rules.  GATE  - NLP workbench with Weka interface.  GeneticProgramming  - Genetic Programming Classifier for Weka  Graph RAT  - A framework for combining graph and non-graph algorithms.  GroovyLab  - Groovy based Matlab-like interface to Weka's algorithms.  http://sourceforge.net/projects/wekainterfacetranslator/  - A translation tool to facilitate the creation of message.properties files for Weka 3.6. Courtesy of the University of Jordan.  Java Framework to crate ARFF from JPA Entity  - Use the JPA Entities to create automatically you ARFF file.  Kea  - automatic keyphrase extraction.  KeplerWeka  - Weka module for the  Kepler  workflow environment.  Learning Vector Quanization  - and more with Weka.  Matlab Weka Interface  - A module for using Weka from Matlab.  Mayday  - Machine Learning for Microarrays - plugin for the WEKA machine Learning Library.  Meka  - A multi-label extension for Weka.  Milk  - a workbench for multi-instance learning.  MOA  - Massive online analysis for data streams.  Modified version  of Weka, including time series mining and visualization tools.  Mulan  - Multi-label classification.  Pattern Miner  - Integrated Pattern Management (extraction, storage, retrieval and comparison of data mining patterns)  pHMM4weka  - Profile Hidden Markov Models for Weka.  PROMPT  - Statistical comparison and mapping of protein sets. Import/Export of WEKA arff data files.  python-arff  - Pure Python library for parsing and writing ARFF files.  python-weka-wrapper  - Wrapper for conveniently using Weka from Python.  RWeka  - an R interface to Weka.  ScalaLab  - Scala based Matlab-like interface to Weka's algorithms.  Semi-Supervised and Collective Classification  using Weka.  Spectral clustering  by Luigi Dragone.  StarSystem  command-line tool implementing best practices in supervised classification, including \"agnostic\" feature selection.  TClass  - classifying multivariate time series.  Tertius  - a system for rule discovery.  TUBE  - Tree-based Density Estimation Algorithms.  TunedIT  - Automated tests of machine-learning algorithms. Repository of datasets, algorithms and benchmarks.  Weka for Computational Genetics  - Multifactor Dimensionality Reduction (MDR) added to the Weka package.  Weka Proper - Database propositionalization for Weka.  weka4WS  - distributed data mining.  Weka-GDPM and Weka-STPM  - Weka for geographic data processing and Moving Object Data Analysis and Mining.  WekaMetal  - a meta-learning extension to Weka.  Weka-Parallel  - parallel processing for Weka.  Word sense disambiguation  by Ted Pedersen.  x2arff  - A simple VB application to convert data stored in excel files into Attribute-Relation File Format.  YALE  - Yet Another Learning Environment.",
            "title": "Active links"
        },
        {
            "location": "/related_projects/#dead-links-information-potentially-accessible-through-internet-archive",
            "text": "BenchMarking Via Weka  - Programming-language agnostic experimental framework.  CMAC  - The Cerebellar Model Articulation Controller  CVS to ARFF converter  - an online tool for the conversion from CSV files to ARFF files  Epitopes Toolkit (EpiT)  - A platform for developing epitope prediction tools.  FAEHIM  - Data Mining Web services.  FastKMeans  - a faster version of k-means clustering (.zip file).  Fuzzy algorithms  - for clustering and classification.  GRB Tool Shed  - a tool to aid gamma ray burst research.  Grid Weka  - grid computing with Weka.  Instance-based Classifiers  - a collection of Instance-based Classifiers.  Judge  - software for document classification and clustering.  KDDML-MQL  - support for KDD process.  LocBoost  classification demo applet.  MARFF  - extension of ARFF for Multi-Relational Applications.  Mathematica interface  for Weka.  maxdView  visualisation tool for microarray data.  Mobile Data Mining  (in portuguese) - more in  this post .  OlexSuite  - Text classification methods.  OpenSubspace  - Subspace-clustering for Weka.  Rarff  - A Ruby library for manipulating ARFF files.  RSW  - sequential classification with Weka.  TagHelper Tools  - a tool for analysis of conversational data.  Weka on Text  - software for text mining.  Weka Visualization tools  - using PMML, VisWiz, and ROCOn.  Weka-GDPM  - extended version of Weka 3.4 to support automatic geographic data preprocessing for spatial data mining.  Faster  implementation  of a NeuralNet with backpropagation (only nominal classes).",
            "title": "Dead links (information potentially accessible through Internet Archive)"
        },
        {
            "location": "/related_research_groups/",
            "text": "Gregory Piatetsky-Shapiro's \nKnowledge Discovery Mine\n\n\nJournal of Data Mining and Knowledge Discovery\n\n\nMultivariate Data Analysis Software and Resources\n\n\n\n\nMachine Learning\n\n\n\n\nMLC++, A Machine Learning Library in C++\n\n\nUCI - Machine Learning information\n, software and databases\n\n\nUTCS Machine Learning Research Group\n\n\nMachine Learning Journal\n\n\n\n\nOther Sites\n\n\n\n\nAAAI Home Page\n\n\nSRI Artificial Intelligence Center\n\n\nClassification Society of North America\n\n\nMIT Artificial Intelligence Laboratory\n\n\nSIGART Electronic Information Service\n\n\nStatLib Index\n\n\n\n\nDead Links\n\n\n\n\nMicrosoft Belief Networks Tools",
            "title": " Related Research Groups"
        },
        {
            "location": "/related_research_groups/#machine-learning",
            "text": "MLC++, A Machine Learning Library in C++  UCI - Machine Learning information , software and databases  UTCS Machine Learning Research Group  Machine Learning Journal",
            "title": "Machine Learning"
        },
        {
            "location": "/related_research_groups/#other-sites",
            "text": "AAAI Home Page  SRI Artificial Intelligence Center  Classification Society of North America  MIT Artificial Intelligence Laboratory  SIGART Electronic Information Service  StatLib Index",
            "title": "Other Sites"
        },
        {
            "location": "/related_research_groups/#dead-links",
            "text": "Microsoft Belief Networks Tools",
            "title": "Dead Links"
        },
        {
            "location": "/remote_experiment/",
            "text": "Remote experiments enable you to distribute the computing load across multiple computers. In the following we will discuss the setup and operation for \nHSQLDB\n and \nMySQL\n.\n\n\nPreparation\n\n\nTo run a remote experiment you will need:\n\n\n\n\nA database server.\n\n\nA number of computers to run remote engines on.\n\n\nTo edit the remote engine policy file included in the Weka distribution to allow class and dataset loading from your home directory.\n\n\nAn invocation of the Experimenter on a machine somewhere (any will do).\n\n\n\n\nFor the following examples, we assume a user called \njohndoe\n with this setup:\n\n\n\n\nAccess to a set of computers running a flavour of Unix (pathnames need to be changed for Windows).\n\n\nThe home directory is located at \n/home/johndoe\n.\n\n\nWeka is found in \n/home/johndoe/weka\n.\n\n\nAdditional jar archives, i.e., JDBC drivers, are stored in \n/home/johndoe/jars\n.\n\n\nThe directory for the datasets is \n/home/johndoe/datasets\n.\n\n\n\n\nNote:\n The example policy file \nremote.policy.example\n is using this setup (available in \nweka/experiment\n). \n\n\nDatabase Server Setup\n\n\n\n\nHSQLDB\n\n Download the JDBC driver for HSQLDB, extract the \nhsqldb.jar\n and place it in the directory \n/home/johndoe/jars\n.\n\n To set up the database server, choose or create a directory to run the database server from, and start the server with:\n\n\n\n\n java -classpath /home/johndoe/jars/hsqldb.jar \\\n   org.hsqldb.Server -database.0 experiment -dbname.0 experiment\n\n\n\n\n\n\nNote:\n This will start up a database with the alias \nexperiment\n (\n-dbname.0 <alias>\n) and create a properties and a log file at the current location prefixed with \nexperiment\n (\n-database.0 <file>\n).\n\n\n\n\n\n\nMySQL\n\n\nWe won't go into details of setting up a MySQL server, but this is rather straightforward and includes the following steps:\n\n\n\n\n\n\n\n\n Download a suitable version of MySQL for your server machine.\n\n Start the MySQL server.\n\n Create a database - for our example we will use experiment as database name.\n\n Download the appropriate JDBC driver, extract the JDBC jar and place it as \nmysql.jar\n in \n/home/johndoe/jars\n.\n\n\nRemote Engine Setup\n\n\n\n\nFirst, set up a directory for scripts and policy files:\n\n\n\n\n /home/johndoe/remote_engine\n\n\n\n\n\n\nUnzip the \nremoteExperimentServer.jar\n (from the Weka distribution; or build it from the sources with \nant remotejar\n) into a temporary directory.\n\n\nNext, copy the remoteEngine.jar to the \n/home/johndoe/remote_engine\n directory.\n\n\nCreate a script, called \n/home/johndoe/remote_engine/startRemoteEngine\n, with the following content (don't forget to make it executable with \nchmod a+x startRemoteEngine\n):\n\n\n\n\n\n\n\n\nHSQLDB\n\n\n\n\n\n\n java -Xmx256m \\\n   -classpath /home/johndoe/jars/hsqldb.jar:remoteEngine.jar \\\n   -Djava.security.policy=remote.policy \\\n   weka.experiment.RemoteEngine &\n\n\n\n\n\n\n\n\nMySQL\n\n\n\n\n\n\n java -Xmx256m \\\n   -classpath /home/johndoe/jars/mysql.jar:remoteEngine.jar \\\n   -Djava.security.policy=remote.policy \\\n   weka.experiment.RemoteEngine &\n\n\n\n\n\n\n\n\nFrom Weka 3.7.2 you will need to include the core weka.jar file in the classpath for the RemoteEngine. Assuming that the weka.jar file has been copied to \n/home/johndoe/remote_engine\n:\n\n\n\n\n\n\njava -Xmx256m \\\n  -classpath /home/johndoe/jars/hsqldb.jar:remoteEngine.jar:weka.jar \\\n  -Djava.security.policy=remote.policy \\\n  weka.experiment.RemoteEngine &\n\n\n\n\n\n\n\n\nNow we will start the remote engines (note that the same version of Java must be used for the Experimenter and remote engines) :\n\n\n\n\nCopy the \nremote.policy.example\n file to \n/home/johndoe/remote_engine\n as \nremote.policy\n.\n\n\n\n\nFor each machine you want to run an engine on:\n\n\n\n\nssh\n to the machine.\n\n\ncd\n to \n/home/johndoe/remote_engine\n.\n\n\nRun \n/home/johndoe/startRemoteEngine\n (to enable the remote engines to use more memory, modify the \n-Xmx\n option in the \nstartRemoteEngine\n script) .\n\n\n\n\n\n\n\n\n\n\n\n\nConfiguring the Experimenter\n\n\nNow we will run the Experimenter:\n\n\n\n\nHSQLDB\n\n\nCopy the \nDatabaseUtils.props.hsql\n file to the \n/home/johndoe/remote_engine\n directory and rename it to \nDatabaseUtils.props\n - a copy comes with your Weka distribution in \nweka/experiment\n.\n\n\nEdit this file and change the \"\njdbcURL=jdbc:hsqldb:hsql://server_name/database_name\n\" entry to include the name of the machine that is running your database server (e.g., \njdbcURL=jdbc:hsqldb:hsql://dodo.company.com/experiment\n).\n\n\nNow start the experimenter (inside this directory):\n\n\n\n\n\n\n\n\n java \\\n   -cp /home/johndoe/jars/hsqldb.jar:remoteEngine.jar:/home/johndoe/weka/weka.jar \\\n   -Djava.rmi.server.codebase=file:/home/johndoe/weka/weka.jar \\\n   weka.gui.experiment.Experimenter\n\n\n\n\n\n\nMySQL\n\n\nCopy the \nDatabaseUtils.props.mysql\n file to the \n/home/johndoe/remote_engine\n directory and rename it to \nDatabaseUtils.props\n - a copy comes with your Weka distribution in \nweka/experiment\n.\n\n\nEdit this file and change the \"\njdbcURL=jdbc:mysql://server_name:3306/database_name\n\" entry to include the name of the machine that is running your database server and the name of the database the result will be stored in (e.g., \njdbcURL=jdbc:mysql://dodo.company.com:3306/experiment\n).\n\n\nNow start the experimenter (inside this directory):\n\n\n\n\n\n\n\n\n java \\\n   -cp /home/johndoe/jars/mysql.jar:remoteEngine.jar:/home/johndoe/weka/weka.jar \\\n   -Djava.rmi.server.codebase=file:/home/johndoe/weka/weka.jar \\\n   weka.gui.experiment.Experimenter\n\n\n\n\n\n\nNote:\n the database name experiment can be still modified in the Experimenter, this is just the default setup. \n\n\n\n\nNow we will configure the experiment:\n\n\n\n\nFirst of all select the \nAdvanced\n mode in the Setup tab\n\n\n\n\nNow choose the \nDatabaseResultListener\n in the \nDestination\n panel. Configure this result producer:\n\n\n\n\nHSQLDB\n\n\n\n\nSupply the value \nsa\n for the username and leave the password empty.\n\n\n\n\n\n\n\n\nMySQL\n\n\n\n\nProvide username and password that you need for connecting to the database.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the Result generator panel choose either the \nCrossValidationResultProducer\n or the \nRandomSplitResultProducer\n (these are the most commonly used ones) and then configure the remaining experiment details (e.g., datasets and classifiers).\n\n\n\n\nNow enable the \nDistribute Experiment\n panel by checking the tick box.\n\n\nClick on the \nHosts\n button and enter the names of the machines that you started remote engines on (\n<Enter>\n adds the host to the list).\n\n\nYou can choose to distribute by run or dataset (try to get a balance).\n\n\nSave your experiment configuration.\n\n\nNow start your experiment as you would do normally.\n\n\nCheck your results in the \nAnalyse\n tab by clicking either the \nDatabase\n or \nExperiment\n buttons.\n\n\n\n\nMulti-core support\n\n\nIf you want to utilize all the cores on a multi-core machine, then you can do so with Weka version 3.6.x and developer versions later than 3.5.7. All you have to do, is define the port alongside the hostname in the Experimenter (format: \n\nhostname:port\n) and then start the \nRemoteEngine\n with the \n-p\n option, specifying the port to listen on.\nSee also \nthis\n post on the \nWekalist\n.\n\n\nTroubleshooting\n\n\n\n\nIf you get an error at the start of an experiment that looks a bit like this:\n\n\n\n\n{{01:13:19: RemoteExperiment (//blabla.company.com/RemoteEngine) (sub)experiment (datataset vineyard.arff) failed : java.sql.SQLException: Table already exists: EXPERIMENT_INDEX in statement [CREATE TABLE Experiment_index ( Experiment_type LONGVARCHAR, Experiment_setup LONGVARCHAR, Result_table INT )]\n\n01:13:19: dataset :vineyard.arff RemoteExperiment\n\n       (//blabla.company.com/RemoteEngine) (sub)experiment (datataset vineyard.arff) failed : java.sql.SQLException: Table already exists: EXPERIMENT_INDEX in statement [CREATE TABLE Experiment_index ( Experiment_type LONGVARCHAR, Experiment_setup LONGVARCHAR, Result_table INT )]. Scheduling for execution on another host.}}\n\n\n\n\n\n\nthen do not panic - this happens because multiple remote machines are trying to create the same table and are temporarily locked out - this will resolve itself so just leave your experiment running - in fact, it is a sign that the experiment is working!\n\n\n\n\n\n\nIf you serialized an experiment and then modify your \nDatabaseUtils.props\n file due to an error (e.g., a missing type-mapping), the Experimenter will use the \nDatabaseUtils.props\n you had at the time you serialized the experiment. Keep in mind that the serialization process also serializes the DatabaseUtils class and therefore stored your props-file! This is another reason for storing your experiments as XML and not in the properietary binary format the Java serialization produces.\n\n\nUsing a corrupt or incomplete \nDatabaseUtils.props\n file can cause peculiar interface errors, for example disabling the use of the \nUser\n button alongside the database URL. If in doubt copy a clean \nDatabaseUtils.props\n from \nSubversion\n.\n\n\nIf you get \nNullPointerException at java.util.Hashtable.get()\n in the Remote Engine do not be alarmed. This will have no effect on the results of your experiment.\n\n\n\n\nLinks\n\n\n\n\nDatabases\n\n\nweka/experiment/DatabaseUtils.props\n\n\nSubversion\n\n\nHSQLDB\n\n\nMySQL",
            "title": " Remote Experiment"
        },
        {
            "location": "/remote_experiment/#preparation",
            "text": "To run a remote experiment you will need:   A database server.  A number of computers to run remote engines on.  To edit the remote engine policy file included in the Weka distribution to allow class and dataset loading from your home directory.  An invocation of the Experimenter on a machine somewhere (any will do).   For the following examples, we assume a user called  johndoe  with this setup:   Access to a set of computers running a flavour of Unix (pathnames need to be changed for Windows).  The home directory is located at  /home/johndoe .  Weka is found in  /home/johndoe/weka .  Additional jar archives, i.e., JDBC drivers, are stored in  /home/johndoe/jars .  The directory for the datasets is  /home/johndoe/datasets .   Note:  The example policy file  remote.policy.example  is using this setup (available in  weka/experiment ).",
            "title": "Preparation"
        },
        {
            "location": "/remote_experiment/#database-server-setup",
            "text": "HSQLDB  Download the JDBC driver for HSQLDB, extract the  hsqldb.jar  and place it in the directory  /home/johndoe/jars .  To set up the database server, choose or create a directory to run the database server from, and start the server with:    java -classpath /home/johndoe/jars/hsqldb.jar \\\n   org.hsqldb.Server -database.0 experiment -dbname.0 experiment   Note:  This will start up a database with the alias  experiment  ( -dbname.0 <alias> ) and create a properties and a log file at the current location prefixed with  experiment  ( -database.0 <file> ).    MySQL  We won't go into details of setting up a MySQL server, but this is rather straightforward and includes the following steps:      Download a suitable version of MySQL for your server machine.  Start the MySQL server.  Create a database - for our example we will use experiment as database name.  Download the appropriate JDBC driver, extract the JDBC jar and place it as  mysql.jar  in  /home/johndoe/jars .",
            "title": "Database Server Setup"
        },
        {
            "location": "/remote_experiment/#remote-engine-setup",
            "text": "First, set up a directory for scripts and policy files:    /home/johndoe/remote_engine   Unzip the  remoteExperimentServer.jar  (from the Weka distribution; or build it from the sources with  ant remotejar ) into a temporary directory.  Next, copy the remoteEngine.jar to the  /home/johndoe/remote_engine  directory.  Create a script, called  /home/johndoe/remote_engine/startRemoteEngine , with the following content (don't forget to make it executable with  chmod a+x startRemoteEngine ):     HSQLDB     java -Xmx256m \\\n   -classpath /home/johndoe/jars/hsqldb.jar:remoteEngine.jar \\\n   -Djava.security.policy=remote.policy \\\n   weka.experiment.RemoteEngine &    MySQL     java -Xmx256m \\\n   -classpath /home/johndoe/jars/mysql.jar:remoteEngine.jar \\\n   -Djava.security.policy=remote.policy \\\n   weka.experiment.RemoteEngine &    From Weka 3.7.2 you will need to include the core weka.jar file in the classpath for the RemoteEngine. Assuming that the weka.jar file has been copied to  /home/johndoe/remote_engine :    java -Xmx256m \\\n  -classpath /home/johndoe/jars/hsqldb.jar:remoteEngine.jar:weka.jar \\\n  -Djava.security.policy=remote.policy \\\n  weka.experiment.RemoteEngine &    Now we will start the remote engines (note that the same version of Java must be used for the Experimenter and remote engines) :   Copy the  remote.policy.example  file to  /home/johndoe/remote_engine  as  remote.policy .   For each machine you want to run an engine on:   ssh  to the machine.  cd  to  /home/johndoe/remote_engine .  Run  /home/johndoe/startRemoteEngine  (to enable the remote engines to use more memory, modify the  -Xmx  option in the  startRemoteEngine  script) .",
            "title": "Remote Engine Setup"
        },
        {
            "location": "/remote_experiment/#configuring-the-experimenter",
            "text": "Now we will run the Experimenter:   HSQLDB  Copy the  DatabaseUtils.props.hsql  file to the  /home/johndoe/remote_engine  directory and rename it to  DatabaseUtils.props  - a copy comes with your Weka distribution in  weka/experiment .  Edit this file and change the \" jdbcURL=jdbc:hsqldb:hsql://server_name/database_name \" entry to include the name of the machine that is running your database server (e.g.,  jdbcURL=jdbc:hsqldb:hsql://dodo.company.com/experiment ).  Now start the experimenter (inside this directory):      java \\\n   -cp /home/johndoe/jars/hsqldb.jar:remoteEngine.jar:/home/johndoe/weka/weka.jar \\\n   -Djava.rmi.server.codebase=file:/home/johndoe/weka/weka.jar \\\n   weka.gui.experiment.Experimenter   MySQL  Copy the  DatabaseUtils.props.mysql  file to the  /home/johndoe/remote_engine  directory and rename it to  DatabaseUtils.props  - a copy comes with your Weka distribution in  weka/experiment .  Edit this file and change the \" jdbcURL=jdbc:mysql://server_name:3306/database_name \" entry to include the name of the machine that is running your database server and the name of the database the result will be stored in (e.g.,  jdbcURL=jdbc:mysql://dodo.company.com:3306/experiment ).  Now start the experimenter (inside this directory):      java \\\n   -cp /home/johndoe/jars/mysql.jar:remoteEngine.jar:/home/johndoe/weka/weka.jar \\\n   -Djava.rmi.server.codebase=file:/home/johndoe/weka/weka.jar \\\n   weka.gui.experiment.Experimenter   Note:  the database name experiment can be still modified in the Experimenter, this is just the default setup.    Now we will configure the experiment:   First of all select the  Advanced  mode in the Setup tab   Now choose the  DatabaseResultListener  in the  Destination  panel. Configure this result producer:   HSQLDB   Supply the value  sa  for the username and leave the password empty.     MySQL   Provide username and password that you need for connecting to the database.        From the Result generator panel choose either the  CrossValidationResultProducer  or the  RandomSplitResultProducer  (these are the most commonly used ones) and then configure the remaining experiment details (e.g., datasets and classifiers).   Now enable the  Distribute Experiment  panel by checking the tick box.  Click on the  Hosts  button and enter the names of the machines that you started remote engines on ( <Enter>  adds the host to the list).  You can choose to distribute by run or dataset (try to get a balance).  Save your experiment configuration.  Now start your experiment as you would do normally.  Check your results in the  Analyse  tab by clicking either the  Database  or  Experiment  buttons.",
            "title": "Configuring the Experimenter"
        },
        {
            "location": "/remote_experiment/#multi-core-support",
            "text": "If you want to utilize all the cores on a multi-core machine, then you can do so with Weka version 3.6.x and developer versions later than 3.5.7. All you have to do, is define the port alongside the hostname in the Experimenter (format:  hostname:port ) and then start the  RemoteEngine  with the  -p  option, specifying the port to listen on.\nSee also  this  post on the  Wekalist .",
            "title": "Multi-core support"
        },
        {
            "location": "/remote_experiment/#troubleshooting",
            "text": "If you get an error at the start of an experiment that looks a bit like this:   {{01:13:19: RemoteExperiment (//blabla.company.com/RemoteEngine) (sub)experiment (datataset vineyard.arff) failed : java.sql.SQLException: Table already exists: EXPERIMENT_INDEX in statement [CREATE TABLE Experiment_index ( Experiment_type LONGVARCHAR, Experiment_setup LONGVARCHAR, Result_table INT )]\n\n01:13:19: dataset :vineyard.arff RemoteExperiment\n\n       (//blabla.company.com/RemoteEngine) (sub)experiment (datataset vineyard.arff) failed : java.sql.SQLException: Table already exists: EXPERIMENT_INDEX in statement [CREATE TABLE Experiment_index ( Experiment_type LONGVARCHAR, Experiment_setup LONGVARCHAR, Result_table INT )]. Scheduling for execution on another host.}}   then do not panic - this happens because multiple remote machines are trying to create the same table and are temporarily locked out - this will resolve itself so just leave your experiment running - in fact, it is a sign that the experiment is working!    If you serialized an experiment and then modify your  DatabaseUtils.props  file due to an error (e.g., a missing type-mapping), the Experimenter will use the  DatabaseUtils.props  you had at the time you serialized the experiment. Keep in mind that the serialization process also serializes the DatabaseUtils class and therefore stored your props-file! This is another reason for storing your experiments as XML and not in the properietary binary format the Java serialization produces.  Using a corrupt or incomplete  DatabaseUtils.props  file can cause peculiar interface errors, for example disabling the use of the  User  button alongside the database URL. If in doubt copy a clean  DatabaseUtils.props  from  Subversion .  If you get  NullPointerException at java.util.Hashtable.get()  in the Remote Engine do not be alarmed. This will have no effect on the results of your experiment.",
            "title": "Troubleshooting"
        },
        {
            "location": "/remote_experiment/#links",
            "text": "Databases  weka/experiment/DatabaseUtils.props  Subversion  HSQLDB  MySQL",
            "title": "Links"
        },
        {
            "location": "/remove_attributes/",
            "text": "The following code removes specified attributes from an \nARFF\n file and prints the result to stdout.\n\n\nThe class takes the following parameters:\n\n\n\n\nARFF\n file\n\n\nattribute index/indices\n\n\ninversion of index/indices (false/true)\n\n\n\n\n import weka.core.Instances;\n import weka.filters.Filter;\n import weka.filters.unsupervised.attribute.Remove;\n\n import java.io.BufferedReader;\n import java.io.FileReader;\n\n public class RemoveTest {\n   /**\n    * takes an ARFF file as first argument, the number of indices to remove\n    * as second and thirdly whether to invert or not (true/false).\n    * Dumps the generated data to stdout.\n    */\n   public static void main(String[] args) throws Exception {\n     Instances       inst;\n     Instances       instNew;\n     Remove          remove;\n\n     inst   = new Instances(new BufferedReader(new FileReader(args[0])));\n     remove = new Remove();\n     remove.setAttributeIndices(args[1]);\n     remove.setInvertSelection(new Boolean(args[2]).booleanValue());\n     remove.setInputFormat(inst);\n     instNew = Filter.useFilter(inst, remove);\n     System.out.println(instNew);\n   }\n }\n\n\n\n\nThe same can be achieved with the following filter commandline (add \n-V\n to invert the selection):\n\n\n java weka.filters.unsupervised.attribute.Remove -R <indices> -i <input> -o <output>\n\n\n\n\nDownloads\n\n\n\n\nRemoveTest.java\n (\nbook\n, \nstable-3.6\n, \ndeveloper\n)",
            "title": " Remove Attributes"
        },
        {
            "location": "/remove_attributes/#downloads",
            "text": "RemoveTest.java  ( book ,  stable-3.6 ,  developer )",
            "title": "Downloads"
        },
        {
            "location": "/removing_misclassified_instances_from_dataset/",
            "text": "Sometimes it is necessary to clean out the instances misclassified by a classifier from a dataset. The following example loads a dataset, runs the \nRemoveMisclassified\n filter and saves the resulting dataset in another file again:\n\n\n RemoveMisclassifiedTest <input.arff> <classname> <output.arff>\n\n\n\n\nSource code:\n\n\n  import weka.classifiers.Classifier;\n  import weka.core.Instances;\n  import weka.filters.Filter;\n  import weka.filters.unsupervised.instance.RemoveMisclassified;\n\n  import java.io.BufferedReader;\n  import java.io.BufferedWriter;\n  import java.io.FileReader;\n  import java.io.FileWriter;\n\n  /**\n   * Runs the RemoveMisclassified filter over a given ARFF file.\n   * First parameter is the input file, the second the classifier\n   * to use and the third one is the output file.\n   *\n   * Usage: RemoveMisclassifiedTest input.arff classname output.arff\n   *\n   * @author FracPete (fracpete at waikato dot ac dot nz)\n   */\n  public class RemoveMisclassifiedTest {\n    public static void main(String[] args) throws Exception {\n      if (args.length != 3) {\n        System.out.println(\"\\nUsage: RemoveMisclassifiedTest input.arff classname output.arff\\n\");\n        System.exit(1);\n      }\n\n      // get data\n      Instances input = new Instances(\n          new BufferedReader(new FileReader(args[0])));\n      input.setClassIndex(input.numAttributes() - 1);\n\n      // get classifier\n      Classifier c = Classifier.forName(args[1], new String[0]);\n\n      // setup and run filter\n      RemoveMisclassified filter = new RemoveMisclassified();\n      filter.setClassifier(c);\n      filter.setClassIndex(-1);\n      filter.setNumFolds(0);\n      filter.setThreshold(0.1);\n      filter.setMaxIterations(0);\n      filter.setInputFormat(input);\n      Instances output = Filter.useFilter(input, filter);\n\n      // output file\n      BufferedWriter writer = new BufferedWriter(new FileWriter(args[2]));\n      writer.write(output.toString());\n      writer.newLine();\n      writer.flush();\n      writer.close();\n    }\n  }\n\n\n\n\nSee also\n\n\n\n\nUse Weka in your Java code\n - for general use of the Weka API\n\n\nSave Instances to an ARFF File\n - for saving an Instances object to a file\n\n\n\n\nDownloads\n\n\n\n\nRemoveMisclassifiedTest.java",
            "title": " Removing misclassified instances from dataset"
        },
        {
            "location": "/removing_misclassified_instances_from_dataset/#see-also",
            "text": "Use Weka in your Java code  - for general use of the Weka API  Save Instances to an ARFF File  - for saving an Instances object to a file",
            "title": "See also"
        },
        {
            "location": "/removing_misclassified_instances_from_dataset/#downloads",
            "text": "RemoveMisclassifiedTest.java",
            "title": "Downloads"
        },
        {
            "location": "/rename_attribute_values/",
            "text": "After discretizing an attribute you might want to rename the values of the newly created nominal attribute. E.g., after discretizing a numeric attribute with values ranging from 1 to 100 you might end up with a nominal attribute that has the following values:\n\n\n '1-15','16-18','29-100'\n\n\n\n\nYou can use the \nrenameAttributeValue(...)\n method of the \nweka.core.Instances\n class (see \nAPI\n) to rename this values into, e.g., 0, 1 and 2. Here's a code snippet how to do this (\narff\n is an Instances object, \natt\n is an attribute of the same instances object):\n\n\n for (int n = 0; n < att.numValues(); n++) {\n    arff.renameAttributeValue(att, att.value(n), \"\" + n);\n\n\n\n\nThe \nRename.java\n like mentioned above.\n\n\nSee also\n\n\n\n\nUse Weka in your Java code\n\n\n\n\nDownloads\n\n\n\n\nRename.java",
            "title": " Rename Attribute Values"
        },
        {
            "location": "/rename_attribute_values/#see-also",
            "text": "Use Weka in your Java code",
            "title": "See also"
        },
        {
            "location": "/rename_attribute_values/#downloads",
            "text": "Rename.java",
            "title": "Downloads"
        },
        {
            "location": "/running_an_experiment_using_clusterers/",
            "text": "Using the advanced mode of the Experimenter you can now run experiments on clustering algorithms as well as classifiers (Note: \nthis is a new feature available with Weka 3.5.8). The main evaluation metric for this type of experiment is the log likelihood of the clusters found by each clusterer. Here is an example of setting up a cross-validation experiment using clusterers.\n\n\nChoose \nCrossValidationResultProducer\n from the \nResult generator\n panel.\n\n\n\n\nNext, choose \nDensityBasedClustererSplitEvaluator\n as the split evaluator to use.\n\n\n\n\nIf you click on \nDensityBasedClustererSplitEvaluator\n you will see its options. \nNote\n that there is an option for removing the class column from the data. In the Experimenter, the class column is set to be the last column by default. Turn this off if you want to keep this column in the data.\n\n\n\n\nOnce \nDensityBasedClustererSplitEvaluator\n has been selected, you will notice that the \nGenerator properties\n have become disabled. Enable them again and expand \nsplitEvaluator\n. Select the \nclusterer\n node.\n\n\n\n\nNow you will see that \nEM\n becomes the default clusterer and gets added to the list of schemes. You can now add/delete other clusterers. \n\n\nIMPORTANT\n: in order to any clusterer that does not produce density estimates (i.e. most other clusterers in Weka), they will have to wrapped in the \nMakeDensityBasedClusterer\n.\n\n\n\n\nOnce and experiment has been run, you can analyze results in the \nAnalyse\n panel. In the \nComparison field\n you will need to scroll down and select \"Log_likelihood\".",
            "title": " Running an Experiment Using Clusterers"
        },
        {
            "location": "/saving_and_loading_models/",
            "text": "You save a trained classifier with the \n-d\n option (\ndumping\n), e.g.:\n\n\n java weka.classifiers.trees.J48 -C 0.25 -M 2 -t /some/where/train.arff -d /other/place/j48.model\n\n\n\n\nAnd you can load it with \n-l\n and use it on a test set, e.g.:\n\n\n java weka.classifiers.trees.J48 -l /other/place/j48.model -T /some/where/test.arff\n\n\n\n\nNote, when loading a model you no longer need to supply specific parameters to the classifier.\n\n\nExplorer\n\n\nA trained model can be saved like this, e.g., J48:\n\n\n\n\ntrain your model on the training data \n/some/where/train.arff\n\n\nright-click in the \nResults list\n on the item which model you want to save\n\n\nselect \nSave model\n and save it to \n/other/place/j48.model\n\n\n\n\nYou can load the previously saved model with the following steps:\n\n\n\n\nload your test data \n/some/where/test.arff\n via the \nSupplied test set\n button\n\n\nright-click in the \nResults list\n, select \nLoad model\n and choose \n/other/place/j48.model\n\n\nselect \nRe-evaluate model on current test set\n\n\n\n\nBased on \nthis\n \nWeka Mailing List\n post.\n\n\nMaking Predictions with your model without retraining\n\n\nSee the \nMaking predictions\n article for detailed information.\n\n\nSource code\n\n\nSee \nSerialization\n for code examples.\n\n\nSee also\n\n\n\n\nSerialization",
            "title": " Saving and loading models"
        },
        {
            "location": "/saving_and_loading_models/#explorer",
            "text": "A trained model can be saved like this, e.g., J48:   train your model on the training data  /some/where/train.arff  right-click in the  Results list  on the item which model you want to save  select  Save model  and save it to  /other/place/j48.model   You can load the previously saved model with the following steps:   load your test data  /some/where/test.arff  via the  Supplied test set  button  right-click in the  Results list , select  Load model  and choose  /other/place/j48.model  select  Re-evaluate model on current test set   Based on  this   Weka Mailing List  post.",
            "title": "Explorer"
        },
        {
            "location": "/saving_and_loading_models/#making-predictions-with-your-model-without-retraining",
            "text": "See the  Making predictions  article for detailed information.",
            "title": "Making Predictions with your model without retraining"
        },
        {
            "location": "/saving_and_loading_models/#source-code",
            "text": "See  Serialization  for code examples.",
            "title": "Source code"
        },
        {
            "location": "/saving_and_loading_models/#see-also",
            "text": "Serialization",
            "title": "See also"
        },
        {
            "location": "/stemmers/",
            "text": "Introduction\n\n\nWeka supports stemming algorithms in the developer version. The stemming algorithms are located in the following package:\n\n\n weka.core.stemmers\n\n\n\n\nCurrently, the Lovins Stemmer (+ iterated version) and support for the Snowball stemmers are included.\n\n\nSnowball stemmers\n\n\nWeka contains a wrapper class for the \nSnowball\n stemmers (containing the Porter stemmer and several other stemmers for different languages). The relevant class is weka.core.stemmers.SnowballStemmer.\n\n\nThe Snowball classes are not included, they only have to be present in the classpath. The reason for this is, that the Weka team doesn't have to watch out for new versions of the stemmers and update them.\n\n\nThere are \nthree\n ways of getting hold of the Snowball stemmers:\n\n\n\n\nFor Weka 3.7.x you can install an \nunofficial package\n\n\nYou can add the \npre-compiled \nsnowball-20051019.jar\n archive\n to your classpath and you're set.\n\n\n(based on source code from 2005-10-19, compiled 2005-10-22)\n\n\n\n\n\n\nYou can \ncompile the stemmers yourself\n with the newest sources.\n\n\nJust download the \nsnowball-20051019.zip\n.\n\nNote:\n the \npatch\n target is specific to the source code from 2005-10-19.\n\n\n\n\n\n\n\n\nPTStemmer\n\n\nPTStemmer\n is a stemmer library for Portuguese developed by Pedro Oliveira.\n\n\nIn order to use this library:\n\n\n\n\nyou can install the \nunofficial package\n when using Weka 3.7.x\n\n\nyou just need to download the \nptstemmer.jar\n and add them to your classpath.\n\n\n\n\nThe source code of the wrapper project is also available: \n\nptstemmer-weka-src-20091105.tar.gz\n.\n\nNB:\n the source code and the resulting jars are based on version 1.0 of the PTStemmer library.\n\n\nUsing stemmers\n\n\nThe stemmers can either be used:\n\n\n\n\nfrom commandline\n\n\nwithin the \nStringToWordVector\n (package \nweka.filters.unsupervised.attribute\n)\n\n\n\n\nCommandline\n\n\nAll stemmers support the following options:\n\n\n\n\n-h\n\n\nfor displaying a brief help\n\n\n\n\n\n\n-i <input-file>\n\n\nThe file to process\n\n\n\n\n\n\n-o <output-file>\n\n\nThe file to output the processed data to (default \nstdout\n)\n\n\n\n\n\n\n-l\n\n\nUses lowercase strings, i.e. the input is automatically converted to lower case\n\n\n\n\n\n\n\n\nStringToWordVector\n\n\nJust use the \nGenericObjectEditor\n to choose the right stemmer and the desired options (if the stemmer offers these).\n\n\nAdding new stemmers\n\n\nYou can easily add new stemmers, if you follow these guidelines (for use in the \nGenericObjectEditor\n):\n\n\n\n\nthey should be located in the \nweka.core.stemmers\n package and\n\n\nthey must implement the interface \nweka.core.stemmers.Stemmer\n.\n\n\n\n\nLinks\n\n\n\n\nSnowball homepage\n\n\nANT homepage",
            "title": " Stemmers"
        },
        {
            "location": "/stemmers/#introduction",
            "text": "Weka supports stemming algorithms in the developer version. The stemming algorithms are located in the following package:   weka.core.stemmers  Currently, the Lovins Stemmer (+ iterated version) and support for the Snowball stemmers are included.",
            "title": "Introduction"
        },
        {
            "location": "/stemmers/#snowball-stemmers",
            "text": "Weka contains a wrapper class for the  Snowball  stemmers (containing the Porter stemmer and several other stemmers for different languages). The relevant class is weka.core.stemmers.SnowballStemmer.  The Snowball classes are not included, they only have to be present in the classpath. The reason for this is, that the Weka team doesn't have to watch out for new versions of the stemmers and update them.  There are  three  ways of getting hold of the Snowball stemmers:   For Weka 3.7.x you can install an  unofficial package  You can add the  pre-compiled  snowball-20051019.jar  archive  to your classpath and you're set.  (based on source code from 2005-10-19, compiled 2005-10-22)    You can  compile the stemmers yourself  with the newest sources.  Just download the  snowball-20051019.zip . Note:  the  patch  target is specific to the source code from 2005-10-19.",
            "title": "Snowball stemmers"
        },
        {
            "location": "/stemmers/#ptstemmer",
            "text": "PTStemmer  is a stemmer library for Portuguese developed by Pedro Oliveira.  In order to use this library:   you can install the  unofficial package  when using Weka 3.7.x  you just need to download the  ptstemmer.jar  and add them to your classpath.   The source code of the wrapper project is also available:  ptstemmer-weka-src-20091105.tar.gz . NB:  the source code and the resulting jars are based on version 1.0 of the PTStemmer library.",
            "title": "PTStemmer"
        },
        {
            "location": "/stemmers/#using-stemmers",
            "text": "The stemmers can either be used:   from commandline  within the  StringToWordVector  (package  weka.filters.unsupervised.attribute )",
            "title": "Using stemmers"
        },
        {
            "location": "/stemmers/#commandline",
            "text": "All stemmers support the following options:   -h  for displaying a brief help    -i <input-file>  The file to process    -o <output-file>  The file to output the processed data to (default  stdout )    -l  Uses lowercase strings, i.e. the input is automatically converted to lower case",
            "title": "Commandline"
        },
        {
            "location": "/stemmers/#stringtowordvector",
            "text": "Just use the  GenericObjectEditor  to choose the right stemmer and the desired options (if the stemmer offers these).",
            "title": "StringToWordVector"
        },
        {
            "location": "/stemmers/#adding-new-stemmers",
            "text": "You can easily add new stemmers, if you follow these guidelines (for use in the  GenericObjectEditor ):   they should be located in the  weka.core.stemmers  package and  they must implement the interface  weka.core.stemmers.Stemmer .",
            "title": "Adding new stemmers"
        },
        {
            "location": "/stemmers/#links",
            "text": "Snowball homepage  ANT homepage",
            "title": "Links"
        },
        {
            "location": "/stop_word_filtering_and_attributes/",
            "text": "I have WEKA 3 6 3\n\n\nWhen I do stop word filtering I get more attributes than before, however the program performs better and is faster and more accure.\n\n\nHercules Dalianis\nhercules@dsv.su.se",
            "title": " Stop word filtering and attributes"
        },
        {
            "location": "/text_categorization_with_weka/",
            "text": "In the following one can find some information of how to use Weka for \ntext categorization\n.\n\n\nImport\n\n\nWeka needs the data to be present in \nARFF\n or \nXRFF\n format in order to perform any classification tasks.\n\n\nDirectories\n\n\nOne can transform the text files with the following tools into ARFF format (depending on the version of Weka you are using):\n\n\n\n\nTextDirectoryToArff\n tool (3.4.x and >= 3.5.3)\n\n\nthis Java class transforms a directory of files into an ARFF file\n\n\n\n\n\n\nTextDirectoryLoader\n converter (> 3.5.3)\n\n\nthis converter is based on the \nTextDirectoryToArff\n tool and located in the \nweka.core.converters\n package\n\n\n\n\n\n\n\n\nExample directory layout for \nTextDirectoryLoader\n:\n\n\n ...\n |\n +- text_example\n    |\n    +- class1\n    |  |\n    |  + file1.txt\n    |  |\n    |  + file2.txt\n    |  |\n    |  ...\n    |\n    +- class2\n    |  |\n    |  + another_file1.txt\n    |  |\n    |  + another_file2.txt\n    |  |\n    |  ...\n\n\n\n\nThe above directory structure can be turned into an ARFF file like this:\n\n\njava weka.core.converters.TextDirectoryLoader -dir text_example > text_example.arff\n\n\n\n\nCSV files\n\n\nCSV files can be imported in Weka easily via the Weka Explorer or via commandline via the \nCSVLoader\n class:\n\n\n java weka.core.converters.CSVLoader file.csv > file.arff\n\n\n\n\nBy default, non-numerical attributes get imported as \nNOMINAL\n attributes, which is not necessarily desired for textual data, especially if one wants to use the \nStringToWordVector\n filter. In order to change the attribute to \nSTRING\n, one can run the \nNominalToString\n filter (package \nweka.filters.unsupervised.attribute\n) on the data, specifying the attribute index or range of indices that should be converted (NB: \nthis filter does \nnot\n exclude the class attribute from conversion!). In order to retain the attribute types, one needs to save the file in \nARFF\n or \nXRFF\n format (or in the compressed version of these formats).\n\n\nThird-party tools\n\n\n\n\nTagHelper Tools\n, which allows one to transform texts into vectors of stemmed or unstemmed unigrams, bigrams, part-of-speech bigrams, and some user defined features, and then saves this representation to \nARFF\n. Currently processes English, German, and Chinese. Spanish and Portugese are in progress.\n\n\n\n\nWorking with textual data\n\n\nConversion\n\n\nMost classifiers in Weka cannot handle \nString\n attributes. For these learning schemes one has to process the data with appropriate filters, e.g., the \nStringToWordVector\n filter which can perform \nTF/IDF transformation\n.\n\n\nThe \nStringToWordVector\n filter places the class attribute of the generated output data at the beginning. In case you'd to like to have it as last attribute again, you can use the \nReorder\n filter with the following setup:\n\n\nweka.filters.unsupervised.attribute.Reorder -R 2-last,first\n\n\n\n\nAnd with the \nMultiFilter\n you can also apply both filters in one go, instead of subsequently. Makes it easier in the Explorer for instance.\n\n\nStopwords\n\n\nThe \nStringToWordVector\n filter can also work with a different stopword list than the built-in one (based on the Rainbow system). One can use the \n-stopwords\n option to load the external stopwords file. The format for such a stopword file is one stopword per line, lines starting with '#' are interpreted as comments and ignored.\n\n\nNote:\n There was a bug in Weka 3.5.6 (which introduced the support of external stopwords lists), which ignored the external stopwords list. Later versions or \nsnapshot\ns from 21/07/2007 on will work correctly.\n\n\nUTF-8\n\n\nIn case you are working with text files containing non-ASCII characters, e.g., Arabic, you might encounter some display problems under Windows. Java was designed to display \nUTF-8\n, which should include arabic characters. By default, Java uses \ncode page 1252\n under Windows, which garbles the display of other characters. In order to fix this, you will have to modify the java command-line with which you start up Weka (taken from \nthis\n post):\n\n\n  java -Dfile.encoding=utf-8 -classpath ...\n\n\n\n\nThe \n-Dfile.encoding=utf-8\n tells Java to explicitly use \nUTF-8\n encoding instead of the default \nCP1252\n.\nIf you are starting Weka via start menu and you use a recent version (at least 3.5.8 or 3.4.13), then you will just have to modify the \nfileEncoding\n placeholder in the \nRunWeka.ini\n accordingly.\n\n\nExamples\n\n\n\n\ntext_example.zip\n - contains a directory structure and example files that can be imported with the \nTextDirectoryLoader\n converter.\n\n\nTextCategorizationTest.java\n - uses the \nTextDirectoryLoader\n converter to turn a directory structure into a dataset, applies the \nStringToWordVector\n and builds a classifier with the filtered data.\n\n\n\n\nSee also\n\n\n\n\nBatch filtering\n - for generating a test set with the same dictionary as the training set\n\n\nAll text categorization articles\n\n\n\n\nLinks\n\n\n\n\nJavadoc\n\n\nStringToWordVector\n\n\nTextDirectoryLoader",
            "title": " Text categorization with Weka"
        },
        {
            "location": "/text_categorization_with_weka/#import",
            "text": "Weka needs the data to be present in  ARFF  or  XRFF  format in order to perform any classification tasks.",
            "title": "Import"
        },
        {
            "location": "/text_categorization_with_weka/#directories",
            "text": "One can transform the text files with the following tools into ARFF format (depending on the version of Weka you are using):   TextDirectoryToArff  tool (3.4.x and >= 3.5.3)  this Java class transforms a directory of files into an ARFF file    TextDirectoryLoader  converter (> 3.5.3)  this converter is based on the  TextDirectoryToArff  tool and located in the  weka.core.converters  package     Example directory layout for  TextDirectoryLoader :   ...\n |\n +- text_example\n    |\n    +- class1\n    |  |\n    |  + file1.txt\n    |  |\n    |  + file2.txt\n    |  |\n    |  ...\n    |\n    +- class2\n    |  |\n    |  + another_file1.txt\n    |  |\n    |  + another_file2.txt\n    |  |\n    |  ...  The above directory structure can be turned into an ARFF file like this:  java weka.core.converters.TextDirectoryLoader -dir text_example > text_example.arff",
            "title": "Directories"
        },
        {
            "location": "/text_categorization_with_weka/#csv-files",
            "text": "CSV files can be imported in Weka easily via the Weka Explorer or via commandline via the  CSVLoader  class:   java weka.core.converters.CSVLoader file.csv > file.arff  By default, non-numerical attributes get imported as  NOMINAL  attributes, which is not necessarily desired for textual data, especially if one wants to use the  StringToWordVector  filter. In order to change the attribute to  STRING , one can run the  NominalToString  filter (package  weka.filters.unsupervised.attribute ) on the data, specifying the attribute index or range of indices that should be converted (NB: \nthis filter does  not  exclude the class attribute from conversion!). In order to retain the attribute types, one needs to save the file in  ARFF  or  XRFF  format (or in the compressed version of these formats).",
            "title": "CSV files"
        },
        {
            "location": "/text_categorization_with_weka/#third-party-tools",
            "text": "TagHelper Tools , which allows one to transform texts into vectors of stemmed or unstemmed unigrams, bigrams, part-of-speech bigrams, and some user defined features, and then saves this representation to  ARFF . Currently processes English, German, and Chinese. Spanish and Portugese are in progress.",
            "title": "Third-party tools"
        },
        {
            "location": "/text_categorization_with_weka/#working-with-textual-data",
            "text": "",
            "title": "Working with textual data"
        },
        {
            "location": "/text_categorization_with_weka/#conversion",
            "text": "Most classifiers in Weka cannot handle  String  attributes. For these learning schemes one has to process the data with appropriate filters, e.g., the  StringToWordVector  filter which can perform  TF/IDF transformation .  The  StringToWordVector  filter places the class attribute of the generated output data at the beginning. In case you'd to like to have it as last attribute again, you can use the  Reorder  filter with the following setup:  weka.filters.unsupervised.attribute.Reorder -R 2-last,first  And with the  MultiFilter  you can also apply both filters in one go, instead of subsequently. Makes it easier in the Explorer for instance.",
            "title": "Conversion"
        },
        {
            "location": "/text_categorization_with_weka/#stopwords",
            "text": "The  StringToWordVector  filter can also work with a different stopword list than the built-in one (based on the Rainbow system). One can use the  -stopwords  option to load the external stopwords file. The format for such a stopword file is one stopword per line, lines starting with '#' are interpreted as comments and ignored.  Note:  There was a bug in Weka 3.5.6 (which introduced the support of external stopwords lists), which ignored the external stopwords list. Later versions or  snapshot s from 21/07/2007 on will work correctly.",
            "title": "Stopwords"
        },
        {
            "location": "/text_categorization_with_weka/#utf-8",
            "text": "In case you are working with text files containing non-ASCII characters, e.g., Arabic, you might encounter some display problems under Windows. Java was designed to display  UTF-8 , which should include arabic characters. By default, Java uses  code page 1252  under Windows, which garbles the display of other characters. In order to fix this, you will have to modify the java command-line with which you start up Weka (taken from  this  post):    java -Dfile.encoding=utf-8 -classpath ...  The  -Dfile.encoding=utf-8  tells Java to explicitly use  UTF-8  encoding instead of the default  CP1252 .\nIf you are starting Weka via start menu and you use a recent version (at least 3.5.8 or 3.4.13), then you will just have to modify the  fileEncoding  placeholder in the  RunWeka.ini  accordingly.",
            "title": "UTF-8"
        },
        {
            "location": "/text_categorization_with_weka/#examples",
            "text": "text_example.zip  - contains a directory structure and example files that can be imported with the  TextDirectoryLoader  converter.  TextCategorizationTest.java  - uses the  TextDirectoryLoader  converter to turn a directory structure into a dataset, applies the  StringToWordVector  and builds a classifier with the filtered data.",
            "title": "Examples"
        },
        {
            "location": "/text_categorization_with_weka/#see-also",
            "text": "Batch filtering  - for generating a test set with the same dictionary as the training set  All text categorization articles",
            "title": "See also"
        },
        {
            "location": "/text_categorization_with_weka/#links",
            "text": "Javadoc  StringToWordVector  TextDirectoryLoader",
            "title": "Links"
        },
        {
            "location": "/text_classification_with_weka/",
            "text": "see \nText categorization with Weka",
            "title": " Text classification with Weka"
        },
        {
            "location": "/transferring_an_arff_file_into_a_database/",
            "text": "This example transfers a dataset stored in an \nARFF\n file into the MySQL \ndatabase\n \nweka_test\n on the MySQL server running on the same machine. In order to make this work, the MySQL JDBC driver must be in the \nCLASSPATH\n and the \nDatabaseUtils.props\n file must be configured accordingly.\n\n\nUsage:\n\n\n Arff2Database <input.arff>\n\n\n\n\nSource code:\n\n\nimport weka.core.*;\n  import weka.core.converters.*;\n  import java.io.*;\n\n  /**\n   * A simple API example of transferring an ARFF file into a MySQL table.\n   * It loads the data into the database \"weka_test\" on the MySQL server\n   * running on the same machine. Instead of using the relation name of the\n   * database as the table name, \"mytable\" is used instead. The\n   * DatabaseUtils.props file must be configured accordingly.\n   *\n   * Usage: Arff2Database input.arff\n   *\n   * @author FracPete (fracpete at waikato dot ac dot nz)\n   */\n  public class Arff2Database {\n\n    /**\n     * loads a dataset into a MySQL database\n     *\n     * @param args    the commandline arguments\n     */\n    public static void main(String[] args) throws Exception {\n      Instances data = new Instances(new BufferedReader(new FileReader(args[0])));\n      data.setClassIndex(data.numAttributes() - 1);\n\n      DatabaseSaver save = new DatabaseSaver();\n      save.setUrl(\"jdbc:mysql://localhost:3306/weka_test\");\n      save.setUser(\"fracpete\");\n      save.setPassword(\"fracpete\");\n      save.setInstances(data);\n      save.setRelationForTableName(false);\n      save.setTableName(\"mytable\");\n      save.connectToDatabase();\n      save.writeBatch();\n    }\n  }\n\n\n\n\nSee also\n\n\n\n\nDatabases\n - explains how to use databases within Weka\n\n\nweka/experiment/DatabaseUtils.props\n - the properties file explained in detail\n\n\n\n\nDownloads\n\n\n\n\nArff2Database.java\n\n\n\n\nThe \nWeka Examples\n collection contains several example classes:\n\n\n\n\nSaveDataToDbBatch.java\n (\nbook\n, \nstable-3.6\n, \ndeveloper\n)\n\n\nSaveDataToDbIncremental.java\n (\nbook\n, \nstable-3.6\n, \ndeveloper\n)\n\n\n\n\n\n\n\n\nLinks\n\n\n\n\nMySQL homepage",
            "title": " Transferring an ARFF file into a database"
        },
        {
            "location": "/transferring_an_arff_file_into_a_database/#see-also",
            "text": "Databases  - explains how to use databases within Weka  weka/experiment/DatabaseUtils.props  - the properties file explained in detail",
            "title": "See also"
        },
        {
            "location": "/transferring_an_arff_file_into_a_database/#downloads",
            "text": "Arff2Database.java   The  Weka Examples  collection contains several example classes:   SaveDataToDbBatch.java  ( book ,  stable-3.6 ,  developer )  SaveDataToDbIncremental.java  ( book ,  stable-3.6 ,  developer )",
            "title": "Downloads"
        },
        {
            "location": "/transferring_an_arff_file_into_a_database/#links",
            "text": "MySQL homepage",
            "title": "Links"
        },
        {
            "location": "/tuning_classifier_parameters/",
            "text": "see \nOptimizing parameters",
            "title": " Tuning classifier parameters"
        },
        {
            "location": "/use_weka_with_the_microsoft_net_framework/",
            "text": "So you want to use Weka with your existing Microsoft .NET code?  Or you want to use your .NET components in a Java-based system that uses Weka?  Achieving .NET and Java interoperability is possible, but there is no 'one size fits all' solution.  The sheer number of different ways to attempt this should give an indication of the difficulty of the problem.\n\n\nThat said, here is a summary of some of the possibilities that you can try.\n\n\nDirect Interoperability\n\n\nIKVM\n\n\nIKVM\n is an implementation of Java for .NET.  It allows you to call your Java classes from directly from your .NET code, and via the \nGNU Classpath\n provides most of the standard Java API for use in .NET.  It also provides a .NET version of the Java Virtual Machine.\n\n\nIn other words, with this software you can now use almost any Java class in your .NET code!  You could even develop for .NET using Java, and then easily import your classes into your .NET system.\n\n\nEven better, IKVM is Open Source software and is freely available.  The only disadvantage is that its functionality is limited to the extent of the GNU Classpath; however this now covers most of the API, and is rapidly expanding.  It also doesn't appear to have any functionality in the other direction - you can't run .NET code in Java.\n\n\nFor use with Weka, IKVM has successfully been tested on a simple C# program that runs a classifier on a dataset.  The GUI will not load at the time of writing, but I suspect that most of the Weka API will work fine.  Because of this, IKVM is recommended for use in small Open Source research projects using Weka.  See this \nIKVM with Weka tutorial\n for more detail.\n\n\nBridging Software\n\n\nBridging software allows you to use your Java classes in your .NET code, and your .NET classes in your java code.  This works by running both the .NET and Java Virtual Machines simultaneously, and creating proxy classes that 'stand in' for each class in the alternative framework.\n\n\nRuntime bridges are relatively computationally-efficient, and provide seamless and flexible interoperability solutions.  The main disadvantage with this method is that the software tools that facilitate this are generally expensive third-party programs that must be purchased.\n\n\nSome .NET / Java bridging tools:\n\n\n\n\nJNBridge\n\n\nJBind2.net\n\n\nJuggerNET\n\n\nJ-Integra for .NET\n\n\nJACOB\n\n\nAn open-source project that provides allows you to call COM components from Java (but not vice-versa).\n\n\n\n\n\n\nHosting .NET Controls in Java\n\n\nThis tutorial explains how to write your own custom COM bridges using JNI.  This is similar to what JACOB does, except that you will have to code it yourself.\n\n\n\n\n\n\n\n\nJava Native Interface SDK for .NET and tools that use it:\n\n\n\n\nObject-Oriented JNI for .NET \n\n\nThis .NET library implements regular JNI SDK in .NET.\n\n\n\n\n\n\nOOJNI .NET Add-in for MS Visual Studio\n\n\nGenerates wrappers for java classes from Java Bytecode selected in C++, Managed C++, C#, J#, VB.\n\n\n\n\n\n\n\n\nIndirect Interoperability\n\n\nInteroperability using a Database\n\n\nIf both your .NET components and your Java components only need to asynchronously interface with a database, interoperability is very simple.  The components from the different frameworks do not have to know about each other - they just interact with the database as they normally would.  For more detail, see\n\nDatabase Interoperability\n.\n\n\nUsing Enterprise Messaging Services\n\n\nMessaging services are used to facilitate asynchronous communication between different components in a system.  They provide an API for sending messages between components, and provide security, data integrity checking and error handling to ensure reliable information transfer.  To use these systems for interoperability between .NET and Java, you may need a messaging system for each framework that has the capability of talking to messaging systems from the other framework.\n\n\nThe disadvantage of this type of system is that it may be expensive and difficult to set up.  However, if you already have such a system in place, it makes sense to use it for interoperability purposes.\n\n\n.NET Enterprise Messaging Services\n\n\n\n\nMicrosoft BizTalk Server\n\n\n\n\nJava Enterprise Messaging Services\n\n\n\n\nIBM Websphere MQ\n\n\nFiorano MQ\n\n\n\n\nWeb Services\n\n\nWeb Services are a common method used for achieving interoperability.  The main attractions are that web-based systems are both platform and language independant, and there exist standard protocols to facilitate the communication.\n\n\nA Web Services based approach generally involves setting up a web server, and some proxy classes in each framework to communicate with this server.  Communication is generally achieved through XML-based protocols such as SOAP.  The problem with this method is that serialization to XML can create large files that must be sent to the web server, so there may be efficiency issues.\n\n\nMicrosoft WSE\n and \nJWSDP\n are freely available extensions to .NET and Java respectively, for the purposes of developing web-server based solutions.\n\n\nTutorials for using this method for interoperability can be found \nhere\n, \nhere\n and \nhere\n.\n\n\nOther Useful Links\n\n\n\n\nAn introduction to IKVM\n - This article supplies a code example of Java/.NET interoperability using IKVM.\n\n\nIKVM.NET\n - An open source implementation of Java for .NET (Highly recommended).\n\n\nIKVM with Weka tutorial\n - A tutorial on using Weka with C# using IKVM.  This tutorial is part of the WekaWiki.\n\n\nJava/.NET Interop: Bridging Muddled Waters\n - A very comprehensive set of articles on the interoperability problem.  Watch out for the various authors' biases towards their own companies and products though.\n\n\nMicrosoft .NET and Java/J2EE Interoperability\n - An MSDN directory of articles on .NET / Java interopererability.\n\n\nMono\n - An open source .NET development environment, cross platform.\n\n\nJava/.NET Integration as Simple as Possible\n - the article, which describes the simplest way to embed .NET controls into a Java GUI with OOJNI\u00ae\n\n\nProblems using weka from VB .NET in VS 2005\n - you may experience some hiccups when attempting to use an IKVM generated assembly in your VB .Net code in Visual Studio 2005...",
            "title": " Use Weka with the Microsoft .NET Framework"
        },
        {
            "location": "/use_weka_with_the_microsoft_net_framework/#direct-interoperability",
            "text": "",
            "title": "Direct Interoperability"
        },
        {
            "location": "/use_weka_with_the_microsoft_net_framework/#ikvm",
            "text": "IKVM  is an implementation of Java for .NET.  It allows you to call your Java classes from directly from your .NET code, and via the  GNU Classpath  provides most of the standard Java API for use in .NET.  It also provides a .NET version of the Java Virtual Machine.  In other words, with this software you can now use almost any Java class in your .NET code!  You could even develop for .NET using Java, and then easily import your classes into your .NET system.  Even better, IKVM is Open Source software and is freely available.  The only disadvantage is that its functionality is limited to the extent of the GNU Classpath; however this now covers most of the API, and is rapidly expanding.  It also doesn't appear to have any functionality in the other direction - you can't run .NET code in Java.  For use with Weka, IKVM has successfully been tested on a simple C# program that runs a classifier on a dataset.  The GUI will not load at the time of writing, but I suspect that most of the Weka API will work fine.  Because of this, IKVM is recommended for use in small Open Source research projects using Weka.  See this  IKVM with Weka tutorial  for more detail.",
            "title": "IKVM"
        },
        {
            "location": "/use_weka_with_the_microsoft_net_framework/#bridging-software",
            "text": "Bridging software allows you to use your Java classes in your .NET code, and your .NET classes in your java code.  This works by running both the .NET and Java Virtual Machines simultaneously, and creating proxy classes that 'stand in' for each class in the alternative framework.  Runtime bridges are relatively computationally-efficient, and provide seamless and flexible interoperability solutions.  The main disadvantage with this method is that the software tools that facilitate this are generally expensive third-party programs that must be purchased.  Some .NET / Java bridging tools:   JNBridge  JBind2.net  JuggerNET  J-Integra for .NET  JACOB  An open-source project that provides allows you to call COM components from Java (but not vice-versa).    Hosting .NET Controls in Java  This tutorial explains how to write your own custom COM bridges using JNI.  This is similar to what JACOB does, except that you will have to code it yourself.     Java Native Interface SDK for .NET and tools that use it:   Object-Oriented JNI for .NET   This .NET library implements regular JNI SDK in .NET.    OOJNI .NET Add-in for MS Visual Studio  Generates wrappers for java classes from Java Bytecode selected in C++, Managed C++, C#, J#, VB.",
            "title": "Bridging Software"
        },
        {
            "location": "/use_weka_with_the_microsoft_net_framework/#indirect-interoperability",
            "text": "",
            "title": "Indirect Interoperability"
        },
        {
            "location": "/use_weka_with_the_microsoft_net_framework/#interoperability-using-a-database",
            "text": "If both your .NET components and your Java components only need to asynchronously interface with a database, interoperability is very simple.  The components from the different frameworks do not have to know about each other - they just interact with the database as they normally would.  For more detail, see Database Interoperability .",
            "title": "Interoperability using a Database"
        },
        {
            "location": "/use_weka_with_the_microsoft_net_framework/#using-enterprise-messaging-services",
            "text": "Messaging services are used to facilitate asynchronous communication between different components in a system.  They provide an API for sending messages between components, and provide security, data integrity checking and error handling to ensure reliable information transfer.  To use these systems for interoperability between .NET and Java, you may need a messaging system for each framework that has the capability of talking to messaging systems from the other framework.  The disadvantage of this type of system is that it may be expensive and difficult to set up.  However, if you already have such a system in place, it makes sense to use it for interoperability purposes.  .NET Enterprise Messaging Services   Microsoft BizTalk Server   Java Enterprise Messaging Services   IBM Websphere MQ  Fiorano MQ",
            "title": "Using Enterprise Messaging Services"
        },
        {
            "location": "/use_weka_with_the_microsoft_net_framework/#web-services",
            "text": "Web Services are a common method used for achieving interoperability.  The main attractions are that web-based systems are both platform and language independant, and there exist standard protocols to facilitate the communication.  A Web Services based approach generally involves setting up a web server, and some proxy classes in each framework to communicate with this server.  Communication is generally achieved through XML-based protocols such as SOAP.  The problem with this method is that serialization to XML can create large files that must be sent to the web server, so there may be efficiency issues.  Microsoft WSE  and  JWSDP  are freely available extensions to .NET and Java respectively, for the purposes of developing web-server based solutions.  Tutorials for using this method for interoperability can be found  here ,  here  and  here .",
            "title": "Web Services"
        },
        {
            "location": "/use_weka_with_the_microsoft_net_framework/#other-useful-links",
            "text": "An introduction to IKVM  - This article supplies a code example of Java/.NET interoperability using IKVM.  IKVM.NET  - An open source implementation of Java for .NET (Highly recommended).  IKVM with Weka tutorial  - A tutorial on using Weka with C# using IKVM.  This tutorial is part of the WekaWiki.  Java/.NET Interop: Bridging Muddled Waters  - A very comprehensive set of articles on the interoperability problem.  Watch out for the various authors' biases towards their own companies and products though.  Microsoft .NET and Java/J2EE Interoperability  - An MSDN directory of articles on .NET / Java interopererability.  Mono  - An open source .NET development environment, cross platform.  Java/.NET Integration as Simple as Possible  - the article, which describes the simplest way to embed .NET controls into a Java GUI with OOJNI\u00ae  Problems using weka from VB .NET in VS 2005  - you may experience some hiccups when attempting to use an IKVM generated assembly in your VB .Net code in Visual Studio 2005...",
            "title": "Other Useful Links"
        },
        {
            "location": "/using_a_new_java_framework_to_create_arff_from_jpa_entity/",
            "text": "A new framework to create ARFF from JPA Entities.\n\n\nWhat makes me want to develop this project:\n\n\n\n\nNot having to worry about parameters of access to the database, since having my application integrated to JPA\n\n\nUse Entities (POJO) JPA and its metadata to generate the ARFF when you need to outsource information for use by third parties.\n\n\nGenerate new Entity (POJO) JPA based on information obtained in the ARFF (planned to release V0.9.0)\n\n\n\n\nSite About Project: \nhttp://socialsla.github.io/Weka-JPA-Persistence\n\n\nSource on GitHub: \nhttps://github.com/SocialSLA/Weka-JPA-Persistence\n\n\nHow code\n\n\ntry {\n  Weka2JPAHelper l_helper; // inject with CDI or create a class with new Weka2JPAHelper(Logger,EntityManager);\n\n  l_helper.save(new File(\"Teste.arff\"), A_JPA_Entity.class);\n}catch(IOException e){}\n\n\n\n\n\nIf you want create the data to create a ARFF file from other source, you can use a JPA Entity like the example:\n\n\ntry{\n        Weka2JPAHelper l_helper = CDIManager.get(Weka2JPAHelper.class);\n\n        Collection<Entity2Weka> l_list = new ArrayList<Entity2Weka>();\n\n        Classification l_positivo = new Classification(1, \"Positivo\");\n        Classification l_negativo = new Classification(-1, \"Negativo\");\n        Classification l_neutro = new Classification(0, \"Neutro\");\n\n        l_list.add(new Entity2Weka(\"Post numero 1 para teste do helper\", l_positivo));\n        l_list.add(new Entity2Weka(\"Post numero 2 para teste do helper\", l_negativo));\n        l_list.add(new Entity2Weka(\"Post numero 3 para teste do helper\", l_neutro));\n        l_list.add(new Entity2Weka(\"Post numero 4 para teste do helper\", l_negativo));\n        l_list.add(new Entity2Weka(\"Post numero 5 para teste do helper\", l_positivo));\n        l_list.add(new Entity2Weka(\"Post numero 6 para teste do helper\", l_negativo));\n        l_list.add(new Entity2Weka(\"Post numero 7 para teste do helper\", l_positivo));\n        l_list.add(new Entity2Weka(\"Post numero 8 para teste do helper\", l_neutro));\n        l_list.add(new Entity2Weka(\"Post numero 9 para teste do helper\", l_negativo));\n\n        l_list.add(new Entity2Weka(null, l_positivo));\n        l_list.add(new Entity2Weka(null, l_neutro));\n        l_list.add(new Entity2Weka(null, l_negativo));\n\n        l_list.add(new Entity2Weka(\"Post numero 10 para teste do helper\", null));\n        l_list.add(new Entity2Weka(\"Post numero 16 para teste do helper\", null));\n        l_list.add(new Entity2Weka(\"Post numero 7 para teste do helper\", null));\n        l_list.add(new Entity2Weka(\"Post numero 8 para teste do helper\", null));\n        l_list.add(new Entity2Weka(\"Post numero 9 para teste do helper\", null));\n\n        l_helper.save(new File(\"Test.arff\"), Entity2Weka.class, l_list);\n\n        CDIManager.stop();\n    }catch{IOException e){}\n\n\n\n\n\nEntity Example\n\n\nThe Entity class with name Entity2Weka is a Entity Base used to get attributes and make header file:\n\n\n@Entity\npublic class Entity2Weka {\n\n    @Id\n    private Integer id; // id not is used\n    @Column\n    private String post; // generate a Sring Attribute\n    @ManyToOne\n    private Classification classification; // generate a relational attribute\n\n        public Entity2Weka(){}\n\n    public Entity2Weka(String p_string, Classification p_classification) {\n        post = p_string;\n        classification = p_classification;\n    }\n}\n\n\n\n\nClassification Entity is a slave entity from entity base:\n\n\n@Entity\n@NamedQueries({ @NamedQuery(name = Classification.NAMED_QUERY_FIND_ALL, query = \"SELECT c FROM Classification c\"),\n        @NamedQuery(name = Classification.NAMED_QUERY_FIND_BY_NAME, query = \"SELECT c FROM Classification c WHERE c.name = ?\") })\npublic class Classification {\n\n    public static final String NAMED_QUERY_FIND_ALL = \"Classification.findAll\";\n    public static final String NAMED_QUERY_FIND_BY_NAME = \"Classification.findByName\";\n\n    @Id\n    private Integer id;\n    @Column(length = 50, nullable = false, unique = true)\n    private String name;\n    @Column(length = 200, nullable = true)\n    private String description;\n\n    public Classification() {\n\n    }\n\n    public Classification(int p_i, String p_name) {\n        id = p_i;\n        name = p_name;\n    }\n.....\n    public toString(){\n        return name;\n    }\n}\n\n\n\n\nResult this ARFF\n\n\n@relation Entity2Weka\n\n@attribute post string\n@attribute classification {'Desconhecido (-888)','Negativo (-1)','Neutro (0)','Positivo (1)','Ambiguo (999)'}\n\n@data\n'Post numero 1 para teste do helper','Positivo (1)'\n'Post numero 2 para teste do helper','Negativo (-1)'\n'Post numero 3 para teste do helper','Neutro (0)'\n'Post numero 4 para teste do helper','Negativo (-1)'\n'Post numero 5 para teste do helper','Positivo (1)'\n'Post numero 6 para teste do helper','Negativo (-1)'\n'Post numero 7 para teste do helper','Positivo (1)'\n'Post numero 8 para teste do helper','Neutro (0)'\n'Post numero 9 para teste do helper','Negativo (-1)'\n?,'Positivo (1)'\n?,'Neutro (0)'\n?,'Negativo (-1)'\n'Post numero 10 para teste do helper',?\n'Post numero 16 para teste do helper',?\n'Post numero 7 para teste do helper',?\n'Post numero 8 para teste do helper',?\n'Post numero 9 para teste do helper',?\n\n\n\n\nDownload source\n\n\nDownload this two pakages:\n\n\n\n\nWEKA JPA Persistence - V0.0.4\n\n\nCDI Utils - for Configuration Injection",
            "title": " Using a new Java framework to create ARFF from JPA Entity"
        },
        {
            "location": "/using_a_new_java_framework_to_create_arff_from_jpa_entity/#how-code",
            "text": "try {\n  Weka2JPAHelper l_helper; // inject with CDI or create a class with new Weka2JPAHelper(Logger,EntityManager);\n\n  l_helper.save(new File(\"Teste.arff\"), A_JPA_Entity.class);\n}catch(IOException e){}  If you want create the data to create a ARFF file from other source, you can use a JPA Entity like the example:  try{\n        Weka2JPAHelper l_helper = CDIManager.get(Weka2JPAHelper.class);\n\n        Collection<Entity2Weka> l_list = new ArrayList<Entity2Weka>();\n\n        Classification l_positivo = new Classification(1, \"Positivo\");\n        Classification l_negativo = new Classification(-1, \"Negativo\");\n        Classification l_neutro = new Classification(0, \"Neutro\");\n\n        l_list.add(new Entity2Weka(\"Post numero 1 para teste do helper\", l_positivo));\n        l_list.add(new Entity2Weka(\"Post numero 2 para teste do helper\", l_negativo));\n        l_list.add(new Entity2Weka(\"Post numero 3 para teste do helper\", l_neutro));\n        l_list.add(new Entity2Weka(\"Post numero 4 para teste do helper\", l_negativo));\n        l_list.add(new Entity2Weka(\"Post numero 5 para teste do helper\", l_positivo));\n        l_list.add(new Entity2Weka(\"Post numero 6 para teste do helper\", l_negativo));\n        l_list.add(new Entity2Weka(\"Post numero 7 para teste do helper\", l_positivo));\n        l_list.add(new Entity2Weka(\"Post numero 8 para teste do helper\", l_neutro));\n        l_list.add(new Entity2Weka(\"Post numero 9 para teste do helper\", l_negativo));\n\n        l_list.add(new Entity2Weka(null, l_positivo));\n        l_list.add(new Entity2Weka(null, l_neutro));\n        l_list.add(new Entity2Weka(null, l_negativo));\n\n        l_list.add(new Entity2Weka(\"Post numero 10 para teste do helper\", null));\n        l_list.add(new Entity2Weka(\"Post numero 16 para teste do helper\", null));\n        l_list.add(new Entity2Weka(\"Post numero 7 para teste do helper\", null));\n        l_list.add(new Entity2Weka(\"Post numero 8 para teste do helper\", null));\n        l_list.add(new Entity2Weka(\"Post numero 9 para teste do helper\", null));\n\n        l_helper.save(new File(\"Test.arff\"), Entity2Weka.class, l_list);\n\n        CDIManager.stop();\n    }catch{IOException e){}",
            "title": "How code"
        },
        {
            "location": "/using_a_new_java_framework_to_create_arff_from_jpa_entity/#entity-example",
            "text": "The Entity class with name Entity2Weka is a Entity Base used to get attributes and make header file:  @Entity\npublic class Entity2Weka {\n\n    @Id\n    private Integer id; // id not is used\n    @Column\n    private String post; // generate a Sring Attribute\n    @ManyToOne\n    private Classification classification; // generate a relational attribute\n\n        public Entity2Weka(){}\n\n    public Entity2Weka(String p_string, Classification p_classification) {\n        post = p_string;\n        classification = p_classification;\n    }\n}  Classification Entity is a slave entity from entity base:  @Entity\n@NamedQueries({ @NamedQuery(name = Classification.NAMED_QUERY_FIND_ALL, query = \"SELECT c FROM Classification c\"),\n        @NamedQuery(name = Classification.NAMED_QUERY_FIND_BY_NAME, query = \"SELECT c FROM Classification c WHERE c.name = ?\") })\npublic class Classification {\n\n    public static final String NAMED_QUERY_FIND_ALL = \"Classification.findAll\";\n    public static final String NAMED_QUERY_FIND_BY_NAME = \"Classification.findByName\";\n\n    @Id\n    private Integer id;\n    @Column(length = 50, nullable = false, unique = true)\n    private String name;\n    @Column(length = 200, nullable = true)\n    private String description;\n\n    public Classification() {\n\n    }\n\n    public Classification(int p_i, String p_name) {\n        id = p_i;\n        name = p_name;\n    }\n.....\n    public toString(){\n        return name;\n    }\n}",
            "title": "Entity Example"
        },
        {
            "location": "/using_a_new_java_framework_to_create_arff_from_jpa_entity/#result-this-arff",
            "text": "@relation Entity2Weka\n\n@attribute post string\n@attribute classification {'Desconhecido (-888)','Negativo (-1)','Neutro (0)','Positivo (1)','Ambiguo (999)'}\n\n@data\n'Post numero 1 para teste do helper','Positivo (1)'\n'Post numero 2 para teste do helper','Negativo (-1)'\n'Post numero 3 para teste do helper','Neutro (0)'\n'Post numero 4 para teste do helper','Negativo (-1)'\n'Post numero 5 para teste do helper','Positivo (1)'\n'Post numero 6 para teste do helper','Negativo (-1)'\n'Post numero 7 para teste do helper','Positivo (1)'\n'Post numero 8 para teste do helper','Neutro (0)'\n'Post numero 9 para teste do helper','Negativo (-1)'\n?,'Positivo (1)'\n?,'Neutro (0)'\n?,'Negativo (-1)'\n'Post numero 10 para teste do helper',?\n'Post numero 16 para teste do helper',?\n'Post numero 7 para teste do helper',?\n'Post numero 8 para teste do helper',?\n'Post numero 9 para teste do helper',?",
            "title": "Result this ARFF"
        },
        {
            "location": "/using_a_new_java_framework_to_create_arff_from_jpa_entity/#download-source",
            "text": "Download this two pakages:   WEKA JPA Persistence - V0.0.4  CDI Utils - for Configuration Injection",
            "title": "Download source"
        },
        {
            "location": "/using_cluster_algorithms/",
            "text": "This article discusses the use of cluster schemes in Weka from the commandline. This functionality is, of course, also available from the GUI, namely the Explorer and the KnowledgeFlow.\n\n\nCommandline\n\n\nClusterers can be used in a similar fashion Weka's classifiers:\n\n\n\n\n-t <file>\n\n\nspecifies the training file\n\n\n\n\n\n\n-T <file>\n\n\nspecifies the test file\n\n\n\n\n\n\n-p <attribute range>\n\n\nfor outputting predictions (if a test file is present, then for this one, otherwise the train file)\n\n\n\n\n\n\n-c <index>\n\n\nperforms a \nclasses to clusters\n evaluation (during training, the class attribute will be automatically ignored)\n\n\n\n\n\n\n-x <folds>\n\n\nperforms cross-validation for density-based clusterers (no \nclasses to clusters\n evaluation possible!). With \nweka.clusterers.MakeDensityBasedClusterer\n, any clusterer can be turned into a density-based one.\n\n\n\n\n\n\n-d <file> and -l <file>\n\n\nfor saving and loading \nserialized models\n\n\n\n\n\n\n\n\nSome examples:\n\n\n\n\nEM\n with train and test file:\n\n\n\n\n java weka.clusterers.EM \\\n    -I 10 \\             # only 10 iterations\n    -t train.arff \\\n    -T test.arff\n\n\n\n\n\n\nSimpleKMeans\n with \nclasses to clusters\n evaluation:\n\n\n\n\n java weka.clusterers.SimpleKMeans \n   -t train.arff \\\n   -c last             # the class attribute is the last\n\n\n\n\n\n\nSample output:\n\n\n\n\n ...\n Class attribute: \nclass\n Classes to Clusters:\n\n\n    0   1  <-- assigned to cluster\n  242 442 | 3\n   22  77 | 2\n\n Cluster 0 <-- 2\n Cluster 1 <-- 3\n\n Incorrectly clustered instances : \n      319.0    40.7407 %\n\n\n\n\n\n\nrunning 2-fold cross-validation on \nSimpleKMeans\n (we need to use \nMakeDensityBasedClusterer\n, since \nSimpleKMeans\n is no density-based clusterer!):\n\n\n\n\n java weka.clusterers.MakeDensityBasedClusterer \n    -W weka.clusterers.SimpleKMeans \\\n    -t train.arff \\\n    -x 2                # 2 folds\n\n\n\n\n\n\nSample output:\n\n\n\n\n ...\n 2 fold CV Log Likelihood: \n-40.9751\n\n\n\n\nFilters\n\n\nWeka contains some filters that make life easier with the cluster algorithms.\n\n\nAddCluster\n\n\nThe filter \nweka.filters.unsupervised.attribute.AddCluster\n adds the cluster number as nominal attribute to the data processed by the filter. This makes the post-processing or analyzing of the cluster assignments easier than with the \n-p X\n option.\n\n\nHere's an example for the UCI \ndataset\n \nanneal\n using \nSimpleKMeans\n:\n\n\n java weka.filters.unsupervised.attribute.AddCluster \\\n    -W \"weka.clusterers.SimpleKMeans -N 6 -S 42\" \\\n    -I last \\                 # we want to ignore the class attribute\n    -i anneal.arff \\\n    -o out.arff\n\n\n\n\nAnd some example output:\n\n\n @relation ...\n\n @attribute family {'?',GB,GK,GS,TN,ZA,ZF,ZH,ZM,ZS}\n @attribute product-type {C,H,G}\n ...\n @attribute class {1,2,3,4,5,U}\n @attribute cluster {cluster1,cluster2,cluster3,cluster4,cluster5,cluster6}\n\n @data\n '?',C,A,8,0,'?',S,'?',0,'?','?',G,'?','?','?','?','?','?','?','?','?','?','?','?','?','?','?','?','?','?','?',COIL,0.7,610,0,'?',0,'?',3,cluster2\n '?',C,R,0,0,'?',S,2,0,'?','?',E,'?','?','?','?','?','?','?','?','?','?','?','?','?','?','?',Y,'?','?','?',COIL,3.2,610,0,'?',0,'?',3,cluster2\n\n\n\n\nClusterMembership\n\n\nIf you're more interested in the probability for each each cluster an instance gets assigned, you can use the filter \nweka.filters.unsupervised.attribute.ClusterMembership\n.\n\n\nHere's an example for the UCI \ndataset\n \nanneal\n using \nEM\n:\n\n\n java weka.filters.unsupervised.attribute.ClusterMembership \\\n    -W weka.clusterers.EM \\\n    -I last \\                 # we want to ignore the class attribute\n    -i anneal.arff \\\n    -o out.arff \\\n    -- \\                      # additional options for EM follow after the --\n    -I 10\n\n\n\n\nAnd some example output:\n\n\n @relation ...\n\n @attribute pCluster_0_0 numeric\n @attribute pCluster_0_1 numeric\n @attribute pCluster_0_2 numeric\n @attribute pCluster_0_3 numeric\n @attribute pCluster_0_4 numeric\n @attribute pCluster_0_5 numeric\n\n @data\n 0.000147,0.009863,0,0.98999,0.000001,0\n 0.00292,0.000002,0,0.997078,0,0\n ...\n\n\n\n\nClassifiers\n\n\nClassificationViaClustering\n\n\nA new meta-classifier, \nweka.classifiers.meta.ClassificationViaClustering\n, got introduced in the developer version (>3.5.6 or \nsnapshot\n), which mimics the \nclusters to classes\n functionality of the \nweka.core.ClusterEvaluation\n class. A user defined cluster algorithm is built with the training data presented to the meta-classifier (after the class attribute got removed, of course) and then the mapping between classes and clusters is determined. This mapping is then used for predicting class labels of unseen instances.\n\n\nHere's an example for the UCI \ndataset\n \nbalance-scale\n:\n\n\n java weka.classifiers.meta.ClassificationViaClustering \\\n    -t balance-scale.arff \\\n    -W weka.clusterers.SimpleKMeans \\\n    -- \\\n    -N 3           # additional parameters for SimpleKMeans, since the dataset has 3 class labels\n\n\n\n\nAnd some sample output:\n\n\n ...\n Clusters to classes mapping:\n\n   1. Cluster: \nB (2)\n   2. Cluster: \nR (3)\n   3. Cluster: \nL (1)\n\n Classes to clusters mapping:\n\n   1. Class (L): \n3. Cluster\n   2. Class (B): \n1. Cluster\n   3. Class (R): \n2. Cluster\n ...\n\n\n\n\nNote:\n In order to obtain a useful model, you have to make sure that the cluster algorithms are properly setup for the dataset you are using. E.g., \nSimpleKMeans\n has a fixed number of clusters that it should determine. Trying to determine 2 clusters on a dataset with 5 class labels isn't very useful.\n\n\nNotes\n\n\n\n\nAll examples are for a Linux bash. For Windows or the SimplCLI, just remove the backslashes and collapse all the lines into a single one.\n\n\nComments in the examples follow a \n#\n and need to be removed, of course.\n\n\n...\n denotes an omission of unnecessary content.\n\n\n\n\nSee also\n\n\n\n\nUse Weka in your Java code\n, section \nClustering\n explains using the Weka API for clusterers\n\n\nBatch filtering\n - shows how to use filters in batch mode\n\n\nSerialization\n - for using serialized/saved models",
            "title": " Using cluster algorithms"
        },
        {
            "location": "/using_cluster_algorithms/#commandline",
            "text": "Clusterers can be used in a similar fashion Weka's classifiers:   -t <file>  specifies the training file    -T <file>  specifies the test file    -p <attribute range>  for outputting predictions (if a test file is present, then for this one, otherwise the train file)    -c <index>  performs a  classes to clusters  evaluation (during training, the class attribute will be automatically ignored)    -x <folds>  performs cross-validation for density-based clusterers (no  classes to clusters  evaluation possible!). With  weka.clusterers.MakeDensityBasedClusterer , any clusterer can be turned into a density-based one.    -d <file> and -l <file>  for saving and loading  serialized models     Some examples:   EM  with train and test file:    java weka.clusterers.EM \\\n    -I 10 \\             # only 10 iterations\n    -t train.arff \\\n    -T test.arff   SimpleKMeans  with  classes to clusters  evaluation:    java weka.clusterers.SimpleKMeans \n   -t train.arff \\\n   -c last             # the class attribute is the last   Sample output:    ...\n Class attribute: \nclass\n Classes to Clusters:\n\n\n    0   1  <-- assigned to cluster\n  242 442 | 3\n   22  77 | 2\n\n Cluster 0 <-- 2\n Cluster 1 <-- 3\n\n Incorrectly clustered instances : \n      319.0    40.7407 %   running 2-fold cross-validation on  SimpleKMeans  (we need to use  MakeDensityBasedClusterer , since  SimpleKMeans  is no density-based clusterer!):    java weka.clusterers.MakeDensityBasedClusterer \n    -W weka.clusterers.SimpleKMeans \\\n    -t train.arff \\\n    -x 2                # 2 folds   Sample output:    ...\n 2 fold CV Log Likelihood: \n-40.9751",
            "title": "Commandline"
        },
        {
            "location": "/using_cluster_algorithms/#filters",
            "text": "Weka contains some filters that make life easier with the cluster algorithms.",
            "title": "Filters"
        },
        {
            "location": "/using_cluster_algorithms/#addcluster",
            "text": "The filter  weka.filters.unsupervised.attribute.AddCluster  adds the cluster number as nominal attribute to the data processed by the filter. This makes the post-processing or analyzing of the cluster assignments easier than with the  -p X  option.  Here's an example for the UCI  dataset   anneal  using  SimpleKMeans :   java weka.filters.unsupervised.attribute.AddCluster \\\n    -W \"weka.clusterers.SimpleKMeans -N 6 -S 42\" \\\n    -I last \\                 # we want to ignore the class attribute\n    -i anneal.arff \\\n    -o out.arff  And some example output:   @relation ...\n\n @attribute family {'?',GB,GK,GS,TN,ZA,ZF,ZH,ZM,ZS}\n @attribute product-type {C,H,G}\n ...\n @attribute class {1,2,3,4,5,U}\n @attribute cluster {cluster1,cluster2,cluster3,cluster4,cluster5,cluster6}\n\n @data\n '?',C,A,8,0,'?',S,'?',0,'?','?',G,'?','?','?','?','?','?','?','?','?','?','?','?','?','?','?','?','?','?','?',COIL,0.7,610,0,'?',0,'?',3,cluster2\n '?',C,R,0,0,'?',S,2,0,'?','?',E,'?','?','?','?','?','?','?','?','?','?','?','?','?','?','?',Y,'?','?','?',COIL,3.2,610,0,'?',0,'?',3,cluster2",
            "title": "AddCluster"
        },
        {
            "location": "/using_cluster_algorithms/#clustermembership",
            "text": "If you're more interested in the probability for each each cluster an instance gets assigned, you can use the filter  weka.filters.unsupervised.attribute.ClusterMembership .  Here's an example for the UCI  dataset   anneal  using  EM :   java weka.filters.unsupervised.attribute.ClusterMembership \\\n    -W weka.clusterers.EM \\\n    -I last \\                 # we want to ignore the class attribute\n    -i anneal.arff \\\n    -o out.arff \\\n    -- \\                      # additional options for EM follow after the --\n    -I 10  And some example output:   @relation ...\n\n @attribute pCluster_0_0 numeric\n @attribute pCluster_0_1 numeric\n @attribute pCluster_0_2 numeric\n @attribute pCluster_0_3 numeric\n @attribute pCluster_0_4 numeric\n @attribute pCluster_0_5 numeric\n\n @data\n 0.000147,0.009863,0,0.98999,0.000001,0\n 0.00292,0.000002,0,0.997078,0,0\n ...",
            "title": "ClusterMembership"
        },
        {
            "location": "/using_cluster_algorithms/#classifiers",
            "text": "",
            "title": "Classifiers"
        },
        {
            "location": "/using_cluster_algorithms/#classificationviaclustering",
            "text": "A new meta-classifier,  weka.classifiers.meta.ClassificationViaClustering , got introduced in the developer version (>3.5.6 or  snapshot ), which mimics the  clusters to classes  functionality of the  weka.core.ClusterEvaluation  class. A user defined cluster algorithm is built with the training data presented to the meta-classifier (after the class attribute got removed, of course) and then the mapping between classes and clusters is determined. This mapping is then used for predicting class labels of unseen instances.  Here's an example for the UCI  dataset   balance-scale :   java weka.classifiers.meta.ClassificationViaClustering \\\n    -t balance-scale.arff \\\n    -W weka.clusterers.SimpleKMeans \\\n    -- \\\n    -N 3           # additional parameters for SimpleKMeans, since the dataset has 3 class labels  And some sample output:   ...\n Clusters to classes mapping:\n\n   1. Cluster: \nB (2)\n   2. Cluster: \nR (3)\n   3. Cluster: \nL (1)\n\n Classes to clusters mapping:\n\n   1. Class (L): \n3. Cluster\n   2. Class (B): \n1. Cluster\n   3. Class (R): \n2. Cluster\n ...  Note:  In order to obtain a useful model, you have to make sure that the cluster algorithms are properly setup for the dataset you are using. E.g.,  SimpleKMeans  has a fixed number of clusters that it should determine. Trying to determine 2 clusters on a dataset with 5 class labels isn't very useful.",
            "title": "ClassificationViaClustering"
        },
        {
            "location": "/using_cluster_algorithms/#notes",
            "text": "All examples are for a Linux bash. For Windows or the SimplCLI, just remove the backslashes and collapse all the lines into a single one.  Comments in the examples follow a  #  and need to be removed, of course.  ...  denotes an omission of unnecessary content.",
            "title": "Notes"
        },
        {
            "location": "/using_cluster_algorithms/#see-also",
            "text": "Use Weka in your Java code , section  Clustering  explains using the Weka API for clusterers  Batch filtering  - shows how to use filters in batch mode  Serialization  - for using serialized/saved models",
            "title": "See also"
        },
        {
            "location": "/using_clusterers/",
            "text": "see \nUsing cluster algorithms",
            "title": " Using clusterers"
        },
        {
            "location": "/using_the_experiment_api/",
            "text": "General\n\n\nThe \nExperimentDemo.java\n class demonstrates the use of the Experiment API (stable 3.6 or developer version):\n\n\n\n\nsetting up an experiment\n\n\none classifier\n\n\none or more datasets\n\n\nclassification or regression\n\n\ncross-validation or random split\n\n\n\n\n\n\nrunning the experiment\n\n\nevaluating the experiment and outputting the results\n\n\n\n\nClasses of the Experiment API being used:\n\n\n\n\nweka.experiment.Experiment\n - the class for peforming experiments\n\n\nweka.experiment.ClassifierSplitEvaluator\n - for classification\n\n\nweka.experiment.RegressionSplitEvaluator\n - for regression\n\n\nweka.experiment.CrossValidationResultProducer\n - for cross-validation\n\n\nweka.experiment.RandomSplitResultProducer\n - for random splits\n\n\nweka.experiment.InstancesResultListener\n - for storing the results of the experiment, used as input for the TTester algorithm\n\n\nweka.experiment.PairedCorrectedTTester\n - for generating the statistics\n\n\nsee \nClaude Nadeau, Yoshua Bengio (2001). Inference for the Generalization Error. Machine Learning.\n\n\n\n\n\n\nweka.experiment.ResultMatrixPlainText\n - for storing the statistics\n\n\n\n\nExamples\n\n\nUsage: \n\n\n java ExperimentDemo\n   -classifier <classifier incl. parameters>\n   -exptype <classification|regression>\n   -splittype <crossvalidation|randomsplit>\n   -runs <# of runs>\n   -folds <folds for CV>\n   -percentage <percentage for randomsplit>\n   -result <ARFF file for storing the results>\n   -t <dataset> (can be supplied multiple times)\n\n\n\n\nClassification\n\n\nAn example run with J48 and two UCI datasets:\n\n\n java ExperimentDemo\n   -classifier weka.classifiers.trees.J48\n   -exptype classification\n   -splittype crossvalidation\n   -runs 10\n   -folds 10\n   -result /some/where/results.arff\n   -t vote.arff\n   -t iris.arff\n\n\n\n\nAnd the output:\n\n\n  Setting up...\n  Initializing...\n  Running...\n  Finishing...\n  Evaluating...\n\n  Result:\n\n  (1) vote\n      Perc. correct: \n96.57135311000002\n      StdDev: \n2.560851001842444\n  (2) iris\n      Perc. correct: \n94.73333325999994\n      StdDev: \n5.300826810632913\n\n\n\n\nRegression\n\n\nAnother example with M5P and two numeric UCI datasets:\n\n\n java ExperimentDemo\n   -classifier weka.classifiers.trees.M5P\n   -exptype regression\n   -splittype randomsplit\n   -runs 10\n   -percentage 66\n   -result /some/where/results.arff\n   -t bolts.arff\n   -t bodyfat.arff\n\n\n\n\nAnd the associated output:\n\n\n  Setting up...\n  Initializing...\n  Running...\n  Finishing...\n  Evaluating...\n\n  Result:\n\n  (1) bolts\n      Perc. correct: \n0.9701825\n      StdDev: \n0.017970627641614084\n  (2) bodyfat.names\n      Perc. correct: \n0.9795883\n      StdDev: \n0.011646527074622525\n\n\n\n\nSee also\n\n\n\n\nUse Weka in your Java code\n - for general use of the Weka API\n\n\n\n\nDownloads\n\n\n\n\nExperimentDemo.java",
            "title": " Using the Experiment API"
        },
        {
            "location": "/using_the_experiment_api/#general",
            "text": "The  ExperimentDemo.java  class demonstrates the use of the Experiment API (stable 3.6 or developer version):   setting up an experiment  one classifier  one or more datasets  classification or regression  cross-validation or random split    running the experiment  evaluating the experiment and outputting the results   Classes of the Experiment API being used:   weka.experiment.Experiment  - the class for peforming experiments  weka.experiment.ClassifierSplitEvaluator  - for classification  weka.experiment.RegressionSplitEvaluator  - for regression  weka.experiment.CrossValidationResultProducer  - for cross-validation  weka.experiment.RandomSplitResultProducer  - for random splits  weka.experiment.InstancesResultListener  - for storing the results of the experiment, used as input for the TTester algorithm  weka.experiment.PairedCorrectedTTester  - for generating the statistics  see  Claude Nadeau, Yoshua Bengio (2001). Inference for the Generalization Error. Machine Learning.    weka.experiment.ResultMatrixPlainText  - for storing the statistics",
            "title": "General"
        },
        {
            "location": "/using_the_experiment_api/#examples",
            "text": "Usage:    java ExperimentDemo\n   -classifier <classifier incl. parameters>\n   -exptype <classification|regression>\n   -splittype <crossvalidation|randomsplit>\n   -runs <# of runs>\n   -folds <folds for CV>\n   -percentage <percentage for randomsplit>\n   -result <ARFF file for storing the results>\n   -t <dataset> (can be supplied multiple times)",
            "title": "Examples"
        },
        {
            "location": "/using_the_experiment_api/#classification",
            "text": "An example run with J48 and two UCI datasets:   java ExperimentDemo\n   -classifier weka.classifiers.trees.J48\n   -exptype classification\n   -splittype crossvalidation\n   -runs 10\n   -folds 10\n   -result /some/where/results.arff\n   -t vote.arff\n   -t iris.arff  And the output:    Setting up...\n  Initializing...\n  Running...\n  Finishing...\n  Evaluating...\n\n  Result:\n\n  (1) vote\n      Perc. correct: \n96.57135311000002\n      StdDev: \n2.560851001842444\n  (2) iris\n      Perc. correct: \n94.73333325999994\n      StdDev: \n5.300826810632913",
            "title": "Classification"
        },
        {
            "location": "/using_the_experiment_api/#regression",
            "text": "Another example with M5P and two numeric UCI datasets:   java ExperimentDemo\n   -classifier weka.classifiers.trees.M5P\n   -exptype regression\n   -splittype randomsplit\n   -runs 10\n   -percentage 66\n   -result /some/where/results.arff\n   -t bolts.arff\n   -t bodyfat.arff  And the associated output:    Setting up...\n  Initializing...\n  Running...\n  Finishing...\n  Evaluating...\n\n  Result:\n\n  (1) bolts\n      Perc. correct: \n0.9701825\n      StdDev: \n0.017970627641614084\n  (2) bodyfat.names\n      Perc. correct: \n0.9795883\n      StdDev: \n0.011646527074622525",
            "title": "Regression"
        },
        {
            "location": "/using_the_experiment_api/#see-also",
            "text": "Use Weka in your Java code  - for general use of the Weka API",
            "title": "See also"
        },
        {
            "location": "/using_the_experiment_api/#downloads",
            "text": "ExperimentDemo.java",
            "title": "Downloads"
        },
        {
            "location": "/using_the_mathexpression_filter/",
            "text": "The filter \nMathExpression\n can be found in this package:\n\n\n weka.filters.unsupervised.attribute\n\n\n\n\nIt provides a powerful means of performing mathematical transformations of numeric attributes. The following operators are supported:\n\n\n +, -, *, /, (, ), \n pow, log, abs, cos, exp, sqrt, tan, sin, ceil, floor, rint, \n MEAN, MAX, MIN, SD, COUNT, SUM, SUMSQUARED, ifelse\n\n\n\n\nThe attribute value that is being processed, can be referenced as \nA\n.\n\n\nManual discretization\n\n\nOne can even use the filter for manually discretizing numeric attributes, if the other \nDiscretize\n filters (supervised and unsupervised) cannot be used. This works thanks to the \nifelse\n operator. \n\n\nIt is basically a two-step-process:\n\n\n\n\nrun \nMathExpression\n to turn all the values into discrete ones\n\n\nrun \nNumericToNominal\n to turn the numeric values then into nominal labels\n\n\n\n\nHere's an example:\n\n\n\n\na dataset where the first attribute needs to be discretized into 3 bins\n\n\nthe bins need to be as follows\n\n\n\n\n (-inf-20.0]\n (20.0-80.0]\n (80.0-inf)\n\n\n\n\n\n\nusing \nMathExpression\n to create discrete values\n\n\n\n\n weka.filters.unsupervised.attribute.MathExpression \\\n   -E \"ifelse(A>20, ifelse(A>80, 3, 2), 1)\" \\\n   -V \\\n   -R 1\n\n\n\n\n\n\nNote:\n \n-V -R 1\n means we only want to transform the first attribute. Without \n-V\n all the numeric attributes would be transformed according to this expression.\n* this results in the following transformation\n\n\n\n\n (-inf-20.0] -> 1\n (20.0-80.0] -> 2\n (80.0-inf)  -> 3\n\n\n\n\n\n\nusing \nNumericToBinary\n to create a nominal attribute from the numeric one\n\n\n\n\n weka.filters.unsupervised.attribute.NumericToNominal \\\n   -R 1\n\n\n\n\n\n\noptional:\n if one wants to rename those labels, one can use the class listed in the \nRename Attribute Values\n article for that\n\n\n\n\nNote:\n the \"\\\" at the end of the lines tell a *nix \nbash\n to continue on the next line.",
            "title": " Using the MathExpression filter"
        },
        {
            "location": "/using_the_mathexpression_filter/#manual-discretization",
            "text": "One can even use the filter for manually discretizing numeric attributes, if the other  Discretize  filters (supervised and unsupervised) cannot be used. This works thanks to the  ifelse  operator.   It is basically a two-step-process:   run  MathExpression  to turn all the values into discrete ones  run  NumericToNominal  to turn the numeric values then into nominal labels   Here's an example:   a dataset where the first attribute needs to be discretized into 3 bins  the bins need to be as follows    (-inf-20.0]\n (20.0-80.0]\n (80.0-inf)   using  MathExpression  to create discrete values    weka.filters.unsupervised.attribute.MathExpression \\\n   -E \"ifelse(A>20, ifelse(A>80, 3, 2), 1)\" \\\n   -V \\\n   -R 1   Note:   -V -R 1  means we only want to transform the first attribute. Without  -V  all the numeric attributes would be transformed according to this expression.\n* this results in the following transformation    (-inf-20.0] -> 1\n (20.0-80.0] -> 2\n (80.0-inf)  -> 3   using  NumericToBinary  to create a nominal attribute from the numeric one    weka.filters.unsupervised.attribute.NumericToNominal \\\n   -R 1   optional:  if one wants to rename those labels, one can use the class listed in the  Rename Attribute Values  article for that   Note:  the \"\\\" at the end of the lines tell a *nix  bash  to continue on the next line.",
            "title": "Manual discretization"
        },
        {
            "location": "/using_weka_from_groovy/",
            "text": "Groovy...\n\n\n\n\nis an agile and dynamic language for the Java Virtual Machine\n\n\nbuilds upon the strengths of Java but has additional power features inspired by languages like Python, Ruby and Smalltalk\n\n\nmakes modern programming features available to Java developers with almost-zero learning curve\n\n\nsupports Domain-Specific Languages and other compact syntax so your code becomes easy to read and maintain\n\n\nmakes writing shell and build scripts easy with its powerful processing primitives, OO abilities and an Ant DSL\n\n\nincreases developer productivity by reducing scaffolding code when developing web, GUI, database or console applications\n\n\nsimplifies testing by supporting unit testing and mocking out-of-the-box\n\n\nseamlessly integrates with all existing Java objects and libraries\n\n\ncompiles straight to Java bytecode so you can use it anywhere you can use Java\n\n\n\n\n-- taken from the \nGroovy homepage\n\n\nThis article explains how to use Weka classes within Groovy.\n\n\nGroovy CLASSPATH\n\n\nAdditional jars can be added to Groovy in various ways:\n\n\n\n\n$GROOVY_HOME/lib\n\n\nAny jar that is placed in this directory will be available in Groovy\n\n\n\n\n\n\n$GROOVY_HOME/conf/groovy-starter.conf\n\n\nThis file lists jar files and directories from which to include jar files. The syntax is fairly easy:\n\n\n\n\n\n\n\n\n\n\n\n\nload <path>\n\n\nFor example, loading the \nweka.jar\n that is located in $HOME/myjars:\n\n\nload !{user.home}/myjars/weka.jar\n\n\nOr loading all jars in a directory, e.g., $HOME/myjars:\n\n\nload !{user.home}/myjars/*.jar\n\n\n\n\n\n\n\n\nJava \nCLASSPATH\n\n\n\n\n\n\nGroovy automatically imports the Java \nCLASSPATH\n, i.e., everything that is listed in the CLASSPATH environment variable.\n\n\n\n\n\n\n-classpath option\n\n\n\n\n\n\nWhen running a Groovy script, you can explicitly tell Groovy, what CLASSPATH to use:\n\n\n\n\ngroovy -classpath <jars, etc=\"\"> <script.groovy>\n\n\n\n\n\n\nThe Grape:\n\n\nGroovy dependency manager \n\n\nGrape is a JAR dependency manager embedded into Groovy. Grape lets you quickly add maven repository dependencies to your classpath, making scripting even easier. Weka can be added as a dependency in your Groovy script\n\n\nYou can also search for dependencies and versions numbers on \nmvnrepository.com\n and it will provide you the \nGrab\n annotation form of the \npom.xml\n entry.\n\n\nAccessing Weka classes from Groovy\n\n\nRequirements\n\n\n\n\nGroovy 1.5.7 or later\n\n\nWeka 3.5.4 or later\n\n\n\n\nImplementation\n\n\nIf your Groovy CLASSPATH has been setup correctly, you can use all those classes in Groovy straight away. E.g., after including the \nweka.jar\n, I can run the following little script (\nUsingJ48.groovy\n to train J48 on a supplied dataset and output its model:\n\n\nimport weka.classifiers.trees.J48\nimport weka.core.converters.ConverterUtils.DataSource\nimport weka.core.Instances\n\nif (args.size() == 0) {\n  println \"Usage: UsingJ48.groovy <arff-file>\"\n  System.exit(0)\n}\n\n// load data and set class index\ndata = DataSource.read(args[0])\ndata.setClassIndex(data.numAttributes() - 1)\n\n// create the model\nj48 = new J48()\nj48.buildClassifier(data)\n\n// print out the built model\nprintln j48\n\n\n\n\nA slightly more elaborate example can be found in \nUsingJ48Ext.groovy\n, which uses more methods of the \nweka.classifiers.Evaluation\n class.\n\n\nNB:\n The example \nUsingJ48Ext.groovy\n needs Weka 3.6.x (or a snapshot of the developer version) to run, due to some changes in the API.\n\n\nImplementing a Groovy classifier\n\n\nRequirements\n\n\n\n\nGroovy 1.5.7 or later\n\n\ndeveloper version of Weka later than 3.5.8, \nsnapshots\n later than 02/02/2009\n\n\n\n\nImplementation\n\n\nImplementing a Groovy classifier is pretty straight-forward, since the syntax is almost the same and Java classes are imported/used in Groovy just like in Java. The \nGeroR.groovy\n file re-implements the \nweka.classifiers.rules.ZeroR\n classifier as Groovy script.\n\n\nThe class declaration for \nGeroR.groovy\n looks like this, for instance:\n\n\nclass GeroR\n  extends Classifier\n  implements WeightedInstancesHandler {\n    ...\n}\n\n\n\n\nFor more information on implementing classifiers, see the \nWriting your own Classifier\n article.\n\n\nExecution\n\n\nGroovy\n\n\nAs long as you have the \nweka.jar\n in your Groovy environment, you can directly run these scripts using the \ngroovy\n command.\nHere is an example, executing the \nFunkyClassifier.groovy\n script in a Linux bash:\n\n\ngroovy -classpath weka.jar \\\n  /some/where/FunkyClassifier.groovy \\\n  -t /my/datasets/data.arff \\\n  <more options for the Groovy script>\n\n\n\n\nWeka\n\n\nIn order to execute classifiers written in Groovy, you have to use the \nweka.classifiers.scripting.GroovyClassifier\n classifier.\nHere is an example, executing the \nFunkyClassifier.groovy\n script in a Linux bash:\n\n\njava weka.jar:groovy-all-1.5.7.jar \\\n  weka.classifiers.scripting.GroovyClassifier \\\n  -t /my/datasets/data.arff \\\n  -G /some/where/FunkyClassifier.groovy \\\n  -- <more options for the Groovy script>\n\n\n\n\nDownloads\n\n\n\n\nUsingJ48.groovy\n - simple example for using J48 in a Groovy script\n\n\nUsingJ48Ext.groovy\n - a slightly more elaborate example script for using J48\n\n\nCustomCV.groovy\n thread\n\n\nGeroR.groovy\n - \nweka.classifiers.rules.ZeroR\n implemented in Groovy\n\n\nLibsvmWeights.groovy\n and outputs the best weights\n\n\nmultiple_eval.groovy\n - evaluates multiple classifiers on multiple train/test pairs and outputs statistics\n\n\nAttributeStatistics.groovy\n - Outputs statistics for each attribute in a dataset\n\n\n\n\nLinks\n\n\n\n\nGroovy\n\n\nHomepage\n\n\nGetting Started\n\n\nStyle Guide\n\n\nSupport",
            "title": " Using Weka from Groovy"
        },
        {
            "location": "/using_weka_from_groovy/#groovy-classpath",
            "text": "Additional jars can be added to Groovy in various ways:   $GROOVY_HOME/lib  Any jar that is placed in this directory will be available in Groovy    $GROOVY_HOME/conf/groovy-starter.conf  This file lists jar files and directories from which to include jar files. The syntax is fairly easy:       load <path>  For example, loading the  weka.jar  that is located in $HOME/myjars:  load !{user.home}/myjars/weka.jar  Or loading all jars in a directory, e.g., $HOME/myjars:  load !{user.home}/myjars/*.jar     Java  CLASSPATH    Groovy automatically imports the Java  CLASSPATH , i.e., everything that is listed in the CLASSPATH environment variable.    -classpath option    When running a Groovy script, you can explicitly tell Groovy, what CLASSPATH to use:   groovy -classpath <jars, etc=\"\"> <script.groovy>",
            "title": "Groovy CLASSPATH"
        },
        {
            "location": "/using_weka_from_groovy/#the-grape",
            "text": "Groovy dependency manager   Grape is a JAR dependency manager embedded into Groovy. Grape lets you quickly add maven repository dependencies to your classpath, making scripting even easier. Weka can be added as a dependency in your Groovy script  You can also search for dependencies and versions numbers on  mvnrepository.com  and it will provide you the  Grab  annotation form of the  pom.xml  entry.",
            "title": "The Grape:"
        },
        {
            "location": "/using_weka_from_groovy/#accessing-weka-classes-from-groovy",
            "text": "",
            "title": "Accessing Weka classes from Groovy"
        },
        {
            "location": "/using_weka_from_groovy/#requirements",
            "text": "Groovy 1.5.7 or later  Weka 3.5.4 or later",
            "title": "Requirements"
        },
        {
            "location": "/using_weka_from_groovy/#implementation",
            "text": "If your Groovy CLASSPATH has been setup correctly, you can use all those classes in Groovy straight away. E.g., after including the  weka.jar , I can run the following little script ( UsingJ48.groovy  to train J48 on a supplied dataset and output its model:  import weka.classifiers.trees.J48\nimport weka.core.converters.ConverterUtils.DataSource\nimport weka.core.Instances\n\nif (args.size() == 0) {\n  println \"Usage: UsingJ48.groovy <arff-file>\"\n  System.exit(0)\n}\n\n// load data and set class index\ndata = DataSource.read(args[0])\ndata.setClassIndex(data.numAttributes() - 1)\n\n// create the model\nj48 = new J48()\nj48.buildClassifier(data)\n\n// print out the built model\nprintln j48  A slightly more elaborate example can be found in  UsingJ48Ext.groovy , which uses more methods of the  weka.classifiers.Evaluation  class.  NB:  The example  UsingJ48Ext.groovy  needs Weka 3.6.x (or a snapshot of the developer version) to run, due to some changes in the API.",
            "title": "Implementation"
        },
        {
            "location": "/using_weka_from_groovy/#implementing-a-groovy-classifier",
            "text": "",
            "title": "Implementing a Groovy classifier"
        },
        {
            "location": "/using_weka_from_groovy/#requirements_1",
            "text": "Groovy 1.5.7 or later  developer version of Weka later than 3.5.8,  snapshots  later than 02/02/2009",
            "title": "Requirements"
        },
        {
            "location": "/using_weka_from_groovy/#implementation_1",
            "text": "Implementing a Groovy classifier is pretty straight-forward, since the syntax is almost the same and Java classes are imported/used in Groovy just like in Java. The  GeroR.groovy  file re-implements the  weka.classifiers.rules.ZeroR  classifier as Groovy script.  The class declaration for  GeroR.groovy  looks like this, for instance:  class GeroR\n  extends Classifier\n  implements WeightedInstancesHandler {\n    ...\n}  For more information on implementing classifiers, see the  Writing your own Classifier  article.",
            "title": "Implementation"
        },
        {
            "location": "/using_weka_from_groovy/#execution",
            "text": "",
            "title": "Execution"
        },
        {
            "location": "/using_weka_from_groovy/#groovy",
            "text": "As long as you have the  weka.jar  in your Groovy environment, you can directly run these scripts using the  groovy  command.\nHere is an example, executing the  FunkyClassifier.groovy  script in a Linux bash:  groovy -classpath weka.jar \\\n  /some/where/FunkyClassifier.groovy \\\n  -t /my/datasets/data.arff \\\n  <more options for the Groovy script>",
            "title": "Groovy"
        },
        {
            "location": "/using_weka_from_groovy/#weka",
            "text": "In order to execute classifiers written in Groovy, you have to use the  weka.classifiers.scripting.GroovyClassifier  classifier.\nHere is an example, executing the  FunkyClassifier.groovy  script in a Linux bash:  java weka.jar:groovy-all-1.5.7.jar \\\n  weka.classifiers.scripting.GroovyClassifier \\\n  -t /my/datasets/data.arff \\\n  -G /some/where/FunkyClassifier.groovy \\\n  -- <more options for the Groovy script>",
            "title": "Weka"
        },
        {
            "location": "/using_weka_from_groovy/#downloads",
            "text": "UsingJ48.groovy  - simple example for using J48 in a Groovy script  UsingJ48Ext.groovy  - a slightly more elaborate example script for using J48  CustomCV.groovy  thread  GeroR.groovy  -  weka.classifiers.rules.ZeroR  implemented in Groovy  LibsvmWeights.groovy  and outputs the best weights  multiple_eval.groovy  - evaluates multiple classifiers on multiple train/test pairs and outputs statistics  AttributeStatistics.groovy  - Outputs statistics for each attribute in a dataset",
            "title": "Downloads"
        },
        {
            "location": "/using_weka_from_groovy/#links",
            "text": "Groovy  Homepage  Getting Started  Style Guide  Support",
            "title": "Links"
        },
        {
            "location": "/using_weka_from_jython/",
            "text": "Jython is an implementation of the high-level, dynamic, object-oriented language Python written in 100% Pure Java, and seamlessly integrated with the Java platform. It thus allows you to run Python on any Java platform.\n\n\n-- taken from the \nJython homepage\n\n\nThis article explains how use Weka classes from within Jython and how to write a classifier in Jython that can be used within the Weka framework.\n\n\nAccessing Weka classes from Jython\n\n\nRequirements\n\n\nIn order for Jython to find the Weka classes, you must export them in your \nCLASSPATH\n. Here is an example for adding the \nweka.jar\n located in the directory \n/some/where\n to the CLASSPATH in a bash under Linux:\n\n\nexport CLASSPATH=$CLASSPATH:/some/where/weka.jar\n\n\n\n\nNote:\n Windows users must just the backslash (\"\\\") in the command prompt instead of the slash (\"/\") in paths.\n\n\nImplementation\n\n\nAs soon as one imports classes in a Jython module one can use that class just like in Java. E.g., if one wants to use the J48 classifier, one only needs to import it as follows:\n\n\n import weka.classifiers.trees.J48 as J48\n\n\n\n\nHere's a Jython module (\nUsingJ48.py\n):\n\n\n import sys\n\n import java.io.FileReader as FileReader\n import weka.core.Instances as Instances\n import weka.classifiers.trees.J48 as J48\n\n # load data file\n file = FileReader(\"/some/where/file.arff\")\n data = Instances(file)\n data.setClassIndex(data.numAttributes() - 1)\n\n # create the model\n j48 = J48()\n j48.buildClassifier(data)\n\n # print out the built model\n print j48\n\n\n\n\nA slightly more elaborate example can be found in \nUsingJ48Ext.py\n, which uses more methods of the \nweka.classifiers.Evaluation\n class.\n\n\nNB:\n The example \nUsingJ48Ext.py\n needs Weka 3.6.x to run, due to some changes in the API.\n\n\nImplementing a Jython classifier\n\n\nRequirements\n\n\n\n\nWeka >3.5.6 (or \nsnapshot\n from 02/08/2007 or later)\n\n\nJython 2.2rc2 (later versions should work as well)\n\n\n\n\nImplementation\n\n\nThis section covers the implementation of \nweka.classifiers.rules.ZeroR\n in Python, \nJeroR.py\n:\n\n\n\n\nSubclass an abstract superclass of Weka classifiers (in this case \nweka.classifiers.Classifier\n):\n\n\n\n\n\n\nclass JeroR (**Classifier**, JythonSerializableObject):\n\n\nNote:\n the \nJythonSerializableObject\n interface is necessary for Serialization purposes (Weka creates copies of classifiers via \nserialization\n)\n\n\n\n\n\n\n\n\nYou have to implement the following methods:\n\n\n\n\ndef listOptions(self):\n\n\n\n\nReturns an \njava.util.Enumeration\n of \nweka.core.Option\n objects of all available options. Calling the superclass method is done with \n*<superclass>*.listOptions()\n, e.g., \nClassifier.listOptions()\n.\n\n\n\n\n\n\n\n\ndef setOptions(self, options):\n\n\n\n\nSets the commandline options, with the parameter \noptions\n being an array of strings.\n\n\n\n\n\n\n\n\ndef getOptions(self):\n\n\n\n\nReturns an array of strings, containing all the currently set options (to be used with \nsetOptions(self,options)\n).\n\n\n\n\n\n\n\n\ndef getCapabilities(self):\n\n\n\n\nReturns a \nweka.core.Capabilities\n object with information about what attributes and classes can be processed by this algorithm.\n\n\n\n\n\n\n\n\ndef buildClassifier(self, instances):\n\n\n\n\nThis method builds the actual model based on the data provided. The first statements in this method should be the ones checking the capabilities of the algorithm against the data and removing all instances with a missing class value:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# check the capabilities\nself.getCapabilities().testWithFail(instances)\n# remove instances with missing class\ninstances = Instances(instances)\ninstances.deleteWithMissingClass()\n\n\n\n\n\n\n\n\nat least \none\n of the following two:\n\n\n\n\ndef classifyInstance(self, instance):\n\n\n\n\nReturns either the index of the predicted class label (for nominal classes) or the regression result (for numeric classes)\n\n\n\n\n\n\n\n\ndef distributionForInstance(self, instance):\n\n\n\n\nThis method returns an array of doubles containing the probabilities for all class labels. In case of a numeric class attribute, the length of this array is 1. In Jython, you can use the \n[jarray](http://www.jython.org/docs/jarray.html)\n module to generate a double array. With the following line you can create the correct array to be returned by this method (you still need to fill it with values):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nresult = jarray.zeros(instance.numClasses(), 'd')\n\n\nOf course, the elements of this array must sum up to 1.\n    * \ndef toString(self):\n\nReturns a string describing the not-yet-built or built model.\n\n\n\n\n\n\n\n\nThe following code snippet simulates the \"main\" method; it creates an instance of the classifier and passes it on to the \nClassifier.runClassifier\n method:\n\n\n\n\nif __name__ = \"__main__\":\n\n    Classifier.runClassifier(JeroR(), sys.argv[1:])\n\n\n\n\n\n\nThis doesn't work right out-of-the-box, since Jython cannot access protected static methods in superclasses. One has to set the following value in the \nJython registry\n to make it work (taken from \nthis\n FAQ):\n\n\n\n\npython.security.respectJavaAccessibility=false\n\n\n\n\nDocumentation\n\n\nDocumentation in Python is done with the so-called \ndoc strings\n within the class or method the documentation is for. Using \nHappyDoc\n, one can use \nstructured text\n to output nice HTML, similar to \nJavadoc\n.\n\n\n\n\nClass doc string:\n\n\n\n\n class JeroR (Classifier, JythonSerializableObject):\n\n     \"\"\"\n     JeroR is a Jython implementation of the Weka classifier ZeroR\n\n     'author' -- FracPete (fracpete at waikato dot ac dot nz)\n\n     'version' -- $Revision$\n     \"\"\"\n\n\n\n\n\n\nNote:\n the \n$Revision$\n tag is filled in by a source control system like CVS or \nSubversion\n.\n\n\n\n\n\n\nMethod doc string:\n\n\n\n\n def classifyInstance(self, instance):\n\n     \"\"\"\n     returns the prediction for the given instance\n\n     Parameter(s):\n\n\n         'instance' -- the instance to predict the class value for\n\n     Return:\n\n\n         the prediction for the given instance\n     \"\"\"\n\n\n\n\nExecution\n\n\nNote:\n The commands listed here for a Linux/Unix bash, for Windows remove all the backslashes (\"\\\") at the end of the lines and assemble the command in a single line. Under Windows, the path separator \":\" used in the \nCLASSPATH\n needs to be replaced with \";\" as well.\n\n\nJython\n\n\nThe Jython classifier, e.g., \nFunkyClassifier.py\n, can be run like this from commandline, with only the \nweka.jar\n and the \njython.jar\n in the \nCLASSPATH\n:\n\n\n java -classpath weka.jar:jython.jar \\\n   org.python.util.jython \\\n   /some/place/FunkyClassifier.py \\\n   -t /some/where/file.arff\n\n\n\n\nWeka\n\n\nIn order to execute the Jython classifier \nFunkyClassifier.py\n with Weka, one basically only needs to have the \nweka.jar\n and the \njython.jar\n in the \nCLASSPATH\n and call the \nweka.classifiers.JythonClassifier\n classifier with the Jython classifier, i.e., \nFunkyClassifier.py\n, as parameter (\"\n-J\n\"):\n\n\n java -classpath weka.jar:jython.jar \\\n   weka.classifiers.JythonClassifier \\\n   -J /some/place/FunkyClassifier.py \\\n   -t /some/where/file.arff\n\n\n\n\nDownloads\n\n\n\n\nUsingJ48.py\n\n\nUsingJ48Ext.py\n\n\nJeroR.py\n - \nweka.classifiers.rules.ZeroR\n as Jython script implemented\n\n\n\n\nSee also\n\n\n\n\nUse Weka in your Java code\n - for general information on how to use the Weka API\n\n\nUsing Weka via Jepp\n - using the \njavax.script\n approach to interface Java and Python\n\n\n\n\nLinks\n\n\n\n\nJython\n\n\nHomepage\n\n\nJava arrays\n\n\nRegistry\n\n\nEmbedding Jython\n\n\n\n\n\n\nPython\n\n\nHomepage\n\n\nHappyDoc\n - generating documentation from Jython/Python modules\n\n\n\n\n\n\nJava\n\n\nHomepage\n\n\nJavadoc\n\n\n\n\n\n\nEclipse\n\n\nHomepage\n\n\nPyDev plugin",
            "title": " Using Weka from Jython"
        },
        {
            "location": "/using_weka_from_jython/#accessing-weka-classes-from-jython",
            "text": "",
            "title": "Accessing Weka classes from Jython"
        },
        {
            "location": "/using_weka_from_jython/#requirements",
            "text": "In order for Jython to find the Weka classes, you must export them in your  CLASSPATH . Here is an example for adding the  weka.jar  located in the directory  /some/where  to the CLASSPATH in a bash under Linux:  export CLASSPATH=$CLASSPATH:/some/where/weka.jar  Note:  Windows users must just the backslash (\"\\\") in the command prompt instead of the slash (\"/\") in paths.",
            "title": "Requirements"
        },
        {
            "location": "/using_weka_from_jython/#implementation",
            "text": "As soon as one imports classes in a Jython module one can use that class just like in Java. E.g., if one wants to use the J48 classifier, one only needs to import it as follows:   import weka.classifiers.trees.J48 as J48  Here's a Jython module ( UsingJ48.py ):   import sys\n\n import java.io.FileReader as FileReader\n import weka.core.Instances as Instances\n import weka.classifiers.trees.J48 as J48\n\n # load data file\n file = FileReader(\"/some/where/file.arff\")\n data = Instances(file)\n data.setClassIndex(data.numAttributes() - 1)\n\n # create the model\n j48 = J48()\n j48.buildClassifier(data)\n\n # print out the built model\n print j48  A slightly more elaborate example can be found in  UsingJ48Ext.py , which uses more methods of the  weka.classifiers.Evaluation  class.  NB:  The example  UsingJ48Ext.py  needs Weka 3.6.x to run, due to some changes in the API.",
            "title": "Implementation"
        },
        {
            "location": "/using_weka_from_jython/#implementing-a-jython-classifier",
            "text": "",
            "title": "Implementing a Jython classifier"
        },
        {
            "location": "/using_weka_from_jython/#requirements_1",
            "text": "Weka >3.5.6 (or  snapshot  from 02/08/2007 or later)  Jython 2.2rc2 (later versions should work as well)",
            "title": "Requirements"
        },
        {
            "location": "/using_weka_from_jython/#implementation_1",
            "text": "This section covers the implementation of  weka.classifiers.rules.ZeroR  in Python,  JeroR.py :   Subclass an abstract superclass of Weka classifiers (in this case  weka.classifiers.Classifier ):    class JeroR (**Classifier**, JythonSerializableObject):  Note:  the  JythonSerializableObject  interface is necessary for Serialization purposes (Weka creates copies of classifiers via  serialization )     You have to implement the following methods:   def listOptions(self):   Returns an  java.util.Enumeration  of  weka.core.Option  objects of all available options. Calling the superclass method is done with  *<superclass>*.listOptions() , e.g.,  Classifier.listOptions() .     def setOptions(self, options):   Sets the commandline options, with the parameter  options  being an array of strings.     def getOptions(self):   Returns an array of strings, containing all the currently set options (to be used with  setOptions(self,options) ).     def getCapabilities(self):   Returns a  weka.core.Capabilities  object with information about what attributes and classes can be processed by this algorithm.     def buildClassifier(self, instances):   This method builds the actual model based on the data provided. The first statements in this method should be the ones checking the capabilities of the algorithm against the data and removing all instances with a missing class value:        # check the capabilities\nself.getCapabilities().testWithFail(instances)\n# remove instances with missing class\ninstances = Instances(instances)\ninstances.deleteWithMissingClass()    at least  one  of the following two:   def classifyInstance(self, instance):   Returns either the index of the predicted class label (for nominal classes) or the regression result (for numeric classes)     def distributionForInstance(self, instance):   This method returns an array of doubles containing the probabilities for all class labels. In case of a numeric class attribute, the length of this array is 1. In Jython, you can use the  [jarray](http://www.jython.org/docs/jarray.html)  module to generate a double array. With the following line you can create the correct array to be returned by this method (you still need to fill it with values):          result = jarray.zeros(instance.numClasses(), 'd')  Of course, the elements of this array must sum up to 1.\n    *  def toString(self): \nReturns a string describing the not-yet-built or built model.     The following code snippet simulates the \"main\" method; it creates an instance of the classifier and passes it on to the  Classifier.runClassifier  method:   if __name__ = \"__main__\":\n\n    Classifier.runClassifier(JeroR(), sys.argv[1:])   This doesn't work right out-of-the-box, since Jython cannot access protected static methods in superclasses. One has to set the following value in the  Jython registry  to make it work (taken from  this  FAQ):   python.security.respectJavaAccessibility=false",
            "title": "Implementation"
        },
        {
            "location": "/using_weka_from_jython/#documentation",
            "text": "Documentation in Python is done with the so-called  doc strings  within the class or method the documentation is for. Using  HappyDoc , one can use  structured text  to output nice HTML, similar to  Javadoc .   Class doc string:    class JeroR (Classifier, JythonSerializableObject):\n\n     \"\"\"\n     JeroR is a Jython implementation of the Weka classifier ZeroR\n\n     'author' -- FracPete (fracpete at waikato dot ac dot nz)\n\n     'version' -- $Revision$\n     \"\"\"   Note:  the  $Revision$  tag is filled in by a source control system like CVS or  Subversion .    Method doc string:    def classifyInstance(self, instance):\n\n     \"\"\"\n     returns the prediction for the given instance\n\n     Parameter(s):\n\n\n         'instance' -- the instance to predict the class value for\n\n     Return:\n\n\n         the prediction for the given instance\n     \"\"\"",
            "title": "Documentation"
        },
        {
            "location": "/using_weka_from_jython/#execution",
            "text": "Note:  The commands listed here for a Linux/Unix bash, for Windows remove all the backslashes (\"\\\") at the end of the lines and assemble the command in a single line. Under Windows, the path separator \":\" used in the  CLASSPATH  needs to be replaced with \";\" as well.",
            "title": "Execution"
        },
        {
            "location": "/using_weka_from_jython/#jython",
            "text": "The Jython classifier, e.g.,  FunkyClassifier.py , can be run like this from commandline, with only the  weka.jar  and the  jython.jar  in the  CLASSPATH :   java -classpath weka.jar:jython.jar \\\n   org.python.util.jython \\\n   /some/place/FunkyClassifier.py \\\n   -t /some/where/file.arff",
            "title": "Jython"
        },
        {
            "location": "/using_weka_from_jython/#weka",
            "text": "In order to execute the Jython classifier  FunkyClassifier.py  with Weka, one basically only needs to have the  weka.jar  and the  jython.jar  in the  CLASSPATH  and call the  weka.classifiers.JythonClassifier  classifier with the Jython classifier, i.e.,  FunkyClassifier.py , as parameter (\" -J \"):   java -classpath weka.jar:jython.jar \\\n   weka.classifiers.JythonClassifier \\\n   -J /some/place/FunkyClassifier.py \\\n   -t /some/where/file.arff",
            "title": "Weka"
        },
        {
            "location": "/using_weka_from_jython/#downloads",
            "text": "UsingJ48.py  UsingJ48Ext.py  JeroR.py  -  weka.classifiers.rules.ZeroR  as Jython script implemented",
            "title": "Downloads"
        },
        {
            "location": "/using_weka_from_jython/#see-also",
            "text": "Use Weka in your Java code  - for general information on how to use the Weka API  Using Weka via Jepp  - using the  javax.script  approach to interface Java and Python",
            "title": "See also"
        },
        {
            "location": "/using_weka_from_jython/#links",
            "text": "Jython  Homepage  Java arrays  Registry  Embedding Jython    Python  Homepage  HappyDoc  - generating documentation from Jython/Python modules    Java  Homepage  Javadoc    Eclipse  Homepage  PyDev plugin",
            "title": "Links"
        },
        {
            "location": "/using_weka_via_jepp/",
            "text": "Jepp embeds \nCPython\n in Java. It is safe to use in a heavily threaded environment, it is quite fast and its stability is a main feature and goal. \n\n\n--taken from the \nJepp homepage\n\n\nPrerequisites\n\n\n\n\nJava 6 (jepp makes use of the \njavax.script\n package)\n\n\nJepp 2.2 or higher\n\n\nsuggested fix\n for the \nmissing sys.argv\n problem\n\n\n\n\nLimitations\n\n\nJepp doesn't seem to be able to import third-party libraries like \nscipy\n, \nnumpy\n or \nwx\n (pure Python modules can be imported, though).\n\n\nAccessing Weka classes within Jepp\n\n\nJava classes are imported in one's Python script as follows:\n\n\n from <package> import <class>\n\n\n\n\nE.g., importing J48 looks like this:\n\n\n from weka.classifiers.trees import J48\n\n\n\n\nIn the following a little example script for loading a dataset, cross-validating J48 with it and outputting the results of the cross-validation in the console:\n\n\n # import classes\n from weka.core import Instances\n from weka.classifiers import Evaluation\n from weka.classifiers.trees import J48\n\n from java.io import BufferedReader\n from java.io import FileReader\n from java.util import Random\n\n # load data\n reader = BufferedReader(FileReader('/some/where/file.arff'))\n data   = Instances(reader)\n data.setClassIndex(data.numAttributes() - 1)\n reader.close()\n\n # train classifier\n j48  = J48()\n eval = Evaluation(data)\n rand = Random(1)\n eval.crossValidateModel(j48, data, 10, rand)\n\n # output summary\n print eval.toSummaryString()\n\n\n\n\nThe script can be started like this (you will have to adjust the paths for the jars and the Python script):\n\n\n java -classpath jep.jar:weka.jar some_script.py\n\n\n\n\nSee also\n\n\n\n\nUse Weka in your Java code\n - for general information on how to use the Weka API\n\n\nUsing Weka from Jython\n\n\n\n\nLinks\n\n\n\n\nJepp homepage\n\n\nPython homepage\n\n\nLinux.com\n - more examples",
            "title": " Using Weka via Jepp"
        },
        {
            "location": "/using_weka_via_jepp/#prerequisites",
            "text": "Java 6 (jepp makes use of the  javax.script  package)  Jepp 2.2 or higher  suggested fix  for the  missing sys.argv  problem",
            "title": "Prerequisites"
        },
        {
            "location": "/using_weka_via_jepp/#limitations",
            "text": "Jepp doesn't seem to be able to import third-party libraries like  scipy ,  numpy  or  wx  (pure Python modules can be imported, though).",
            "title": "Limitations"
        },
        {
            "location": "/using_weka_via_jepp/#accessing-weka-classes-within-jepp",
            "text": "Java classes are imported in one's Python script as follows:   from <package> import <class>  E.g., importing J48 looks like this:   from weka.classifiers.trees import J48  In the following a little example script for loading a dataset, cross-validating J48 with it and outputting the results of the cross-validation in the console:   # import classes\n from weka.core import Instances\n from weka.classifiers import Evaluation\n from weka.classifiers.trees import J48\n\n from java.io import BufferedReader\n from java.io import FileReader\n from java.util import Random\n\n # load data\n reader = BufferedReader(FileReader('/some/where/file.arff'))\n data   = Instances(reader)\n data.setClassIndex(data.numAttributes() - 1)\n reader.close()\n\n # train classifier\n j48  = J48()\n eval = Evaluation(data)\n rand = Random(1)\n eval.crossValidateModel(j48, data, 10, rand)\n\n # output summary\n print eval.toSummaryString()  The script can be started like this (you will have to adjust the paths for the jars and the Python script):   java -classpath jep.jar:weka.jar some_script.py",
            "title": "Accessing Weka classes within Jepp"
        },
        {
            "location": "/using_weka_via_jepp/#see-also",
            "text": "Use Weka in your Java code  - for general information on how to use the Weka API  Using Weka from Jython",
            "title": "See also"
        },
        {
            "location": "/using_weka_via_jepp/#links",
            "text": "Jepp homepage  Python homepage  Linux.com  - more examples",
            "title": "Links"
        },
        {
            "location": "/visualizing_a_tree/",
            "text": "The following code sample (\nVisualizeJ48.java\n) takes an \nARFF\n file as input, trains a \n[J48](http://weka.sourceforge.net/doc/weka/classifiers/trees/j48.html)\n and displays the generated tree with the \n[TreeVisualizer](http://weka.sourceforge.net/doc/weka/gui/treevisualizer/treevisualizer.html)\n class.\n\n\nThis can be done with all classifiers that implement the \n[weka.core.Drawable](http://weka.sourceforge.net/doc/weka/core/drawable.html)\n interface.\n\n\nSource code\n\n\nimport java.awt.BorderLayout;\n import java.awt.event.WindowAdapter;\n import java.awt.event.WindowEvent;\n import java.io.BufferedReader;\n import java.io.FileReader;\n\n import javax.swing.JFrame;\n\n import weka.classifiers.trees.J48;\n import weka.core.Instances;\n import weka.gui.treevisualizer.PlaceNode2;\n import weka.gui.treevisualizer.TreeVisualizer;\n\n /**\n  * Displays a trained J48 as tree.\n  * Expects an ARFF filename as first argument.\n  *\n  * @author FracPete (fracpete at waikato dot ac dot nz)\n  */\n public class VisualizeJ48 {\n   public static void main(String args[]) throws Exception {\n     // train classifier\n     J48 cls = new J48();\n     Instances data = new Instances(new BufferedReader(new FileReader(args[0])));\n     data.setClassIndex(data.numAttributes() - 1);\n     cls.buildClassifier(data);\n\n     // display classifier\n     final javax.swing.JFrame jf = \n       new javax.swing.JFrame(\"Weka Classifier Tree Visualizer: J48\");\n     jf.setSize(500,400);\n     jf.getContentPane().setLayout(new BorderLayout());\n     TreeVisualizer tv = new TreeVisualizer(null,\n         cls.graph(),\n         new PlaceNode2());\n     jf.getContentPane().add(tv, BorderLayout.CENTER);\n     jf.addWindowListener(new java.awt.event.WindowAdapter() {\n       public void windowClosing(java.awt.event.WindowEvent e) {\n         jf.dispose();\n       }\n     });\n\n     jf.setVisible(true);\n     tv.fitToScreen();\n   }\n }\n\n\n\n\nDownloads\n\n\n\n\nVisualizeJ48.java",
            "title": " Visualizing a Tree"
        },
        {
            "location": "/visualizing_a_tree/#source-code",
            "text": "import java.awt.BorderLayout;\n import java.awt.event.WindowAdapter;\n import java.awt.event.WindowEvent;\n import java.io.BufferedReader;\n import java.io.FileReader;\n\n import javax.swing.JFrame;\n\n import weka.classifiers.trees.J48;\n import weka.core.Instances;\n import weka.gui.treevisualizer.PlaceNode2;\n import weka.gui.treevisualizer.TreeVisualizer;\n\n /**\n  * Displays a trained J48 as tree.\n  * Expects an ARFF filename as first argument.\n  *\n  * @author FracPete (fracpete at waikato dot ac dot nz)\n  */\n public class VisualizeJ48 {\n   public static void main(String args[]) throws Exception {\n     // train classifier\n     J48 cls = new J48();\n     Instances data = new Instances(new BufferedReader(new FileReader(args[0])));\n     data.setClassIndex(data.numAttributes() - 1);\n     cls.buildClassifier(data);\n\n     // display classifier\n     final javax.swing.JFrame jf = \n       new javax.swing.JFrame(\"Weka Classifier Tree Visualizer: J48\");\n     jf.setSize(500,400);\n     jf.getContentPane().setLayout(new BorderLayout());\n     TreeVisualizer tv = new TreeVisualizer(null,\n         cls.graph(),\n         new PlaceNode2());\n     jf.getContentPane().add(tv, BorderLayout.CENTER);\n     jf.addWindowListener(new java.awt.event.WindowAdapter() {\n       public void windowClosing(java.awt.event.WindowEvent e) {\n         jf.dispose();\n       }\n     });\n\n     jf.setVisible(true);\n     tv.fitToScreen();\n   }\n }",
            "title": "Source code"
        },
        {
            "location": "/visualizing_a_tree/#downloads",
            "text": "VisualizeJ48.java",
            "title": "Downloads"
        },
        {
            "location": "/visualizing_cluster_assignments/",
            "text": "The following code sample (\nVisualizeClusterAssignments.java\n displays the cluster assignments of a clusterer on a particular dataset.\n\n\nThis is the same functionality as you get with the right-click menu in the Explorer, choosing \nVisualize cluster assignments\n.\n\n\nExample command-line:\n\n\njava -classpath .:weka.jar VisualizeClusterAssignments -t /some/where/data.arff -W \"weka.clusterers.EM -I 50\"\n\n\n\n\nNote:\n The command above is for Linux/Unix. For the Windows platform, you have to use semicolons in the \nCLASSPATH\n and backlashes instead of forward slashes in the paths.\n\n\nSource code\n\n\nimport weka.clusterers.*;\nimport weka.core.*;\nimport weka.core.converters.ConverterUtils.*;\nimport weka.gui.explorer.ClustererPanel;\nimport weka.gui.visualize.*;\n\nimport java.awt.*;\nimport java.io.*;\nimport java.text.*;\nimport java.util.*;\n\nimport javax.swing.*;\n\n/**\n * Runs a clusterer on a dataset and visualizes the cluster assignments, \n * like with right-click menu in Explorer.\n * <p/>\n * Takes two arguments:\n * <ol>\n *   <li>-t dataset</li>\n *   <li>-W cluster algorithm with options</li>\n * </ol>\n *\n * Note: code should work with Weka 3.6.0 and 3.5.8.\n * \n * @author FracPete (fracpete at waikato dot ac dot nz)\n */\npublic class VisualizeClusterAssignments {\n  public static void main(String[] args) throws Exception {\n    // load data\n    Instances train = DataSource.read(Utils.getOption('t', args));\n    // some data formats store the class attribute information as well\n    if (train.classIndex() != -1)\n      throw new IllegalArgumentException(\"Data cannot have class attribute!\");\n\n    // instantiate clusterer\n    String[] options = Utils.splitOptions(Utils.getOption('W', args));\n    String classname = options[0];\n    options[0] = \"\";\n    Clusterer clusterer = AbstractClusterer.forName(classname, options);\n\n    // evaluate clusterer\n    clusterer.buildClusterer(train);\n    ClusterEvaluation eval = new ClusterEvaluation();\n    eval.setClusterer(clusterer);\n    eval.evaluateClusterer(train);\n\n    // setup visualization\n    // taken from: ClustererPanel.startClusterer()\n    PlotData2D predData = ClustererPanel.setUpVisualizableInstances(train, eval);\n    String name = (new SimpleDateFormat(\"HH:mm:ss - \")).format(new Date());\n    String cname = clusterer.getClass().getName();\n    if (cname.startsWith(\"weka.clusterers.\"))\n      name += cname.substring(\"weka.clusterers.\".length());\n    else\n      name += cname;\n\n    VisualizePanel vp = new VisualizePanel();\n    vp.setName(name + \" (\" + train.relationName() + \")\");\n    predData.setPlotName(name + \" (\" + train.relationName() + \")\");\n    vp.addPlot(predData);\n\n    // display data\n    // taken from: ClustererPanel.visualizeClusterAssignments(VisualizePanel)\n    String plotName = vp.getName();\n    final javax.swing.JFrame jf = \n      new javax.swing.JFrame(\"Weka Clusterer Visualize: \" + plotName);\n    jf.setSize(500,400);\n    jf.getContentPane().setLayout(new BorderLayout());\n    jf.getContentPane().add(vp, BorderLayout.CENTER);\n    jf.addWindowListener(new java.awt.event.WindowAdapter() {\n      public void windowClosing(java.awt.event.WindowEvent e) {\n        jf.dispose();\n      }\n    });\n    jf.setVisible(true);\n  }\n}\n\n\n\n\nDownloads\n\n\n\n\nVisualizeClusterAssignments.java",
            "title": " Visualizing cluster assignments"
        },
        {
            "location": "/visualizing_cluster_assignments/#source-code",
            "text": "import weka.clusterers.*;\nimport weka.core.*;\nimport weka.core.converters.ConverterUtils.*;\nimport weka.gui.explorer.ClustererPanel;\nimport weka.gui.visualize.*;\n\nimport java.awt.*;\nimport java.io.*;\nimport java.text.*;\nimport java.util.*;\n\nimport javax.swing.*;\n\n/**\n * Runs a clusterer on a dataset and visualizes the cluster assignments, \n * like with right-click menu in Explorer.\n * <p/>\n * Takes two arguments:\n * <ol>\n *   <li>-t dataset</li>\n *   <li>-W cluster algorithm with options</li>\n * </ol>\n *\n * Note: code should work with Weka 3.6.0 and 3.5.8.\n * \n * @author FracPete (fracpete at waikato dot ac dot nz)\n */\npublic class VisualizeClusterAssignments {\n  public static void main(String[] args) throws Exception {\n    // load data\n    Instances train = DataSource.read(Utils.getOption('t', args));\n    // some data formats store the class attribute information as well\n    if (train.classIndex() != -1)\n      throw new IllegalArgumentException(\"Data cannot have class attribute!\");\n\n    // instantiate clusterer\n    String[] options = Utils.splitOptions(Utils.getOption('W', args));\n    String classname = options[0];\n    options[0] = \"\";\n    Clusterer clusterer = AbstractClusterer.forName(classname, options);\n\n    // evaluate clusterer\n    clusterer.buildClusterer(train);\n    ClusterEvaluation eval = new ClusterEvaluation();\n    eval.setClusterer(clusterer);\n    eval.evaluateClusterer(train);\n\n    // setup visualization\n    // taken from: ClustererPanel.startClusterer()\n    PlotData2D predData = ClustererPanel.setUpVisualizableInstances(train, eval);\n    String name = (new SimpleDateFormat(\"HH:mm:ss - \")).format(new Date());\n    String cname = clusterer.getClass().getName();\n    if (cname.startsWith(\"weka.clusterers.\"))\n      name += cname.substring(\"weka.clusterers.\".length());\n    else\n      name += cname;\n\n    VisualizePanel vp = new VisualizePanel();\n    vp.setName(name + \" (\" + train.relationName() + \")\");\n    predData.setPlotName(name + \" (\" + train.relationName() + \")\");\n    vp.addPlot(predData);\n\n    // display data\n    // taken from: ClustererPanel.visualizeClusterAssignments(VisualizePanel)\n    String plotName = vp.getName();\n    final javax.swing.JFrame jf = \n      new javax.swing.JFrame(\"Weka Clusterer Visualize: \" + plotName);\n    jf.setSize(500,400);\n    jf.getContentPane().setLayout(new BorderLayout());\n    jf.getContentPane().add(vp, BorderLayout.CENTER);\n    jf.addWindowListener(new java.awt.event.WindowAdapter() {\n      public void windowClosing(java.awt.event.WindowEvent e) {\n        jf.dispose();\n      }\n    });\n    jf.setVisible(true);\n  }\n}",
            "title": "Source code"
        },
        {
            "location": "/visualizing_cluster_assignments/#downloads",
            "text": "VisualizeClusterAssignments.java",
            "title": "Downloads"
        },
        {
            "location": "/weka_for_newbies/",
            "text": "This page has been filled mostly thanks to answers on the wekalist (\nhttp://list.waikato.ac.nz/mailman/listinfo/wekalist\n). So thanks to the participants (in particular Mark, Eibe, Tom & jmgomezhidalgo)!\nFor any addition to this page, you can either create an account & edit it yourself or send an email to gm [AT] presans [DOT] com\n\n\nImportant Tips\n\n\n\n\nSince many people use Weka, lots of (basic & advanced) questions have already been asked on the mailing list. Therefore, using \"\n wekalist\" in your preferred search engine might help you get an answer faster than asking the same question again on the list \nbefore\n doing any research on your own first.\n\n\nIn case you would be \nreally lazy\n: \nhttp://www.google.com/search?hl=en&q=\n%20wekalist\n\n\n\n\nJava programming helps\n\n\n\n\nhttp://mindprod.com/jgloss/jcheat.html\n\n\nhttp://mindprod.com/jgloss/jgloss.html\n\n\nIn French:\n\n\nhttp://www.jmdoudoux.fr/accueil_java.htm\n\n\n\n\n\n\n\n\nArtificial Intelligence and Machine Learning Courses\n\n\n\n\nArtificial Intelligence\n\n\nhttps://www.udacity.com/wiki/cs271\n\n\n\n\n\n\nMachine Learning\n\n\nhttps://www.coursera.org/course/ml\n\n\nhttp://www.cs.cornell.edu/Courses/cs4780/2013fa/#lectures\n\n\nhttp://shop.oreilly.com/product/0636920025610.do\n\n\n(Neural Networks) \nhttps://www.coursera.org/course/neuralnets\n - \ndead\n\n\n\n\n\n\nSpecialized\n\n\n(Probabilistic Graphical Models - PGM: Bayesian and Markov networks) \nhttps://www.coursera.org/course/pgm\n\n\n(Social Network Analysis - SNA) \nhttps://www.coursera.org/course/sna\n\n\n(Natural Language Processing - NLP)\n\n\nhttps://www.coursera.org/course/nlp\n - \ndead\n\n\nhttps://www.coursera.org/course/nlangp\n\n\n\n\n\n\n\n\n\n\nWeka\n\n\nhttps://www.youtube.com/channel/ucxyxsgq6oz21b43hpw2dcvw\n - \ndead\n\n\nhttps://www.youtube.com/user/rushdishams\n\n\n\n\n\n\n\n\nMachine Learning Intro Books\n\n\n\n\nhttp://www.amazon.com/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_1?ie=UTF8&qid=1394186512&sr=8-1&keywords=tom+mitchell\n\n\nhttp://www.amazon.com/Pattern-Classification-Pt-1-Richard-Duda/dp/0471056693/ref=pd_sim_b_5?ie=UTF8&refRID=1Z8Y81J1WHER2HDRYGP3\n\n\nhttp://www.amazon.com/Artificial-Intelligence-Modern-Approach-Edition/dp/0136042597/ref=pd_sim_b_6?ie=UTF8&refRID=1Z8Y81J1WHER2HDRYGP3\n\n\nhttp://www.amazon.com/Introduction-Machine-Learning-Adaptive-Computation/dp/026201243X/ref=sr_1_1?s=books&ie=UTF8&qid=1394186894&sr=1-1&keywords=Alpaydin-Introduction+to+Machine+Learning\n\n\nhttp://www.amazon.com/Genetic-Programming-Computers-Selection-Adaptive/dp/0262111705/ref=sr_1_1?s=books&ie=UTF8&qid=1394186950&sr=1-1&keywords=Koza+Genetic_Programming_On_the_Programming_of_Computers_by_Means_of_Natural_Selection_Complex_Adaptive_Systems\n\n\nhttp://www.amazon.com/Machine-Learning-Hackers-Drew-Conway/dp/1449303714\n\n\nhttp://www.amazon.com/Mining-Social-Web-Facebook-LinkedIn/dp/1449367615/ref=pd_sim_b_6?ie=UTF8&refRID=0J9TP17C75CRAY0ETH2T\n\n\nhttp://www.amazon.com/Machine-Learning-Email-Filtering-Priority/dp/1449314309/ref=sr_1_2?ie=UTF8&qid=1394186854&sr=8-2&keywords=Machine+Learning+Email+Filtering\n\n\nhttp://www.amazon.com/Building-Machine-Learning-Systems-Python/dp/1782161406/ref=sr_1_1?s=books&ie=UTF8&qid=1394187002&sr=1-1&keywords=Coelho-Building+Machine+Learning+Systems+with+Python\n\n\nhttp://www.amazon.com/Machine-Learning-Action-Peter-Harrington/dp/1617290181/ref=pd_sim_b_3?ie=UTF8&refRID=1DHDKNY77W2YM0W55HY9\n\n\nIn French:    \n\n\nhttp://www.amazon.com/Apprentissage-artificiel-algorithmes-Antoine-Cornu%C3%A9jols/dp/2212110200/ref=sr_1_6?ie=UTF8&qid=1394186532&sr=8-6&keywords=cornu%C3%A9jols\n\n\n\n\n\n\n\n\nWeka Reference Book\n\n\n\n\nhttp://www.amazon.com/Data-Mining-Practical-Techniques-Management/dp/0123748569/ref=dp_ob_title_bk\n\n\n\n\nWeka Intro & General Documentation\n\n\n\n\nIntro\n\n\nhttps://www.youtube.com/channel/UCXYXSGq6Oz21b43hpW2DCvw\n\n\nhttp://www.ibm.com/developerworks/library/os-weka1/\n\n\nhttp://downloads.sourceforge.net/project/weka/documentation/3.7.x/WekaManual-3-7-10.pdf\n\n\nhttps://waikato.github.io/weka-wiki/faq/\n\n\n\n\n\n\nGeneral Documentation\n\n\nhttp://www.cs.waikato.ac.nz/ml/weka/documentation.html\n\n\nhttp://www.cs.waikato.ac.nz/ml/weka/help.html\n\n\nhttps://waikato.github.io/weka-wiki/\n\n\n\n\n\n\n\n\nWeka Development\n\n\n\n\nhttps://waikato.github.io/weka-wiki/use_weka_in_your_java_code/\n\n\nAPIs\n\n\nhttps://waikato.github.io/weka-wiki/use_weka_in_your_java_code/\n\n\nhttp://weka.sourceforge.net/doc.stable/\n\n\nhttp://weka.sourceforge.net/doc.dev/\n\n\n\n\n\n\nCode Examples\n\n\nA Simple Text Classifier in Java with WEKA presents and discuses two little programs as examples of how to integrate WEKA into your Java code for text mining: \nhttp://jmgomezhidalgo.blogspot.com.es/2013/04/a-simple-text-classifier-in-java-with.html\n\n\nLanguage Identification as Text Classification with WEKA explains how to build an automated language guesser for texts as a complete example of a Text Mining process with WEKA, and in order to demonstrate a more advanced usage of the StringToWordVector class: \nhttp://jmgomezhidalgo.blogspot.com.es/2013/05/language-identification-as-text.html\n\n\nSample Code for Text Indexing with WEKA shows how to index a text dataset using your own Java code and the StringToWordVector filter in WEKA: \nhttp://jmgomezhidalgo.blogspot.com.es/2013/06/sample-code-for-text-indexing-with-weka.html\n\n\nhttp://permalink.gmane.org/gmane.comp.ai.weka/33249\n - \ndead\n\n\nhttp://permalink.gmane.org/gmane.comp.ai.weka/33302\n (text classification full example) - \ndead\n\n\n[TO BE COMPLETED there are lots of examples on this same site, just find them :)]\n\n\n\n\n\n\nAdditions/Modification to inner code\n\n\nModification of Weka \n\n\nhttps://waikato.github.io/weka-wiki/subversion/\n (get source from SVN)\n\n\n\n\n\n\nCreating a package \n\n\nhttp://weka.8497.n7.nabble.com/Contributing-a-package-doubts-td30164.html\n\n\n\n\n\n\n\n\n\n\nMigration\n\n\nSerialize from 3.6.8 (stable) / Deserialize to 3.7.10 (development) -> \nhttp://article.gmane.org/gmane.comp.ai.weka/33368\n - \ndead\n\n\n\n\n\n\n\n\nWeka Input & Output\n\n\n\n\nWeka Input\n\n\nWeka Configuration Files\n\n\nhttps://waikato.github.io/weka-wiki/weka_gui_explorer_explorer.props\n\n\n\n\n\n\nWeka & Excel (you should really use \"flat\" files like CSVs!)\n\n\nhttp://weka.sourceforge.net/packageMetaData/WekaExcel/index.html\n\n\n\n\n\n\nWeka & CSV\n\n\nhttps://waikato.github.io/weka-wiki/faqs/csv_file_conversion/\n\n\nhttps://waikato.github.io/weka-wiki/converting_csv_to_arff/\n\n\n\n\n\n\n\n\n\n\nWeka Output\n\n\nMaking Predictions\n\n\nhttps://waikato.github.io/weka-wiki/making_predictions/\n\n\n\n\n\n\nSaving Models \n\n\nhttps://waikato.github.io/weka-wiki/saving_and_loading_models/\n\n\n\n\n\n\nEvaluation \n\n\nhttp://blog.gmane.org/gmane.comp.ai.weka/month=20140301\n - \ndead\n\n\nhttp://weka.8497.n7.nabble.com/Evaluating-a-classifier-using-a-log-loss-function-td30182.html\n\n\n\n\n\n\n\n\n\n\n\n\nWeka Server Usage and Development\n\n\n\n\nhttp://wiki.pentaho.com/display/DATAMINING/Weka+Server\n\n\nhttps://waikato.github.io/weka-wiki/remote_experiment/\n\n\nhttp://khusainr.myweb.port.ac.uk/weka/gweka-howto.html\n - \ndead\n\n\nhttp://forums.pentaho.com/showthread.php?143747-weka-server\n\n\n\n\nWeka & Memory\n\n\n\n\nhttp://weka.8497.n7.nabble.com/Memory-Issues-and-Weka-td30220.html\n\n\n\n\nWeka Platform-Specifics Problems\n\n\n\n\nMac\n\n\nhttp://weka.8497.n7.nabble.com/weka-jar-does-not-run-on-mac-td30146.html\n\n\n\n\n\n\nWindows\n\n\nhttps://waikato.github.io/weka-wiki/lib_svm/\n\n\n\n\n\n\nLinux\n\n\n\n\nWeka & Attribute Selection\n\n\n\n\nhttps://waikato.github.io/weka-wiki/performing_attribute_selection/\n\n\n\n\nWeka & Clustering\n\n\n\n\nhttp://micans.org/mcl/\n\n\nMetrics for validation/comparison: there is none in Weka as of 2014/03, but some ideas can be found here:\n\n\nhttp://permalink.gmane.org/gmane.comp.ai.weka/33317\n - \ndead\n\n\nhttp://mirror.ibcp.fr/pub/CRAN/web/views/Cluster.html\n - \ndead\n (in section \" Additional Functionality\").  For instance:\n\n\nhttp://mirror.ibcp.fr/pub/CRAN/web/packages/clValid/index.html\n - \ndead\n\n\nhttp://mirror.ibcp.fr/pub/CRAN/web/packages/clv/index.html\n - \ndead\n\n\nhttp://mirror.ibcp.fr/pub/CRAN/web/packages/fpc/index.html\n - \ndead\n\n\n\n\n\n\n\n\nWeka & Classification\n\n\n\n\nWeka & SMO/SVM\n\n\nhttp://permalink.gmane.org/gmane.comp.ai.weka/33211\n - \ndead\n\n\n\n\n\n\n\n\nSpecific Applications and other tools\n\n\n\n\nWorking with Text\n\n\nhttps://waikato.github.io/weka-wiki/text_categorization_with_weka/\n\n\nhttp://www.esp.uem.es/jmgomez/tmweka/index.html\n - \ndead\n\n\nhttp://jmgomezhidalgo.blogspot.com.es/2013/04/a-simple-text-classifier-in-java-with.html\n\n\nhttp://jmgomezhidalgo.blogspot.com.es/2013/02/text-mining-in-weka-revisited-selecting.html\n\n\nhttp://www.youtube.com/watch?v=IY29uC4uem8\n (e.g. the text classification of the \"Imdb data set\")\n\n\nhttp://weka.8497.n7.nabble.com/How-to-use-probabilistic-latent-semantic-analysis-PLSA-td30145.html\n (LSA)\n\n\nStemming\n\n\nhttp://comments.gmane.org/gmane.comp.ai.weka/33202\n - \ndead\n\n\n\n\n\n\n\n\n\n\nSearch Engine / Reranking \n\n\nhttp://lemurproject.org/components.php\n\n\nhttp://www.uni-marburg.de/fb12/kebi/research/software/WEKA-LR-PAGE\n\n\nhttp://fantail.quansun.com/\n (ranking prediction, multi-target regression, label ranking and metalearning) - \ndead\n\n\nhttp://svmlight.joachims.org/\n (reformulation of SVM to do reranking)\n\n\n\n\n\n\nGraphs \n\n\nhttp://micans.org/mcl/\n\n\n\n\n\n\nBigData\n\n\nhttps://mahout.apache.org/\n\n\nhttp://www.cs.waikato.ac.nz/ml/weka/bigdata.html\n\n\nhttp://wiki.pentaho.com/display/DATAMINING/Handling+Large+Data+Sets+with+Weka\n\n\n\n\n\n\nNamed Entity Recognition (POS, NER) \n\n\n(2006) \nhttp://gate.ac.uk/\n\n\n(2002) \nhttp://www.cnts.ua.ac.be/conll2002/ner/\n - \ndead\n\n\n\n\n\n\nExtraction, Transformation Loading (ETL) \n\n\nhttp://community.pentaho.com/projects/data-integration/\n\n\n\n\n\n\nOther ML tools \n\n\nhttp://java-ml.sourceforge.net/\n (Data manipulation, Clustering, Feature selection, Classification, Databases: Bayes, MCL, KD-tree, DTW...)\n\n\nhttp://mallet.cs.umass.edu/\n (document classification, sequence tagging, topic modeling, numerical optimization: LDA, GRMM, CRF...)\n\n\nhttp://www.spagobi.org/\n (Business Intelligence: OLAP, KPI, data visualization, geospatial analytics...)\n\n\nhttp://cran.r-project.org/web/views/MachineLearning.html\n (all sorts of algorithms in R)",
            "title": " Weka For Newbies"
        },
        {
            "location": "/weka_for_newbies/#important-tips",
            "text": "Since many people use Weka, lots of (basic & advanced) questions have already been asked on the mailing list. Therefore, using \"  wekalist\" in your preferred search engine might help you get an answer faster than asking the same question again on the list  before  doing any research on your own first.  In case you would be  really lazy :  http://www.google.com/search?hl=en&q= %20wekalist",
            "title": "Important Tips"
        },
        {
            "location": "/weka_for_newbies/#java-programming-helps",
            "text": "http://mindprod.com/jgloss/jcheat.html  http://mindprod.com/jgloss/jgloss.html  In French:  http://www.jmdoudoux.fr/accueil_java.htm",
            "title": "Java programming helps"
        },
        {
            "location": "/weka_for_newbies/#artificial-intelligence-and-machine-learning-courses",
            "text": "Artificial Intelligence  https://www.udacity.com/wiki/cs271    Machine Learning  https://www.coursera.org/course/ml  http://www.cs.cornell.edu/Courses/cs4780/2013fa/#lectures  http://shop.oreilly.com/product/0636920025610.do  (Neural Networks)  https://www.coursera.org/course/neuralnets  -  dead    Specialized  (Probabilistic Graphical Models - PGM: Bayesian and Markov networks)  https://www.coursera.org/course/pgm  (Social Network Analysis - SNA)  https://www.coursera.org/course/sna  (Natural Language Processing - NLP)  https://www.coursera.org/course/nlp  -  dead  https://www.coursera.org/course/nlangp      Weka  https://www.youtube.com/channel/ucxyxsgq6oz21b43hpw2dcvw  -  dead  https://www.youtube.com/user/rushdishams",
            "title": "Artificial Intelligence and Machine Learning Courses"
        },
        {
            "location": "/weka_for_newbies/#machine-learning-intro-books",
            "text": "http://www.amazon.com/Machine-Learning-Tom-M-Mitchell/dp/0070428077/ref=sr_1_1?ie=UTF8&qid=1394186512&sr=8-1&keywords=tom+mitchell  http://www.amazon.com/Pattern-Classification-Pt-1-Richard-Duda/dp/0471056693/ref=pd_sim_b_5?ie=UTF8&refRID=1Z8Y81J1WHER2HDRYGP3  http://www.amazon.com/Artificial-Intelligence-Modern-Approach-Edition/dp/0136042597/ref=pd_sim_b_6?ie=UTF8&refRID=1Z8Y81J1WHER2HDRYGP3  http://www.amazon.com/Introduction-Machine-Learning-Adaptive-Computation/dp/026201243X/ref=sr_1_1?s=books&ie=UTF8&qid=1394186894&sr=1-1&keywords=Alpaydin-Introduction+to+Machine+Learning  http://www.amazon.com/Genetic-Programming-Computers-Selection-Adaptive/dp/0262111705/ref=sr_1_1?s=books&ie=UTF8&qid=1394186950&sr=1-1&keywords=Koza+Genetic_Programming_On_the_Programming_of_Computers_by_Means_of_Natural_Selection_Complex_Adaptive_Systems  http://www.amazon.com/Machine-Learning-Hackers-Drew-Conway/dp/1449303714  http://www.amazon.com/Mining-Social-Web-Facebook-LinkedIn/dp/1449367615/ref=pd_sim_b_6?ie=UTF8&refRID=0J9TP17C75CRAY0ETH2T  http://www.amazon.com/Machine-Learning-Email-Filtering-Priority/dp/1449314309/ref=sr_1_2?ie=UTF8&qid=1394186854&sr=8-2&keywords=Machine+Learning+Email+Filtering  http://www.amazon.com/Building-Machine-Learning-Systems-Python/dp/1782161406/ref=sr_1_1?s=books&ie=UTF8&qid=1394187002&sr=1-1&keywords=Coelho-Building+Machine+Learning+Systems+with+Python  http://www.amazon.com/Machine-Learning-Action-Peter-Harrington/dp/1617290181/ref=pd_sim_b_3?ie=UTF8&refRID=1DHDKNY77W2YM0W55HY9  In French:      http://www.amazon.com/Apprentissage-artificiel-algorithmes-Antoine-Cornu%C3%A9jols/dp/2212110200/ref=sr_1_6?ie=UTF8&qid=1394186532&sr=8-6&keywords=cornu%C3%A9jols",
            "title": "Machine Learning Intro Books"
        },
        {
            "location": "/weka_for_newbies/#weka-reference-book",
            "text": "http://www.amazon.com/Data-Mining-Practical-Techniques-Management/dp/0123748569/ref=dp_ob_title_bk",
            "title": "Weka Reference Book"
        },
        {
            "location": "/weka_for_newbies/#weka-intro-general-documentation",
            "text": "Intro  https://www.youtube.com/channel/UCXYXSGq6Oz21b43hpW2DCvw  http://www.ibm.com/developerworks/library/os-weka1/  http://downloads.sourceforge.net/project/weka/documentation/3.7.x/WekaManual-3-7-10.pdf  https://waikato.github.io/weka-wiki/faq/    General Documentation  http://www.cs.waikato.ac.nz/ml/weka/documentation.html  http://www.cs.waikato.ac.nz/ml/weka/help.html  https://waikato.github.io/weka-wiki/",
            "title": "Weka Intro &amp; General Documentation"
        },
        {
            "location": "/weka_for_newbies/#weka-development",
            "text": "https://waikato.github.io/weka-wiki/use_weka_in_your_java_code/  APIs  https://waikato.github.io/weka-wiki/use_weka_in_your_java_code/  http://weka.sourceforge.net/doc.stable/  http://weka.sourceforge.net/doc.dev/    Code Examples  A Simple Text Classifier in Java with WEKA presents and discuses two little programs as examples of how to integrate WEKA into your Java code for text mining:  http://jmgomezhidalgo.blogspot.com.es/2013/04/a-simple-text-classifier-in-java-with.html  Language Identification as Text Classification with WEKA explains how to build an automated language guesser for texts as a complete example of a Text Mining process with WEKA, and in order to demonstrate a more advanced usage of the StringToWordVector class:  http://jmgomezhidalgo.blogspot.com.es/2013/05/language-identification-as-text.html  Sample Code for Text Indexing with WEKA shows how to index a text dataset using your own Java code and the StringToWordVector filter in WEKA:  http://jmgomezhidalgo.blogspot.com.es/2013/06/sample-code-for-text-indexing-with-weka.html  http://permalink.gmane.org/gmane.comp.ai.weka/33249  -  dead  http://permalink.gmane.org/gmane.comp.ai.weka/33302  (text classification full example) -  dead  [TO BE COMPLETED there are lots of examples on this same site, just find them :)]    Additions/Modification to inner code  Modification of Weka   https://waikato.github.io/weka-wiki/subversion/  (get source from SVN)    Creating a package   http://weka.8497.n7.nabble.com/Contributing-a-package-doubts-td30164.html      Migration  Serialize from 3.6.8 (stable) / Deserialize to 3.7.10 (development) ->  http://article.gmane.org/gmane.comp.ai.weka/33368  -  dead",
            "title": "Weka Development"
        },
        {
            "location": "/weka_for_newbies/#weka-input-output",
            "text": "Weka Input  Weka Configuration Files  https://waikato.github.io/weka-wiki/weka_gui_explorer_explorer.props    Weka & Excel (you should really use \"flat\" files like CSVs!)  http://weka.sourceforge.net/packageMetaData/WekaExcel/index.html    Weka & CSV  https://waikato.github.io/weka-wiki/faqs/csv_file_conversion/  https://waikato.github.io/weka-wiki/converting_csv_to_arff/      Weka Output  Making Predictions  https://waikato.github.io/weka-wiki/making_predictions/    Saving Models   https://waikato.github.io/weka-wiki/saving_and_loading_models/    Evaluation   http://blog.gmane.org/gmane.comp.ai.weka/month=20140301  -  dead  http://weka.8497.n7.nabble.com/Evaluating-a-classifier-using-a-log-loss-function-td30182.html",
            "title": "Weka Input &amp; Output"
        },
        {
            "location": "/weka_for_newbies/#weka-server-usage-and-development",
            "text": "http://wiki.pentaho.com/display/DATAMINING/Weka+Server  https://waikato.github.io/weka-wiki/remote_experiment/  http://khusainr.myweb.port.ac.uk/weka/gweka-howto.html  -  dead  http://forums.pentaho.com/showthread.php?143747-weka-server",
            "title": "Weka Server Usage and Development"
        },
        {
            "location": "/weka_for_newbies/#weka-memory",
            "text": "http://weka.8497.n7.nabble.com/Memory-Issues-and-Weka-td30220.html",
            "title": "Weka &amp; Memory"
        },
        {
            "location": "/weka_for_newbies/#weka-platform-specifics-problems",
            "text": "Mac  http://weka.8497.n7.nabble.com/weka-jar-does-not-run-on-mac-td30146.html    Windows  https://waikato.github.io/weka-wiki/lib_svm/    Linux",
            "title": "Weka Platform-Specifics Problems"
        },
        {
            "location": "/weka_for_newbies/#weka-attribute-selection",
            "text": "https://waikato.github.io/weka-wiki/performing_attribute_selection/",
            "title": "Weka &amp; Attribute Selection"
        },
        {
            "location": "/weka_for_newbies/#weka-clustering",
            "text": "http://micans.org/mcl/  Metrics for validation/comparison: there is none in Weka as of 2014/03, but some ideas can be found here:  http://permalink.gmane.org/gmane.comp.ai.weka/33317  -  dead  http://mirror.ibcp.fr/pub/CRAN/web/views/Cluster.html  -  dead  (in section \" Additional Functionality\").  For instance:  http://mirror.ibcp.fr/pub/CRAN/web/packages/clValid/index.html  -  dead  http://mirror.ibcp.fr/pub/CRAN/web/packages/clv/index.html  -  dead  http://mirror.ibcp.fr/pub/CRAN/web/packages/fpc/index.html  -  dead",
            "title": "Weka &amp; Clustering"
        },
        {
            "location": "/weka_for_newbies/#weka-classification",
            "text": "Weka & SMO/SVM  http://permalink.gmane.org/gmane.comp.ai.weka/33211  -  dead",
            "title": "Weka &amp; Classification"
        },
        {
            "location": "/weka_for_newbies/#specific-applications-and-other-tools",
            "text": "Working with Text  https://waikato.github.io/weka-wiki/text_categorization_with_weka/  http://www.esp.uem.es/jmgomez/tmweka/index.html  -  dead  http://jmgomezhidalgo.blogspot.com.es/2013/04/a-simple-text-classifier-in-java-with.html  http://jmgomezhidalgo.blogspot.com.es/2013/02/text-mining-in-weka-revisited-selecting.html  http://www.youtube.com/watch?v=IY29uC4uem8  (e.g. the text classification of the \"Imdb data set\")  http://weka.8497.n7.nabble.com/How-to-use-probabilistic-latent-semantic-analysis-PLSA-td30145.html  (LSA)  Stemming  http://comments.gmane.org/gmane.comp.ai.weka/33202  -  dead      Search Engine / Reranking   http://lemurproject.org/components.php  http://www.uni-marburg.de/fb12/kebi/research/software/WEKA-LR-PAGE  http://fantail.quansun.com/  (ranking prediction, multi-target regression, label ranking and metalearning) -  dead  http://svmlight.joachims.org/  (reformulation of SVM to do reranking)    Graphs   http://micans.org/mcl/    BigData  https://mahout.apache.org/  http://www.cs.waikato.ac.nz/ml/weka/bigdata.html  http://wiki.pentaho.com/display/DATAMINING/Handling+Large+Data+Sets+with+Weka    Named Entity Recognition (POS, NER)   (2006)  http://gate.ac.uk/  (2002)  http://www.cnts.ua.ac.be/conll2002/ner/  -  dead    Extraction, Transformation Loading (ETL)   http://community.pentaho.com/projects/data-integration/    Other ML tools   http://java-ml.sourceforge.net/  (Data manipulation, Clustering, Feature selection, Classification, Databases: Bayes, MCL, KD-tree, DTW...)  http://mallet.cs.umass.edu/  (document classification, sequence tagging, topic modeling, numerical optimization: LDA, GRMM, CRF...)  http://www.spagobi.org/  (Business Intelligence: OLAP, KPI, data visualization, geospatial analytics...)  http://cran.r-project.org/web/views/MachineLearning.html  (all sorts of algorithms in R)",
            "title": "Specific Applications and other tools"
        },
        {
            "location": "/weka_on_a_memory_stick/",
            "text": "The following guide explains how to put Weka on a USB memory stick. This works both for Linux and Windows (Mac OSX as well).\n\n\nFor simplicity, this example is demonstrated with the following versions:\n\n\n\n\nWeka: \n3.5.3\n\n\nJRE: \n1.5.0_10\n\n\n\n\nPreliminaries\n\n\n\n\ndownload\n a Weka ZIP (Windows: \ndon't download the Installer!)\n\n\ndownload\n the JRE (Java Runtime Environemnt) that works with the downloaded Weka version. (Linux: \ndon't download the \nRPM\n, but the \nLinux self-extracting file\n)\n\n\ninstall the downloaded JRE\n\n\nWindows: \n\n\nthe JRE location is normally in \nC:\\Program Files\\Java\n\n\n\n\n\n\nLinux: \n\n\nthe self-extracting file creates a directory containing the JRE at the same location as the installation file\n\n\n\n\n\n\n\n\n\n\n\n\nSetup\n\n\n\n\ncreate a directory on your memory stick that will hold Weka and the JRE:\n\n\n\n\n weka\n\n\n\n\n\n\nunzip the Weka ZIP into the \nweka\n directory, which will create the following sub-directory;\n\n\n\n\n weka-3-5-3\n\n\n\n\n\n\ncopy the JRE onto the stick in the \nweka\n directory, which will be this sub-directory:\n\n\n\n\n jre1.5.0_10\n\n\n\n\nScript\n\n\nAs a final step, create a script to start Weka:\n\n\n\n\nWindows\n\n\ncreate a new batch file called \nweka.bat\n in the directory \nweka\n with the following content\n\n\n\n\n\n\n\n\n @echo off\n set CP=%CLASSPATH%;.\\weka-3-5-3\\weka.jar\n start .\\jre1.5.0_10\\bin\\javaw -classpath \"%CP%\" weka.gui.GUIChooser\n\n\n\n\n\n\nNote:\n If \nstart\n is not available in your flavor of Windows, you can drop it. It is only used to get rid of the DOS-Box.\n\n\n\n\n\n\nLinux\n\n\ncreate a new bash script called \nweka.sh\n in the directory \nweka\n with the following content\n\n\n\n\n\n\n\n\n #!/bin/bash\n CP=$CLASSPATH:./weka-3-5-3/weka.jar\n ./jre1.5.0_10/bin/java -classpath $CP weka.gui.GUIChooser\n\n\n\n\n\n\nNote:\n since memory sticks normally use the \nFAT32\n file-system you probably won't need to make it executable\n\n\n\n\nExecution\n\n\n\n\n\n\nWindows: \n\n\n\n\njust double-click on the batch file\n\n\n\n\n\n\n\n\nLinux: \n\n\n\n\nopen a terminal and execute the bash script\n\n\n\n\n\n\n\n\nLinks\n\n\n\n\nWeka homepage\n\n\nSun Java homepage\n\n\nstart command\n in Windows NT/2000",
            "title": " Weka on a memory stick"
        },
        {
            "location": "/weka_on_a_memory_stick/#preliminaries",
            "text": "download  a Weka ZIP (Windows: \ndon't download the Installer!)  download  the JRE (Java Runtime Environemnt) that works with the downloaded Weka version. (Linux: \ndon't download the  RPM , but the  Linux self-extracting file )  install the downloaded JRE  Windows:   the JRE location is normally in  C:\\Program Files\\Java    Linux:   the self-extracting file creates a directory containing the JRE at the same location as the installation file",
            "title": "Preliminaries"
        },
        {
            "location": "/weka_on_a_memory_stick/#setup",
            "text": "create a directory on your memory stick that will hold Weka and the JRE:    weka   unzip the Weka ZIP into the  weka  directory, which will create the following sub-directory;    weka-3-5-3   copy the JRE onto the stick in the  weka  directory, which will be this sub-directory:    jre1.5.0_10",
            "title": "Setup"
        },
        {
            "location": "/weka_on_a_memory_stick/#script",
            "text": "As a final step, create a script to start Weka:   Windows  create a new batch file called  weka.bat  in the directory  weka  with the following content      @echo off\n set CP=%CLASSPATH%;.\\weka-3-5-3\\weka.jar\n start .\\jre1.5.0_10\\bin\\javaw -classpath \"%CP%\" weka.gui.GUIChooser   Note:  If  start  is not available in your flavor of Windows, you can drop it. It is only used to get rid of the DOS-Box.    Linux  create a new bash script called  weka.sh  in the directory  weka  with the following content      #!/bin/bash\n CP=$CLASSPATH:./weka-3-5-3/weka.jar\n ./jre1.5.0_10/bin/java -classpath $CP weka.gui.GUIChooser   Note:  since memory sticks normally use the  FAT32  file-system you probably won't need to make it executable",
            "title": "Script"
        },
        {
            "location": "/weka_on_a_memory_stick/#execution",
            "text": "Windows:    just double-click on the batch file     Linux:    open a terminal and execute the bash script",
            "title": "Execution"
        },
        {
            "location": "/weka_on_a_memory_stick/#links",
            "text": "Weka homepage  Sun Java homepage  start command  in Windows NT/2000",
            "title": "Links"
        },
        {
            "location": "/where_does_weka_look_for_props_files/",
            "text": "WEKA not only uses the \n.props\n files that are present in the \nweka.jar\n archive, but also the ones in the user's home directory and the current directory, i.e., the one WEKA was started from.\n\n\nFor a complete overview, see the section \nPrecedence\n in the \nProperties file\n article. The same article also explains how to \nmodify\n these \n.props\n files in section \nHow to modify a .props file?\n.",
            "title": " Where does WEKA look for .props files?"
        },
        {
            "location": "/where_is_my_home_directory_located/",
            "text": "Where a user's home directory is located varies from platform to platform and among the users on a single computer. But the actual location of the home directory is available through special environment variables:\n\n\n\n\nUnix/Linux\n\n\n$HOME\n\n\n\n\n\n\nWindows\n\n\n%USERPROFILE%\n\n\n\n\n\n\nCygwin\n\n\n$USERPROFILE\n\n\n\n\n\n\n\n\nIn order to find out where these environment variables actually point to, do the following:\n\n\n\n\non \nUnix/Linux\n, open a terminal and type the following command\n\n\necho $HOME\n\n\n\n\n\n\non \nWindows\n, open a command-prompt and type the following command\n\n\necho %USERPROFILE%\n\n\n\n\n\n\non \nCygwin\n, open a bash and type the following command\n\n\necho $USERPROFILE",
            "title": " Where is my home directory located?"
        },
        {
            "location": "/why_am_i_missing_certain_nominal_or_string_values_from_sparse_instances/",
            "text": "Internally, Weka stores all attribute values as double precision floating point numbers. In the case of nominal or string attributes these numbers are interpreted as indexes into the set of values for the attribute in question, with 0 corresponding to the first value, 1 the second and so forth. Because sparse data does not explicitly store zeros, any instances containing the first value (with index 0) of a nominal or string attribute does not show this value when printing out an ARFF file that is sparse format.",
            "title": " Why am I missing certain nominal or string values from sparse instances?"
        },
        {
            "location": "/windows_databases/",
            "text": "A common query we get from our users is how to open a Windows database in the Weka Explorer. This page is intended as a guide to help you achieve this. It is a complicated process and we cannot guarantee that it will work for you. The process described makes use of the JDBC-ODBC bridge that is part of Sun's JRE/JDK 1.3 (and higher).\n\n\nThe following instructions are for Windows 2000. Under other Windows versions there may be slight differences.\n\n\nStep 1: Create a User DSN\n\n\n\n\nGo to the \nControl Panel\n\n\nChoose \nAdminstrative Tools\n\n\nChoose \nData Sources (ODBC)\n\n\nAt the \nUser DSN\n tab, choose \nAdd...\n\n\nChoose database\n\n\nMicrosoft Access\n\n\nNote:\n Make sure your database is not open in another application before following the steps below.\n\n\nChoose the \nMicrosoft Access\n driver and click \nFinish\n\n\nGive the source a name by typing it into the \nData Source Name\n field\n\n\nIn the \nDatabase\n section, choose \nSelect...\n\n\nBrowse to find your database file, select it and click \nOK\n\n\nClick \nOK\n to finalize your DSN\n\n\n\n\n\n\nMicrosoft SQL Server 2000 (Desktop Engine)\n\n\nChoose the \nSQL Server\n driver and click \nFinish\n\n\nGive the source a name by typing it into the \nName\n field\n\n\nAdd a description for this source in the \nDescription\n field\n\n\nSelect the server you're connecting to from the \nServer\n combobox\n\n\nFor the verification of the authenticity of the login ID choose \nWith SQL Server...\n\n\nCheck \nConnect to SQL Server to obtain default settings...\n and supply the user ID and password with which you installed the Desktop Engine\n\n\nJust click on \nNext\n until it changes into \nFinish\n and click this, too\n\n\nFor testing purposes, click on \nTest Data Source...\n - the result should be \nTESTS COMPLETED SUCCESSFULLY!\n\n\nClick on \nOK\n\n\n\n\n\n\nMySQL\n\n\nChoose the \nMySQL ODBC\n driver and click \nFinish\n\n\nGive the source a name by typing it into the \nData Source Name\n field\n\n\nAdd a description for this source in the \nDescription\n field\n\n\nSpecify the server you're connecting to in \nServer\n\n\nFill in the user to use for connecting to the database in the \nUser\n field, the same for the password\n\n\nChoose the database for this DSN from the \nDatabase\n combobox\n\n\nClick on \nOK\n\n\n\n\n\n\n\n\n\n\nYour DSN should now be listed in the \nUser Data Sources\n list\n\n\n\n\nStep 2: Set up the DatabaseUtils.props file\n\n\nYou will need to configure a file called \nDatabaseUtils.props\n. This file already exists under the path \nweka/experiment/\n in the \nweka.jar\n file (which is just a ZIP file) that is part of the Weka download. In this directory you will also find a sample file for ODBC connectivity, called \nDatabaseUtils.props.odbc\n, and one specifically for MS Access, called \nDatabaseUtils.props.msaccess\n (>3.4.14, >3.5.8, >3.6.0), also using ODBC. You should use one of the sample files as basis for your setup, since they already contain default values specific to ODBC access.\n\n\nThis file needs to be recognized when the Explorer starts. You can achieve this by making sure it is in the working directory or the home directory (if you are unsure what the terms \nworking directory\n and \nhome directory\n mean, see the \nNotes\n section). The easiest is probably the second alternative, as the setup will apply to all the Weka instances on your machine.\n\n\nJust make sure that the file contains the following lines at least:\n\n\n jdbcDriver=sun.jdbc.odbc.JdbcOdbcDriver\n jdbcURL=jdbc:odbc:dbname\n\n\n\n\nwhere \ndbname\n is the name you gave the user DSN. (This can also be changed once the Explorer is running.)\n\n\nStep 3: Open the database\n\n\nBook version\n\n\n\n\nStart up the Weka Explorer.\n\n\nChoose \nOpen DB...\n\n\nEdit the \nquery\n field to read \nselect * from tablename\n where \ntablename\n is the name of the database table you want to read, or you could put a more complicated SQL query here instead.\n\n\nThe \ndatabaseURL\n should read \"jdbc:odbc:\ndbname\n\" where \ndbname\n is the name you gave the user DSN.\n\n\nClick \nOK\n \n\n\n\n\nAt this point the data should be read from the database.\n\n\nStable 3.6 and developer version\n\n\n\n\nStart up the Weka Explorer.\n\n\nChoose \nOpen DB...\n\n\nThe \nURL\n should read \"jdbc:odbc:\ndbname\n\" where \ndbname\n is the name you gave the user DSN.\n\n\nClick \nConnect\n\n\nEnter a \nQuery\n, e.g., \"\nselect * from tablename\n\" where \ntablename\n is the name of the database table you want to read. Or you could put a more complicated SQL query here instead.\n\n\nClick \nExecute\n\n\nWhen you're satisfied with the returned data, click \nOK\n to load the data into the Preprocess panel.\n\n\n\n\nNotes\n\n\n\n\nWorking directory\n\n\nThe directory a process is started from. When you start Weka from the Windows Start Menu, then this directory would be Weka's installation directory (the \njava\n process is started from that directory).\n\n\n\n\n\n\n\n\nHome directory\n\n\n\n\nThe directory that contains all the user's data. The exact location depends on the operating system and the version of the operating system. It is stored in the following environment variable:\n\n\n\n\n\n\nUnix/Linux\n\n\n\n\n$HOME\n\n\n\n\n\n\n\n\nWindows\n\n\n\n\n%USERPROFILE%\n\n\n\n\n\n\n\n\nCygwin\n\n\n\n\n$USERPROFILE\n\nYou should be able output the value in a command prompt/terminal with the \necho\n command. E.g., for Windows this would be:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\necho %USERPROFILE%\n\n\n\n\n\n\nSee also\n\n\n\n\nDatabases\n\n\nCLASSPATH\n\n\nDatabaseUtils.props",
            "title": " Windows Databases"
        },
        {
            "location": "/windows_databases/#step-1-create-a-user-dsn",
            "text": "Go to the  Control Panel  Choose  Adminstrative Tools  Choose  Data Sources (ODBC)  At the  User DSN  tab, choose  Add...  Choose database  Microsoft Access  Note:  Make sure your database is not open in another application before following the steps below.  Choose the  Microsoft Access  driver and click  Finish  Give the source a name by typing it into the  Data Source Name  field  In the  Database  section, choose  Select...  Browse to find your database file, select it and click  OK  Click  OK  to finalize your DSN    Microsoft SQL Server 2000 (Desktop Engine)  Choose the  SQL Server  driver and click  Finish  Give the source a name by typing it into the  Name  field  Add a description for this source in the  Description  field  Select the server you're connecting to from the  Server  combobox  For the verification of the authenticity of the login ID choose  With SQL Server...  Check  Connect to SQL Server to obtain default settings...  and supply the user ID and password with which you installed the Desktop Engine  Just click on  Next  until it changes into  Finish  and click this, too  For testing purposes, click on  Test Data Source...  - the result should be  TESTS COMPLETED SUCCESSFULLY!  Click on  OK    MySQL  Choose the  MySQL ODBC  driver and click  Finish  Give the source a name by typing it into the  Data Source Name  field  Add a description for this source in the  Description  field  Specify the server you're connecting to in  Server  Fill in the user to use for connecting to the database in the  User  field, the same for the password  Choose the database for this DSN from the  Database  combobox  Click on  OK      Your DSN should now be listed in the  User Data Sources  list",
            "title": "Step 1: Create a User DSN"
        },
        {
            "location": "/windows_databases/#step-2-set-up-the-databaseutilsprops-file",
            "text": "You will need to configure a file called  DatabaseUtils.props . This file already exists under the path  weka/experiment/  in the  weka.jar  file (which is just a ZIP file) that is part of the Weka download. In this directory you will also find a sample file for ODBC connectivity, called  DatabaseUtils.props.odbc , and one specifically for MS Access, called  DatabaseUtils.props.msaccess  (>3.4.14, >3.5.8, >3.6.0), also using ODBC. You should use one of the sample files as basis for your setup, since they already contain default values specific to ODBC access.  This file needs to be recognized when the Explorer starts. You can achieve this by making sure it is in the working directory or the home directory (if you are unsure what the terms  working directory  and  home directory  mean, see the  Notes  section). The easiest is probably the second alternative, as the setup will apply to all the Weka instances on your machine.  Just make sure that the file contains the following lines at least:   jdbcDriver=sun.jdbc.odbc.JdbcOdbcDriver\n jdbcURL=jdbc:odbc:dbname  where  dbname  is the name you gave the user DSN. (This can also be changed once the Explorer is running.)",
            "title": "Step 2: Set up the DatabaseUtils.props file"
        },
        {
            "location": "/windows_databases/#step-3-open-the-database",
            "text": "",
            "title": "Step 3: Open the database"
        },
        {
            "location": "/windows_databases/#book-version",
            "text": "Start up the Weka Explorer.  Choose  Open DB...  Edit the  query  field to read  select * from tablename  where  tablename  is the name of the database table you want to read, or you could put a more complicated SQL query here instead.  The  databaseURL  should read \"jdbc:odbc: dbname \" where  dbname  is the name you gave the user DSN.  Click  OK     At this point the data should be read from the database.",
            "title": "Book version"
        },
        {
            "location": "/windows_databases/#stable-36-and-developer-version",
            "text": "Start up the Weka Explorer.  Choose  Open DB...  The  URL  should read \"jdbc:odbc: dbname \" where  dbname  is the name you gave the user DSN.  Click  Connect  Enter a  Query , e.g., \" select * from tablename \" where  tablename  is the name of the database table you want to read. Or you could put a more complicated SQL query here instead.  Click  Execute  When you're satisfied with the returned data, click  OK  to load the data into the Preprocess panel.",
            "title": "Stable 3.6 and developer version"
        },
        {
            "location": "/windows_databases/#notes",
            "text": "Working directory  The directory a process is started from. When you start Weka from the Windows Start Menu, then this directory would be Weka's installation directory (the  java  process is started from that directory).     Home directory   The directory that contains all the user's data. The exact location depends on the operating system and the version of the operating system. It is stored in the following environment variable:    Unix/Linux   $HOME     Windows   %USERPROFILE%     Cygwin   $USERPROFILE \nYou should be able output the value in a command prompt/terminal with the  echo  command. E.g., for Windows this would be:          echo %USERPROFILE%",
            "title": "Notes"
        },
        {
            "location": "/windows_databases/#see-also",
            "text": "Databases  CLASSPATH  DatabaseUtils.props",
            "title": "See also"
        },
        {
            "location": "/xml/",
            "text": "Weka now supports \nXML\n (e\nX\ntensible \nM\narkup \nL\nanguage) in several places:\n\n\nCommand Line\n\n\nWEKA now allows to start Classifiers and Experiments with the \n-xml\n option followed by a filename to retrieve the command line options from the XML file instead of the command line.\n\n\nFor such simple classifiers like e.g. J48 this looks like overkill, but as soon as one uses Meta-Classifiers or Meta-Meta-Classifiers the handling gets tricky and one spends a lot of time looking for missing quotes. With the hierarchical structure of XML files it is simple to plug in other classifiers by just exchanging tags.\n\n\nThe DTD for the XML options is quite simple:\n\n\n <!DOCTYPE options\n [\n    <!ELEMENT options (option)*>\n    <!ATTLIST options type CDATA \"classifier\">\n    <!ATTLIST options value CDATA \"\">\n    <!ELEMENT option (#PCDATA | options)*>\n    <!ATTLIST option name CDATA #REQUIRED>\n    <!ATTLIST option type (flag | single | hyphens | quotes) \"single\">\n ]\n >\n\n\n\n\nThe type attribute of the option tag needs some explanations. There are currently four different types of options in WEKA:\n\n\n\n\nflag\n\n\nThe simplest option that takes no arguments, like e.g. the \n-V\n flag for inversing an selection.\n\n\n\n\n\n\n\n\n <option name=\"V\" type=\"flag\"/>\n\n\n\n\n\n\nsingle\n\n\nThe option takes exactly one parameter, directly following after the option, e.g., for specifying the trainings file with \n-t somefile.arff\n. Here the parameter value is just put between the opening and closing tag. Since single is the default value for the type  tag we don't need to specify it explicitly.\n\n\n\n\n\n\n\n\n <option name=\"t\">somefile.arff</option>\n\n\n\n\n\n\nhyphens\n\n\nMeta-Classifiers like \nAdaBoostM1\n take another classifier as option with the \n-W\n option, where the options for the base classifier follow after the \n--\n. And here it is where the fun starts;\n\nwhere to put parameters for the base classifier if the Meta-Classifier itself is a base classifier for another Meta-Classifier?\nE.g., does \n-W weka.classifiers.trees.J48 -- -C 0.001\n become this:\n\n\n\n\n\n\n\n\n <option name=\"W\" type=\"hyphens\">\n    <options type=\"classifier\" value=\"weka.classifiers.trees.J48\">\n       <option name=\"C\">0.001</option>\n    </options>\n </option>\n\n\n\n\n\n\nInternally, all the options enclosed by the \noptions\n tag are pushed to the end after the \n--\n if one transforms the XML into a command line string.\n\n\n\n\n\n\nquotes\n\n\nA Meta-Classifier like \nStacking\n can take several \n-B\n options, where each single one encloses other options in quotes (this itself can contain a Meta-Classifier!). From \n-B \"weka.classifiers.trees.J48\"\n we then get this XML:\n\n\n\n\n\n\n\n\n <option name=\"B\" type=\"quotes\">\n    <options type=\"classifier\" value=\"weka.classifiers.trees.J48\"/>\n </option>\n\n\n\n\n\n\nWith the XML representation one doesn't have to worry anymore about the level of quotes one is using and therefore doesn't have to care about the correct escaping (i.e. \" ... \\\" ... \\\" ...\") since this is done automatically.\n\n\n\n\nAnd if we now put all together we can transform this more complicated command line (\njava\n and the CLASSPATH omitted):\n\n\nweka.classifiers.meta.Stacking -B \"weka.classifiers.meta.AdaBoostM1 -W weka.classifiers.trees.J48 -- -C 0.001\" -B \"weka.classifiers.meta.Bagging -W weka.classifiers.meta.AdaBoostM1 -- -W weka.classifiers.trees.J48\" -B \"weka.classifiers.meta.Stacking -B \\\"weka.classifiers.trees.J48\\\"\" -t test/datasets/hepatitis.arff\n\n\n\n\ninto XML:\n\n\n <options type=\"class\" value=\"weka.classifiers.meta.Stacking\">\n    <option name=\"B\" type=\"quotes\">\n       <options type=\"classifier\" value=\"weka.classifiers.meta.AdaBoostM1\">\n          <option name=\"W\" type=\"hyphens\">\n             <options type=\"classifier\" value=\"weka.classifiers.trees.J48\">\n                <option name=\"C\">0.001</option>\n             </options>\n          </option>\n       </options>\n    </option>\n\n    <option name=\"B\" type=\"quotes\">\n       <options type=\"classifier\" value=\"weka.classifiers.meta.Bagging\">\n          <option name=\"W\" type=\"hyphens\">\n             <options type=\"classifier\" value=\"weka.classifiers.meta.AdaBoostM1\">\n                <option name=\"W\" type=\"hyphens\">\n                   <options type=\"classifier\" value=\"weka.classifiers.trees.J48\"/>\n                </option>\n             </options>\n          </option>\n       </options>\n    </option>\n\n    <option name=\"B\" type=\"quotes\">\n       <options type=\"classifier\" value=\"weka.classifiers.meta.Stacking\">\n          <option name=\"B\" type=\"quotes\">\n             <options type=\"classifier\" value=\"weka.classifiers.trees.J48\"/>\n          </option>\n       </options>\n    </option>\n\n    <option name=\"t\">test/datasets/hepatitis.arff</option>\n </options>\n\n\n\n\nNote:\n\n\n\n\nThe \ntype\n and \nvalue\n attribute of the outermost \noptions\n tag is not used while reading the parameters. It is merely for documentation purposes, so that one knows which class was actually started from the command line.\n\n\n\n\nResponsible Class(es):\n\n\nweka.core.xml.XMLOptions\n\n\nExample(s):\n \ncommandline.xml\n\n\nSerialization of Experiments\n\n\nIt is now possible to serialize the Experiments from the \nWEKA Experimenter\n not only in the proprietary binary format Java offers with serialization (with this you run into problems trying to read old experiments with a newer WEKA version, due to different SerialUIDs), but also in XML. There are currently two different ways to do this:\n\n\n\n\nbuilt-in\n\n\nThe built-in serialization captures only the necessary informations of an experiment and doesn't serialize anything else. It's sole purpose is to save the setup of a specific experiment and can therefore not store any built models. Thanks to this limitation we'll never run into problems with mismatching SerialUIDs.\n\n\n\n\n\n\n\n\n\n\nThis kind of serialization is always available and can be selected via a Filter (*.xml) in the Save/Open-Dialog of the Experimenter.\n\n\nThe DTD is very simple and looks like this (for version 3.4.5):\n\n\n\n\n <!DOCTYPE object[\n    <!ELEMENT object (#PCDATA | object)*>\n    <!ATTLIST object name      CDATA #REQUIRED>\n    <!ATTLIST object class     CDATA #REQUIRED>\n    <!ATTLIST object primitive CDATA \"no\">\n    <!ATTLIST object array     CDATA \"no\">   <!-- the dimensions of the array; no=0, yes=1 -->\n    <!ATTLIST object null      CDATA \"no\">   \n    <!ATTLIST object version   CDATA \"3.4.5\">\n ]>\n\n\n\n\n\n\nPrior to versions 3.4.5 and 3.5.0 it looked like this:\n\n\n\n\n <!DOCTYPE object\n [\n    <!ELEMENT object (#PCDATA | object)*>\n    <!ATTLIST object name      CDATA #REQUIRED>\n    <!ATTLIST object class     CDATA #REQUIRED>\n    <!ATTLIST object primitive CDATA \"yes\">\n    <!ATTLIST object array     CDATA \"no\">\n ]\n >\n\n\n\n\n\n\nResponsible Class(es):\n\n \nweka.experiment.xml.XMLExperiment\n\n\nfor general Serialization:\n\n \nweka.core.xml.XMLSerialization\n\n \nweka.core.xml.XMLBasicSerialization\n\n\nExample(s):\n \nserialization.xml\n\n\n\n\n\n\nKOML\n\n\nThe Koala Object Markup Language (KOML) is published under the \nLGPL\n and is an alternative way of serializing and derserializing Java Objects in an XML file. Like the normal serialization it serializes everything into XML via an ObjectOutputStream, including the SerialUID of each class. Even though we have the same problems with mismatching SerialUIDs it is at least possible edit the XML files by hand and replace the offending IDs with the new ones.\n\n\n\n\n\n\n\n\n\n\nIn order to use KOML one only has to assure that the KOML classes are in the CLASSPATH with which the Experimenter is launched. As soon as KOML is present another Filter (*.koml) will show up in the Save/Open-Dialog.\n\n\nThe DTD for KOML can be found \nhere\n.\n\n\nResponsible Class(es):\n\n \nweka.core.xml.KOML\n\n\nExample(s):\n \nserialization.koml\n\n\n\n\nThe experiment class can of course read those XML files if passed as input or output file (see options of \nweka.experiment.Experiment\n and \nweka.experiment.RemoteExperiment\n).\n\n\nSerialization of Classifiers\n\n\nThe options for models of a classifier, \n-l\n for the input model and \n-d\n for the output model, now also supports XML serialized files. Here we have to differentiate between two different formats:\n\n\n\n\n\n\nbuilt-in\n\n\n\n\nThe built-in serialization captures only the options of a classifier but not the built model. With the \n-l\n one still has to provide a training file, since we only retrieve the options from the XML file. It is possible to add more options on the command line, but it is no check performed whether they collide with the ones stored in the XML file.\nThe file is expected to end with \n.xml\n.\n\n\n\n\n\n\n\n\nKOML\n\n\n\n\nSince the KOML serialization captures everything of a Java Object we can use it just like the normal Java serialization.\nThe file is expected to end with \n.koml\n.\n\n\n\n\n\n\n\n\nThe \nbuilt-in\n serialization can be used in the \nExperimenter\n for loading/saving options from algorithms that have been added to a Simple Experiment. Unfortunately it is not possible to create such a hierarchical structure like mentioned in \nCommand Line\n. This is because of the loss of information caused by the \ngetOptions()\n method of classifiers: \nit returns only a flat String-Array and not a tree structure.\n\n\nResponsible Class(es):\n\n\nweka.core.xml.KOML\n\n\nweka.classifiers.xml.XMLClassifier\n\n\nExample(s):\n \ncommandline_inputmodel.xml\n\n\nBayesian Networks\n\n\nThe GraphVisualizer (\nweka.gui.graphvisualizer.GraphVisualizer\n) can save graphs into the \nInterchange Format\n for Bayesian Networks (BIF). If started from command line with an XML filename as first parameter and not from the Explorer it can display the given file directly.\n\n\nThe DTD for BIF is this:\n\n\n <!DOCTYPE BIF [\n    <!ELEMENT BIF ( NETWORK )*>\n          <!ATTLIST BIF VERSION CDATA #REQUIRED>\n    <!ELEMENT NETWORK ( NAME, ( PROPERTY | VARIABLE | DEFINITION )* )>\n    <!ELEMENT NAME (#PCDATA)>\n\n    <!ELEMENT VARIABLE ( NAME, ( OUTCOME |  PROPERTY )* ) >\n          <!ATTLIST VARIABLE TYPE (nature|decision|utility) \"nature\">\n    <!ELEMENT OUTCOME (#PCDATA)>\n    <!ELEMENT DEFINITION ( FOR | GIVEN | TABLE | PROPERTY )* >\n    <!ELEMENT FOR (#PCDATA)>\n    <!ELEMENT GIVEN (#PCDATA)>\n\n    <!ELEMENT TABLE (#PCDATA)>\n    <!ELEMENT PROPERTY (#PCDATA)>\n ]>\n\n\n\n\nResponsible Class(es):\n\n\nweka.classifiers.bayes.BayesNet#toXMLBIF03()\n\n\nweka.classifiers.bayes.net.BIFReader\n\n\nweka.gui.graphvisualizer.BIFParser\n\n\nExample(s):\n \nbif.xml\n\n\nTools\n\n\n\n\n\n\nExperimenter options\n\n\n\n\nThe XSLT script \noptions.xsl\n parses an XML file for the experimenter and outputs the options in two ways:\n\n\n\n\n\n\nin an array-like fashion, i.e., each option on a separate line; the class is output first.\n\n\ncommandline-like, i.e., the class followed by all its parameters; at each end of a line a \"\\\" is appended. (works only on *nix and \nCygwin\n)\n\n\n(Use \noptions_single.xsl\n\n \nUsage:\n\n\n\n\n\n\n\n\n\n\n\n\n xsltproc options.xsl <xml file>\n\n\n\n\n\n\nNote: \nyou can use any XSLT processor, e.g., xt; xsltproc is just one.\n\n\n\n\nDownloads\n\n\n\n\nKOML\n\n\nkoml12.dtd\n - local copy of the KOML DTD 1.2\n\n\nkoml_bin.zip\n\n\nkoml_sources.zip\n - the KOML source code",
            "title": " XML"
        },
        {
            "location": "/xml/#command-line",
            "text": "WEKA now allows to start Classifiers and Experiments with the  -xml  option followed by a filename to retrieve the command line options from the XML file instead of the command line.  For such simple classifiers like e.g. J48 this looks like overkill, but as soon as one uses Meta-Classifiers or Meta-Meta-Classifiers the handling gets tricky and one spends a lot of time looking for missing quotes. With the hierarchical structure of XML files it is simple to plug in other classifiers by just exchanging tags.  The DTD for the XML options is quite simple:   <!DOCTYPE options\n [\n    <!ELEMENT options (option)*>\n    <!ATTLIST options type CDATA \"classifier\">\n    <!ATTLIST options value CDATA \"\">\n    <!ELEMENT option (#PCDATA | options)*>\n    <!ATTLIST option name CDATA #REQUIRED>\n    <!ATTLIST option type (flag | single | hyphens | quotes) \"single\">\n ]\n >  The type attribute of the option tag needs some explanations. There are currently four different types of options in WEKA:   flag  The simplest option that takes no arguments, like e.g. the  -V  flag for inversing an selection.      <option name=\"V\" type=\"flag\"/>   single  The option takes exactly one parameter, directly following after the option, e.g., for specifying the trainings file with  -t somefile.arff . Here the parameter value is just put between the opening and closing tag. Since single is the default value for the type  tag we don't need to specify it explicitly.      <option name=\"t\">somefile.arff</option>   hyphens  Meta-Classifiers like  AdaBoostM1  take another classifier as option with the  -W  option, where the options for the base classifier follow after the  -- . And here it is where the fun starts; \nwhere to put parameters for the base classifier if the Meta-Classifier itself is a base classifier for another Meta-Classifier?\nE.g., does  -W weka.classifiers.trees.J48 -- -C 0.001  become this:      <option name=\"W\" type=\"hyphens\">\n    <options type=\"classifier\" value=\"weka.classifiers.trees.J48\">\n       <option name=\"C\">0.001</option>\n    </options>\n </option>   Internally, all the options enclosed by the  options  tag are pushed to the end after the  --  if one transforms the XML into a command line string.    quotes  A Meta-Classifier like  Stacking  can take several  -B  options, where each single one encloses other options in quotes (this itself can contain a Meta-Classifier!). From  -B \"weka.classifiers.trees.J48\"  we then get this XML:      <option name=\"B\" type=\"quotes\">\n    <options type=\"classifier\" value=\"weka.classifiers.trees.J48\"/>\n </option>   With the XML representation one doesn't have to worry anymore about the level of quotes one is using and therefore doesn't have to care about the correct escaping (i.e. \" ... \\\" ... \\\" ...\") since this is done automatically.   And if we now put all together we can transform this more complicated command line ( java  and the CLASSPATH omitted):  weka.classifiers.meta.Stacking -B \"weka.classifiers.meta.AdaBoostM1 -W weka.classifiers.trees.J48 -- -C 0.001\" -B \"weka.classifiers.meta.Bagging -W weka.classifiers.meta.AdaBoostM1 -- -W weka.classifiers.trees.J48\" -B \"weka.classifiers.meta.Stacking -B \\\"weka.classifiers.trees.J48\\\"\" -t test/datasets/hepatitis.arff  into XML:   <options type=\"class\" value=\"weka.classifiers.meta.Stacking\">\n    <option name=\"B\" type=\"quotes\">\n       <options type=\"classifier\" value=\"weka.classifiers.meta.AdaBoostM1\">\n          <option name=\"W\" type=\"hyphens\">\n             <options type=\"classifier\" value=\"weka.classifiers.trees.J48\">\n                <option name=\"C\">0.001</option>\n             </options>\n          </option>\n       </options>\n    </option>\n\n    <option name=\"B\" type=\"quotes\">\n       <options type=\"classifier\" value=\"weka.classifiers.meta.Bagging\">\n          <option name=\"W\" type=\"hyphens\">\n             <options type=\"classifier\" value=\"weka.classifiers.meta.AdaBoostM1\">\n                <option name=\"W\" type=\"hyphens\">\n                   <options type=\"classifier\" value=\"weka.classifiers.trees.J48\"/>\n                </option>\n             </options>\n          </option>\n       </options>\n    </option>\n\n    <option name=\"B\" type=\"quotes\">\n       <options type=\"classifier\" value=\"weka.classifiers.meta.Stacking\">\n          <option name=\"B\" type=\"quotes\">\n             <options type=\"classifier\" value=\"weka.classifiers.trees.J48\"/>\n          </option>\n       </options>\n    </option>\n\n    <option name=\"t\">test/datasets/hepatitis.arff</option>\n </options>  Note:   The  type  and  value  attribute of the outermost  options  tag is not used while reading the parameters. It is merely for documentation purposes, so that one knows which class was actually started from the command line.   Responsible Class(es):  weka.core.xml.XMLOptions  Example(s):   commandline.xml",
            "title": "Command Line"
        },
        {
            "location": "/xml/#serialization-of-experiments",
            "text": "It is now possible to serialize the Experiments from the  WEKA Experimenter  not only in the proprietary binary format Java offers with serialization (with this you run into problems trying to read old experiments with a newer WEKA version, due to different SerialUIDs), but also in XML. There are currently two different ways to do this:   built-in  The built-in serialization captures only the necessary informations of an experiment and doesn't serialize anything else. It's sole purpose is to save the setup of a specific experiment and can therefore not store any built models. Thanks to this limitation we'll never run into problems with mismatching SerialUIDs.      This kind of serialization is always available and can be selected via a Filter (*.xml) in the Save/Open-Dialog of the Experimenter.  The DTD is very simple and looks like this (for version 3.4.5):    <!DOCTYPE object[\n    <!ELEMENT object (#PCDATA | object)*>\n    <!ATTLIST object name      CDATA #REQUIRED>\n    <!ATTLIST object class     CDATA #REQUIRED>\n    <!ATTLIST object primitive CDATA \"no\">\n    <!ATTLIST object array     CDATA \"no\">   <!-- the dimensions of the array; no=0, yes=1 -->\n    <!ATTLIST object null      CDATA \"no\">   \n    <!ATTLIST object version   CDATA \"3.4.5\">\n ]>   Prior to versions 3.4.5 and 3.5.0 it looked like this:    <!DOCTYPE object\n [\n    <!ELEMENT object (#PCDATA | object)*>\n    <!ATTLIST object name      CDATA #REQUIRED>\n    <!ATTLIST object class     CDATA #REQUIRED>\n    <!ATTLIST object primitive CDATA \"yes\">\n    <!ATTLIST object array     CDATA \"no\">\n ]\n >   Responsible Class(es): \n  weka.experiment.xml.XMLExperiment  for general Serialization: \n  weka.core.xml.XMLSerialization \n  weka.core.xml.XMLBasicSerialization  Example(s):   serialization.xml    KOML  The Koala Object Markup Language (KOML) is published under the  LGPL  and is an alternative way of serializing and derserializing Java Objects in an XML file. Like the normal serialization it serializes everything into XML via an ObjectOutputStream, including the SerialUID of each class. Even though we have the same problems with mismatching SerialUIDs it is at least possible edit the XML files by hand and replace the offending IDs with the new ones.      In order to use KOML one only has to assure that the KOML classes are in the CLASSPATH with which the Experimenter is launched. As soon as KOML is present another Filter (*.koml) will show up in the Save/Open-Dialog.  The DTD for KOML can be found  here .  Responsible Class(es): \n  weka.core.xml.KOML  Example(s):   serialization.koml   The experiment class can of course read those XML files if passed as input or output file (see options of  weka.experiment.Experiment  and  weka.experiment.RemoteExperiment ).",
            "title": "Serialization of Experiments"
        },
        {
            "location": "/xml/#serialization-of-classifiers",
            "text": "The options for models of a classifier,  -l  for the input model and  -d  for the output model, now also supports XML serialized files. Here we have to differentiate between two different formats:    built-in   The built-in serialization captures only the options of a classifier but not the built model. With the  -l  one still has to provide a training file, since we only retrieve the options from the XML file. It is possible to add more options on the command line, but it is no check performed whether they collide with the ones stored in the XML file.\nThe file is expected to end with  .xml .     KOML   Since the KOML serialization captures everything of a Java Object we can use it just like the normal Java serialization.\nThe file is expected to end with  .koml .     The  built-in  serialization can be used in the  Experimenter  for loading/saving options from algorithms that have been added to a Simple Experiment. Unfortunately it is not possible to create such a hierarchical structure like mentioned in  Command Line . This is because of the loss of information caused by the  getOptions()  method of classifiers: \nit returns only a flat String-Array and not a tree structure.  Responsible Class(es):  weka.core.xml.KOML  weka.classifiers.xml.XMLClassifier  Example(s):   commandline_inputmodel.xml",
            "title": "Serialization of Classifiers"
        },
        {
            "location": "/xml/#bayesian-networks",
            "text": "The GraphVisualizer ( weka.gui.graphvisualizer.GraphVisualizer ) can save graphs into the  Interchange Format  for Bayesian Networks (BIF). If started from command line with an XML filename as first parameter and not from the Explorer it can display the given file directly.  The DTD for BIF is this:   <!DOCTYPE BIF [\n    <!ELEMENT BIF ( NETWORK )*>\n          <!ATTLIST BIF VERSION CDATA #REQUIRED>\n    <!ELEMENT NETWORK ( NAME, ( PROPERTY | VARIABLE | DEFINITION )* )>\n    <!ELEMENT NAME (#PCDATA)>\n\n    <!ELEMENT VARIABLE ( NAME, ( OUTCOME |  PROPERTY )* ) >\n          <!ATTLIST VARIABLE TYPE (nature|decision|utility) \"nature\">\n    <!ELEMENT OUTCOME (#PCDATA)>\n    <!ELEMENT DEFINITION ( FOR | GIVEN | TABLE | PROPERTY )* >\n    <!ELEMENT FOR (#PCDATA)>\n    <!ELEMENT GIVEN (#PCDATA)>\n\n    <!ELEMENT TABLE (#PCDATA)>\n    <!ELEMENT PROPERTY (#PCDATA)>\n ]>  Responsible Class(es):  weka.classifiers.bayes.BayesNet#toXMLBIF03()  weka.classifiers.bayes.net.BIFReader  weka.gui.graphvisualizer.BIFParser  Example(s):   bif.xml",
            "title": "Bayesian Networks"
        },
        {
            "location": "/xml/#tools",
            "text": "Experimenter options   The XSLT script  options.xsl  parses an XML file for the experimenter and outputs the options in two ways:    in an array-like fashion, i.e., each option on a separate line; the class is output first.  commandline-like, i.e., the class followed by all its parameters; at each end of a line a \"\\\" is appended. (works only on *nix and  Cygwin )  (Use  options_single.xsl \n  Usage:        xsltproc options.xsl <xml file>   Note: \nyou can use any XSLT processor, e.g., xt; xsltproc is just one.",
            "title": "Tools"
        },
        {
            "location": "/xml/#downloads",
            "text": "KOML  koml12.dtd  - local copy of the KOML DTD 1.2  koml_bin.zip  koml_sources.zip  - the KOML source code",
            "title": "Downloads"
        },
        {
            "location": "/xrff/",
            "text": "The \nXRFF\n (e\nX\ntensible attribute-\nR\nelation \nF\nile \nF\normat) is an XML-based extension of the \nARFF\n format.\n\n\nFile extensions\n\n\n\n\n.xrff\n\n\nthe default extension of \nXRFF\n files\n\n\n\n\n\n\n.xrff.gz\n\n\nthe extension for \ngzip\n compressed \nXRFF\n files (see \nCompression\n for more details)\n\n\n\n\n\n\n\n\nComparison\n\n\nARFF\n\n\nIn the following a snippet of the UCI dataset \niris\n in \nARFF\n format:\n\n\n @relation iris\n\n @attribute sepallength numeric\n @attribute sepalwidth numeric\n @attribute petallength numeric\n @attribute petalwidth numeric\n @attribute class {Iris-setosa,Iris-versicolor,Iris-virginica}\n\n @data\n 5.1,3.5,1.4,0.2,Iris-setosa\n 4.9,3,1.4,0.2,Iris-setosa\n ...\n\n\n\n\nXRFF\n\n\nAnd the same dataset represented as \nXRFF\n file:\n\n\n <?xml version=\"1.0\" encoding=\"utf-8\"?>\n\n <!DOCTYPE dataset\n [\n    <!ELEMENT dataset (header,body)>\n    <!ATTLIST dataset name CDATA #REQUIRED>\n    <!ATTLIST dataset version CDATA \"3.5.4\">\n\n    <!ELEMENT header (notes?,attributes)>\n    <!ELEMENT body (instances)>\n    <!ELEMENT notes ANY>   <!--  comments, information, copyright, etc. -->\n\n    <!ELEMENT attributes (attribute+)>\n    <!ELEMENT attribute (labels?,metadata?,attributes?)>\n    <!ATTLIST attribute name CDATA #REQUIRED>\n    <!ATTLIST attribute type (numeric|date|nominal|string|relational) #REQUIRED>\n    <!ATTLIST attribute format CDATA #IMPLIED>\n    <!ATTLIST attribute class (yes|no) \"no\">\n    <!ELEMENT labels (label*)>   <!-- only for type \"nominal\" -->\n    <!ELEMENT label ANY>\n    <!ELEMENT metadata (property*)>\n    <!ELEMENT property ANY>\n    <!ATTLIST property name CDATA #REQUIRED>\n\n    <!ELEMENT instances (instance*)>\n    <!ELEMENT instance (value*)>\n    <!ATTLIST instance type (normal|sparse) \"normal\">\n    <!ATTLIST instance weight CDATA #IMPLIED>\n    <!ELEMENT value (#PCDATA|instances)*>\n    <!ATTLIST value index CDATA #IMPLIED>   <!-- 1-based index (only used for instance format \"sparse\") -->\n    <!ATTLIST value missing (yes|no) \"no\">\n ]\n >\n\n <dataset name=\"iris\" version=\"3.5.3\">\n    <header>\n       <attributes>\n          <attribute name=\"sepallength\" type=\"numeric\"/>\n          <attribute name=\"sepalwidth\" type=\"numeric\"/>\n          <attribute name=\"petallength\" type=\"numeric\"/>\n          <attribute name=\"petalwidth\" type=\"numeric\"/>\n          <attribute class=\"yes\" name=\"class\" type=\"nominal\">\n             <labels>\n                <label>Iris-setosa</label>\n                <label>Iris-versicolor</label>\n                <label>Iris-virginica</label>\n             </labels>\n          </attribute>\n       </attributes>\n    </header>\n\n    <body>\n       <instances>\n          <instance>\n             <value>5.1</value>\n             <value>3.5</value>\n             <value>1.4</value>\n             <value>0.2</value>\n             <value>Iris-setosa</value>\n          </instance>\n          <instance>\n             <value>4.9</value>\n             <value>3</value>\n             <value>1.4</value>\n             <value>0.2</value>\n             <value>Iris-setosa</value>\n          </instance>\n          ...\n       </instances>\n    </body>\n </dataset>\n\n\n\n\nSparse format\n\n\nThe \nXRFF\n format also supports a sparse data representation. Even though the iris dataset does not contain sparse data, the above example will be used here to illustrate the sparse format:\n\n\n ...\n <instances>\n    <instance type=\"sparse\">\n       <value index=\"1\">5.1</value>\n       <value index=\"2\">3.5</value>\n       <value index=\"3\">1.4</value>\n       <value index=\"4\">0.2</value>\n       <value index=\"5\">Iris-setosa</value>\n    </instance>\n    <instance type=\"sparse\">\n       <value index=\"1\">4.9</value>\n       <value index=\"2\">3</value>\n       <value index=\"3\">1.4</value>\n       <value index=\"4\">0.2</value>\n       <value index=\"5\">Iris-setosa</value>\n    </instance>\n    ...\n </instances>\n ...\n\n\n\n\nIn contrast to the \nnormal\n data format, each sparse \ninstance\n tag contains a \ntype\n attribute with the value \nsparse\n:\n\n\n <instance type=\"sparse\">\n\n\n\n\nAnd each \nvalue\n tag needs to specify the \nindex\n attribute, which contains the 1-based index of this value. \n\n\n <value index=\"1\">5.1</value>\n\n\n\n\nCompression\n\n\nSince the XML representation takes up considerably more space than the rather compact \nARFF\n format, one can also compress the data via \ngzip\n. Weka automatically recognizes a file being gzip compressed, if the file's extension is \n.xrff.gz\n instead of \n.xrff\n.\n\n\nThe Weka Explorer now allows to load/save compressed and uncompressed XRFF files (this applies also to \nARFF\n files).\n\n\nAdditional features\n\n\nIn addition to all the features of the \nARFF\n format, the \nXRFF\n format contains the following additional features:\n\n\n\n\nclass attribute specification\n\n\nattribute weights\n\n\ninstance weights\n\n\n\n\nClass attribute specification\n\n\nVia the \nclass=\"yes\"\n attribute in the attribute specification in the header, one can define which attribute should act as class attribute. A feature that can be used on the command line as well as in the Experimenter, which now can also load other data formats, and removing the limitation of the class attribute always having to be the last one.\n\n\nSnippet from the iris dataset:\n\n\n <attribute **class=\"yes\"** name=\"class\" type=\"nominal\">\n\n\n\n\nAttribute weights\n\n\nAttribute weights are stored in an attributes meta-data tag (in the \nheader\n section). Here's an example of the \npetalwidth\n attribute with a weight of 0.9:\n\n\n <attribute name=\"petalwidth\" type=\"numeric\">\n     <metadata>\n        <property name=\"weight\">0.9</property>\n     </metadata>\n </attribute>\n\n\n\n\nInstance weights\n\n\nInstance weights are defined via the \nweight\n attribute in each \ninstance\n tag. By default, the weight is 1. Here's an example:\n\n\n <instance weight=\"0.75\">\n    <value>5.1</value>\n    <value>3.5</value>\n    <value>1.4</value>\n    <value>0.2</value>\n    <value>Iris-setosa</value>\n </instance>",
            "title": " XRFF"
        },
        {
            "location": "/xrff/#file-extensions",
            "text": ".xrff  the default extension of  XRFF  files    .xrff.gz  the extension for  gzip  compressed  XRFF  files (see  Compression  for more details)",
            "title": "File extensions"
        },
        {
            "location": "/xrff/#comparison",
            "text": "",
            "title": "Comparison"
        },
        {
            "location": "/xrff/#arff",
            "text": "In the following a snippet of the UCI dataset  iris  in  ARFF  format:   @relation iris\n\n @attribute sepallength numeric\n @attribute sepalwidth numeric\n @attribute petallength numeric\n @attribute petalwidth numeric\n @attribute class {Iris-setosa,Iris-versicolor,Iris-virginica}\n\n @data\n 5.1,3.5,1.4,0.2,Iris-setosa\n 4.9,3,1.4,0.2,Iris-setosa\n ...",
            "title": "ARFF"
        },
        {
            "location": "/xrff/#xrff",
            "text": "And the same dataset represented as  XRFF  file:   <?xml version=\"1.0\" encoding=\"utf-8\"?>\n\n <!DOCTYPE dataset\n [\n    <!ELEMENT dataset (header,body)>\n    <!ATTLIST dataset name CDATA #REQUIRED>\n    <!ATTLIST dataset version CDATA \"3.5.4\">\n\n    <!ELEMENT header (notes?,attributes)>\n    <!ELEMENT body (instances)>\n    <!ELEMENT notes ANY>   <!--  comments, information, copyright, etc. -->\n\n    <!ELEMENT attributes (attribute+)>\n    <!ELEMENT attribute (labels?,metadata?,attributes?)>\n    <!ATTLIST attribute name CDATA #REQUIRED>\n    <!ATTLIST attribute type (numeric|date|nominal|string|relational) #REQUIRED>\n    <!ATTLIST attribute format CDATA #IMPLIED>\n    <!ATTLIST attribute class (yes|no) \"no\">\n    <!ELEMENT labels (label*)>   <!-- only for type \"nominal\" -->\n    <!ELEMENT label ANY>\n    <!ELEMENT metadata (property*)>\n    <!ELEMENT property ANY>\n    <!ATTLIST property name CDATA #REQUIRED>\n\n    <!ELEMENT instances (instance*)>\n    <!ELEMENT instance (value*)>\n    <!ATTLIST instance type (normal|sparse) \"normal\">\n    <!ATTLIST instance weight CDATA #IMPLIED>\n    <!ELEMENT value (#PCDATA|instances)*>\n    <!ATTLIST value index CDATA #IMPLIED>   <!-- 1-based index (only used for instance format \"sparse\") -->\n    <!ATTLIST value missing (yes|no) \"no\">\n ]\n >\n\n <dataset name=\"iris\" version=\"3.5.3\">\n    <header>\n       <attributes>\n          <attribute name=\"sepallength\" type=\"numeric\"/>\n          <attribute name=\"sepalwidth\" type=\"numeric\"/>\n          <attribute name=\"petallength\" type=\"numeric\"/>\n          <attribute name=\"petalwidth\" type=\"numeric\"/>\n          <attribute class=\"yes\" name=\"class\" type=\"nominal\">\n             <labels>\n                <label>Iris-setosa</label>\n                <label>Iris-versicolor</label>\n                <label>Iris-virginica</label>\n             </labels>\n          </attribute>\n       </attributes>\n    </header>\n\n    <body>\n       <instances>\n          <instance>\n             <value>5.1</value>\n             <value>3.5</value>\n             <value>1.4</value>\n             <value>0.2</value>\n             <value>Iris-setosa</value>\n          </instance>\n          <instance>\n             <value>4.9</value>\n             <value>3</value>\n             <value>1.4</value>\n             <value>0.2</value>\n             <value>Iris-setosa</value>\n          </instance>\n          ...\n       </instances>\n    </body>\n </dataset>",
            "title": "XRFF"
        },
        {
            "location": "/xrff/#sparse-format",
            "text": "The  XRFF  format also supports a sparse data representation. Even though the iris dataset does not contain sparse data, the above example will be used here to illustrate the sparse format:   ...\n <instances>\n    <instance type=\"sparse\">\n       <value index=\"1\">5.1</value>\n       <value index=\"2\">3.5</value>\n       <value index=\"3\">1.4</value>\n       <value index=\"4\">0.2</value>\n       <value index=\"5\">Iris-setosa</value>\n    </instance>\n    <instance type=\"sparse\">\n       <value index=\"1\">4.9</value>\n       <value index=\"2\">3</value>\n       <value index=\"3\">1.4</value>\n       <value index=\"4\">0.2</value>\n       <value index=\"5\">Iris-setosa</value>\n    </instance>\n    ...\n </instances>\n ...  In contrast to the  normal  data format, each sparse  instance  tag contains a  type  attribute with the value  sparse :   <instance type=\"sparse\">  And each  value  tag needs to specify the  index  attribute, which contains the 1-based index of this value.    <value index=\"1\">5.1</value>",
            "title": "Sparse format"
        },
        {
            "location": "/xrff/#compression",
            "text": "Since the XML representation takes up considerably more space than the rather compact  ARFF  format, one can also compress the data via  gzip . Weka automatically recognizes a file being gzip compressed, if the file's extension is  .xrff.gz  instead of  .xrff .  The Weka Explorer now allows to load/save compressed and uncompressed XRFF files (this applies also to  ARFF  files).",
            "title": "Compression"
        },
        {
            "location": "/xrff/#additional-features",
            "text": "In addition to all the features of the  ARFF  format, the  XRFF  format contains the following additional features:   class attribute specification  attribute weights  instance weights",
            "title": "Additional features"
        },
        {
            "location": "/xrff/#class-attribute-specification",
            "text": "Via the  class=\"yes\"  attribute in the attribute specification in the header, one can define which attribute should act as class attribute. A feature that can be used on the command line as well as in the Experimenter, which now can also load other data formats, and removing the limitation of the class attribute always having to be the last one.  Snippet from the iris dataset:   <attribute **class=\"yes\"** name=\"class\" type=\"nominal\">",
            "title": "Class attribute specification"
        },
        {
            "location": "/xrff/#attribute-weights",
            "text": "Attribute weights are stored in an attributes meta-data tag (in the  header  section). Here's an example of the  petalwidth  attribute with a weight of 0.9:   <attribute name=\"petalwidth\" type=\"numeric\">\n     <metadata>\n        <property name=\"weight\">0.9</property>\n     </metadata>\n </attribute>",
            "title": "Attribute weights"
        },
        {
            "location": "/xrff/#instance-weights",
            "text": "Instance weights are defined via the  weight  attribute in each  instance  tag. By default, the weight is 1. Here's an example:   <instance weight=\"0.75\">\n    <value>5.1</value>\n    <value>3.5</value>\n    <value>1.4</value>\n    <value>0.2</value>\n    <value>Iris-setosa</value>\n </instance>",
            "title": "Instance weights"
        },
        {
            "location": "/zero_r/",
            "text": "Description\n\n\nClass for building and using a 0-R classifier. Predicts the mean\n(for a numeric class) or the mode (for a nominal class).\n\n\nReference\n\n\n-none-\n\n\nPackage\n\n\nweka.classifiers.rules\n\n\nDownload\n\n\nSource code: \n\nZeroR.java\n\n\nAdditional Information\n\n\n-none-",
            "title": " ZeroR"
        },
        {
            "location": "/zero_r/#description",
            "text": "Class for building and using a 0-R classifier. Predicts the mean\n(for a numeric class) or the mode (for a nominal class).",
            "title": "Description"
        },
        {
            "location": "/zero_r/#reference",
            "text": "-none-",
            "title": "Reference"
        },
        {
            "location": "/zero_r/#package",
            "text": "weka.classifiers.rules",
            "title": "Package"
        },
        {
            "location": "/zero_r/#download",
            "text": "Source code:  ZeroR.java",
            "title": "Download"
        },
        {
            "location": "/zero_r/#additional-information",
            "text": "-none-",
            "title": "Additional Information"
        },
        {
            "location": "/weka_core_capabilities.props/",
            "text": "File\n\n\nweka/core/Capabilities.props\n\n\nDescription\n\n\nCustomization of the Capabilities support in Weka.\n\n\nCAUTION:\n disabling any of these properties can lead to unreliable results within Weka!\n\n\nVersion\n\n\n\n\n\n\n\n\n3.5.3\n\n\n\n\n\n\n\n\nFields\n\n\n\n\nTest\n\n\ngeneral \nswitch\n for Capabilities tests\n\n\n\n\n\n\nInstancesTest\n\n\nenable/disables tests that are based on the data\n\n\n\n\n\n\nAttributeTest\n\n\nenable/disables tests that work only on the type of attribute\n\n\n\n\n\n\nMissingValuesTest\n\n\ntest for missing values\n\n\n\n\n\n\nMissingClassValuesTest\n\n\ntest for missing class values\n\n\n\n\n\n\nMinimumNumberInstancesTest\n\n\ntest for minimum number of instances\n\n\n\n\n\n\n\n\nSee also\n\n\n\n\nProperties file\n\n\n\n\nLinks\n\n\n\n\nWekalist posts\n\n\nproblem\n\n\ninquiry\n\n\nfurther discussion",
            "title": " weka/core/Capabilities.props"
        },
        {
            "location": "/weka_core_capabilities.props/#file",
            "text": "weka/core/Capabilities.props",
            "title": "File"
        },
        {
            "location": "/weka_core_capabilities.props/#description",
            "text": "Customization of the Capabilities support in Weka.  CAUTION:  disabling any of these properties can lead to unreliable results within Weka!",
            "title": "Description"
        },
        {
            "location": "/weka_core_capabilities.props/#version",
            "text": "3.5.3",
            "title": "Version"
        },
        {
            "location": "/weka_core_capabilities.props/#fields",
            "text": "Test  general  switch  for Capabilities tests    InstancesTest  enable/disables tests that are based on the data    AttributeTest  enable/disables tests that work only on the type of attribute    MissingValuesTest  test for missing values    MissingClassValuesTest  test for missing class values    MinimumNumberInstancesTest  test for minimum number of instances",
            "title": "Fields"
        },
        {
            "location": "/weka_core_capabilities.props/#see-also",
            "text": "Properties file",
            "title": "See also"
        },
        {
            "location": "/weka_core_capabilities.props/#links",
            "text": "Wekalist posts  problem  inquiry  further discussion",
            "title": "Links"
        },
        {
            "location": "/weka_core_logging_logging.props/",
            "text": "File\n\n\nweka/core/logging/Logging.props\n\n\nDescription\n\n\nDefines the type of logging Weka performs. The default is to log regular log messages from the GUI and everything that is output to \nstdout\n and \nstderr\n to \n$HOME/weka.log\n.\n\n\nVersion\n\n\n\n\n\n\n\n\n3.5.8\n\n\n\n\n\n\n\n\nFields\n\n\n\n\nLogger\n\n\nThe logger class to use for logging. See Javadoc of respective logger class (derived from \nweka.core.logging.Logger\n) for more information.\n\n\n\n\n\n\nMinLevel\n\n\nSets the minimum level a log messages needs to have in order to end up in the log. \nALL\n will log everything, \nOFF\n turns logging off.\n\n\n\n\n\n\n\n\nDateFormat\n\n\n\n\nThe \nISO-8601\n format of the date. Here is the default value:\n\nyyyy-MM-dd HH:mm:ss\n\n\n\n\n\n\n\n\nLogFile\n\n\n\n\nIn case a logger class logs to a file, like \nweka.core.logging.FileLogger\n and \nweka.core.logging.OutputLogger\n, this file will be used. These loggers clear the log-file everytime Weka is started.\n\n\n\n\n\n\n\n\nSee also\n\n\n\n\nProperties file\n\n\n\n\nLinks\n\n\n\n\nISO-8601",
            "title": " weka/core/logging/Logging.props"
        },
        {
            "location": "/weka_core_logging_logging.props/#file",
            "text": "weka/core/logging/Logging.props",
            "title": "File"
        },
        {
            "location": "/weka_core_logging_logging.props/#description",
            "text": "Defines the type of logging Weka performs. The default is to log regular log messages from the GUI and everything that is output to  stdout  and  stderr  to  $HOME/weka.log .",
            "title": "Description"
        },
        {
            "location": "/weka_core_logging_logging.props/#version",
            "text": "3.5.8",
            "title": "Version"
        },
        {
            "location": "/weka_core_logging_logging.props/#fields",
            "text": "Logger  The logger class to use for logging. See Javadoc of respective logger class (derived from  weka.core.logging.Logger ) for more information.    MinLevel  Sets the minimum level a log messages needs to have in order to end up in the log.  ALL  will log everything,  OFF  turns logging off.     DateFormat   The  ISO-8601  format of the date. Here is the default value: yyyy-MM-dd HH:mm:ss     LogFile   In case a logger class logs to a file, like  weka.core.logging.FileLogger  and  weka.core.logging.OutputLogger , this file will be used. These loggers clear the log-file everytime Weka is started.",
            "title": "Fields"
        },
        {
            "location": "/weka_core_logging_logging.props/#see-also",
            "text": "Properties file",
            "title": "See also"
        },
        {
            "location": "/weka_core_logging_logging.props/#links",
            "text": "ISO-8601",
            "title": "Links"
        },
        {
            "location": "/weka_gui_generic_object_editor.props/",
            "text": "File\n\n\nweka/gui/GenericObjectEditor.props\n\n\nDescription\n\n\nHas been superceded by \nweka/gui/GenericPropertiesCreator.props\n which performs a dynamic discovery of classes.\nVersion 3.5.8 of Weka \nturned off\n the dynamic discovery of classes by default, i.e., one has to add the class in this file again or turn dynamic discovery on again.\n\n\nVersion\n\n\n\n\n>= 3.1.3\n\n\n\n\nSee also\n\n\n\n\nweka/gui/GenericPropertiesCreator.props\n\n\nGenericObjectEditor\n (explains how to add new schemes)\n\n\nProperties File",
            "title": " weka/gui/GenericObjectEditor.props"
        },
        {
            "location": "/weka_gui_generic_object_editor.props/#file",
            "text": "weka/gui/GenericObjectEditor.props",
            "title": "File"
        },
        {
            "location": "/weka_gui_generic_object_editor.props/#description",
            "text": "Has been superceded by  weka/gui/GenericPropertiesCreator.props  which performs a dynamic discovery of classes.\nVersion 3.5.8 of Weka  turned off  the dynamic discovery of classes by default, i.e., one has to add the class in this file again or turn dynamic discovery on again.",
            "title": "Description"
        },
        {
            "location": "/weka_gui_generic_object_editor.props/#version",
            "text": ">= 3.1.3",
            "title": "Version"
        },
        {
            "location": "/weka_gui_generic_object_editor.props/#see-also",
            "text": "weka/gui/GenericPropertiesCreator.props  GenericObjectEditor  (explains how to add new schemes)  Properties File",
            "title": "See also"
        },
        {
            "location": "/weka_gui_gui_editors.props/",
            "text": "File\n\n\nweka/gui/GUIEditors.props\n\n\nDescription\n\n\nLists what classes are handled by which GUI editor. Formerly done statically (but centralized) by the class \nweka.gui.GenericObjectEditor\n or in versions older than 3.5.2 all over the place.\n\n\nVersion\n\n\n\n\n\n\n\n\n3.5.3\n\n\n\n\n\n\n\n\nFields\n\n\n\n\nFormat\n\n\n<class>=<editor>\n\n\n\n\ne.g., \njava.io.File=weka.gui.FileEditor\n\n\n\n\n\n\n\n\n<class[]>=<editor>\n (for arrays)\n\n\n\n\ne.g., \njava.lang.Object[]=weka.gui.GenericArrayEditor\n\n\n\n\n\n\n\n\n\n\n\n\nAvailable editors\n\n\nGeneral editors\n\n\nweka.gui.GenericObjectEditor\n\n\nweka.gui.GenericArrayEditor\n\n\n\n\n\n\nSpecialized editors\n\n\nweka.gui.CostMatrixEditor\n\n\nweka.gui.EnsembleSelectionLibraryEditor\n\n\nweka.gui.FileEditor\n\n\nweka.gui.SelectedTagEditor\n\n\nweka.gui.SimpleDateFormatEditor\n\n\n\n\n\n\n\n\n\n\n\n\nSee also\n\n\n\n\nGenericObjectEditor\n\n\nProperties File",
            "title": " weka/gui/GUIEditors.props"
        },
        {
            "location": "/weka_gui_gui_editors.props/#file",
            "text": "weka/gui/GUIEditors.props",
            "title": "File"
        },
        {
            "location": "/weka_gui_gui_editors.props/#description",
            "text": "Lists what classes are handled by which GUI editor. Formerly done statically (but centralized) by the class  weka.gui.GenericObjectEditor  or in versions older than 3.5.2 all over the place.",
            "title": "Description"
        },
        {
            "location": "/weka_gui_gui_editors.props/#version",
            "text": "3.5.3",
            "title": "Version"
        },
        {
            "location": "/weka_gui_gui_editors.props/#fields",
            "text": "Format  <class>=<editor>   e.g.,  java.io.File=weka.gui.FileEditor     <class[]>=<editor>  (for arrays)   e.g.,  java.lang.Object[]=weka.gui.GenericArrayEditor       Available editors  General editors  weka.gui.GenericObjectEditor  weka.gui.GenericArrayEditor    Specialized editors  weka.gui.CostMatrixEditor  weka.gui.EnsembleSelectionLibraryEditor  weka.gui.FileEditor  weka.gui.SelectedTagEditor  weka.gui.SimpleDateFormatEditor",
            "title": "Fields"
        },
        {
            "location": "/weka_gui_gui_editors.props/#see-also",
            "text": "GenericObjectEditor  Properties File",
            "title": "See also"
        },
        {
            "location": "/weka_gui_generic_properties_creator.props/",
            "text": "File\n\n\nweka/gui/GenericPropertiesCreator.props\n\n\nDescription\n\n\nLists all the packages to look in for subclasses of a certain superclass to be displayed in the \nGenericObjectEditor\n.\n\n\nNote:\n Weka 3.5.8 turned the automatic discovery off by default. In this case, the \nweka/gui/GenericObjectEditor.props\n is used again.\n\n\nVersion\n\n\n\n\n>= 3.4.4\n\n\n\n\nFields\n\n\n\n\nenable/disable dynamic discovery (> 3.5.5)\n\n\nUseDynamic=true|false\n\n\n\n\n\n\nFormat (a \nbackslash\n at the end continues the package list on the next line)\n\n\n<superclass>=<package>[,package[,...]]\n\n\n\n\n\n\nFilter example\n\n\n\n\n weka.filters.Filter= \\\n  weka.filters, \\\n  weka.filters.supervised.attribute, \\\n  weka.filters.supervised.instance, \\\n  weka.filters.unsupervised.attribute, \\\n  weka.filters.unsupervised.instance\n\n\n\n\n\n\nClassifier example\n\n\n\n\n weka.classifiers.Classifier=\\\n  weka.classifiers.bayes,\\\n  weka.classifiers.functions,\\\n  weka.classifiers.lazy,\\\n  weka.classifiers.meta,\\\n  weka.classifiers.meta.nestedDichotomies,\\\n  weka.classifiers.mi,\\\n  weka.classifiers.misc,\\\n  weka.classifiers.trees,\\\n  weka.classifiers.rules\n\n\n\n\nSee also\n\n\n\n\nGenericObjectEditor\n (explains how to add new schemes)\n\n\nProperties file",
            "title": " weka/gui/GenericPropertiesCreator.props"
        },
        {
            "location": "/weka_gui_generic_properties_creator.props/#file",
            "text": "weka/gui/GenericPropertiesCreator.props",
            "title": "File"
        },
        {
            "location": "/weka_gui_generic_properties_creator.props/#description",
            "text": "Lists all the packages to look in for subclasses of a certain superclass to be displayed in the  GenericObjectEditor .  Note:  Weka 3.5.8 turned the automatic discovery off by default. In this case, the  weka/gui/GenericObjectEditor.props  is used again.",
            "title": "Description"
        },
        {
            "location": "/weka_gui_generic_properties_creator.props/#version",
            "text": ">= 3.4.4",
            "title": "Version"
        },
        {
            "location": "/weka_gui_generic_properties_creator.props/#fields",
            "text": "enable/disable dynamic discovery (> 3.5.5)  UseDynamic=true|false    Format (a  backslash  at the end continues the package list on the next line)  <superclass>=<package>[,package[,...]]    Filter example    weka.filters.Filter= \\\n  weka.filters, \\\n  weka.filters.supervised.attribute, \\\n  weka.filters.supervised.instance, \\\n  weka.filters.unsupervised.attribute, \\\n  weka.filters.unsupervised.instance   Classifier example    weka.classifiers.Classifier=\\\n  weka.classifiers.bayes,\\\n  weka.classifiers.functions,\\\n  weka.classifiers.lazy,\\\n  weka.classifiers.meta,\\\n  weka.classifiers.meta.nestedDichotomies,\\\n  weka.classifiers.mi,\\\n  weka.classifiers.misc,\\\n  weka.classifiers.trees,\\\n  weka.classifiers.rules",
            "title": "Fields"
        },
        {
            "location": "/weka_gui_generic_properties_creator.props/#see-also",
            "text": "GenericObjectEditor  (explains how to add new schemes)  Properties file",
            "title": "See also"
        },
        {
            "location": "/weka_gui_generic_properties_creator.excludes/",
            "text": "File\n\n\nweka/gui/GenericPropertiesCreator.excludes\n\n\nDescription\n\n\nList classes for corresponding superclasses in \nGenericPropertiesCreator.props\n that shouldn't appear in the \nGenericObjectEditor\n popup tree.\n\n\nVersion\n\n\n\n\n>= 3.5.3\n\n\n\n\nFields\n\n\n\n\nFormat\n\n\n<key>=<prefix>:<class>[,<prefix>:<class>]\n\n\n\n\n\n\n<key>\n\n\n\n\nthe key from \nGenericPropertiesCreator.props\n (class or interface)\n\n\n\n\n\n\n\n\n<prefix>\n\n\nS\n (\"Superclass\"): \nany class derived from this one will be excluded\n\n\nI\n (\"Interface\"): \nany class implementing this interface will be excluded\n\n\nC\n (\"Class\"): \nexactly this class will be excluded\n\n\n\n\n\n\n\n\n\n\nExample\n\n\nweka.experiment.ResultListener=I:weka.experiment.ResultProducer\n\n\n\n\n\n\n\n\nSee also\n\n\n\n\nweka/gui/GenericPropertiesCreator.props\n\n\nGenericObjectEditor\n\n\nProperties file",
            "title": " weka/gui/GenericPropertiesCreator.excludes"
        },
        {
            "location": "/weka_gui_generic_properties_creator.excludes/#file",
            "text": "weka/gui/GenericPropertiesCreator.excludes",
            "title": "File"
        },
        {
            "location": "/weka_gui_generic_properties_creator.excludes/#description",
            "text": "List classes for corresponding superclasses in  GenericPropertiesCreator.props  that shouldn't appear in the  GenericObjectEditor  popup tree.",
            "title": "Description"
        },
        {
            "location": "/weka_gui_generic_properties_creator.excludes/#version",
            "text": ">= 3.5.3",
            "title": "Version"
        },
        {
            "location": "/weka_gui_generic_properties_creator.excludes/#fields",
            "text": "Format  <key>=<prefix>:<class>[,<prefix>:<class>]    <key>   the key from  GenericPropertiesCreator.props  (class or interface)     <prefix>  S  (\"Superclass\"): \nany class derived from this one will be excluded  I  (\"Interface\"): \nany class implementing this interface will be excluded  C  (\"Class\"): \nexactly this class will be excluded      Example  weka.experiment.ResultListener=I:weka.experiment.ResultProducer",
            "title": "Fields"
        },
        {
            "location": "/weka_gui_generic_properties_creator.excludes/#see-also",
            "text": "weka/gui/GenericPropertiesCreator.props  GenericObjectEditor  Properties file",
            "title": "See also"
        },
        {
            "location": "/weka_gui_look_and_feel.props/",
            "text": "File\n\n\nweka/gui/LookAndFeel.props\n\n\nDescription\n\n\nDefines with what theme Weka is displayed. By default Weka starts with the system default one, but under Java 5.0 and Linux/Gnome this can lead to a \ncrash\n due to a NullPointerException.\n\n\nVersion\n\n\n\n\n>= 3.4.5\n\n\n>= 3.5.0\n\n\n\n\nFields\n\n\n\n\nTheme\n\n\na few possible values\n\n\n\n\n\n\njavax.swing.plaf.metal.MetalLookAndFeel\n\n\nworks on all platforms\n\n\nfixes problem with Java 5/6 and Linux/Gnome\n\n\n\n\n\n\ncom.sun.java.swing.plaf.windows.WindowsLookAndFeel\n\n\ntheme for Win32\n\n\n\n\n\n\n...\n\n\n\n\n\n\n\n\nSee also\n\n\n\n\nTroubleshooting (GUIChooser starts but not Experimenter or Explorer)\n\n\nTroubleshooting (KnowledgeFlow toolbars are empty)\n\n\nProperties file\n\n\n\n\nLinks\n\n\n\n\nOriginal Wekalist post",
            "title": " weka/gui/LookAndFeel.props"
        },
        {
            "location": "/weka_gui_look_and_feel.props/#file",
            "text": "weka/gui/LookAndFeel.props",
            "title": "File"
        },
        {
            "location": "/weka_gui_look_and_feel.props/#description",
            "text": "Defines with what theme Weka is displayed. By default Weka starts with the system default one, but under Java 5.0 and Linux/Gnome this can lead to a  crash  due to a NullPointerException.",
            "title": "Description"
        },
        {
            "location": "/weka_gui_look_and_feel.props/#version",
            "text": ">= 3.4.5  >= 3.5.0",
            "title": "Version"
        },
        {
            "location": "/weka_gui_look_and_feel.props/#fields",
            "text": "Theme  a few possible values    javax.swing.plaf.metal.MetalLookAndFeel  works on all platforms  fixes problem with Java 5/6 and Linux/Gnome    com.sun.java.swing.plaf.windows.WindowsLookAndFeel  theme for Win32    ...",
            "title": "Fields"
        },
        {
            "location": "/weka_gui_look_and_feel.props/#see-also",
            "text": "Troubleshooting (GUIChooser starts but not Experimenter or Explorer)  Troubleshooting (KnowledgeFlow toolbars are empty)  Properties file",
            "title": "See also"
        },
        {
            "location": "/weka_gui_look_and_feel.props/#links",
            "text": "Original Wekalist post",
            "title": "Links"
        },
        {
            "location": "/weka_gui_memory_usage.props/",
            "text": "File\n\n\nweka/gui/MemoryUsage.props\n\n\nDescription\n\n\nContains properties for the memory usage panel/frame, which can be launched from:\n\n\n\n\nweka.gui.Main\n\n\nFile -> Memory usage\n\n\n\n\n\n\nweka.gui.GUIChooser\n\n\nclick on the \nMemory usage\n button\n\n\n\n\n\n\n\n\nVersion\n\n\n\n\n\n\n\n\n3.5.7\n\n\n\n\n\n\n\n\nFields\n\n\n\n\nWidth\n\n\nthe width of the panel in pixel. The default is \n400\n.\n\n\n\n\n\n\nHeight\n\n\nthe height of the panel - normally not set, since the height of the garbage collector button is used\n\n\n\n\n\n\nLeft\n, \nTop\n\n\nspecify a fixed location on the screen in pixel. Both of these parameters have to be different from -1 to place the window. The default is \n-1\n for both, i.e., top-left corner of the screen.\n\n\n\n\n\n\nBackgroundColor\n\n\ndefines the background color of the panel, can either be the name of a \nstandard Java color\n or an RGB triplet (= \nR,G,B\n). Default is \nwhite\n.\n\n\n\n\n\n\nInterval\n\n\nthe refresh interval in milli-second. The default is \n1000\n.\n\n\n\n\n\n\nPercentages\n\n\ncomma-separated list of percentage number that indicate when to switch colors. E.g., \n70,80,90\n (which is the default) indicates to change the color of the bar from the default one (see \nDefaultColor\n) as soon as the percentage reaches this threshold. For example, if the percentage reaches 70 percent, then the color specified with the key \n70\n is used (default is \nyellow\n for \n70\n). If it reaches 80 or more then \norange\n is used (the default for \n80\n is \norange\n) and everything above and including \n90\n will be displayed \nred\n (\nred\n is the default for \n90\n).\n\n\n\n\n\n\nDefaultColor\n\n\nthe default color to display the percentage bar with, i.e., if the percentage is below the lowest percentage listed in \nPercentages\n.\n\n\n\n\n\n\n\n\nSee also\n\n\n\n\nProperties file",
            "title": " weka/gui/MemoryUsage.props"
        },
        {
            "location": "/weka_gui_memory_usage.props/#file",
            "text": "weka/gui/MemoryUsage.props",
            "title": "File"
        },
        {
            "location": "/weka_gui_memory_usage.props/#description",
            "text": "Contains properties for the memory usage panel/frame, which can be launched from:   weka.gui.Main  File -> Memory usage    weka.gui.GUIChooser  click on the  Memory usage  button",
            "title": "Description"
        },
        {
            "location": "/weka_gui_memory_usage.props/#version",
            "text": "3.5.7",
            "title": "Version"
        },
        {
            "location": "/weka_gui_memory_usage.props/#fields",
            "text": "Width  the width of the panel in pixel. The default is  400 .    Height  the height of the panel - normally not set, since the height of the garbage collector button is used    Left ,  Top  specify a fixed location on the screen in pixel. Both of these parameters have to be different from -1 to place the window. The default is  -1  for both, i.e., top-left corner of the screen.    BackgroundColor  defines the background color of the panel, can either be the name of a  standard Java color  or an RGB triplet (=  R,G,B ). Default is  white .    Interval  the refresh interval in milli-second. The default is  1000 .    Percentages  comma-separated list of percentage number that indicate when to switch colors. E.g.,  70,80,90  (which is the default) indicates to change the color of the bar from the default one (see  DefaultColor ) as soon as the percentage reaches this threshold. For example, if the percentage reaches 70 percent, then the color specified with the key  70  is used (default is  yellow  for  70 ). If it reaches 80 or more then  orange  is used (the default for  80  is  orange ) and everything above and including  90  will be displayed  red  ( red  is the default for  90 ).    DefaultColor  the default color to display the percentage bar with, i.e., if the percentage is below the lowest percentage listed in  Percentages .",
            "title": "Fields"
        },
        {
            "location": "/weka_gui_memory_usage.props/#see-also",
            "text": "Properties file",
            "title": "See also"
        },
        {
            "location": "/weka_gui_simple_cli.props/",
            "text": "File\n\n\nweka/gui/SimpleCLI.props\n\n\nDescription\n\n\nContains properties for the SimpleCLI, e.g., the command history. Whenever the user issues a command, the history gets saved in the user's home directory (\n$HOME/SimpleCLI.props\n on Linux or \n%USERPROFILE%/SimpleCLI.props\n on Windows).\n\n\nVersion\n\n\n\n\n\n\n\n\n3.5.6\n\n\n\n\n\n\n\n\nFields\n\n\n\n\nHistorySize\n\n\nthe maximum number of most recent commands to store in the properties file (in the user's home directory).\n\n\n\n\n\n\nCommand X\n\n\nlists command \nX\n of the history, with \nX\n being an integer starting from 0.\n\n\n\n\n\n\n\n\nSee also\n\n\n\n\nProperties file",
            "title": " weka/gui/SimpleCLI.props"
        },
        {
            "location": "/weka_gui_simple_cli.props/#file",
            "text": "weka/gui/SimpleCLI.props",
            "title": "File"
        },
        {
            "location": "/weka_gui_simple_cli.props/#description",
            "text": "Contains properties for the SimpleCLI, e.g., the command history. Whenever the user issues a command, the history gets saved in the user's home directory ( $HOME/SimpleCLI.props  on Linux or  %USERPROFILE%/SimpleCLI.props  on Windows).",
            "title": "Description"
        },
        {
            "location": "/weka_gui_simple_cli.props/#version",
            "text": "3.5.6",
            "title": "Version"
        },
        {
            "location": "/weka_gui_simple_cli.props/#fields",
            "text": "HistorySize  the maximum number of most recent commands to store in the properties file (in the user's home directory).    Command X  lists command  X  of the history, with  X  being an integer starting from 0.",
            "title": "Fields"
        },
        {
            "location": "/weka_gui_simple_cli.props/#see-also",
            "text": "Properties file",
            "title": "See also"
        },
        {
            "location": "/weka_gui_beans_beans.props/",
            "text": "File\n\n\nweka/gui/beans/Beans.props\n\n\nDescription\n\n\nProperties file that customizes the KnowledgeFlow.\n\n\nVersion\n\n\n\n\n>= 3.3.2\n\n\n\n\nFields\n\n\n\n\nweka.gui.beans.KnowledgeFlow.standardToolBars\n\n\nlist of standard toolbars (containing bean tools that do not wrap weka base class types)\n\n\n\n\n\n\n*.order\n\n\ntoolbar ordering information for wrapper types\n\n\n\n\n\n\n*.alias\n\n\ntoolbar naming aliases for weka algorithm classes\n\n\n\n\n\n\nGUI behavior (>= 3.5.1)\n\n\nScrollBarIncrementLayout\n\n\n\n\nthe increment for scrollbars using the mouse's scroll-wheel in the layout area\n\n\n\n\n\n\n\n\nScrollBarIncrementComponents\n\n\n\n\nthe increment for scrollbars using the mouse's scroll-wheel in the component area (i.e., the toolbars)\n\n\n\n\n\n\n\n\nFlowWidth\n\n\n\n\nthe size of the layout area\n\n\n\n\n\n\n\n\nFlowHeight\n\n\n\n\nthe size of the layout area\n\n\n\n\n\n\n\n\nPreferredExtension\n\n\n\n\nthe preferred file extension (and therefore format) for saving a flow layout\n\n\n\n\n\n\n\n\nUserComponentsInXML\n\n\n\n\nwhether the user components (\"meta\"-beans) are saved in XML (i.e., \n.kfml\n) or not\n\n\n\n\n\n\n\n\n\n\n\n\nColours (> 3.5.7)\n\n\nweka.gui.beans.StripChart.backgroundColour\n\n\n\n\nthe background color of the StripChart, default is \nblack\n (can use R,G,B format)\n\n\n\n\n\n\n\n\nweka.gui.beans.StripChart$LegendPanel.borderColour\n\n\n\n\nthe color of the text on the legend's border, default is \nblue\n (can use R,G,B format)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSee also\n\n\n\n\nProperties File",
            "title": " weka/gui/beans/Beans.props"
        },
        {
            "location": "/weka_gui_beans_beans.props/#file",
            "text": "weka/gui/beans/Beans.props",
            "title": "File"
        },
        {
            "location": "/weka_gui_beans_beans.props/#description",
            "text": "Properties file that customizes the KnowledgeFlow.",
            "title": "Description"
        },
        {
            "location": "/weka_gui_beans_beans.props/#version",
            "text": ">= 3.3.2",
            "title": "Version"
        },
        {
            "location": "/weka_gui_beans_beans.props/#fields",
            "text": "weka.gui.beans.KnowledgeFlow.standardToolBars  list of standard toolbars (containing bean tools that do not wrap weka base class types)    *.order  toolbar ordering information for wrapper types    *.alias  toolbar naming aliases for weka algorithm classes    GUI behavior (>= 3.5.1)  ScrollBarIncrementLayout   the increment for scrollbars using the mouse's scroll-wheel in the layout area     ScrollBarIncrementComponents   the increment for scrollbars using the mouse's scroll-wheel in the component area (i.e., the toolbars)     FlowWidth   the size of the layout area     FlowHeight   the size of the layout area     PreferredExtension   the preferred file extension (and therefore format) for saving a flow layout     UserComponentsInXML   whether the user components (\"meta\"-beans) are saved in XML (i.e.,  .kfml ) or not       Colours (> 3.5.7)  weka.gui.beans.StripChart.backgroundColour   the background color of the StripChart, default is  black  (can use R,G,B format)     weka.gui.beans.StripChart$LegendPanel.borderColour   the color of the text on the legend's border, default is  blue  (can use R,G,B format)",
            "title": "Fields"
        },
        {
            "location": "/weka_gui_beans_beans.props/#see-also",
            "text": "Properties File",
            "title": "See also"
        },
        {
            "location": "/weka_gui_experiment_experimenter.props/",
            "text": "File\n\n\nweka/gui/experiment/Experimenter.props\n\n\nDescription\n\n\nUsed for customizing the initial Experimenter settings.\n\n\nVersion\n\n\n\n\n>= 3.4.6\n\n\n>= 3.5.1\n\n\n\n\nFields\n\n\n\n\nExtension\n\n\nthe default extension in the file-dialog (and therefore format)\n\n\n\n\n\n\n.exp\n - uses Java \nSerialization\n\n\n[.xml](xml#serialization of experiments.md)\n\n\n[.koml](xml#serialization of experiments.md)\n\n\n\n\n\n\nDestination\n - \nsimple\n\n\nthe default destination\n\n\n\n\n\n\nARFF file\n\n\nCSV file\n\n\nJDBC database\n\n\n\n\n\n\nExperimentType\n - \nsimple\n\n\nthe experiment type\n\n\n\n\n\n\nCross-validation\n\n\nTrain/Test Percentage Split (data randomized)\n\n\nTrain/Test Percentage Split (order preserved)\n\n\n\n\n\n\nUseClassification\n - \nsimple\n\n\nwhether classification is the default (\ntrue\n) or regression (\nfalse\n)\n\n\n\n\n\n\nFolds\n - \nsimple\n\n\nthe default number of CV folds\n\n\n\n\n\n\nTrainPercentage\n - \nsimple\n\n\nthe default percentage for training (0 - 100)\n\n\n\n\n\n\nRepetitions\n - \nsimple\n\n\nthe default number of repetitions\n\n\n\n\n\n\nDatasetsFirst\n\n\nwhether datasets are first iterated (\ntrue\n) or the algorithms (\nfalse\n)\n\n\n\n\n\n\nInitialDatasetsDirectory\n\n\nthe initial datasets directory\nNote for Win32: \nthe path backslashes have to written as \"\\\"\n\n\n\n\n\n\nUseRelativePaths\n\n\nwhether to use relative paths (\ntrue\n) or absolute ones (\nfalse\n)\n\n\n\n\n\n\nTester\n\n\nthe default tester to use\n\n\n\n\n\n\nPaired T-Tester (corrected)\n\n\nPaired T-Tester\n\n\n\n\n\n\nRow\n\n\nthe row selection\n\n\n\n\n\n\nColumn\n\n\nthe column selection\n\n\n\n\n\n\nComparisonField\n\n\nthe default comparison field (lower case!), cf. combobox\n\n\n\n\n\n\nSignificance\n\n\nthe default significance (0.0 - 1.0)\n\n\n\n\n\n\nSorting\n\n\nthe default sorting, left empty means no sorting at all\n\n\n\n\n\n\nShowStdDev\n\n\nwhether stddevs are displayed by default\n\n\n\n\n\n\nShowAverage\n\n\nwhether the Average is displayed by default (prints an additional list in the results)\n\n\n\n\n\n\nMeanPrecision\n\n\nthe default precision for the mean\n\n\n\n\n\n\nStdDevPrecision\n\n\nthe default precision for the stdev\n\n\n\n\n\n\nOutputFormat\n\n\nthe classname of the ResultMatrix, responsible for the default output format (see \nweka.experiment\n package)\n\n\n\n\n\n\nRemoveFilterClassnames\n\n\nwhether filter classnames are removed by default\n\n\n\n\n\n\n\n\nNote:\n \nsimple\n means that this option is only available in the \nsimple\n version of the Experimenter, not the \nadvanced\n one\n\n\nSee also\n\n\n\n\nProperties File",
            "title": " weka/gui/experiment/Experimenter.props"
        },
        {
            "location": "/weka_gui_experiment_experimenter.props/#file",
            "text": "weka/gui/experiment/Experimenter.props",
            "title": "File"
        },
        {
            "location": "/weka_gui_experiment_experimenter.props/#description",
            "text": "Used for customizing the initial Experimenter settings.",
            "title": "Description"
        },
        {
            "location": "/weka_gui_experiment_experimenter.props/#version",
            "text": ">= 3.4.6  >= 3.5.1",
            "title": "Version"
        },
        {
            "location": "/weka_gui_experiment_experimenter.props/#fields",
            "text": "Extension  the default extension in the file-dialog (and therefore format)    .exp  - uses Java  Serialization  [.xml](xml#serialization of experiments.md)  [.koml](xml#serialization of experiments.md)    Destination  -  simple  the default destination    ARFF file  CSV file  JDBC database    ExperimentType  -  simple  the experiment type    Cross-validation  Train/Test Percentage Split (data randomized)  Train/Test Percentage Split (order preserved)    UseClassification  -  simple  whether classification is the default ( true ) or regression ( false )    Folds  -  simple  the default number of CV folds    TrainPercentage  -  simple  the default percentage for training (0 - 100)    Repetitions  -  simple  the default number of repetitions    DatasetsFirst  whether datasets are first iterated ( true ) or the algorithms ( false )    InitialDatasetsDirectory  the initial datasets directory\nNote for Win32: \nthe path backslashes have to written as \"\\\"    UseRelativePaths  whether to use relative paths ( true ) or absolute ones ( false )    Tester  the default tester to use    Paired T-Tester (corrected)  Paired T-Tester    Row  the row selection    Column  the column selection    ComparisonField  the default comparison field (lower case!), cf. combobox    Significance  the default significance (0.0 - 1.0)    Sorting  the default sorting, left empty means no sorting at all    ShowStdDev  whether stddevs are displayed by default    ShowAverage  whether the Average is displayed by default (prints an additional list in the results)    MeanPrecision  the default precision for the mean    StdDevPrecision  the default precision for the stdev    OutputFormat  the classname of the ResultMatrix, responsible for the default output format (see  weka.experiment  package)    RemoveFilterClassnames  whether filter classnames are removed by default     Note:   simple  means that this option is only available in the  simple  version of the Experimenter, not the  advanced  one",
            "title": "Fields"
        },
        {
            "location": "/weka_gui_experiment_experimenter.props/#see-also",
            "text": "Properties File",
            "title": "See also"
        },
        {
            "location": "/weka_gui_explorer_explorer.props/",
            "text": "File\n\n\nweka/gui/explorer/Explorer.props\n\n\nDescription\n\n\nThis props file determines what schemes and options are initially set in the Explorer.\n\n\nVersion\n\n\n\n\n\n\n\n\n3.5.3\n\n\n\n\n\n\n\n\nFields\n\n\nPreprocess panel\n\n\n\n\nInitGenericObjectEditorFilter\n\n\n\n\nif set to true the Capabilities filters in the GOE will be initialized based on the full dataset that has been loaded into the Explorer otherwise only the header\n\n\n\n\n\n\n\n\nTabs\n\n\n\n\nLists all the tabs that should be displayed in the Explorer. Apart from the Preprocess panel itself, all other panels are basically plugins.\nSee the \nAdding tabs in the Explorer\n article for more details on adding custom panels.\n\n\n\n\n\n\n\n\n\n\nInitialDirectory (> 3.6.0, developer version and \nsnapshots\n later than 07/02/2009)\n\n\n\n\n\n\nDefines the initial directory for opening datasets in the Preprocess panel.\nThe following placeholders are recognized (work across platforms):\n\n\n\n\n\n\n\n\n%t - the temp directory\n\n\n%h - the user's home directory\n\n\n%c - the current directory \n(the default setting)\n\n\n%% - gets replaced by a single percentage sign\n\n\n\n\n\n\n\n\nenableUndo (> 3.6.5, > 3.7.4 and \nsnapshots\n later than 09/01/2011)\n\n\n\n\n\n\nEnable/disable the creation of undo files (default is enabled)\n\n\n\n\n\n\n\n\n\n\nundoDirectory (> 3.6.5, > 3.7.4 and \nsnapshots\n later than 09/01/2011)\n\n\n\n\n\n\nSpecify the directory to use for saving undo files\nThe following placeholders are recognized (work across platforms):\n\n\n\n\n\n\n\n\n%t - the temp directory\n\n\n\n\n\n\n\n\nFilter\n\n\n\n\n\n\nthe filter to use, none if left empty\n\n\n\n\n\n\n\n\n\n\nClassify panel\n\n\n\n\nClassifier\n\n\n\n\nthe classifier to use\n\n\n\n\n\n\n\n\nClassifierTestMode\n\n\n\n\nthe default test mode in the classify tab\n\n\n\n\n\n\n\n\n1 - cross-validation (default)\n\n\n2 - percentage split\n\n\n3 - use training set\n\n\n4 - supplied test set\n\n\n\n\n\n\nClassifierCrossvalidationFolds\n\n\n\n\nthe default number of folds for CV\n\n\n\n\n\n\n\n\nClassifierCostSensitiveEval\n\n\n\n\nwhether the evaluation of the classifier is done cost-sensitively\na cost matrix still has to be provided!\n\n\n\n\n\n\n\n\nClassifierOutputConfusionMatrix\n\n\n\n\nwhether the confusion matrix is output\n\n\n\n\n\n\n\n\nClassifierOutputEntropyEvalMeasures\n\n\n\n\nwhether the entropy based evaluation measures of the classifier model are output\n\n\n\n\n\n\n\n\nClassifierOutputModel\n\n\n\n\nwhether the classifier model is output\n\n\n\n\n\n\n\n\nClassifierOutputPerClassStats\n\n\n\n\nwhether additional per-class stats of the classifier model are output\n\n\n\n\n\n\n\n\nClassifierOutputPredictions\n\n\n\n\nwhether the predictions of the classifier output as well\n\n\n\n\n\n\n\n\nClassifierPercentageSplit\n\n\n\n\nthe default percentage split in %\n\n\n\n\n\n\n\n\nClassifierPreserveOrder\n\n\n\n\nwhether the order is preserved in case of percentage split\n\n\n\n\n\n\n\n\nClassifierRandomSeed\n\n\n\n\nthe default random seed\n\n\n\n\n\n\n\n\nClassifierStorePredictionsForVis\n\n\n\n\nwhether the predictions of the classifier are stored for visulization purposes\n\n\n\n\n\n\n\n\nClassifierOutputSourceCode (> 3.5.5)\n\n\n\n\nwhether to output Java source code for classifiers that implement the weka.classifiers.Sourcable interface\n\n\n\n\n\n\n\n\nClassifierSourceCodeClass (> 3.5.5)\n\n\n\n\nthe default classname of the generated Java source code\n\n\n\n\n\n\n\n\nClassifierErrorsPlotInstances (> 3.7.0)\n\n\n\n\nthe default classname for the class generating the plot instances of the classifier errors\n\n\n\n\n\n\n\n\nClassifierErrorsMinimumPlotSizeNumeric (> 3.7.0)\n\n\n\n\nthe minimum size for the crosses that display the classifier errors for numeric class attributes\n\n\n\n\n\n\n\n\nClassifierErrorsMaximumPlotSizeNumeric (> 3.7.0)\n\n\n\n\nthe maximum size for the crosses that display the classifier errors for numeric class attributes\n\n\n\n\n\n\n\n\n\n\nCluster panel\n\n\n\n\nClusterer\n\n\n\n\nthe clusterer to use\n\n\n\n\n\n\n\n\nClustererTestMode\n\n\n\n\nthe default test mode\n\n\n\n\n\n\n\n\n2 - percentage split\n\n\n3 - use training set (default)\n\n\n4 - supplied test set\n\n\n5 - classes to clusters evaluation\n\n\n\n\n\n\nClustererStoreClustersForVis\n\n\n\n\nwhether the clusters are stored for visualization purposes\n\n\n\n\n\n\n\n\nAssociations panel\n\n\nAssociator\n\n\n\n\nthe default associator\n\n\n\n\n\n\n\n\nAttribute selection panel\n\n\nASEvaluation\n\n\n\n\nthe default attribute evaluator\n\n\n\n\n\n\n\n\nASSearch\n\n\n\n\nthe default attribute selection search scheme\n\n\n\n\n\n\n\n\nASTestMode\n\n\n\n\nthe default test mode\n\n\n\n\n\n\n\n\n0 - use full training set (default)\n\n\n1 - cross-validation\n\n\n\n\n\n\nASCrossvalidationFolds\n\n\n\n\nthe default number of folds for CV\n\n\n\n\n\n\n\n\nASRandomSeed\n\n\n\n\nthe default random seed\n\n\n\n\n\n\n\n\n\n\nSee also\n\n\n\n\nProperties File",
            "title": " weka/gui/explorer/Explorer.props"
        },
        {
            "location": "/weka_gui_explorer_explorer.props/#file",
            "text": "weka/gui/explorer/Explorer.props",
            "title": "File"
        },
        {
            "location": "/weka_gui_explorer_explorer.props/#description",
            "text": "This props file determines what schemes and options are initially set in the Explorer.",
            "title": "Description"
        },
        {
            "location": "/weka_gui_explorer_explorer.props/#version",
            "text": "3.5.3",
            "title": "Version"
        },
        {
            "location": "/weka_gui_explorer_explorer.props/#fields",
            "text": "",
            "title": "Fields"
        },
        {
            "location": "/weka_gui_explorer_explorer.props/#preprocess-panel",
            "text": "InitGenericObjectEditorFilter   if set to true the Capabilities filters in the GOE will be initialized based on the full dataset that has been loaded into the Explorer otherwise only the header     Tabs   Lists all the tabs that should be displayed in the Explorer. Apart from the Preprocess panel itself, all other panels are basically plugins.\nSee the  Adding tabs in the Explorer  article for more details on adding custom panels.      InitialDirectory (> 3.6.0, developer version and  snapshots  later than 07/02/2009)    Defines the initial directory for opening datasets in the Preprocess panel.\nThe following placeholders are recognized (work across platforms):     %t - the temp directory  %h - the user's home directory  %c - the current directory  (the default setting)  %% - gets replaced by a single percentage sign     enableUndo (> 3.6.5, > 3.7.4 and  snapshots  later than 09/01/2011)    Enable/disable the creation of undo files (default is enabled)      undoDirectory (> 3.6.5, > 3.7.4 and  snapshots  later than 09/01/2011)    Specify the directory to use for saving undo files\nThe following placeholders are recognized (work across platforms):     %t - the temp directory     Filter    the filter to use, none if left empty",
            "title": "Preprocess panel"
        },
        {
            "location": "/weka_gui_explorer_explorer.props/#classify-panel",
            "text": "Classifier   the classifier to use     ClassifierTestMode   the default test mode in the classify tab     1 - cross-validation (default)  2 - percentage split  3 - use training set  4 - supplied test set    ClassifierCrossvalidationFolds   the default number of folds for CV     ClassifierCostSensitiveEval   whether the evaluation of the classifier is done cost-sensitively\na cost matrix still has to be provided!     ClassifierOutputConfusionMatrix   whether the confusion matrix is output     ClassifierOutputEntropyEvalMeasures   whether the entropy based evaluation measures of the classifier model are output     ClassifierOutputModel   whether the classifier model is output     ClassifierOutputPerClassStats   whether additional per-class stats of the classifier model are output     ClassifierOutputPredictions   whether the predictions of the classifier output as well     ClassifierPercentageSplit   the default percentage split in %     ClassifierPreserveOrder   whether the order is preserved in case of percentage split     ClassifierRandomSeed   the default random seed     ClassifierStorePredictionsForVis   whether the predictions of the classifier are stored for visulization purposes     ClassifierOutputSourceCode (> 3.5.5)   whether to output Java source code for classifiers that implement the weka.classifiers.Sourcable interface     ClassifierSourceCodeClass (> 3.5.5)   the default classname of the generated Java source code     ClassifierErrorsPlotInstances (> 3.7.0)   the default classname for the class generating the plot instances of the classifier errors     ClassifierErrorsMinimumPlotSizeNumeric (> 3.7.0)   the minimum size for the crosses that display the classifier errors for numeric class attributes     ClassifierErrorsMaximumPlotSizeNumeric (> 3.7.0)   the maximum size for the crosses that display the classifier errors for numeric class attributes",
            "title": "Classify panel"
        },
        {
            "location": "/weka_gui_explorer_explorer.props/#cluster-panel",
            "text": "Clusterer   the clusterer to use     ClustererTestMode   the default test mode     2 - percentage split  3 - use training set (default)  4 - supplied test set  5 - classes to clusters evaluation    ClustererStoreClustersForVis   whether the clusters are stored for visualization purposes     Associations panel  Associator   the default associator     Attribute selection panel  ASEvaluation   the default attribute evaluator     ASSearch   the default attribute selection search scheme     ASTestMode   the default test mode     0 - use full training set (default)  1 - cross-validation    ASCrossvalidationFolds   the default number of folds for CV     ASRandomSeed   the default random seed",
            "title": "Cluster panel"
        },
        {
            "location": "/weka_gui_explorer_explorer.props/#see-also",
            "text": "Properties File",
            "title": "See also"
        },
        {
            "location": "/weka_gui_scripting_groovy.props/",
            "text": "File\n\n\nweka/gui/scripting/Groovy.props\n\n\nDescription\n\n\nThis props file determines the look and feel of the minimalistic scripting IDE for \nGroovy\n.\n\n\nVersion\n\n\n\n\n\n\n\n\n3.5.8 or \nsnapshot\n later than 6/3/2009 (only developer version)\n\n\n\n\n\n\n\n\nFields\n\n\n\n\nFontName\n\n\nSpecifies the name of the font for displaying the code.\ndefault: \nmonospaced\n\n\n\n\n\n\nFontSize\n\n\nThe font size.\ndefault: \n12\n\n\n\n\n\n\nForegroundColor\n\n\nThe color of the font (if not comment or keyword). Can take R,G,B format.\ndefault: \nblack\n\n\n\n\n\n\nBackgroundColor\n\n\nThe background color. Can take R,G,B format.\ndefault: \nwhite\n\n\n\n\n\n\nKeywordColor\n\n\nThe color for keywords (see list in field \nKeywords\n). Can take R,G,B format.\ndefault: \nblue\n\n\n\n\n\n\nCommentColor\n\n\nThe color for comments (single-line and multi-line). Can take R,G,B format.\ndefault: \ngray\n\n\n\n\n\n\nStringColor\n\n\nThe color for strings (enclosed in single or double quotes). Can take R,G,B format.\ndefault: \nred\n\n\n\n\n\n\nSyntax\n\n\nWhether syntax highlighting is turned on or not (true|false)\ndefault: \ntrue\n\n\n\n\n\n\nIndentation\n\n\nThe number of spaces to use for indentation.\ndefault: \n2\n\n\n\n\n\n\nTabs\n\n\nThe number of spaces a tab represents.\ndefault: \n2\n\n\n\n\n\n\nUseBlanks\n\n\nWhether to use blanks instead of tabs (true|false).\ndefault: \ntrue\n\n\n\n\n\n\nDelimiters\n\n\nThe characters that define word limits.\ndefault: \n;:{}()[]+-/%<=>!&|^~*\n\n\n\n\n\n\nQuoteDelimiters\n\n\nThe characters that enclose a string.\ndefault: \n\"'\n\n\n\n\n\n\nQuoteEscape\n\n\nThe character to escape the \nQuoteDelimiters\n with.\ndefault: \n\\ (backslash)\n\n\n\n\n\n\nMultiLineComment\n\n\nWhether to enable multi-line comments (true|false).\ndefault: \ntrue\n\n\n\n\n\n\nMultiLineCommentStart\n\n\nThe character sequence that starts a multi-line comment.\ndefault: \n/*\n\n\n\n\n\n\nMultiLineCommentEnd\n\n\nThe character sequence that ends a multi-line comment.\ndefault: \n*/\n\n\n\n\n\n\nSingleLineCommentStart\n\n\nThe character sequence that starts a single-line comment.\ndefault: \n*\n\n\n\n\n\n\nAddMatchingBlockEnd\n\n\nWhether to automatically add matching block end character sequences while typing (true|false).\ndefault: \ntrue\n\n\n\n\n\n\nBlockStart\n\n\nThe character sequence that starts a block.\ndefault: \n{\n\n\n\n\n\n\nBlockEndd\n\n\nThe character sequence that ends a block.\ndefault: \n}\n\n\n\n\n\n\nKeywords\n\n\nComma-separated list of keywords to highlight.\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nRecognized \ncolor names\n\n\nblack\n\n\nblue\n\n\ncyan\n\n\ndarkGray\n\n\ngray\n\n\ngreen\n\n\nlightGray\n\n\nmagenta\n\n\norange\n\n\npink\n\n\nred\n\n\nwhite\n\n\nyellow\n\n\n\n\n\n\nR,G,B format\n\n\nThe RGB format is a comma-separated list three integer values (values ranging from 0-255) for RED, GREEN and BLUE.\n\n\n\n\n\n\n\n\nSee also\n\n\n\n\nProperties File\n\n\nUsing Weka from Groovy\n\n\n\n\nLinks\n\n\n\n\nGroovy homepage",
            "title": " weka/gui/scripting/Groovy.props"
        },
        {
            "location": "/weka_gui_scripting_groovy.props/#file",
            "text": "weka/gui/scripting/Groovy.props",
            "title": "File"
        },
        {
            "location": "/weka_gui_scripting_groovy.props/#description",
            "text": "This props file determines the look and feel of the minimalistic scripting IDE for  Groovy .",
            "title": "Description"
        },
        {
            "location": "/weka_gui_scripting_groovy.props/#version",
            "text": "3.5.8 or  snapshot  later than 6/3/2009 (only developer version)",
            "title": "Version"
        },
        {
            "location": "/weka_gui_scripting_groovy.props/#fields",
            "text": "FontName  Specifies the name of the font for displaying the code.\ndefault: \nmonospaced    FontSize  The font size.\ndefault: \n12    ForegroundColor  The color of the font (if not comment or keyword). Can take R,G,B format.\ndefault: \nblack    BackgroundColor  The background color. Can take R,G,B format.\ndefault: \nwhite    KeywordColor  The color for keywords (see list in field  Keywords ). Can take R,G,B format.\ndefault: \nblue    CommentColor  The color for comments (single-line and multi-line). Can take R,G,B format.\ndefault: \ngray    StringColor  The color for strings (enclosed in single or double quotes). Can take R,G,B format.\ndefault: \nred    Syntax  Whether syntax highlighting is turned on or not (true|false)\ndefault: \ntrue    Indentation  The number of spaces to use for indentation.\ndefault: \n2    Tabs  The number of spaces a tab represents.\ndefault: \n2    UseBlanks  Whether to use blanks instead of tabs (true|false).\ndefault: \ntrue    Delimiters  The characters that define word limits.\ndefault: \n;:{}()[]+-/%<=>!&|^~*    QuoteDelimiters  The characters that enclose a string.\ndefault: \n\"'    QuoteEscape  The character to escape the  QuoteDelimiters  with.\ndefault: \n\\ (backslash)    MultiLineComment  Whether to enable multi-line comments (true|false).\ndefault: \ntrue    MultiLineCommentStart  The character sequence that starts a multi-line comment.\ndefault: \n/*    MultiLineCommentEnd  The character sequence that ends a multi-line comment.\ndefault: \n*/    SingleLineCommentStart  The character sequence that starts a single-line comment.\ndefault: \n*    AddMatchingBlockEnd  Whether to automatically add matching block end character sequences while typing (true|false).\ndefault: \ntrue    BlockStart  The character sequence that starts a block.\ndefault: \n{    BlockEndd  The character sequence that ends a block.\ndefault: \n}    Keywords  Comma-separated list of keywords to highlight.",
            "title": "Fields"
        },
        {
            "location": "/weka_gui_scripting_groovy.props/#notes",
            "text": "Recognized  color names  black  blue  cyan  darkGray  gray  green  lightGray  magenta  orange  pink  red  white  yellow    R,G,B format  The RGB format is a comma-separated list three integer values (values ranging from 0-255) for RED, GREEN and BLUE.",
            "title": "Notes"
        },
        {
            "location": "/weka_gui_scripting_groovy.props/#see-also",
            "text": "Properties File  Using Weka from Groovy",
            "title": "See also"
        },
        {
            "location": "/weka_gui_scripting_groovy.props/#links",
            "text": "Groovy homepage",
            "title": "Links"
        },
        {
            "location": "/weka_gui_scripting_jython.props/",
            "text": "File\n\n\nweka/gui/scripting/Jython.props\n\n\nDescription\n\n\nThis props file determines the look and feel of the minimalistic scripting IDE for \nJython\n.\n\n\nVersion\n\n\n\n\n\n\n\n\n3.5.8 or \nsnapshot\n later than 6/3/2009 (only developer version)\n\n\n\n\n\n\n\n\nFields\n\n\n\n\nFontName\n\n\nSpecifies the name of the font for displaying the code.\ndefault: \nmonospaced\n\n\n\n\n\n\nFontSize\n\n\nThe font size.\ndefault: \n12\n\n\n\n\n\n\nForegroundColor\n\n\nThe color of the font (if not comment or keyword). Can take R,G,B format.\ndefault: \nblack\n\n\n\n\n\n\nBackgroundColor\n\n\nThe background color. Can take R,G,B format.\ndefault: \nwhite\n\n\n\n\n\n\nKeywordColor\n\n\nThe color for keywords (see list in field \nKeywords\n). Can take R,G,B format.\ndefault: \nblue\n\n\n\n\n\n\nCommentColor\n\n\nThe color for comments (single-line and multi-line). Can take R,G,B format.\ndefault: \ngray\n\n\n\n\n\n\nStringColor\n\n\nThe color for strings (enclosed in single or double quotes). Can take R,G,B format.\ndefault: \nred\n\n\n\n\n\n\nSyntax\n\n\nWhether syntax highlighting is turned on or not (true|false)\ndefault: \ntrue\n\n\n\n\n\n\nIndentation\n\n\nThe number of spaces to use for indentation.\ndefault: \n4\n\n\n\n\n\n\nTabs\n\n\nThe number of spaces a tab represents.\ndefault: \n4\n\n\n\n\n\n\nUseBlanks\n\n\nWhether to use blanks instead of tabs (true|false).\ndefault: \ntrue\n\n\n\n\n\n\nDelimiters\n\n\nThe characters that define word limits.\ndefault: \n;:{}()[]+-/%<=>!&|^~*\n\n\n\n\n\n\nQuoteDelimiters\n\n\nThe characters that enclose a string.\ndefault: \n\"'\n\n\n\n\n\n\nQuoteEscape\n\n\nThe character to escape the \nQuoteDelimiters\n with.\ndefault: \n\\ (backslash)\n\n\n\n\n\n\nMultiLineComment\n\n\nWhether to enable multi-line comments (true|false).\ndefault: \ntrue\n\n\n\n\n\n\nMultiLineCommentStart\n\n\nThe character sequence that starts a multi-line comment.\ndefault: \n\"\"\"\n\n\n\n\n\n\nMultiLineCommentEnd\n\n\nThe character sequence that ends a multi-line comment.\ndefault: \n\"\"\"\n\n\n\n\n\n\nSingleLineCommentStart\n\n\nThe character sequence that starts a single-line comment.\ndefault: \n\n\n\n\n\n\n\n\n\n\n\n\nAddMatchingBlockEnd\n\n\nWhether to automatically add matching block end character sequences while typing (true|false).\ndefault: \nfalse\n\n\n\n\n\n\nBlockStart\n\n\nThe character sequence that starts a block.\ndefault: \n\n-none-\n\n\n\n\n\n\nBlockEndd\n\n\nThe character sequence that ends a block.\ndefault: \n\n-none-\n\n\n\n\n\n\nKeywords\n\n\nComma-separated list of keywords to highlight.\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nRecognized \ncolor names\n\n\nblack\n\n\nblue\n\n\ncyan\n\n\ndarkGray\n\n\ngray\n\n\ngreen\n\n\nlightGray\n\n\nmagenta\n\n\norange\n\n\npink\n\n\nred\n\n\nwhite\n\n\nyellow\n\n\n\n\n\n\nR,G,B format\n\n\nThe RGB format is a comma-separated list three integer values (values ranging from 0-255) for RED, GREEN and BLUE.\n\n\n\n\n\n\n\n\nSee also\n\n\n\n\nProperties File\n\n\nUsing Weka from Jython\n\n\n\n\nLinks\n\n\n\n\nJython homepage",
            "title": " weka/gui/scripting/Jython.props"
        },
        {
            "location": "/weka_gui_scripting_jython.props/#file",
            "text": "weka/gui/scripting/Jython.props",
            "title": "File"
        },
        {
            "location": "/weka_gui_scripting_jython.props/#description",
            "text": "This props file determines the look and feel of the minimalistic scripting IDE for  Jython .",
            "title": "Description"
        },
        {
            "location": "/weka_gui_scripting_jython.props/#version",
            "text": "3.5.8 or  snapshot  later than 6/3/2009 (only developer version)",
            "title": "Version"
        },
        {
            "location": "/weka_gui_scripting_jython.props/#fields",
            "text": "FontName  Specifies the name of the font for displaying the code.\ndefault: \nmonospaced    FontSize  The font size.\ndefault: \n12    ForegroundColor  The color of the font (if not comment or keyword). Can take R,G,B format.\ndefault: \nblack    BackgroundColor  The background color. Can take R,G,B format.\ndefault: \nwhite    KeywordColor  The color for keywords (see list in field  Keywords ). Can take R,G,B format.\ndefault: \nblue    CommentColor  The color for comments (single-line and multi-line). Can take R,G,B format.\ndefault: \ngray    StringColor  The color for strings (enclosed in single or double quotes). Can take R,G,B format.\ndefault: \nred    Syntax  Whether syntax highlighting is turned on or not (true|false)\ndefault: \ntrue    Indentation  The number of spaces to use for indentation.\ndefault: \n4    Tabs  The number of spaces a tab represents.\ndefault: \n4    UseBlanks  Whether to use blanks instead of tabs (true|false).\ndefault: \ntrue    Delimiters  The characters that define word limits.\ndefault: \n;:{}()[]+-/%<=>!&|^~*    QuoteDelimiters  The characters that enclose a string.\ndefault: \n\"'    QuoteEscape  The character to escape the  QuoteDelimiters  with.\ndefault: \n\\ (backslash)    MultiLineComment  Whether to enable multi-line comments (true|false).\ndefault: \ntrue    MultiLineCommentStart  The character sequence that starts a multi-line comment.\ndefault: \n\"\"\"    MultiLineCommentEnd  The character sequence that ends a multi-line comment.\ndefault: \n\"\"\"    SingleLineCommentStart  The character sequence that starts a single-line comment.\ndefault:",
            "title": "Fields"
        },
        {
            "location": "/weka_gui_scripting_jython.props/#notes",
            "text": "Recognized  color names  black  blue  cyan  darkGray  gray  green  lightGray  magenta  orange  pink  red  white  yellow    R,G,B format  The RGB format is a comma-separated list three integer values (values ranging from 0-255) for RED, GREEN and BLUE.",
            "title": "Notes"
        },
        {
            "location": "/weka_gui_scripting_jython.props/#see-also",
            "text": "Properties File  Using Weka from Jython",
            "title": "See also"
        },
        {
            "location": "/weka_gui_scripting_jython.props/#links",
            "text": "Jython homepage",
            "title": "Links"
        },
        {
            "location": "/weka_gui_tree_visualizer_tree_visualizes.props/",
            "text": "File\n\n\nweka/gui/treevisualizer/TreeVisualizer.props\n\n\nDescription\n\n\nCustomizes the TreeVisualizer display. The TreeVisualizer is used in the Explorer to display trees, e.g., generated by J48.\n\n\nVersion\n\n\n\n\n\n\n\n\n3.6.0 (stable-3.6 version)\n\n\n\n\n\n\n\n\n\n\n3.5.8 (developer version)\n\n\n\n\n\n\n\n\nFields\n\n\n\n\nFontColor\n (can use R,G,B format)\n\n\nThe color of the text being displayed, node and edge labels.\n\n\n\n\n\n\nBackgroundColor\n (can use R,G,B format)\n\n\nThe background color, by default empty in order to use the platforms default background color.\n\nNote:\n on Mac OS X, this \nseems to result\n in BLACK when saving the tree to an image file. Mac OS X users should fill in a color.\n\n\n\n\n\n\nNodeColor\n (can use R,G,B format)\n\n\nThe color in which the node boxes are painted.\n\n\n\n\n\n\nLineColor\n (can use R,G,B format)\n\n\nThe color of the edges.\n\n\n\n\n\n\nZoomBoxColor\n (can use R,G,B format)\n\n\nThe color of the zoom box.\n\n\n\n\n\n\nZoomBoxXORColor\n (can use R,G,B format)\n\n\nThe XOR color of the zoom box.\n\n\n\n\n\n\nShowBorder\n\n\nIndicates whether to show the border around the graph (labeled \"Tree View\") or not.\n\n\n\n\n\n\n\n\nSee also\n\n\n\n\nProperties File",
            "title": " weka/gui/treevisualizer/TreeVisualizer.props"
        },
        {
            "location": "/weka_gui_tree_visualizer_tree_visualizes.props/#file",
            "text": "weka/gui/treevisualizer/TreeVisualizer.props",
            "title": "File"
        },
        {
            "location": "/weka_gui_tree_visualizer_tree_visualizes.props/#description",
            "text": "Customizes the TreeVisualizer display. The TreeVisualizer is used in the Explorer to display trees, e.g., generated by J48.",
            "title": "Description"
        },
        {
            "location": "/weka_gui_tree_visualizer_tree_visualizes.props/#version",
            "text": "3.6.0 (stable-3.6 version)      3.5.8 (developer version)",
            "title": "Version"
        },
        {
            "location": "/weka_gui_tree_visualizer_tree_visualizes.props/#fields",
            "text": "FontColor  (can use R,G,B format)  The color of the text being displayed, node and edge labels.    BackgroundColor  (can use R,G,B format)  The background color, by default empty in order to use the platforms default background color. Note:  on Mac OS X, this  seems to result  in BLACK when saving the tree to an image file. Mac OS X users should fill in a color.    NodeColor  (can use R,G,B format)  The color in which the node boxes are painted.    LineColor  (can use R,G,B format)  The color of the edges.    ZoomBoxColor  (can use R,G,B format)  The color of the zoom box.    ZoomBoxXORColor  (can use R,G,B format)  The XOR color of the zoom box.    ShowBorder  Indicates whether to show the border around the graph (labeled \"Tree View\") or not.",
            "title": "Fields"
        },
        {
            "location": "/weka_gui_tree_visualizer_tree_visualizes.props/#see-also",
            "text": "Properties File",
            "title": "See also"
        },
        {
            "location": "/weka_gui_visualize_visualize.props/",
            "text": "File\n\n\nweka/gui/visualize/Visualize.props\n\n\nDescription\n\n\nCustomizes display of plots and certain curves in the GUI.\n\n\nVersion\n\n\n\n\n>= 3.1.9\n\n\n\n\nFields\n\n\n\n\nweka.gui.visualize.precision\n\n\nMaximum precision for numeric values\n\n\n\n\n\n\nweka.gui.visualize.Plot2D.axisColour\n\n\nColour for the axis in the 2D plot (can use R,G,B format)\n\n\n\n\n\n\nweka.gui.visualize.Plot2D.backgroundColour\n\n\nColour for the background of the 2D plot (can use R,G,B format)\n\n\n\n\n\n\nweka.gui.visualize.VisualizePanel.displayAttributeBars\n\n\nDisplay the list of one dimensional attribute visualizations\n\n\n\n\n\n\nweka.gui.visualize.AttributePanel.barColour\n\n\nColour for the background of the attribute bars in the AttributePanel (can use R,G,B format)\n\n\n\n\n\n\nweka.gui.visualize.Plot2D.instanceInfoFrame\n (developer version later than 3.5.8 or \nsnapshot\n, not in stable-3.6)\n\n\nLists the classname for displaying the instance info, e.g., when visualizing the classifier errors in the Explorer. Custom classes only need to be derived from \njavax.swing.JFrame\n and implement the \nweka.gui.visualize.InstanceInfo\n interface.\n\n\n\n\n\n\nThreshold curve plots\n\n\nweka.gui.visualize.ThresholdVisualizePanel.ThresholdCurve.XDimension\n\n\nweka.gui.visualize.ThresholdVisualizePanel.ThresholdCurve.YDimension\n\n\nweka.gui.visualize.ThresholdVisualizePanel.ThresholdCurve.ColourDimension\n\n\n\n\n\n\nCost curve plots\n\n\nweka.gui.visualize.VisualizePanel.CostCurve.XDimension\n\n\nweka.gui.visualize.VisualizePanel.CostCurve.YDimension\n\n\nweka.gui.visualize.VisualizePanel.CostCurve.ColourDimension\n\n\n\n\n\n\nMargin curve plots\n\n\nweka.gui.visualize.VisualizePanel.MarginCurve.XDimension\n\n\nweka.gui.visualize.VisualizePanel.MarginCurve.YDimension\n\n\nweka.gui.visualize.VisualizePanel.MarginCurve.ColourDimension\n\n\n\n\n\n\n\n\nNotes\n\n\n\n\nRecognized \ncolor names\n\n\nblack\n\n\nblue\n\n\ncyan\n\n\ndarkGray\n\n\ngray\n\n\ngreen\n\n\nlightGray\n\n\nmagenta\n\n\norange\n\n\npink\n\n\nred\n\n\nwhite\n\n\nyellow\n\n\n\n\n\n\nR,G,B format\n\n\nThe RGB format is a comma-separated list three integer values (values ranging from 0-255) for RED, GREEN and BLUE.\n\n\n\n\n\n\n\n\nSee also\n\n\n\n\nProperties File",
            "title": " weka/gui/visualize/Visualize.props"
        },
        {
            "location": "/weka_gui_visualize_visualize.props/#file",
            "text": "weka/gui/visualize/Visualize.props",
            "title": "File"
        },
        {
            "location": "/weka_gui_visualize_visualize.props/#description",
            "text": "Customizes display of plots and certain curves in the GUI.",
            "title": "Description"
        },
        {
            "location": "/weka_gui_visualize_visualize.props/#version",
            "text": ">= 3.1.9",
            "title": "Version"
        },
        {
            "location": "/weka_gui_visualize_visualize.props/#fields",
            "text": "weka.gui.visualize.precision  Maximum precision for numeric values    weka.gui.visualize.Plot2D.axisColour  Colour for the axis in the 2D plot (can use R,G,B format)    weka.gui.visualize.Plot2D.backgroundColour  Colour for the background of the 2D plot (can use R,G,B format)    weka.gui.visualize.VisualizePanel.displayAttributeBars  Display the list of one dimensional attribute visualizations    weka.gui.visualize.AttributePanel.barColour  Colour for the background of the attribute bars in the AttributePanel (can use R,G,B format)    weka.gui.visualize.Plot2D.instanceInfoFrame  (developer version later than 3.5.8 or  snapshot , not in stable-3.6)  Lists the classname for displaying the instance info, e.g., when visualizing the classifier errors in the Explorer. Custom classes only need to be derived from  javax.swing.JFrame  and implement the  weka.gui.visualize.InstanceInfo  interface.    Threshold curve plots  weka.gui.visualize.ThresholdVisualizePanel.ThresholdCurve.XDimension  weka.gui.visualize.ThresholdVisualizePanel.ThresholdCurve.YDimension  weka.gui.visualize.ThresholdVisualizePanel.ThresholdCurve.ColourDimension    Cost curve plots  weka.gui.visualize.VisualizePanel.CostCurve.XDimension  weka.gui.visualize.VisualizePanel.CostCurve.YDimension  weka.gui.visualize.VisualizePanel.CostCurve.ColourDimension    Margin curve plots  weka.gui.visualize.VisualizePanel.MarginCurve.XDimension  weka.gui.visualize.VisualizePanel.MarginCurve.YDimension  weka.gui.visualize.VisualizePanel.MarginCurve.ColourDimension",
            "title": "Fields"
        },
        {
            "location": "/weka_gui_visualize_visualize.props/#notes",
            "text": "Recognized  color names  black  blue  cyan  darkGray  gray  green  lightGray  magenta  orange  pink  red  white  yellow    R,G,B format  The RGB format is a comma-separated list three integer values (values ranging from 0-255) for RED, GREEN and BLUE.",
            "title": "Notes"
        },
        {
            "location": "/weka_gui_visualize_visualize.props/#see-also",
            "text": "Properties File",
            "title": "See also"
        }
    ]
}