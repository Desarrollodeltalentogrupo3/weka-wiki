{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Please note, the migration from the old wiki in this new format is still work in progress! New to Weka? Have a look at the Frequently Asked Questions (= FAQ), the Troubleshooting article or search the mailing list archives . Don't forget to check out the documentation on the Weka homepage and the Learning Resources . You have questions regarding Weka? You can post questions to the Weka mailing list . Please keep in mind that you cannot expect an immediate answer to your question(s). The questions are mainly answered by volunteers, Weka users just like you. You are looking for packages? With Weka 3.7.2 and later, you can easily install packages through Weka's package manager interface, either official ones or unofficial ones. Have a look at the Packages article for more information on this topic. You want to contribute to the wiki? The wiki is based on Markdown articles, which are turned into static HTML using MkDocs (see here for details on writing articles). The content of the wiki is available as repository on GitHub . Feel free to add/update and then do a pull request . You found a bug? Please post the bug report to the Weka mailing list . The following information will help tracking things down: version of Weka (e.g., 3.9.2) operating system (e.g., Windows 10 or Ubuntu 16.04 64bit) Java version (e.g., 1.8.0_162 64bit) You can also run the following command in the SimpleCLI and attach the generate output as text file to your post: java weka.core.SystemInfo","title":"Home"},{"location":"#new-to-weka","text":"Have a look at the Frequently Asked Questions (= FAQ), the Troubleshooting article or search the mailing list archives . Don't forget to check out the documentation on the Weka homepage and the Learning Resources .","title":"New to Weka?"},{"location":"#you-have-questions-regarding-weka","text":"You can post questions to the Weka mailing list . Please keep in mind that you cannot expect an immediate answer to your question(s). The questions are mainly answered by volunteers, Weka users just like you.","title":"You have questions regarding Weka?"},{"location":"#you-are-looking-for-packages","text":"With Weka 3.7.2 and later, you can easily install packages through Weka's package manager interface, either official ones or unofficial ones. Have a look at the Packages article for more information on this topic.","title":"You are looking for packages?"},{"location":"#you-want-to-contribute-to-the-wiki","text":"The wiki is based on Markdown articles, which are turned into static HTML using MkDocs (see here for details on writing articles). The content of the wiki is available as repository on GitHub . Feel free to add/update and then do a pull request .","title":"You want to contribute to the wiki?"},{"location":"#you-found-a-bug","text":"Please post the bug report to the Weka mailing list . The following information will help tracking things down: version of Weka (e.g., 3.9.2) operating system (e.g., Windows 10 or Ubuntu 16.04 64bit) Java version (e.g., 1.8.0_162 64bit) You can also run the following command in the SimpleCLI and attach the generate output as text file to your post: java weka.core.SystemInfo","title":"You found a bug?"},{"location":"adding_attributes_to_dataset/","text":"The following example class adds a nominal and a numeric attribute to the dataset identified by the filename given as first parameter. The second parameter defines whether the data is manipulated via the Add filter (= filter ) or through the Weka API directly (= java ). Usage: AddAttribute file.arff filter|java Source code: import weka.core.*; import weka.filters.Filter; import weka.filters.unsupervised.attribute.Add; import java.io.*; import java.util.*; /** * Adds a nominal and a numeric attribute to the dataset provided as first * parameter (and fills it with random values) and outputs the result to * stdout. It's either done via the Add filter (first option filter ) * or manual with Java (second option java ). * * Usage: AddAttribute lt;file.arff gt; lt;filter|java gt; * * @author FracPete (fracpete at waikato dot ac dot nz) */ public class AddAttribute { /** * adds the attributes * * @param args the commandline arguments */ public static void main(String[] args) throws Exception { if (args.length != 2) { System.out.println( \\nUsage: AddAttribute file.arff filter|java \\n ); System.exit(1); } // load dataset Instances data = new Instances(new BufferedReader(new FileReader(args[0]))); Instances newData = null; // filter or java? if (args[1].equals( filter )) { Add filter; newData = new Instances(data); // 1. nominal attribute filter = new Add(); filter.setAttributeIndex( last ); filter.setNominalLabels( A,B,C,D ); filter.setAttributeName( NewNominal ); filter.setInputFormat(newData); newData = Filter.useFilter(newData, filter); // 2. numeric attribute filter = new Add(); filter.setAttributeIndex( last ); filter.setAttributeName( NewNumeric ); filter.setInputFormat(newData); newData = Filter.useFilter(newData, filter); } else if (args[1].equals( java )) { newData = new Instances(data); // add new attributes // 1. nominal FastVector values = new FastVector(); /* FastVector is now deprecated. Users can use any java.util.List */ values.addElement( A ); /* implementation now */ values.addElement( B ); values.addElement( C ); values.addElement( D ); newData.insertAttributeAt(new Attribute( NewNominal , values), newData.numAttributes()); // 2. numeric newData.insertAttributeAt(new Attribute( NewNumeric ), newData.numAttributes()); } else { System.out.println( \\nUsage: AddAttribute file.arff filter|java \\n ); System.exit(2); } // random values Random rand = new Random(1); for (int i = 0; i newData.numInstances(); i++) { // 1. nominal // index of labels A:0,B:1,C:2,D:3 newData.instance(i).setValue(newData.numAttributes() - 2, rand.nextInt(4)); // 2. numeric newData.instance(i).setValue(newData.numAttributes() - 1, rand.nextDouble()); } // output on stdout System.out.println(newData); } } See also Creating an ARFF file - explains the creation of all the different attribute types Use Weka in your Java code - for general usage of the Weka API Save Instances to an ARFF File - if you want to save the output to a file instead of printing them to stdout Downloads AddAttribute.java ( stable , developer )","title":" Adding attributes to a dataset"},{"location":"adding_attributes_to_dataset/#see-also","text":"Creating an ARFF file - explains the creation of all the different attribute types Use Weka in your Java code - for general usage of the Weka API Save Instances to an ARFF File - if you want to save the output to a file instead of printing them to stdout","title":"See also"},{"location":"adding_attributes_to_dataset/#downloads","text":"AddAttribute.java ( stable , developer )","title":"Downloads"},{"location":"ant/","text":"What is ANT? This is how the ANT homepage defines its tool: Apache Ant is a Java-based build tool. In theory, it is kind of like Make, but without Make's wrinkles. Basics the ANT build file is based on XML the usual name for the build file is build.xml invocation - the usual build file needs not be specified explicitly, if it's in the current directory; if not target is specified, the default one is used ant [-f build-file ] [ target ] displaying all the available targets of a build file ant [-f build-file ] -projecthelp Weka and ANT a build file for Weka is available from subversion (it has been included in the weka-src.jar since version 3.4.8 and 3.5.3) it is located in the weka directory some targets of interest clean - Removes the build, dist and reports directories; also any class files in the source tree compile - Compile weka and deposit class files in ${path_modifier}/build/classes docs - Make javadocs into {${path_modifier}/doc}} exejar - Create an executable jar file in ${path_modifier}/dist Links ANT homepage XML","title":" Ant"},{"location":"ant/#basics","text":"the ANT build file is based on XML the usual name for the build file is build.xml invocation - the usual build file needs not be specified explicitly, if it's in the current directory; if not target is specified, the default one is used ant [-f build-file ] [ target ] displaying all the available targets of a build file ant [-f build-file ] -projecthelp","title":"Basics"},{"location":"ant/#weka-and-ant","text":"a build file for Weka is available from subversion (it has been included in the weka-src.jar since version 3.4.8 and 3.5.3) it is located in the weka directory some targets of interest clean - Removes the build, dist and reports directories; also any class files in the source tree compile - Compile weka and deposit class files in ${path_modifier}/build/classes docs - Make javadocs into {${path_modifier}/doc}} exejar - Create an executable jar file in ${path_modifier}/dist","title":"Weka and ANT"},{"location":"ant/#links","text":"ANT homepage XML","title":"Links"},{"location":"arff/","text":"Data format A description of the ARFF format can be found in the following articles: ARFF (stable version) ARFF (developer version) Creating an ARFF file How to create an ARFF file on the fly, i.e., inside Java, you can find here: Creating an ARFF file See also ARFF Syntax Highlighting for various editors Links ARFF2DB.py - a Python script for importing an ARFF file into a database (similar functionality to the weka.core.converters.DatabaseSaver class)","title":"ARFF Format"},{"location":"arff/#data-format","text":"A description of the ARFF format can be found in the following articles: ARFF (stable version) ARFF (developer version)","title":"Data format"},{"location":"arff/#creating-an-arff-file","text":"How to create an ARFF file on the fly, i.e., inside Java, you can find here: Creating an ARFF file","title":"Creating an ARFF file"},{"location":"arff/#see-also","text":"ARFF Syntax Highlighting for various editors","title":"See also"},{"location":"arff/#links","text":"ARFF2DB.py - a Python script for importing an ARFF file into a database (similar functionality to the weka.core.converters.DatabaseSaver class)","title":"Links"},{"location":"arff_developer/","text":"An ARFF (Attribute-Relation File Format) file is an ASCII text file that describes a list of instances sharing a set of attributes. Overview ARFF files have two distinct sections. The first section is the Header information, which is followed the Data information. The Header of the ARFF file contains the name of the relation, a list of the attributes (the columns in the data), and their types. An example header on the standard IRIS dataset looks like this: % 1. Title: Iris Plants Database % % 2. Sources: % (a) Creator: R.A. Fisher % (b) Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov) % (c) Date: July, 1988 % @RELATION iris @ATTRIBUTE sepallength NUMERIC @ATTRIBUTE sepalwidth NUMERIC @ATTRIBUTE petallength NUMERIC @ATTRIBUTE petalwidth NUMERIC @ATTRIBUTE class {Iris-setosa,Iris-versicolor,Iris-virginica} The Data of the ARFF file looks like the following: @DATA 5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa 5.0,3.6,1.4,0.2,Iris-setosa 5.4,3.9,1.7,0.4,Iris-setosa 4.6,3.4,1.4,0.3,Iris-setosa 5.0,3.4,1.5,0.2,Iris-setosa 4.4,2.9,1.4,0.2,Iris-setosa 4.9,3.1,1.5,0.1,Iris-setosa Lines that begin with a % are comments. The @RELATION , @ATTRIBUTE and @DATA declarations are case insensitive. Examples Several well-known machine learning datasets are distributed with Weka in the $WEKAHOME/data directory as ARFF files. The ARFF Header Section The ARFF Header section of the file contains the relation declaration and attribute declarations. The @relation Declaration The relation name is defined as the first line in the ARFF file. The format is: @relation [relation-name] where [relation-name] is a string. The string must be quoted if the name includes spaces. Furthermore, relation names or attribute names (see below) cannot begin with a character below \\u0021 '{', '}', ',', or '%' Moreover, it can only begin with a single or double quote if there is a corresponding quote at the end of the name. == The @attribute Declarations == Attribute declarations take the form of an ordered sequence of @attribute statements. Each attribute in the data set has its own @attribute statement which uniquely defines the name of that attribute and its data type. The order the attributes are declared indicates the column position in the data section of the file. For example, if an attribute is the third one declared then Weka expects that all that attributes values will be found in the third comma delimited column. The format for the @attribute statement is: @attribute [attribute-name] [datatype] where the [attribute-name] must adhere to the constraints specified in the above section on the @relation declaration. The [datatype] can be any of the four types supported by Weka: numeric integer is treated as numeric real is treated as numeric [nominal-specification] string date [date-format] relational for multi-instance data (for future use) where [nominal-specification] and [date-format] are defined below. The keywords numeric , real , integer , string and date are case insensitive. Numeric attributes Numeric attributes can be real or integer numbers. Nominal attributes Nominal values are defined by providing an [nominal-specification] listing the possible values: {[nominal-name1], [nominal-name2], [nominal-name3], ...} For example, the class value of the Iris dataset can be defined as follows: @ATTRIBUTE class {Iris-setosa,Iris-versicolor,Iris-virginica} Values that contain spaces must be quoted. String attributes String attributes allow us to create attributes containing arbitrary textual values. This is very useful in text-mining applications, as we can create datasets with string attributes, then write Weka Filters to manipulate strings (like StringToWordVectorFilter ). String attributes are declared as follows: @ATTRIBUTE LCC string Date attributes Date attribute declarations take the form: @attribute [name] date [[date-format]] where [name] is the name for the attribute and [date-format] is an optional string specifying how date values should be parsed and printed (this is the same format used by SimpleDateFormat ). The default format string accepts the ISO-8601 combined date and time format: yyyy-MM-dd'T'HH:mm:ss . Check out the Javadoc of the java.text.SimpleDateFormat class for supported character patterns. Dates must be specified in the data section as the corresponding string representations of the date/time (see example below). Relational attributes Relational attribute declarations take the form: @attribute [name] relational [further attribute definitions] @end [name] For the multi-instance dataset MUSK1 the definition would look like this ( \"...\" denotes an omission): @attribute molecule_name {MUSK-jf78,...,NON-MUSK-199} @attribute bag relational @attribute f1 numeric ... @attribute f166 numeric @end bag @attribute class {0,1} ... The ARFF Data Section The ARFF Data section of the file contains the data declaration line and the actual instance lines. The @data Declaration The @data declaration is a single line denoting the start of the data segment in the file. The format is: @data The instance data Each instance is represented on a single line, with carriage returns denoting the end of the instance. A percent sign (%) introduces a comment, which continues to the end of the line. Attribute values for each instance can be delimited by commas or tabs. A comma/tab may be followed by zero or more spaces. Attribute values must appear in the order in which they were declared in the header section (i.e., the data corresponding to the nth @attribute declaration is always the nth field of the attribute). A missing value is represented by a single question mark, as in: @data 4.4,?,1.5,?,Iris-setosa Values of string and nominal attributes are case sensitive, and any that contain space or the comment-delimiter character % must be quoted. (The code suggests that double-quotes are acceptable and that a backslash will escape individual characters.) An example follows: @relation LCCvsLCSH @attribute LCC string @attribute LCSH string @data AG5, 'Encyclopedias and dictionaries.;Twentieth century.' AS262, 'Science -- Soviet Union -- History.' AE5, 'Encyclopedias and dictionaries.' AS281, 'Astronomy, Assyro-Babylonian.;Moon -- Phases.' AS281, 'Astronomy, Assyro-Babylonian.;Moon -- Tables.' Dates must be specified in the data section using the string representation specified in the attribute declaration. For example: @RELATION Timestamps @ATTRIBUTE timestamp DATE yyyy-MM-dd HH:mm:ss @DATA 2001-04-03 12:12:12 2001-05-03 12:59:55 Relational data must be enclosed within double quotes \" . For example an instance of the MUSK1 dataset ( \"...\" denotes an omission): MUSK-188, 42,...,30 ,1 Sparse ARFF files Sparse ARFF files are very similar to ARFF files, but data with value 0 are not be explicitly represented. Sparse ARFF files have the same header (i.e @relation and @attribute tags) but the data section is different. Instead of representing each value in order, like this: @data 0, X, 0, Y, class A 0, 0, W, 0, class B the non-zero attributes are explicitly identified by attribute number and their value stated, like this: @data {1 X, 3 Y, 4 class A } {2 W, 4 class B } Each instance is surrounded by curly braces, and the format for each entry is: [index] [space] [value] where index is the attribute index (starting from 0). Note that the omitted values in a sparse instance are 0 , they are not \"missing\" values! If a value is unknown, you must explicitly represent it with a question mark (?). Warning: There is a known problem saving SparseInstance objects from datasets that have string attributes. In Weka, string and nominal data values are stored as numbers; these numbers act as indexes into an array of possible attribute values (this is very efficient). However, the first string value is assigned index 0: this means that, internally, this value is stored as a 0. When a SparseInstance is written, string instances with internal value 0 are not output, so their string value is lost (and when the arff file is read again, the default value 0 is the index of a different string value, so the attribute value appears to change). To get around this problem, add a dummy string value at index 0 that is never used whenever you declare string attributes that are likely to be used in SparseInstance objects and saved as Sparse ARFF files. Instance weights in ARFF files A weight can be associated with an instance in a standard ARFF file by appending it to the end of the line for that instance and enclosing the value in curly braces. E.g: @data 0, X, 0, Y, class A , {5} For a sparse instance, this example would look like: @data {1 X, 3 Y, 4 class A }, {5} Note that any instance without a weight value specified is assumed to have a weight of 1 for backwards compatibility. See also [[Add weights to dataset]] ARFF Syntax Highlighting for various editors Links ISO 8601 Javadoc of java.text.SimpleDateFormat (lists the supported character patterns) ANTLR syntax by Staal A. Vinterbo arff.g","title":" ARFF (developer version)"},{"location":"arff_developer/#overview","text":"ARFF files have two distinct sections. The first section is the Header information, which is followed the Data information. The Header of the ARFF file contains the name of the relation, a list of the attributes (the columns in the data), and their types. An example header on the standard IRIS dataset looks like this: % 1. Title: Iris Plants Database % % 2. Sources: % (a) Creator: R.A. Fisher % (b) Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov) % (c) Date: July, 1988 % @RELATION iris @ATTRIBUTE sepallength NUMERIC @ATTRIBUTE sepalwidth NUMERIC @ATTRIBUTE petallength NUMERIC @ATTRIBUTE petalwidth NUMERIC @ATTRIBUTE class {Iris-setosa,Iris-versicolor,Iris-virginica} The Data of the ARFF file looks like the following: @DATA 5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa 5.0,3.6,1.4,0.2,Iris-setosa 5.4,3.9,1.7,0.4,Iris-setosa 4.6,3.4,1.4,0.3,Iris-setosa 5.0,3.4,1.5,0.2,Iris-setosa 4.4,2.9,1.4,0.2,Iris-setosa 4.9,3.1,1.5,0.1,Iris-setosa Lines that begin with a % are comments. The @RELATION , @ATTRIBUTE and @DATA declarations are case insensitive.","title":"Overview"},{"location":"arff_developer/#examples","text":"Several well-known machine learning datasets are distributed with Weka in the $WEKAHOME/data directory as ARFF files.","title":"Examples"},{"location":"arff_developer/#the-arff-header-section","text":"The ARFF Header section of the file contains the relation declaration and attribute declarations.","title":"The ARFF Header Section"},{"location":"arff_developer/#the-relation-declaration","text":"The relation name is defined as the first line in the ARFF file. The format is: @relation [relation-name] where [relation-name] is a string. The string must be quoted if the name includes spaces. Furthermore, relation names or attribute names (see below) cannot begin with a character below \\u0021 '{', '}', ',', or '%' Moreover, it can only begin with a single or double quote if there is a corresponding quote at the end of the name. == The @attribute Declarations == Attribute declarations take the form of an ordered sequence of @attribute statements. Each attribute in the data set has its own @attribute statement which uniquely defines the name of that attribute and its data type. The order the attributes are declared indicates the column position in the data section of the file. For example, if an attribute is the third one declared then Weka expects that all that attributes values will be found in the third comma delimited column. The format for the @attribute statement is: @attribute [attribute-name] [datatype] where the [attribute-name] must adhere to the constraints specified in the above section on the @relation declaration. The [datatype] can be any of the four types supported by Weka: numeric integer is treated as numeric real is treated as numeric [nominal-specification] string date [date-format] relational for multi-instance data (for future use) where [nominal-specification] and [date-format] are defined below. The keywords numeric , real , integer , string and date are case insensitive.","title":"The @relation Declaration"},{"location":"arff_developer/#numeric-attributes","text":"Numeric attributes can be real or integer numbers.","title":"Numeric attributes"},{"location":"arff_developer/#nominal-attributes","text":"Nominal values are defined by providing an [nominal-specification] listing the possible values: {[nominal-name1], [nominal-name2], [nominal-name3], ...} For example, the class value of the Iris dataset can be defined as follows: @ATTRIBUTE class {Iris-setosa,Iris-versicolor,Iris-virginica} Values that contain spaces must be quoted.","title":"Nominal attributes"},{"location":"arff_developer/#string-attributes","text":"String attributes allow us to create attributes containing arbitrary textual values. This is very useful in text-mining applications, as we can create datasets with string attributes, then write Weka Filters to manipulate strings (like StringToWordVectorFilter ). String attributes are declared as follows: @ATTRIBUTE LCC string","title":"String attributes"},{"location":"arff_developer/#date-attributes","text":"Date attribute declarations take the form: @attribute [name] date [[date-format]] where [name] is the name for the attribute and [date-format] is an optional string specifying how date values should be parsed and printed (this is the same format used by SimpleDateFormat ). The default format string accepts the ISO-8601 combined date and time format: yyyy-MM-dd'T'HH:mm:ss . Check out the Javadoc of the java.text.SimpleDateFormat class for supported character patterns. Dates must be specified in the data section as the corresponding string representations of the date/time (see example below).","title":"Date attributes"},{"location":"arff_developer/#relational-attributes","text":"Relational attribute declarations take the form: @attribute [name] relational [further attribute definitions] @end [name] For the multi-instance dataset MUSK1 the definition would look like this ( \"...\" denotes an omission): @attribute molecule_name {MUSK-jf78,...,NON-MUSK-199} @attribute bag relational @attribute f1 numeric ... @attribute f166 numeric @end bag @attribute class {0,1} ...","title":"Relational attributes"},{"location":"arff_developer/#the-arff-data-section","text":"The ARFF Data section of the file contains the data declaration line and the actual instance lines.","title":"The ARFF Data Section"},{"location":"arff_developer/#the-data-declaration","text":"The @data declaration is a single line denoting the start of the data segment in the file. The format is: @data","title":"The @data Declaration"},{"location":"arff_developer/#the-instance-data","text":"Each instance is represented on a single line, with carriage returns denoting the end of the instance. A percent sign (%) introduces a comment, which continues to the end of the line. Attribute values for each instance can be delimited by commas or tabs. A comma/tab may be followed by zero or more spaces. Attribute values must appear in the order in which they were declared in the header section (i.e., the data corresponding to the nth @attribute declaration is always the nth field of the attribute). A missing value is represented by a single question mark, as in: @data 4.4,?,1.5,?,Iris-setosa Values of string and nominal attributes are case sensitive, and any that contain space or the comment-delimiter character % must be quoted. (The code suggests that double-quotes are acceptable and that a backslash will escape individual characters.) An example follows: @relation LCCvsLCSH @attribute LCC string @attribute LCSH string @data AG5, 'Encyclopedias and dictionaries.;Twentieth century.' AS262, 'Science -- Soviet Union -- History.' AE5, 'Encyclopedias and dictionaries.' AS281, 'Astronomy, Assyro-Babylonian.;Moon -- Phases.' AS281, 'Astronomy, Assyro-Babylonian.;Moon -- Tables.' Dates must be specified in the data section using the string representation specified in the attribute declaration. For example: @RELATION Timestamps @ATTRIBUTE timestamp DATE yyyy-MM-dd HH:mm:ss @DATA 2001-04-03 12:12:12 2001-05-03 12:59:55 Relational data must be enclosed within double quotes \" . For example an instance of the MUSK1 dataset ( \"...\" denotes an omission): MUSK-188, 42,...,30 ,1","title":"The instance data"},{"location":"arff_developer/#sparse-arff-files","text":"Sparse ARFF files are very similar to ARFF files, but data with value 0 are not be explicitly represented. Sparse ARFF files have the same header (i.e @relation and @attribute tags) but the data section is different. Instead of representing each value in order, like this: @data 0, X, 0, Y, class A 0, 0, W, 0, class B the non-zero attributes are explicitly identified by attribute number and their value stated, like this: @data {1 X, 3 Y, 4 class A } {2 W, 4 class B } Each instance is surrounded by curly braces, and the format for each entry is: [index] [space] [value] where index is the attribute index (starting from 0). Note that the omitted values in a sparse instance are 0 , they are not \"missing\" values! If a value is unknown, you must explicitly represent it with a question mark (?). Warning: There is a known problem saving SparseInstance objects from datasets that have string attributes. In Weka, string and nominal data values are stored as numbers; these numbers act as indexes into an array of possible attribute values (this is very efficient). However, the first string value is assigned index 0: this means that, internally, this value is stored as a 0. When a SparseInstance is written, string instances with internal value 0 are not output, so their string value is lost (and when the arff file is read again, the default value 0 is the index of a different string value, so the attribute value appears to change). To get around this problem, add a dummy string value at index 0 that is never used whenever you declare string attributes that are likely to be used in SparseInstance objects and saved as Sparse ARFF files.","title":"Sparse ARFF files"},{"location":"arff_developer/#instance-weights-in-arff-files","text":"A weight can be associated with an instance in a standard ARFF file by appending it to the end of the line for that instance and enclosing the value in curly braces. E.g: @data 0, X, 0, Y, class A , {5} For a sparse instance, this example would look like: @data {1 X, 3 Y, 4 class A }, {5} Note that any instance without a weight value specified is assumed to have a weight of 1 for backwards compatibility.","title":"Instance weights in ARFF files"},{"location":"arff_developer/#see-also","text":"[[Add weights to dataset]] ARFF Syntax Highlighting for various editors","title":"See also"},{"location":"arff_developer/#links","text":"ISO 8601 Javadoc of java.text.SimpleDateFormat (lists the supported character patterns) ANTLR syntax by Staal A. Vinterbo arff.g","title":"Links"},{"location":"arff_from_text_collections/","text":"The following utility generates an ARFF file from text documents in a given directory (download link is at the end of this article). The stable/developer version of Weka offer this tool as the weka.core.converters.TextDirectoryLoader converter. This can be used as: java -cp path to weka.jar weka.core.converters.TextDirectoryLoader -dir . For help just type: java -cp path to weka.jar weka.core.converters.TextDirectoryLoader /* * TextDirectoryToArff.java * Copyright (C) 2002 Richard Kirkby * * This program is free software; you can redistribute it and/or modify * it under the terms of the GNU General Public License as published by * the Free Software Foundation; either version 2 of the License, or * (at your option) any later version. * * This program is distributed in the hope that it will be useful, * but WITHOUT ANY WARRANTY; without even the implied warranty of * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the * GNU General Public License for more details. * * You should have received a copy of the GNU General Public License * along with this program; if not, write to the Free Software * Foundation, Inc., 675 Mass Ave, Cambridge, MA 02139, USA. */ import java.io.*; import weka.core.*; /** * Builds an arff dataset from the documents in a given directory. * Assumes that the file names for the documents end with .txt . * * Usage: p/ * * TextDirectoryToArff directory path p/ * * @author Richard Kirkby (rkirkby at cs.waikato.ac.nz) * @version 1.0 */ public class TextDirectoryToArff { public Instances createDataset(String directoryPath) throws Exception { FastVector atts = new FastVector(2); atts.addElement(new Attribute( filename , (FastVector) null)); atts.addElement(new Attribute( contents , (FastVector) null)); Instances data = new Instances( text_files_in_ + directoryPath, atts, 0); File dir = new File(directoryPath); String[] files = dir.list(); for (int i = 0; i files.length; i++) { if (files[i].endsWith( .txt )) { try { double[] newInst = new double[2]; newInst[0] = (double)data.attribute(0).addStringValue(files[i]); File txt = new File(directoryPath + File.separator + files[i]); InputStreamReader is; is = new InputStreamReader(new FileInputStream(txt)); StringBuffer txtStr = new StringBuffer(); int c; while ((c = is.read()) != -1) { txtStr.append((char)c); } newInst[1] = (double)data.attribute(1).addStringValue(txtStr.toString()); data.add(new Instance(1.0, newInst)); } catch (Exception e) { //System.err.println( failed to convert file: + directoryPath + File.separator + files[i]); } } } return data; } public static void main(String[] args) { if (args.length == 1) { TextDirectoryToArff tdta = new TextDirectoryToArff(); try { Instances dataset = tdta.createDataset(args[0]); System.out.println(dataset); } catch (Exception e) { System.err.println(e.getMessage()); e.printStackTrace(); } } else { System.out.println( Usage: java TextDirectoryToArff directory name ); } } } See also [[Text categorization with Weka]] Downloads TextDirectoryToArff.java","title":" ARFF files from Text Collections"},{"location":"arff_from_text_collections/#see-also","text":"[[Text categorization with Weka]]","title":"See also"},{"location":"arff_from_text_collections/#downloads","text":"TextDirectoryToArff.java","title":"Downloads"},{"location":"arff_stable/","text":"An ARFF (Attribute-Relation File Format) file is an ASCII text file that describes a list of instances sharing a set of attributes. Overview ARFF files have two distinct sections. The first section is the Header information, which is followed the Data information. The Header of the ARFF file contains the name of the relation, a list of the attributes (the columns in the data), and their types. An example header on the standard IRIS dataset looks like this: % 1. Title: Iris Plants Database % % 2. Sources: % (a) Creator: R.A. Fisher % (b) Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov) % (c) Date: July, 1988 % @RELATION iris @ATTRIBUTE sepallength NUMERIC @ATTRIBUTE sepalwidth NUMERIC @ATTRIBUTE petallength NUMERIC @ATTRIBUTE petalwidth NUMERIC @ATTRIBUTE class {Iris-setosa,Iris-versicolor,Iris-virginica} The Data of the ARFF file looks like the following: @DATA 5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa 5.0,3.6,1.4,0.2,Iris-setosa 5.4,3.9,1.7,0.4,Iris-setosa 4.6,3.4,1.4,0.3,Iris-setosa 5.0,3.4,1.5,0.2,Iris-setosa 4.4,2.9,1.4,0.2,Iris-setosa 4.9,3.1,1.5,0.1,Iris-setosa Lines that begin with a % are comments. The @RELATION , @ATTRIBUTE and @DATA declarations are case insensitive. Examples Several well-known machine learning datasets are distributed with Weka in the $WEKAHOME/data directory as ARFF files. The ARFF Header Section The ARFF Header section of the file contains the relation declaration and attribute declarations. The @relation Declaration The relation name is defined as the first line in the ARFF file. The format is: @relation [relation-name] where [relation-name] is a string. The string must be quoted if the name includes spaces. Furthermore, relation names or attribute names (see below) cannot begin with a character below \\u0021 '{', '}', ',', or '%' Moreover, it can only begin with a single or double quote if there is a corresponding quote at the end of the name. The @attribute Declarations Attribute declarations take the form of an ordered sequence of @attribute statements. Each attribute in the data set has its own @attribute statement which uniquely defines the name of that attribute and its data type. The order the attributes are declared indicates the column position in the data section of the file. For example, if an attribute is the third one declared then Weka expects that all that attributes values will be found in the third comma delimited column. The format for the @attribute statement is: @attribute [attribute-name] [datatype] where the [attribute-name] must adhere to the constraints specified in the above section on the @relation declaration. The [datatype] can be any of the four types supported by Weka: numeric integer is treated as numeric real is treated as numeric [nominal-specification] string date [date-format] relational for multi-instance data (for future use) where [nominal-specification] and [date-format] are defined below. The keywords numeric , real , integer , string and date are case insensitive. Numeric attributes Numeric attributes can be real or integer numbers. Nominal attributes Nominal values are defined by providing an [nominal-specification] listing the possible values: {[nominal-name1], [nominal-name2], [nominal-name3], ...} For example, the class value of the Iris dataset can be defined as follows: @ATTRIBUTE class {Iris-setosa,Iris-versicolor,Iris-virginica} Values that contain spaces must be quoted. String attributes String attributes allow us to create attributes containing arbitrary textual values. This is very useful in text-mining applications, as we can create datasets with string attributes, then write Weka Filters to manipulate strings (like StringToWordVectorFilter ). String attributes are declared as follows: @ATTRIBUTE LCC string Date attributes Date attribute declarations take the form: @attribute [name] date [[date-format]] where [name] is the name for the attribute and [date-format] is an optional string specifying how date values should be parsed and printed (this is the same format used by SimpleDateFormat ). The default format string accepts the ISO-8601 combined date and time format: yyyy-MM-dd'T'HH:mm:ss . Check out the Javadoc of the java.text.SimpleDateFormat class for supported character patterns. Dates must be specified in the data section as the corresponding string representations of the date/time (see example below). Relational attributes Relational attribute declarations take the form: @attribute [name] relational [further attribute definitions] @end [name] For the multi-instance dataset MUSK1 the definition would look like this ( \"...\" denotes an omission): @attribute molecule_name {MUSK-jf78,...,NON-MUSK-199} @attribute bag relational @attribute f1 numeric ... @attribute f166 numeric @end bag @attribute class {0,1} ... The ARFF Data Section The ARFF Data section of the file contains the data declaration line and the actual instance lines. The @data Declaration The @data declaration is a single line denoting the start of the data segment in the file. The format is: @data The instance data Each instance is represented on a single line, with carriage returns denoting the end of the instance. A percent sign (%) introduces a comment, which continues to the end of the line. Attribute values for each instance can be delimited by commas or tabs. A comma/tab may be followed by zero or more spaces. Attribute values must appear in the order in which they were declared in the header section (i.e., the data corresponding to the nth @attribute declaration is always the nth field of the attribute). A missing value is represented by a single question mark, as in: @data 4.4,?,1.5,?,Iris-setosa Values of string and nominal attributes are case sensitive, and any that contain space or the comment-delimiter character % must be quoted. (The code suggests that double-quotes are acceptable and that a backslash will escape individual characters.) An example follows: @relation LCCvsLCSH @attribute LCC string @attribute LCSH string @data AG5, 'Encyclopedias and dictionaries.;Twentieth century.' AS262, 'Science -- Soviet Union -- History.' AE5, 'Encyclopedias and dictionaries.' AS281, 'Astronomy, Assyro-Babylonian.;Moon -- Phases.' AS281, 'Astronomy, Assyro-Babylonian.;Moon -- Tables.' Dates must be specified in the data section using the string representation specified in the attribute declaration. For example: @RELATION Timestamps @ATTRIBUTE timestamp DATE yyyy-MM-dd HH:mm:ss @DATA 2001-04-03 12:12:12 2001-05-03 12:59:55 Relational data must be enclosed within double quotes \" . For example an instance of the MUSK1 dataset ( \"...\" denotes an omission): MUSK-188, 42,...,30 ,1 Sparse ARFF files Sparse ARFF files are very similar to ARFF files, but data with value 0 are not be explicitly represented. Sparse ARFF files have the same header (i.e @relation and @attribute tags) but the data section is different. Instead of representing each value in order, like this: @data 0, X, 0, Y, class A 0, 0, W, 0, class B the non-zero attributes are explicitly identified by attribute number and their value stated, like this: @data {1 X, 3 Y, 4 class A } {2 W, 4 class B } Each instance is surrounded by curly braces, and the format for each entry is: [index] [space] [value] where index is the attribute index (starting from 0). Note that the omitted values in a sparse instance are 0 , they are not \"missing\" values! If a value is unknown, you must explicitly represent it with a question mark (?). Warning: There is a known problem saving SparseInstance objects from datasets that have string attributes. In Weka, string and nominal data values are stored as numbers; these numbers act as indexes into an array of possible attribute values (this is very efficient). However, the first string value is assigned index 0: this means that, internally, this value is stored as a 0. When a SparseInstance is written, string instances with internal value 0 are not output, so their string value is lost (and when the arff file is read again, the default value 0 is the index of a different string value, so the attribute value appears to change). To get around this problem, add a dummy string value at index 0 that is never used whenever you declare string attributes that are likely to be used in SparseInstance objects and saved as Sparse ARFF files. Instance weights in ARFF files A weight can be associated with an instance in a standard ARFF file by appending it to the end of the line for that instance and enclosing the value in curly braces. E.g: @data 0, X, 0, Y, class A , {5} For a sparse instance, this example would look like: @data {1 X, 3 Y, 4 class A }, {5} Note that any instance without a weight value specified is assumed to have a weight of 1 for backwards compatibility. See also [[Add weights to dataset]] ARFF Syntax Highlighting for various editors Links ISO-8601 Javadoc of java.text.SimpleDateFormat (lists the supported character patterns) ANTLR syntax by Staal A. Vinterbo arff.g","title":" ARFF (stable version)"},{"location":"arff_stable/#overview","text":"ARFF files have two distinct sections. The first section is the Header information, which is followed the Data information. The Header of the ARFF file contains the name of the relation, a list of the attributes (the columns in the data), and their types. An example header on the standard IRIS dataset looks like this: % 1. Title: Iris Plants Database % % 2. Sources: % (a) Creator: R.A. Fisher % (b) Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov) % (c) Date: July, 1988 % @RELATION iris @ATTRIBUTE sepallength NUMERIC @ATTRIBUTE sepalwidth NUMERIC @ATTRIBUTE petallength NUMERIC @ATTRIBUTE petalwidth NUMERIC @ATTRIBUTE class {Iris-setosa,Iris-versicolor,Iris-virginica} The Data of the ARFF file looks like the following: @DATA 5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa 5.0,3.6,1.4,0.2,Iris-setosa 5.4,3.9,1.7,0.4,Iris-setosa 4.6,3.4,1.4,0.3,Iris-setosa 5.0,3.4,1.5,0.2,Iris-setosa 4.4,2.9,1.4,0.2,Iris-setosa 4.9,3.1,1.5,0.1,Iris-setosa Lines that begin with a % are comments. The @RELATION , @ATTRIBUTE and @DATA declarations are case insensitive.","title":"Overview"},{"location":"arff_stable/#examples","text":"Several well-known machine learning datasets are distributed with Weka in the $WEKAHOME/data directory as ARFF files.","title":"Examples"},{"location":"arff_stable/#the-arff-header-section","text":"The ARFF Header section of the file contains the relation declaration and attribute declarations.","title":"The ARFF Header Section"},{"location":"arff_stable/#the-relation-declaration","text":"The relation name is defined as the first line in the ARFF file. The format is: @relation [relation-name] where [relation-name] is a string. The string must be quoted if the name includes spaces. Furthermore, relation names or attribute names (see below) cannot begin with a character below \\u0021 '{', '}', ',', or '%' Moreover, it can only begin with a single or double quote if there is a corresponding quote at the end of the name.","title":"The @relation Declaration"},{"location":"arff_stable/#the-attribute-declarations","text":"Attribute declarations take the form of an ordered sequence of @attribute statements. Each attribute in the data set has its own @attribute statement which uniquely defines the name of that attribute and its data type. The order the attributes are declared indicates the column position in the data section of the file. For example, if an attribute is the third one declared then Weka expects that all that attributes values will be found in the third comma delimited column. The format for the @attribute statement is: @attribute [attribute-name] [datatype] where the [attribute-name] must adhere to the constraints specified in the above section on the @relation declaration. The [datatype] can be any of the four types supported by Weka: numeric integer is treated as numeric real is treated as numeric [nominal-specification] string date [date-format] relational for multi-instance data (for future use) where [nominal-specification] and [date-format] are defined below. The keywords numeric , real , integer , string and date are case insensitive.","title":"The @attribute Declarations"},{"location":"arff_stable/#numeric-attributes","text":"Numeric attributes can be real or integer numbers.","title":"Numeric attributes"},{"location":"arff_stable/#nominal-attributes","text":"Nominal values are defined by providing an [nominal-specification] listing the possible values: {[nominal-name1], [nominal-name2], [nominal-name3], ...} For example, the class value of the Iris dataset can be defined as follows: @ATTRIBUTE class {Iris-setosa,Iris-versicolor,Iris-virginica} Values that contain spaces must be quoted.","title":"Nominal attributes"},{"location":"arff_stable/#string-attributes","text":"String attributes allow us to create attributes containing arbitrary textual values. This is very useful in text-mining applications, as we can create datasets with string attributes, then write Weka Filters to manipulate strings (like StringToWordVectorFilter ). String attributes are declared as follows: @ATTRIBUTE LCC string","title":"String attributes"},{"location":"arff_stable/#date-attributes","text":"Date attribute declarations take the form: @attribute [name] date [[date-format]] where [name] is the name for the attribute and [date-format] is an optional string specifying how date values should be parsed and printed (this is the same format used by SimpleDateFormat ). The default format string accepts the ISO-8601 combined date and time format: yyyy-MM-dd'T'HH:mm:ss . Check out the Javadoc of the java.text.SimpleDateFormat class for supported character patterns. Dates must be specified in the data section as the corresponding string representations of the date/time (see example below).","title":"Date attributes"},{"location":"arff_stable/#relational-attributes","text":"Relational attribute declarations take the form: @attribute [name] relational [further attribute definitions] @end [name] For the multi-instance dataset MUSK1 the definition would look like this ( \"...\" denotes an omission): @attribute molecule_name {MUSK-jf78,...,NON-MUSK-199} @attribute bag relational @attribute f1 numeric ... @attribute f166 numeric @end bag @attribute class {0,1} ...","title":"Relational attributes"},{"location":"arff_stable/#the-arff-data-section","text":"The ARFF Data section of the file contains the data declaration line and the actual instance lines.","title":"The ARFF Data Section"},{"location":"arff_stable/#the-data-declaration","text":"The @data declaration is a single line denoting the start of the data segment in the file. The format is: @data","title":"The @data Declaration"},{"location":"arff_stable/#the-instance-data","text":"Each instance is represented on a single line, with carriage returns denoting the end of the instance. A percent sign (%) introduces a comment, which continues to the end of the line. Attribute values for each instance can be delimited by commas or tabs. A comma/tab may be followed by zero or more spaces. Attribute values must appear in the order in which they were declared in the header section (i.e., the data corresponding to the nth @attribute declaration is always the nth field of the attribute). A missing value is represented by a single question mark, as in: @data 4.4,?,1.5,?,Iris-setosa Values of string and nominal attributes are case sensitive, and any that contain space or the comment-delimiter character % must be quoted. (The code suggests that double-quotes are acceptable and that a backslash will escape individual characters.) An example follows: @relation LCCvsLCSH @attribute LCC string @attribute LCSH string @data AG5, 'Encyclopedias and dictionaries.;Twentieth century.' AS262, 'Science -- Soviet Union -- History.' AE5, 'Encyclopedias and dictionaries.' AS281, 'Astronomy, Assyro-Babylonian.;Moon -- Phases.' AS281, 'Astronomy, Assyro-Babylonian.;Moon -- Tables.' Dates must be specified in the data section using the string representation specified in the attribute declaration. For example: @RELATION Timestamps @ATTRIBUTE timestamp DATE yyyy-MM-dd HH:mm:ss @DATA 2001-04-03 12:12:12 2001-05-03 12:59:55 Relational data must be enclosed within double quotes \" . For example an instance of the MUSK1 dataset ( \"...\" denotes an omission): MUSK-188, 42,...,30 ,1","title":"The instance data"},{"location":"arff_stable/#sparse-arff-files","text":"Sparse ARFF files are very similar to ARFF files, but data with value 0 are not be explicitly represented. Sparse ARFF files have the same header (i.e @relation and @attribute tags) but the data section is different. Instead of representing each value in order, like this: @data 0, X, 0, Y, class A 0, 0, W, 0, class B the non-zero attributes are explicitly identified by attribute number and their value stated, like this: @data {1 X, 3 Y, 4 class A } {2 W, 4 class B } Each instance is surrounded by curly braces, and the format for each entry is: [index] [space] [value] where index is the attribute index (starting from 0). Note that the omitted values in a sparse instance are 0 , they are not \"missing\" values! If a value is unknown, you must explicitly represent it with a question mark (?). Warning: There is a known problem saving SparseInstance objects from datasets that have string attributes. In Weka, string and nominal data values are stored as numbers; these numbers act as indexes into an array of possible attribute values (this is very efficient). However, the first string value is assigned index 0: this means that, internally, this value is stored as a 0. When a SparseInstance is written, string instances with internal value 0 are not output, so their string value is lost (and when the arff file is read again, the default value 0 is the index of a different string value, so the attribute value appears to change). To get around this problem, add a dummy string value at index 0 that is never used whenever you declare string attributes that are likely to be used in SparseInstance objects and saved as Sparse ARFF files.","title":"Sparse ARFF files"},{"location":"arff_stable/#instance-weights-in-arff-files","text":"A weight can be associated with an instance in a standard ARFF file by appending it to the end of the line for that instance and enclosing the value in curly braces. E.g: @data 0, X, 0, Y, class A , {5} For a sparse instance, this example would look like: @data {1 X, 3 Y, 4 class A }, {5} Note that any instance without a weight value specified is assumed to have a weight of 1 for backwards compatibility.","title":"Instance weights in ARFF files"},{"location":"arff_stable/#see-also","text":"[[Add weights to dataset]] ARFF Syntax Highlighting for various editors","title":"See also"},{"location":"arff_stable/#links","text":"ISO-8601 Javadoc of java.text.SimpleDateFormat (lists the supported character patterns) ANTLR syntax by Staal A. Vinterbo arff.g","title":"Links"},{"location":"arff_syntax/","text":"Here you can find syntax highlightings for various editors: Emacs Add the code from the arff.emacs file into your startup file. Notepad++ Copy the contents of tag of the arff.notepadplus file into your %APPDATA%\\Notepad++\\userDefineLang.xml file. (Ensure that you maintain the XML structure). If userDefineLang.xml does not exist, simply rename the arff.notepadplus file to userDefineLang.xml TextPad Copy the file arff.syn into your TEXTPAD-DIR /system directory. Then run the wizard for adding a new document class (Configure - New Document Class...). Ultraedit Just copy/paste the content of the file arff.ultraedit in your ULTRAEDIT-DIR /WORDFILE.TXT file. Adjust the /Lnn language number that it fits into the numbering of your current settings. vim/gvim Save the file arff.vim in your $HOME/.vim/syntax directory. You can enable the syntax with :set syntax=arff . Links Emacs homepage Notepad++ homepage TextPad homepage Ultraedit homepage vim homepage","title":" ARFF Syntax Highlighting"},{"location":"arff_syntax/#emacs","text":"Add the code from the arff.emacs file into your startup file.","title":"Emacs"},{"location":"arff_syntax/#notepad","text":"Copy the contents of tag of the arff.notepadplus file into your %APPDATA%\\Notepad++\\userDefineLang.xml file. (Ensure that you maintain the XML structure). If userDefineLang.xml does not exist, simply rename the arff.notepadplus file to userDefineLang.xml","title":"Notepad++"},{"location":"arff_syntax/#textpad","text":"Copy the file arff.syn into your TEXTPAD-DIR /system directory. Then run the wizard for adding a new document class (Configure - New Document Class...).","title":"TextPad"},{"location":"arff_syntax/#ultraedit","text":"Just copy/paste the content of the file arff.ultraedit in your ULTRAEDIT-DIR /WORDFILE.TXT file. Adjust the /Lnn language number that it fits into the numbering of your current settings.","title":"Ultraedit"},{"location":"arff_syntax/#vimgvim","text":"Save the file arff.vim in your $HOME/.vim/syntax directory. You can enable the syntax with :set syntax=arff .","title":"vim/gvim"},{"location":"arff_syntax/#links","text":"Emacs homepage Notepad++ homepage TextPad homepage Ultraedit homepage vim homepage","title":"Links"},{"location":"auc/","text":"AUC = the A rea U nder the ROC C urve. Weka uses the Mann Whitney statistic to calculate the AUC via the weka.classifiers.evaluation.ThresholdCurve class. Explorer See [[ROC curves]]. KnowledgeFlow See [[ROC curves]]. Commandline Classifiers can output the AUC if the -i option is provided. The -i option provides detailed information per class. Running the J48 classifier on the iris UCI [[Datasets|dataset]] with the following commandline: java [CLASSPATH|-classpath your-classpath ] weka.classifiers.trees.J48 -t /some/where/iris.arff -i produces this output: == Detailed Accuracy By Class == TP Rate FP Rate Precision Recall F-Measure ROC Area Class 0.98 0 1 0.98 0.99 0.99 Iris-setosa 0.94 0.03 0.94 0.94 0.94 0.952 Iris-versicolor 0.96 0.03 0.941 0.96 0.95 0.961 Iris-virginica See also [[ROC curves]] Mann Whitney statistic on WikiPedia Links University of Nebraska Medical Center, Interpreting Diagnostic Tests weka.classifiers.evaluation.ThresholdCurve","title":" Area under the curve"},{"location":"auc/#explorer","text":"See [[ROC curves]].","title":"Explorer"},{"location":"auc/#knowledgeflow","text":"See [[ROC curves]].","title":"KnowledgeFlow"},{"location":"auc/#commandline","text":"Classifiers can output the AUC if the -i option is provided. The -i option provides detailed information per class. Running the J48 classifier on the iris UCI [[Datasets|dataset]] with the following commandline: java [CLASSPATH|-classpath your-classpath ] weka.classifiers.trees.J48 -t /some/where/iris.arff -i produces this output: == Detailed Accuracy By Class == TP Rate FP Rate Precision Recall F-Measure ROC Area Class 0.98 0 1 0.98 0.99 0.99 Iris-setosa 0.94 0.03 0.94 0.94 0.94 0.952 Iris-versicolor 0.96 0.03 0.941 0.96 0.95 0.961 Iris-virginica","title":"Commandline"},{"location":"auc/#see-also","text":"[[ROC curves]] Mann Whitney statistic on WikiPedia","title":"See also"},{"location":"auc/#links","text":"University of Nebraska Medical Center, Interpreting Diagnostic Tests weka.classifiers.evaluation.ThresholdCurve","title":"Links"},{"location":"batch_filtering/","text":"Batch filtering is used if a second dataset, normally the test set, needs to be processed with the same statistics as the the first dataset, normally the training set. For example, performing standardization with the Standardize filter on two datasets separately will most certainly create two differently standardized output files, since the mean and the standard deviation are based on the input data (and those will differ if the datasets are different). The same applies to the StringToWordVector : here the word dictionary will change, since word occurrences will differ in training and test set. The generated output will be two incompatible files. In order to create compatible train and test set, batch filtering is necessary. Here, the first input/output pair ( -i / -o ) initializes the filter's statistics and the second input/output pair ( -r / -s ) gets processed according to those statistics. To enable batch filtering, one has to provide the additional parameter -b on the commandline. Here is an example Java call: java weka.filters.unsupervised.attribute.Standardize \\ -b \\ -i train.arff \\ -o train_std.arff \\ -r test.arff \\ -s test_std.arff Note: The commandline outlined above is for a Linux/Unix bash (the backslash tells the shell that the command isn't finished yet and continues on the next line). In case of Windows or the SimpleCLI, just remove those backslashes and put everything on one line. See also See section Batch filtering in the article Use Weka in your Java code , in case you need to perform batch filtering from within your own code","title":" Batch filtering"},{"location":"batch_filtering/#see-also","text":"See section Batch filtering in the article Use Weka in your Java code , in case you need to perform batch filtering from within your own code","title":"See also"},{"location":"binarize_attribute/","text":"Sometimes one wants to binarize a nominal attribute of a certain dataset by grouping all values except the one of interest together as a negation of this value. E.g., in the {{weather}} data the outlook attribute, where sunny is of interest and the other values, rainy and overcast , are grouped together as not-sunny . Original dataset: @relation weather @attribute outlook {sunny, overcast, rainy} @attribute temperature real @attribute humidity real @attribute windy {TRUE, FALSE} @attribute play {yes, no} @data sunny,85,85,FALSE,no sunny,80,90,TRUE,no overcast,83,86,FALSE,yes rainy,70,96,FALSE,yes rainy,68,80,FALSE,yes rainy,65,70,TRUE,no overcast,64,65,TRUE,yes sunny,72,95,FALSE,no sunny,69,70,FALSE,yes rainy,75,80,FALSE,yes sunny,75,70,TRUE,yes overcast,72,90,TRUE,yes overcast,81,75,FALSE,yes rainy,71,91,TRUE,no Desired output: @relation weather-sunny-and-not_sunny @attribute outlook {sunny,not_sunny} @attribute temperature numeric @attribute humidity numeric @attribute windy {TRUE,FALSE} @attribute play {yes,no} @data sunny,85,85,FALSE,no sunny,80,90,TRUE,no not_sunny,83,86,FALSE,yes not_sunny,70,96,FALSE,yes not_sunny,68,80,FALSE,yes not_sunny,65,70,TRUE,no not_sunny,64,65,TRUE,yes sunny,72,95,FALSE,no sunny,69,70,FALSE,yes not_sunny,75,80,FALSE,yes sunny,75,70,TRUE,yes not_sunny,72,90,TRUE,yes not_sunny,81,75,FALSE,yes not_sunny,71,91,TRUE,no The Weka filter NominalToBinary cannot be used directly, since it generates a new attribute for each value of the nominal attribute. As a postprocessing step one could delete all the attributes that are of no interest, but this is quite cumbersome. The Binarize.java class on the other hand generates directly several ARFF out of a given one in the desired format. Download Binarize.java ( stable , developer","title":" Binarize Attribute"},{"location":"binarize_attribute/#download","text":"Binarize.java ( stable , developer","title":"Download"},{"location":"command_redirection/","text":"Console With command redirection one can redirect standard streams like stdin , stdout and stderr to user-specified locations. Quite often it is useful to redirect the output of a program to a text file. redirecting stdout to a file someProgram /some/where/output.txt (Linux/Unix Bash) someProgram c:\\some\\where\\output.txt (Windows command prompt) redirecting stderr to a file someProgram 2 /some/where/output.txt (Linux/Unix Bash) someProgram 2 c:\\some\\where\\output.txt (Windows command prompt) redirecting stdout and stderr to a file someProgram /some/where/output.txt (Linux/Unix Bash) someProgram c:\\some\\where\\output.txt 2 1 (Windows command prompt) Note: under Weka quite often the output is printed to stderr , e.g., if one is using the -p 0 option from the commandline to print the predicted values for a test file: java weka.classifiers.trees.J48 -t train.arff -T test.arff -p 0 2 j48.txt or if one already has a trained model: java weka.classifiers.trees.J48 -l j48.model -T test.arff -p 0 2 j48.txt SimpleCLI One can perform a basic redirection also in the SimpleCLI, e.g.: java weka.classifiers.trees.J48 -t test.arff j48.txt Note: the must be preceded and followed by a space , otherwise it is not recognized as redirection, but part of another parameter. Links Linux Command redirection under Bash I/O Redirection under Bash Redirection under Unix (WikiPedia) Windows Command redirection under MS Windows Command redirection under MS DOS","title":" Command redirection"},{"location":"command_redirection/#console","text":"With command redirection one can redirect standard streams like stdin , stdout and stderr to user-specified locations. Quite often it is useful to redirect the output of a program to a text file. redirecting stdout to a file someProgram /some/where/output.txt (Linux/Unix Bash) someProgram c:\\some\\where\\output.txt (Windows command prompt) redirecting stderr to a file someProgram 2 /some/where/output.txt (Linux/Unix Bash) someProgram 2 c:\\some\\where\\output.txt (Windows command prompt) redirecting stdout and stderr to a file someProgram /some/where/output.txt (Linux/Unix Bash) someProgram c:\\some\\where\\output.txt 2 1 (Windows command prompt) Note: under Weka quite often the output is printed to stderr , e.g., if one is using the -p 0 option from the commandline to print the predicted values for a test file: java weka.classifiers.trees.J48 -t train.arff -T test.arff -p 0 2 j48.txt or if one already has a trained model: java weka.classifiers.trees.J48 -l j48.model -T test.arff -p 0 2 j48.txt","title":"Console"},{"location":"command_redirection/#simplecli","text":"One can perform a basic redirection also in the SimpleCLI, e.g.: java weka.classifiers.trees.J48 -t test.arff j48.txt Note: the must be preceded and followed by a space , otherwise it is not recognized as redirection, but part of another parameter.","title":"SimpleCLI"},{"location":"command_redirection/#links","text":"Linux Command redirection under Bash I/O Redirection under Bash Redirection under Unix (WikiPedia) Windows Command redirection under MS Windows Command redirection under MS DOS","title":"Links"},{"location":"compiling_weka/","text":"There are several ways of compiling the Weka source code: with ant takes care of compiling all the necessary classes and easily generates jar archives with maven similar to ant with an IDE, like IntelliJ IDEA, Eclipse or NetBeans can be very helpful for debugging tricky bugs","title":" Compiling Weka"},{"location":"creating_arff_file/","text":"The following code generates an Instances object and outputs it to stdout as ARFF file. It generates the following types of attributes: numeric nominal string date relational Example class AttTest : import weka.core.Attribute; import weka.core.FastVector; import weka.core.Instance; import weka.core.Instances; /** * Generates a little ARFF file with different attribute types. * * @author FracPete */ public class AttTest { public static void main(String[] args) throws Exception { FastVector atts; FastVector attsRel; FastVector attVals; FastVector attValsRel; Instances data; Instances dataRel; double[] vals; double[] valsRel; int i; // 1. set up attributes atts = new FastVector(); // - numeric atts.addElement(new Attribute( att1 )); // - nominal attVals = new FastVector(); for (i = 0; i 5; i++) attVals.addElement( val + (i+1)); atts.addElement(new Attribute( att2 , attVals)); // - string atts.addElement(new Attribute( att3 , (FastVector) null)); // - date atts.addElement(new Attribute( att4 , yyyy-MM-dd )); // - relational attsRel = new FastVector(); // -- numeric attsRel.addElement(new Attribute( att5.1 )); // -- nominal attValsRel = new FastVector(); for (i = 0; i 5; i++) attValsRel.addElement( val5. + (i+1)); attsRel.addElement(new Attribute( att5.2 , attValsRel)); dataRel = new Instances( att5 , attsRel, 0); atts.addElement(new Attribute( att5 , dataRel, 0)); // 2. create Instances object data = new Instances( MyRelation , atts, 0); // 3. fill with data // first instance vals = new double[data.numAttributes()]; // - numeric vals[0] = Math.PI; // - nominal vals[1] = attVals.indexOf( val3 ); // - string vals[2] = data.attribute(2).addStringValue( This is a string! ); // - date vals[3] = data.attribute(3).parseDate( 2001-11-09 ); // - relational dataRel = new Instances(data.attribute(4).relation(), 0); // -- first instance valsRel = new double[2]; valsRel[0] = Math.PI + 1; valsRel[1] = attValsRel.indexOf( val5.3 ); dataRel.add(new Instance(1.0, valsRel)); // -- second instance valsRel = new double[2]; valsRel[0] = Math.PI + 2; valsRel[1] = attValsRel.indexOf( val5.2 ); dataRel.add(new Instance(1.0, valsRel)); vals[4] = data.attribute(4).addRelation(dataRel); // add data.add(new Instance(1.0, vals)); // second instance vals = new double[data.numAttributes()]; // important: needs NEW array! // - numeric vals[0] = Math.E; // - nominal vals[1] = attVals.indexOf( val1 ); // - string vals[2] = data.attribute(2).addStringValue( And another one! ); // - date vals[3] = data.attribute(3).parseDate( 2000-12-01 ); // - relational dataRel = new Instances(data.attribute(4).relation(), 0); // -- first instance valsRel = new double[2]; valsRel[0] = Math.E + 1; valsRel[1] = attValsRel.indexOf( val5.4 ); dataRel.add(new Instance(1.0, valsRel)); // -- second instance valsRel = new double[2]; valsRel[0] = Math.E + 2; valsRel[1] = attValsRel.indexOf( val5.1 ); dataRel.add(new Instance(1.0, valsRel)); vals[4] = data.attribute(4).addRelation(dataRel); // add data.add(new Instance(1.0, vals)); // 4. output data System.out.println(data); } } Missing values By default, a new double array will be initialized with 0s. In case you want to be a value missing at a certain position, you have to explicitly set the missing value via the missingValue() method of the weka.core.Instance class. In Weka 3.7.1 Instance is an interface, so missingValue() moved into weka.core.Utils . In case you already have an existing weka.core.Instance object, then you use its setMissing(int) method, which sets a missing value at the given position. Here are examples, which set the third attribute to missing: double array: double[] vals = ... // from somewhere, e.g., from AttTest.java example vals[2] = Instance.missingValue(); // or ... = Utils.missingValue() for Weka 3.7.1 weka.core.Instance object: double[] vals = ... // from somewhere, e.g., from AttTest.java example Instance inst = new Instance(1.0, vals); inst.setMissing(2); Downloads AttTest.java ( stable , developer ) - the above class See also [[Save Instances to an ARFF File]] - if you want to save the data to a file instead of printing it to stdout [[Adding attributes to a dataset]] - shows how to add attributes to an existing dataset ARFF format","title":" Creating an ARFF file"},{"location":"creating_arff_file/#missing-values","text":"By default, a new double array will be initialized with 0s. In case you want to be a value missing at a certain position, you have to explicitly set the missing value via the missingValue() method of the weka.core.Instance class. In Weka 3.7.1 Instance is an interface, so missingValue() moved into weka.core.Utils . In case you already have an existing weka.core.Instance object, then you use its setMissing(int) method, which sets a missing value at the given position. Here are examples, which set the third attribute to missing: double array: double[] vals = ... // from somewhere, e.g., from AttTest.java example vals[2] = Instance.missingValue(); // or ... = Utils.missingValue() for Weka 3.7.1 weka.core.Instance object: double[] vals = ... // from somewhere, e.g., from AttTest.java example Instance inst = new Instance(1.0, vals); inst.setMissing(2);","title":"Missing values"},{"location":"creating_arff_file/#downloads","text":"AttTest.java ( stable , developer ) - the above class","title":"Downloads"},{"location":"creating_arff_file/#see-also","text":"[[Save Instances to an ARFF File]] - if you want to save the data to a file instead of printing it to stdout [[Adding attributes to a dataset]] - shows how to add attributes to an existing dataset ARFF format","title":"See also"},{"location":"creating_instances/","text":"see Creating an ARFF file","title":" Creating Instances on-the-fly"},{"location":"databases/","text":"CLASSPATH See the [[CLASSPATH]] article for how to set up your CLASSPATH environment variable, in order to make the JDBC driver available for Weka. Configuration files Thanks to JDBC it is easy to connect to Databases that provide a JDBC driver. Responsible for the setup is the following properties file, located in the weka.experiment package: DatabaseUtils.props You can get this properties file from the weka.jar or weka-src.jar jar-archive, both part of a normal Weka release. If you open up one of those files, you'll find the properties file in the sub-folder weka/experiment . Weka comes with example files for a wide range of databases: DatabaseUtils.props.hsql - HSQLDB DatabaseUtils.props.msaccess - MS Access (see the Windows Databases article for more information) DatabaseUtils.props.mssqlserver - MS SQL Server 2000 DatabaseUtils.props.mssqlserver2005 - MS SQL Server 2005 DatabaseUtils.props.mysql - MySQL DatabaseUtils.props.odbc - ODBC access via Sun's ODBC/JDBC bridge, e.g., for MS Sql Server (see the Windows Databases article for more information) DatabaseUtils.props.oracle - Oracle 10g DatabaseUtils.props.postgresql - PostgreSQL 7.4 DatabaseUtils.props.sqlite3 - sqlite 3.x The easiest way is just to place the extracted properties file into your HOME directory. For more information on how property files are processed, check out [[Properties File|this]] article. Note: Weka only looks for the DatabaseUtils.props file. If you take one of the example files listed above, you need to rename it first. Setup Under normal circumstances you only have to edit the following two properties: jdbcDriver jdbcURL Driver jdbcDriver is the classname of the JDBC driver, necessary to connect to your database, e.g.: HSQLDB - org.hsqldb.jdbcDriver MS SQL Server 2000 (Desktop Edition) - com.microsoft.jdbc.sqlserver.SQLServerDriver MS SQL Server 2005 - com.microsoft.sqlserver.jdbc.SQLServerDriver MySQL - org.gjt.mm.mysql.Driver (or com.mysql.jdbc.Driver ) ODBC - part of Sun's JDKs/JREs, no external driver necessary - sun.jdbc.odbc.JdbcOdbcDriver Oracle - oracle.jdbc.driver.OracleDriver PostgreSQL - org.postgresql.Driver sqlite 3.x - org.sqlite.JDBC URL jdbcURL specifies the JDBC URL pointing to your database (can be still changed in the Experimenter/Explorer), e.g. for the database MyDatabase on the server server.my.domain : HSQLDB - jdbc:hsqldb:hsql://server.my.domain/MyDatabase MS SQL Server 2000 (Desktop Edition) - jdbc:microsoft:sqlserver://server.my.comain:1433 Note: if you add ;databasename=*db-name* you can connect to a different database than the default one, e.g., MyDatabase MS SQL Server 2005 - jdbc:sqlserver://server.my.domain:1433 MySQL - jdbc:mysql://server.my.domain:3306/MyDatabase ODBC - jdbc:odbc:DSN_name (replace DSN_name with the DSN that you want to use) Oracle (thin driver) - jdbc:oracle:thin:@server.my.domain:1526:orcl Note: @machineName:port:SID for the Express Edition you can use: jdbc:oracle:thin:@server.my.domain:1521:XE PostgreSQL - jdbc:postgresql://server.my.domain:5432/MyDatabase You can also specify user and password directly in the URL: jdbc:postgresql://server.my.domain:5432/MyDatabase?user= ... password= ... where you have to replace the ... with the correct values sqlite 3.x - jdbc:sqlite:/path/to/database.db (you can access only local files) Missing Datatypes Sometimes (e.g. with MySQL) it can happen that a column type cannot be interpreted. In that case it is necessary to map the name of the column type to the Java type it should be interpreted as. E.g. the MySQL type TEXT is returned as BLOB from the JDBC driver and has to be mapped to String ( 0 represents String - the mappings can be found in the comments of the properties file): BLOB=0 The article [[weka_experiment_DatabaseUtils.props|weka/experiment/DatabaseUtils.props]] contains more details on this topic. Stored Procedures Let's say you're tired of typing the same query over and over again. A good way to shorten that, is to create a stored procedure. PostgreSQL 7.4.x The following example creates a procedure called emplyoee_name that returns the names of all the employees in table employee . Even though it doesn't make much sense to create a stored procedure for this query, nonetheless, it shows how to create and call stored procedures in PostgreSQL. Create CREATE OR REPLACE FUNCTION public.employee_name() RETURNS SETOF text AS 'select name from employee' LANGUAGE 'sql' VOLATILE; SQL statement to call procedure SELECT * FROM employee_name() Retrieve data via InstanceQuery java weka.experiment.InstanceQuery -Q \"SELECT * FROM employee_name()\" -U user -P password Troubleshooting In case you're experiencing problems connecting to your database, check out the mailing list . It is possible that somebody else encountered the same problem as you and you'll find a post containing the solution to your problem. Specific [[MS SQL Server 2000 (Desktop Engine)#Troubleshooting|MS SQL Server 2000]] Troubleshooting MS SQL Server 2005: TCP/IP is not enabled for SQL Server, or the server or port number specified is incorrect.Verify that SQL Server is listening with TCP/IP on the specified server and port. This might be reported with an exception similar to: \"The login has failed. The TCP/IP connection to the host has failed.\" This indicates one of the following: SQL Server is installed but TCP/IP has not been installed as a network protocol for SQL Server by using the SQL Server Network Utility for SQL Server 2000, or the SQL Server Configuration Manager for SQL Server 2005 TCP/IP is installed as a SQL Server protocol, but it is not listening on the port specified in the JDBC connection URL. The default port is 1433. The port that is used by the server has not been opened in the firewall The Added driver: ... output on the commandline does not mean that the actual class was found, but only that Weka will attempt to load the class later on in order to establish a database connection. The error message No suitable driver can be caused by the following: The JDBC driver you are attempting to load is not in the [[CLASSPATH]] (Note: using -jar in the java commandline overwrites the CLASSPATH environment variable!). Open the SimpleCLI, run the command java weka.core.SystemInfo and check whether the property java.class.path lists your database jar. If not correct your [[CLASSPATH]] or the Java call you start Weka with. The JDBC driver class is misspelled in the jdbcDriver property or you have multiple entries of jdbcDriver ([[properties file]]s need unique keys!) The jdbcURL property has a spelling error and tries to use a non-existing protocol or you listed it multiple times, which doesn't work either (remember, [[properties file]]s need unique keys!) See also [[weka_experiment_DatabaseUtils.props|weka/experiment/DatabaseUtils.props]] [[Properties File]] [[CLASSPATH]] Links HSQLDB homepage IBM Cloudscape homepage Microsoft SQL Server SQL Server 2000 (Desktop Engine) SQL Server 2000 JDBC Driver SP 3 SQL Server 2005 JDBC Driver MySQL homepage JDBC driver Oracle homepage JDBC driver JDBC FAQ PostgreSQL homepage JDBC driver sqlite homepage JDBC driver Weka Mailing list","title":" Databases"},{"location":"databases/#classpath","text":"See the [[CLASSPATH]] article for how to set up your CLASSPATH environment variable, in order to make the JDBC driver available for Weka.","title":"CLASSPATH"},{"location":"databases/#configuration-files","text":"Thanks to JDBC it is easy to connect to Databases that provide a JDBC driver. Responsible for the setup is the following properties file, located in the weka.experiment package: DatabaseUtils.props You can get this properties file from the weka.jar or weka-src.jar jar-archive, both part of a normal Weka release. If you open up one of those files, you'll find the properties file in the sub-folder weka/experiment . Weka comes with example files for a wide range of databases: DatabaseUtils.props.hsql - HSQLDB DatabaseUtils.props.msaccess - MS Access (see the Windows Databases article for more information) DatabaseUtils.props.mssqlserver - MS SQL Server 2000 DatabaseUtils.props.mssqlserver2005 - MS SQL Server 2005 DatabaseUtils.props.mysql - MySQL DatabaseUtils.props.odbc - ODBC access via Sun's ODBC/JDBC bridge, e.g., for MS Sql Server (see the Windows Databases article for more information) DatabaseUtils.props.oracle - Oracle 10g DatabaseUtils.props.postgresql - PostgreSQL 7.4 DatabaseUtils.props.sqlite3 - sqlite 3.x The easiest way is just to place the extracted properties file into your HOME directory. For more information on how property files are processed, check out [[Properties File|this]] article. Note: Weka only looks for the DatabaseUtils.props file. If you take one of the example files listed above, you need to rename it first.","title":"Configuration files"},{"location":"databases/#setup","text":"Under normal circumstances you only have to edit the following two properties: jdbcDriver jdbcURL","title":"Setup"},{"location":"databases/#driver","text":"jdbcDriver is the classname of the JDBC driver, necessary to connect to your database, e.g.: HSQLDB - org.hsqldb.jdbcDriver MS SQL Server 2000 (Desktop Edition) - com.microsoft.jdbc.sqlserver.SQLServerDriver MS SQL Server 2005 - com.microsoft.sqlserver.jdbc.SQLServerDriver MySQL - org.gjt.mm.mysql.Driver (or com.mysql.jdbc.Driver ) ODBC - part of Sun's JDKs/JREs, no external driver necessary - sun.jdbc.odbc.JdbcOdbcDriver Oracle - oracle.jdbc.driver.OracleDriver PostgreSQL - org.postgresql.Driver sqlite 3.x - org.sqlite.JDBC","title":"Driver"},{"location":"databases/#url","text":"jdbcURL specifies the JDBC URL pointing to your database (can be still changed in the Experimenter/Explorer), e.g. for the database MyDatabase on the server server.my.domain : HSQLDB - jdbc:hsqldb:hsql://server.my.domain/MyDatabase MS SQL Server 2000 (Desktop Edition) - jdbc:microsoft:sqlserver://server.my.comain:1433 Note: if you add ;databasename=*db-name* you can connect to a different database than the default one, e.g., MyDatabase MS SQL Server 2005 - jdbc:sqlserver://server.my.domain:1433 MySQL - jdbc:mysql://server.my.domain:3306/MyDatabase ODBC - jdbc:odbc:DSN_name (replace DSN_name with the DSN that you want to use) Oracle (thin driver) - jdbc:oracle:thin:@server.my.domain:1526:orcl Note: @machineName:port:SID for the Express Edition you can use: jdbc:oracle:thin:@server.my.domain:1521:XE PostgreSQL - jdbc:postgresql://server.my.domain:5432/MyDatabase You can also specify user and password directly in the URL: jdbc:postgresql://server.my.domain:5432/MyDatabase?user= ... password= ... where you have to replace the ... with the correct values sqlite 3.x - jdbc:sqlite:/path/to/database.db (you can access only local files)","title":"URL"},{"location":"databases/#missing-datatypes","text":"Sometimes (e.g. with MySQL) it can happen that a column type cannot be interpreted. In that case it is necessary to map the name of the column type to the Java type it should be interpreted as. E.g. the MySQL type TEXT is returned as BLOB from the JDBC driver and has to be mapped to String ( 0 represents String - the mappings can be found in the comments of the properties file): BLOB=0 The article [[weka_experiment_DatabaseUtils.props|weka/experiment/DatabaseUtils.props]] contains more details on this topic.","title":"Missing Datatypes"},{"location":"databases/#stored-procedures","text":"Let's say you're tired of typing the same query over and over again. A good way to shorten that, is to create a stored procedure.","title":"Stored Procedures"},{"location":"databases/#postgresql-74x","text":"The following example creates a procedure called emplyoee_name that returns the names of all the employees in table employee . Even though it doesn't make much sense to create a stored procedure for this query, nonetheless, it shows how to create and call stored procedures in PostgreSQL. Create CREATE OR REPLACE FUNCTION public.employee_name() RETURNS SETOF text AS 'select name from employee' LANGUAGE 'sql' VOLATILE; SQL statement to call procedure SELECT * FROM employee_name() Retrieve data via InstanceQuery java weka.experiment.InstanceQuery -Q \"SELECT * FROM employee_name()\" -U user -P password","title":"PostgreSQL 7.4.x"},{"location":"databases/#troubleshooting","text":"In case you're experiencing problems connecting to your database, check out the mailing list . It is possible that somebody else encountered the same problem as you and you'll find a post containing the solution to your problem. Specific [[MS SQL Server 2000 (Desktop Engine)#Troubleshooting|MS SQL Server 2000]] Troubleshooting MS SQL Server 2005: TCP/IP is not enabled for SQL Server, or the server or port number specified is incorrect.Verify that SQL Server is listening with TCP/IP on the specified server and port. This might be reported with an exception similar to: \"The login has failed. The TCP/IP connection to the host has failed.\" This indicates one of the following: SQL Server is installed but TCP/IP has not been installed as a network protocol for SQL Server by using the SQL Server Network Utility for SQL Server 2000, or the SQL Server Configuration Manager for SQL Server 2005 TCP/IP is installed as a SQL Server protocol, but it is not listening on the port specified in the JDBC connection URL. The default port is 1433. The port that is used by the server has not been opened in the firewall The Added driver: ... output on the commandline does not mean that the actual class was found, but only that Weka will attempt to load the class later on in order to establish a database connection. The error message No suitable driver can be caused by the following: The JDBC driver you are attempting to load is not in the [[CLASSPATH]] (Note: using -jar in the java commandline overwrites the CLASSPATH environment variable!). Open the SimpleCLI, run the command java weka.core.SystemInfo and check whether the property java.class.path lists your database jar. If not correct your [[CLASSPATH]] or the Java call you start Weka with. The JDBC driver class is misspelled in the jdbcDriver property or you have multiple entries of jdbcDriver ([[properties file]]s need unique keys!) The jdbcURL property has a spelling error and tries to use a non-existing protocol or you listed it multiple times, which doesn't work either (remember, [[properties file]]s need unique keys!)","title":"Troubleshooting"},{"location":"databases/#see-also","text":"[[weka_experiment_DatabaseUtils.props|weka/experiment/DatabaseUtils.props]] [[Properties File]] [[CLASSPATH]]","title":"See also"},{"location":"databases/#links","text":"HSQLDB homepage IBM Cloudscape homepage Microsoft SQL Server SQL Server 2000 (Desktop Engine) SQL Server 2000 JDBC Driver SP 3 SQL Server 2005 JDBC Driver MySQL homepage JDBC driver Oracle homepage JDBC driver JDBC FAQ PostgreSQL homepage JDBC driver sqlite homepage JDBC driver Weka Mailing list","title":"Links"},{"location":"extending_weka/","text":"The following articles describe how you can extend Weka: Writing a new Filter Writing a new Classifier","title":"Extending Weka"},{"location":"faq/","text":"General What are the principal release branches of Weka? Where can I get old versions of WEKA? How do I get the latest bugfixes? Can I check my CLASSPATH from within WEKA? Where is my home directory located? Can I check how much memory is available for WEKA? Can I use WEKA in commercial applications? Basic usage Can I use CSV files? How do I perform CSV file conversion? How do I divide a dataset into training and test set? How do I generate compatible train and test sets that get processed with a filter? How do I perform attribute selection? How do I perform clustering? Where do I find visualization of classifiers, etc.? How do I perform text classification? How can I perform multi-instance learning in WEKA? How do I perform cost-sensitive classification? How do I make predictions with a trained model? Why am I missing certain nominal or string values from sparse instances? Can I use WEKA for time series analysis? Does WEKA support multi-label classification? How do I perform one-class classification? Can I make a screenshot of a plot or graph directly in WEKA? How do I use the package manager? What do I do if the package manager does not start? Advanced usage How can I track instances in WEKA? How do I use ID attributes? How do I connect to a database? How do I use WEKA from command line? Can I tune the parameters of a classifier? How do I generate Learning curves? Where can I find information regarding ROC curves? I have unbalanced data - now what? Can I run an experiment using clusterers in the Experimenter? How can I use transactional data in Weka? How can I use Weka with Matlab or Octave? Customizing Weka Can I change the colors (background, axes, etc.) of the plots in WEKA? How do I add a new classifier, filter, kernel, etc Using third-party tools How do I use libsvm in WEKA? The snowball stemmers don't work, what am I doing wrong? Developing with WEKA Where can I get WEKA's source code? How do I compile WEKA? What is Subversion and what do I need to do to access it? How do I use WEKA's classes in my own code? How do I write a new classifier or filter? Can I compile WEKA into native code? Can I use WEKA from C#? Can I use WEKA from Python? Can I use WEKA from Groovy? Serialization is nice, but what about generating actual Java code from WEKA classes? How are packages structured for the package management system? Pluggable evaluation metrics for classification/regression How can I contribute to WEKA? Windows How do I modify the CLASSPATH? How do I modify the RunWeka.bat file? Can I process UTF-8 datasets or files? How do I run the Windows Weka installer in silent mode? Troubleshooting I have Weka download problems - what's going wrong? My ARFF file doesn't load - why? What does nominal value not declared in header, read Token[X], line Y mean? How do I get rid of this OutOfMemoryException? How do I deal with a StackOverflowError? Why do I get the error message 'training and test set are not compatible'? Couldn't read from database: unknown data type Trying to add JDBC driver: ... - Error, not in CLASSPATH? I cannot process large datasets - any ideas? See Troubleshooting article for more troubleshooting.","title":"FAQ"},{"location":"faq/#general","text":"What are the principal release branches of Weka? Where can I get old versions of WEKA? How do I get the latest bugfixes? Can I check my CLASSPATH from within WEKA? Where is my home directory located? Can I check how much memory is available for WEKA? Can I use WEKA in commercial applications?","title":"General"},{"location":"faq/#basic-usage","text":"Can I use CSV files? How do I perform CSV file conversion? How do I divide a dataset into training and test set? How do I generate compatible train and test sets that get processed with a filter? How do I perform attribute selection? How do I perform clustering? Where do I find visualization of classifiers, etc.? How do I perform text classification? How can I perform multi-instance learning in WEKA? How do I perform cost-sensitive classification? How do I make predictions with a trained model? Why am I missing certain nominal or string values from sparse instances? Can I use WEKA for time series analysis? Does WEKA support multi-label classification? How do I perform one-class classification? Can I make a screenshot of a plot or graph directly in WEKA? How do I use the package manager? What do I do if the package manager does not start?","title":"Basic usage"},{"location":"faq/#advanced-usage","text":"How can I track instances in WEKA? How do I use ID attributes? How do I connect to a database? How do I use WEKA from command line? Can I tune the parameters of a classifier? How do I generate Learning curves? Where can I find information regarding ROC curves? I have unbalanced data - now what? Can I run an experiment using clusterers in the Experimenter? How can I use transactional data in Weka? How can I use Weka with Matlab or Octave?","title":"Advanced usage"},{"location":"faq/#customizing-weka","text":"Can I change the colors (background, axes, etc.) of the plots in WEKA? How do I add a new classifier, filter, kernel, etc","title":"Customizing Weka"},{"location":"faq/#using-third-party-tools","text":"How do I use libsvm in WEKA? The snowball stemmers don't work, what am I doing wrong?","title":"Using third-party tools"},{"location":"faq/#developing-with-weka","text":"Where can I get WEKA's source code? How do I compile WEKA? What is Subversion and what do I need to do to access it? How do I use WEKA's classes in my own code? How do I write a new classifier or filter? Can I compile WEKA into native code? Can I use WEKA from C#? Can I use WEKA from Python? Can I use WEKA from Groovy? Serialization is nice, but what about generating actual Java code from WEKA classes? How are packages structured for the package management system? Pluggable evaluation metrics for classification/regression How can I contribute to WEKA?","title":"Developing with WEKA"},{"location":"faq/#windows","text":"How do I modify the CLASSPATH? How do I modify the RunWeka.bat file? Can I process UTF-8 datasets or files? How do I run the Windows Weka installer in silent mode?","title":"Windows"},{"location":"faq/#troubleshooting","text":"I have Weka download problems - what's going wrong? My ARFF file doesn't load - why? What does nominal value not declared in header, read Token[X], line Y mean? How do I get rid of this OutOfMemoryException? How do I deal with a StackOverflowError? Why do I get the error message 'training and test set are not compatible'? Couldn't read from database: unknown data type Trying to add JDBC driver: ... - Error, not in CLASSPATH? I cannot process large datasets - any ideas? See Troubleshooting article for more troubleshooting.","title":"Troubleshooting"},{"location":"generating_cv_folds/","text":"You have two choices of generating cross-validation folds: Filter approach - uses a bash script to generate the train/test pairs beforehand Java approach - to be used from within your own Java code, creates train/test pairs on the fly","title":" Generating cross-validation folds"},{"location":"generating_cv_folds_filter/","text":"The filter RemoveFolds (package weka.filters.unsupervised.instance ) can be used to generate the train/test splits used in cross-validation (for stratified folds, use weka.filters.supervised.instance.StratifiedRemoveFolds ). The filter has to be used twice for each train/test split, first to generate the train set and then to obtain the test set. Since this is rather cumbersome by hand, one can also put this into a bash script: #!/bin/bash # # expects the weka.jar as first parameter and the datasets to work on as # second parameter. # # FracPete, 2007-04-10 if [ ! $# -eq 2 ] then echo echo usage: folds.sh weka.jar dataset echo exit 1 fi JAR=$1 DATASET=$2 FOLDS=10 FILTER=weka.filters.unsupervised.instance.RemoveFolds SEED=1 for ((i = 1; i = $FOLDS; i++)) do echo Generating pair $i/$FOLDS... OUTFILE=`echo $DATASET | sed s/ \\.arff //g` # train set java -cp $JAR $FILTER -V -N $FOLDS -F $i -S $SEED -i $DATASET -o $OUTFILE-train-$i-of-$FOLDS.arff # test set java -cp $JAR $FILTER -N $FOLDS -F $i -S $SEED -i $DATASET -o $OUTFILE-test-$i-of-$FOLDS.arff done The script expects two parameters: the weka.jar (or the path to the Weka classes) the dataset to generate the train/test pairs from Example: ./folds.sh /some/where/weka.jar /some/where/else/dataset.arff This example will create the train/test splits for a 10-fold cross-validation at the same location as the original dataset, i.e., in the directory /some/where/else/ . Downloads folds.sh","title":" Generating cross-validation folds (Filter approach)"},{"location":"generating_cv_folds_filter/#downloads","text":"folds.sh","title":"Downloads"},{"location":"generating_cv_folds_java/","text":"This article describes how to generate train/test splits for cross-validation using the Weka API directly. The following variables are given: Instances data = ...; // contains the full dataset we wann create train/test sets from int seed = ...; // the seed for randomizing the data int folds = ...; // the number of folds to generate, =2 Randomize the data First, randomize your data: Random rand = new Random(seed); // create seeded number generator randData = new Instances(data); // create copy of original data randData.randomize(rand); // randomize data with number generator In case your data has a nominal class and you wanna perform stratified cross-validation: randData.stratify(folds); Generate the folds Single run Next thing that we have to do is creating the train and the test set: for (int n = 0; n folds; n++) { Instances train = randData.trainCV(folds, n, rand); Instances test = randData.testCV(folds, n); // further processing, classification, etc. ... } Note: the above code is used by the weka.filters.supervised.instance.StratifiedRemoveFolds filter the weka.classifiers.Evaluation class and the Explorer/Experimenter would use this method for obtaining the train set: [[code format=\"java\"]] Instances train = randData.trainCV(folds, n, rand); [[code]] Multiple runs The example above only performs one run of a cross-validation. In case you want to run 10 runs of 10-fold cross-validation, use the following loop: Instances data = ...; // our dataset again, obtained from somewhere int runs = 10; for (int i = 0; i runs; i++) { seed = i+1; // every run gets a new, but defined seed value // see: randomize the data ... // see: generate the folds ... } See also Use Weka in your Java code - for general use of the Weka API Downloads CrossValidationSingleRun.java ( stable , developer ) - simulates a single run of 10-fold cross-validation CrossValidationSingleRunVariant.java ( stable , developer ) - simulates a single run of 10-fold cross-validation, but outputs the confusion matrices for each single train/test pair as well. CrossValidationMultipleRuns.java ( stable , developer ) - simulates 10 runs of 10-fold cross-validation CrossValidationAddPrediction.java ( stable , developer ) - simulates a single run of 10-fold cross-validation, but also adds the classification/distribution/error flag to the test data (uses the AddClassification filter)","title":" Generating cross-validation folds (Java approach)"},{"location":"generating_cv_folds_java/#randomize-the-data","text":"First, randomize your data: Random rand = new Random(seed); // create seeded number generator randData = new Instances(data); // create copy of original data randData.randomize(rand); // randomize data with number generator In case your data has a nominal class and you wanna perform stratified cross-validation: randData.stratify(folds);","title":"Randomize the data"},{"location":"generating_cv_folds_java/#generate-the-folds","text":"","title":"Generate the folds"},{"location":"generating_cv_folds_java/#single-run","text":"Next thing that we have to do is creating the train and the test set: for (int n = 0; n folds; n++) { Instances train = randData.trainCV(folds, n, rand); Instances test = randData.testCV(folds, n); // further processing, classification, etc. ... } Note: the above code is used by the weka.filters.supervised.instance.StratifiedRemoveFolds filter the weka.classifiers.Evaluation class and the Explorer/Experimenter would use this method for obtaining the train set: [[code format=\"java\"]] Instances train = randData.trainCV(folds, n, rand); [[code]]","title":"Single run"},{"location":"generating_cv_folds_java/#multiple-runs","text":"The example above only performs one run of a cross-validation. In case you want to run 10 runs of 10-fold cross-validation, use the following loop: Instances data = ...; // our dataset again, obtained from somewhere int runs = 10; for (int i = 0; i runs; i++) { seed = i+1; // every run gets a new, but defined seed value // see: randomize the data ... // see: generate the folds ... }","title":"Multiple runs"},{"location":"generating_cv_folds_java/#see-also","text":"Use Weka in your Java code - for general use of the Weka API","title":"See also"},{"location":"generating_cv_folds_java/#downloads","text":"CrossValidationSingleRun.java ( stable , developer ) - simulates a single run of 10-fold cross-validation CrossValidationSingleRunVariant.java ( stable , developer ) - simulates a single run of 10-fold cross-validation, but outputs the confusion matrices for each single train/test pair as well. CrossValidationMultipleRuns.java ( stable , developer ) - simulates 10 runs of 10-fold cross-validation CrossValidationAddPrediction.java ( stable , developer ) - simulates a single run of 10-fold cross-validation, but also adds the classification/distribution/error flag to the test data (uses the AddClassification filter)","title":"Downloads"},{"location":"generating_roc_curve/","text":"The following little Java class trains a NaiveBayes classifier with a dataset provided by the user and displays the ROC curve for the first class label. Source code: import java.awt.*; import java.io.*; import java.util.*; import javax.swing.*; import weka.core.*; import weka.classifiers.*; import weka.classifiers.bayes.NaiveBayes; import weka.classifiers.evaluation.Evaluation; import weka.classifiers.evaluation.ThresholdCurve; import weka.gui.visualize.*; /** * Generates and displays a ROC curve from a dataset. Uses a default * NaiveBayes to generate the ROC data. * * @author FracPete */ public class GenerateROC { /** * takes one argument: dataset in ARFF format (expects class to * be last attribute) */ public static void main(String[] args) throws Exception { // load data Instances data = new Instances( new BufferedReader( new FileReader(args[0]))); data.setClassIndex(data.numAttributes() - 1); // train classifier Classifier cl = new NaiveBayes(); Evaluation eval = new Evaluation(data); eval.crossValidateModel(cl, data, 10, new Random(1)); // generate curve ThresholdCurve tc = new ThresholdCurve(); int classIndex = 0; Instances result = tc.getCurve(eval.predictions(), classIndex); // plot curve ThresholdVisualizePanel vmc = new ThresholdVisualizePanel(); vmc.setROCString( (Area under ROC = + Utils.doubleToString(tc.getROCArea(result), 4) + ) ); vmc.setName(result.relationName()); PlotData2D tempd = new PlotData2D(result); tempd.setPlotName(result.relationName()); tempd.addInstanceNumberAttribute(); // specify which points are connected boolean[] cp = new boolean[result.numInstances()]; for (int n = 1; n cp.length; n++) cp[n] = true; tempd.setConnectPoints(cp); // add plot vmc.addPlot(tempd); // display curve String plotName = vmc.getName(); final javax.swing.JFrame jf = new javax.swing.JFrame( Weka Classifier Visualize: +plotName); jf.setSize(500,400); jf.getContentPane().setLayout(new BorderLayout()); jf.getContentPane().add(vmc, BorderLayout.CENTER); jf.addWindowListener(new java.awt.event.WindowAdapter() { public void windowClosing(java.awt.event.WindowEvent e) { jf.dispose(); } }); jf.setVisible(true); } } See also ROC curves Visualizing ROC curve Plotting multiple ROC curves Downloads GenerateROC.java ( stable , developer )","title":" Generating ROC curve"},{"location":"generating_roc_curve/#see-also","text":"ROC curves Visualizing ROC curve Plotting multiple ROC curves","title":"See also"},{"location":"generating_roc_curve/#downloads","text":"GenerateROC.java ( stable , developer )","title":"Downloads"},{"location":"get_latest_bugfixes/","text":"Weka is actively developed, that means that bugs are fixed and new functionality is added (only to the developer version) all the time. Every now and then (about every 6-12 months), when there was a sufficiently large number of improvements or fixes, a release is made and uploaded to Sourceforget.net . If you don't want to wait that long, you have two options: Get the latest source code from Subversion and compile it yourself. See the following articles for more information obtaining the source code from Subversion , either book or developer version compiling the source code Download a snapshot from the download section of the Weka homepage . Snapshots for book and developer version are generated automatically every night, based on the current Subversion code. The ZIP files have the same content as a release, i.e., compiled classes (= weka.jar), source code (= weka-src.jar), Javadoc and other documentation. Note: compare the timestamp of the Weka Mailing List post that reports a bugfix with the one of the snapshot to make sure the bugfix is already included in the snapshot.","title":" Get latest Bugfixes"},{"location":"learning_resources/","text":"Videos Youtube channel of Data Mining with Weka MOOCs Tutorials Learn Data Science Online MOOCs Data Mining with Weka More Data Mining with Weka Advanced Data Mining with Weka","title":"Learning resources"},{"location":"learning_resources/#videos","text":"Youtube channel of Data Mining with Weka MOOCs","title":"Videos"},{"location":"learning_resources/#tutorials","text":"Learn Data Science Online","title":"Tutorials"},{"location":"learning_resources/#moocs","text":"Data Mining with Weka More Data Mining with Weka Advanced Data Mining with Weka","title":"MOOCs"},{"location":"mailing_list/","text":"The WEKA Mailing list can be found here: List for subscribing/unsubscribing to the list Archives ( Mirror 1 , Mirror 2 ) for searching previous posted messages Before posting, please read the Mailing List Etiquette .","title":"Mailing list"},{"location":"maven/","text":"Maven is another build tool. But unlike Ant , it is a more high-level tool. Though its configuration file, pom.xml is written in XML as well, Maven uses a different approach to the build process. In Ant, you tell it where to find Java classes for compilation, what libraries to compile against, where to put the compiled ones and then how to combine them into a jar. With Maven, you only specify dependent libraries, a compile and a jar plugin and maybe tweak the options a bit. For this to work, Maven enforces a strict directory structure (though you can tweak that, if you need to). So why another build tool? Whereas Ant scripts quite often create a fat jar , i.e., a jar that contains not only the project's code, but also the contain of libraries the code was compiled against. Handy if you only want to have a single jar. However, this is a nightmare, if you need to update a single library, but all you have is a single, enormous jar. Maven handles dependencies automatically , relying on libraries (they call them artifacts) to be publicly available, e.g., on Maven Central . It allows you to use newer versions of libraries than defined by the dependent libraries (e.g., critical bug fixes), without having to modify any jars manually. Though Maven can also generate far jar files, it is not considered good practice, as it defeats Maven's automatic version resolution. In order to make Weka, and most of its packages, available to a wider audience (e.g., other software developers), we also publish on Maven Central. Compiling For compiling Weka, you would issue a command like this (in the same directory as pom.xml ): mvn clean install If you don't want the tests to run, use this: mvn clean install -DskipTests=true","title":" Maven"},{"location":"maven/#so-why-another-build-tool","text":"Whereas Ant scripts quite often create a fat jar , i.e., a jar that contains not only the project's code, but also the contain of libraries the code was compiled against. Handy if you only want to have a single jar. However, this is a nightmare, if you need to update a single library, but all you have is a single, enormous jar. Maven handles dependencies automatically , relying on libraries (they call them artifacts) to be publicly available, e.g., on Maven Central . It allows you to use newer versions of libraries than defined by the dependent libraries (e.g., critical bug fixes), without having to modify any jars manually. Though Maven can also generate far jar files, it is not considered good practice, as it defeats Maven's automatic version resolution. In order to make Weka, and most of its packages, available to a wider audience (e.g., other software developers), we also publish on Maven Central.","title":"So why another build tool?"},{"location":"maven/#compiling","text":"For compiling Weka, you would issue a command like this (in the same directory as pom.xml ): mvn clean install If you don't want the tests to run, use this: mvn clean install -DskipTests=true","title":"Compiling"},{"location":"not_so_faq/","text":"Associators How do I use the associator GeneralizedSequentialPatterns? Classifiers What do those numbers mean in a J48 tree?","title":"Not so FAQ"},{"location":"not_so_faq/#associators","text":"How do I use the associator GeneralizedSequentialPatterns?","title":"Associators"},{"location":"not_so_faq/#classifiers","text":"What do those numbers mean in a J48 tree?","title":"Classifiers"},{"location":"plotting_multiple_roc_curves/","text":"KnowledgeFlow Description Comparing different classifiers on one dataset can also be done via ROC curves , not just via Accuracy, Correlation coefficient etc. In the Explorer it is not possible to do that for several classifiers, this is only possible in the KnowledgeFlow . This is the basic setup (based on a Wekalist post): ArffLoader ---dataSet--- ClassAssigner ---dataSet--- ClassValuePicker (the class label you want the plot for) ---dataSet--- CrossValidationFoldMaker ---trainingSet/testSet (i.e. BOTH connections)--- Classifier of your choice ---batchClassifier--- ClassifierPerformanceEvaluator ---thresholdData--- ModelPerformanceChart This setup can be easily extended to host several classifiers, which illustrates the Plotting_multiple_roc.kfml example, containing J48 and RandomForest as classifiers. Java Description The VisualizeMultipleROC.java class lets you display several ROC curves in a single plot. The data it is using for display is from previously saved ROC curves. This example class is just a modified version of the VisualizeROC.java class, which displays only a single ROC curve (see Visualizing ROC curve article). See also Wikipedia article on ROC curve Visualizing ROC curve ROC curves Downloads Plotting_multiple_roc.kfml - Example KnowledgeFlow layout file VisualizeMultipleROC.java ( stable , developer )","title":" Plotting multiple ROC curves"},{"location":"plotting_multiple_roc_curves/#knowledgeflow","text":"","title":"KnowledgeFlow"},{"location":"plotting_multiple_roc_curves/#description","text":"Comparing different classifiers on one dataset can also be done via ROC curves , not just via Accuracy, Correlation coefficient etc. In the Explorer it is not possible to do that for several classifiers, this is only possible in the KnowledgeFlow . This is the basic setup (based on a Wekalist post): ArffLoader ---dataSet--- ClassAssigner ---dataSet--- ClassValuePicker (the class label you want the plot for) ---dataSet--- CrossValidationFoldMaker ---trainingSet/testSet (i.e. BOTH connections)--- Classifier of your choice ---batchClassifier--- ClassifierPerformanceEvaluator ---thresholdData--- ModelPerformanceChart This setup can be easily extended to host several classifiers, which illustrates the Plotting_multiple_roc.kfml example, containing J48 and RandomForest as classifiers.","title":"Description"},{"location":"plotting_multiple_roc_curves/#java","text":"","title":"Java"},{"location":"plotting_multiple_roc_curves/#description_1","text":"The VisualizeMultipleROC.java class lets you display several ROC curves in a single plot. The data it is using for display is from previously saved ROC curves. This example class is just a modified version of the VisualizeROC.java class, which displays only a single ROC curve (see Visualizing ROC curve article).","title":"Description"},{"location":"plotting_multiple_roc_curves/#see-also","text":"Wikipedia article on ROC curve Visualizing ROC curve ROC curves","title":"See also"},{"location":"plotting_multiple_roc_curves/#downloads","text":"Plotting_multiple_roc.kfml - Example KnowledgeFlow layout file VisualizeMultipleROC.java ( stable , developer )","title":"Downloads"},{"location":"roc_curves/","text":"General Weka just varies the threshold on the class probability estimates in each case. What does that mean? In case of a classifier that does not return proper class probabilities (like SMO with the -M option, or IB1), you will end up with only two points in the curve. Using a classifier that returns proper distributions, like BayesNet, J48 or SMO with -M option for building logistic models, you will get nice curves. The class used for calculating the ROC and also the AUC (= area under the curve) is weka.classifiers.evaluation.ThresholdCurve . Commandline You can output the data for the ROC curves with the following options: -threshold-file file The file to save the threshold data to. The format is determined by the extensions, e.g., '.arff' for ARFF format or '.csv' for CSV. -threshold-label label The class label to determine the threshold data for (default is the first label) Here's an example for using J48 on the UCI dataset anneal , generating the ROC curve file for label U from a 10-fold cross-validation: java weka.classifiers.trees.J48 -t /some/where/anneal.arff \\ -threshold-file anneal_roc_U.arff -threshold-label U Explorer Generating The Weka Explorer enables you to plot the ROC ( Receiver operating characteristic ) curve for a certain class label of dataset: run a classifier on a dataset right-click in the result list on the result you want to display the curve for select Visualize threshold curve and choose the class label you want the plot for Note: the AUC for this plot is also displayed, just above the actual plot. Saving You can save the ROC curve in two ways: as an ARFF file, containing the data points (can be displayed again) as an image (using Alt+Shift+Left click to bring up a save dialog) Loading A previously saved ROC data file can be displayed in two ways: without the AUC - with the following command java [CLASSPATH|-classpath your-classpath ] weka.gui.visualize.VisualizePanel file with the AUC - needs this source code KnowledgeFlow See Plotting multiple ROC curves . See also Plotting multiple ROC curves Generating ROC curve Links WikiPedia article on ROC curve weka.classifiers.evaluation.ThresholdCurve","title":" ROC Curves"},{"location":"roc_curves/#general","text":"Weka just varies the threshold on the class probability estimates in each case. What does that mean? In case of a classifier that does not return proper class probabilities (like SMO with the -M option, or IB1), you will end up with only two points in the curve. Using a classifier that returns proper distributions, like BayesNet, J48 or SMO with -M option for building logistic models, you will get nice curves. The class used for calculating the ROC and also the AUC (= area under the curve) is weka.classifiers.evaluation.ThresholdCurve .","title":"General"},{"location":"roc_curves/#commandline","text":"You can output the data for the ROC curves with the following options: -threshold-file file The file to save the threshold data to. The format is determined by the extensions, e.g., '.arff' for ARFF format or '.csv' for CSV. -threshold-label label The class label to determine the threshold data for (default is the first label) Here's an example for using J48 on the UCI dataset anneal , generating the ROC curve file for label U from a 10-fold cross-validation: java weka.classifiers.trees.J48 -t /some/where/anneal.arff \\ -threshold-file anneal_roc_U.arff -threshold-label U","title":"Commandline"},{"location":"roc_curves/#explorer","text":"","title":"Explorer"},{"location":"roc_curves/#generating","text":"The Weka Explorer enables you to plot the ROC ( Receiver operating characteristic ) curve for a certain class label of dataset: run a classifier on a dataset right-click in the result list on the result you want to display the curve for select Visualize threshold curve and choose the class label you want the plot for Note: the AUC for this plot is also displayed, just above the actual plot.","title":"Generating"},{"location":"roc_curves/#saving","text":"You can save the ROC curve in two ways: as an ARFF file, containing the data points (can be displayed again) as an image (using Alt+Shift+Left click to bring up a save dialog)","title":"Saving"},{"location":"roc_curves/#loading","text":"A previously saved ROC data file can be displayed in two ways: without the AUC - with the following command java [CLASSPATH|-classpath your-classpath ] weka.gui.visualize.VisualizePanel file with the AUC - needs this source code","title":"Loading"},{"location":"roc_curves/#knowledgeflow","text":"See Plotting multiple ROC curves .","title":"KnowledgeFlow"},{"location":"roc_curves/#see-also","text":"Plotting multiple ROC curves Generating ROC curve","title":"See also"},{"location":"roc_curves/#links","text":"WikiPedia article on ROC curve weka.classifiers.evaluation.ThresholdCurve","title":"Links"},{"location":"save_instances_to_arff/","text":"DataSink The easiest way to save an weka.core.Instances object to a file is by using the weka.core.converters.ConverterUtils.DataSink class. import weka.core.converters.ConverterUtils.DataSink; import weka.core.Instances; Instances dataset = ... String outputFilename = ... try { DataSink.write(outputFilename, dataset); } catch (Exception e) { System.err.println( Failed to save data to: + outputFilename); e.printStackTrace(); } Converter You can use the ArffSaver class ( weka.core.converters.ArffSaver ) for saving a weka.core.Instances object to a file. Here is the snippet : Instances dataSet = ... ArffSaver saver = new ArffSaver(); saver.setInstances(dataSet); saver.setFile(new File( ./data/test.arff )); saver.writeBatch(); Notes: using the converter approach, one can easily swap the ArffSaver with another saver, e.g., the CSVSaver to output the data in a different format. The Weka Examples collection dedicates quite a few examples to the use of converters in the wekaexamples.core.converters package ( stable , developer ) Java I/O You can also save the weka.core.Instances object directly using Java I/O classes: import java.io.BufferedWriter; import java.io.FileWriter; ... Instances dataSet = ... BufferedWriter writer = new BufferedWriter(new FileWriter( ./data/test.arff )); writer.write(dataSet.toString()); writer.flush(); writer.close(); Note: using the toString() of the weka.core.Instances doesn't scale very well for large datasets, since the complete string has to fit into memory. It is best to use a converter, as described in the previous section, which uses an incremental approach for writing the dataset to disk.","title":" Save Instances to an ARFF File"},{"location":"save_instances_to_arff/#datasink","text":"The easiest way to save an weka.core.Instances object to a file is by using the weka.core.converters.ConverterUtils.DataSink class. import weka.core.converters.ConverterUtils.DataSink; import weka.core.Instances; Instances dataset = ... String outputFilename = ... try { DataSink.write(outputFilename, dataset); } catch (Exception e) { System.err.println( Failed to save data to: + outputFilename); e.printStackTrace(); }","title":"DataSink"},{"location":"save_instances_to_arff/#converter","text":"You can use the ArffSaver class ( weka.core.converters.ArffSaver ) for saving a weka.core.Instances object to a file. Here is the snippet : Instances dataSet = ... ArffSaver saver = new ArffSaver(); saver.setInstances(dataSet); saver.setFile(new File( ./data/test.arff )); saver.writeBatch(); Notes: using the converter approach, one can easily swap the ArffSaver with another saver, e.g., the CSVSaver to output the data in a different format. The Weka Examples collection dedicates quite a few examples to the use of converters in the wekaexamples.core.converters package ( stable , developer )","title":"Converter"},{"location":"save_instances_to_arff/#java-io","text":"You can also save the weka.core.Instances object directly using Java I/O classes: import java.io.BufferedWriter; import java.io.FileWriter; ... Instances dataSet = ... BufferedWriter writer = new BufferedWriter(new FileWriter( ./data/test.arff )); writer.write(dataSet.toString()); writer.flush(); writer.close(); Note: using the toString() of the weka.core.Instances doesn't scale very well for large datasets, since the complete string has to fit into memory. It is best to use a converter, as described in the previous section, which uses an incremental approach for writing the dataset to disk.","title":"Java I/O"},{"location":"serialization/","text":"Serialization is the process of saving an object in a persistent form, e.g., on the harddisk as a bytestream. Deserialization is the process in the opposite direction, creating an object from a persistently saved data structure. In Java , an object can be serialized if it imports the java.io.Serializable interface. Members of an object that are not supposed to be serialized, need to be prefixed with the keyword transient . In the following you'll find some Java code snippets for serializing and deserializing a J48 classifier. Of course, serialization is not limited to classifiers. Most schemes in Weka, like clusterers and filters, are also serializable. Serializing Here we create a J48 classifier cls , train it with a dataset /some/where/data.arff , and save the built model to a file /some/where/j48.model . // create J48 Classifier cls = new J48(); // train Instances inst = new Instances( new BufferedReader( new FileReader( /some/where/data.arff ))); inst.setClassIndex(inst.numAttributes() - 1); cls.buildClassifier(inst); // serialize model ObjectOutputStream oos = new ObjectOutputStream( new FileOutputStream( /some/where/j48.model )); oos.writeObject(cls); oos.flush(); oos.close(); If you use the SerializationHelper class, then this shrinks to: // serialize model weka.core.SerializationHelper.write( /some/where/j48.model , cls); Deserializing Here the previously saved model is deserialized as cls and again available for classification. // deserialize model ObjectInputStream ois = new ObjectInputStream( new FileInputStream( /some/where/j48.model )); Classifier cls = (Classifier) ois.readObject(); ois.close(); Or, with the SerializationHelper class: // deserialize model Classifier cls = (Classifier) weka.core.SerializationHelper.read( /some/where/j48.model ); Serialization in Weka The Explorer serializes the classifier and the training header together. This makes it easy to test whether a dataset is compatible with the dataset the classifier was trained with. The commandline option -d of the developer version stores the training header as well. In order to read serialized models that contain the header information as well, you can use the readAll method of the weka.core.SerializationHelper . For serializing models with their datasets, use writeAll . See also Use Weka in your Java code Links Java Serialization Specifications","title":" Serialization"},{"location":"serialization/#serializing","text":"Here we create a J48 classifier cls , train it with a dataset /some/where/data.arff , and save the built model to a file /some/where/j48.model . // create J48 Classifier cls = new J48(); // train Instances inst = new Instances( new BufferedReader( new FileReader( /some/where/data.arff ))); inst.setClassIndex(inst.numAttributes() - 1); cls.buildClassifier(inst); // serialize model ObjectOutputStream oos = new ObjectOutputStream( new FileOutputStream( /some/where/j48.model )); oos.writeObject(cls); oos.flush(); oos.close(); If you use the SerializationHelper class, then this shrinks to: // serialize model weka.core.SerializationHelper.write( /some/where/j48.model , cls);","title":"Serializing"},{"location":"serialization/#deserializing","text":"Here the previously saved model is deserialized as cls and again available for classification. // deserialize model ObjectInputStream ois = new ObjectInputStream( new FileInputStream( /some/where/j48.model )); Classifier cls = (Classifier) ois.readObject(); ois.close(); Or, with the SerializationHelper class: // deserialize model Classifier cls = (Classifier) weka.core.SerializationHelper.read( /some/where/j48.model );","title":"Deserializing"},{"location":"serialization/#serialization-in-weka","text":"The Explorer serializes the classifier and the training header together. This makes it easy to test whether a dataset is compatible with the dataset the classifier was trained with. The commandline option -d of the developer version stores the training header as well. In order to read serialized models that contain the header information as well, you can use the readAll method of the weka.core.SerializationHelper . For serializing models with their datasets, use writeAll .","title":"Serialization in Weka"},{"location":"serialization/#see-also","text":"Use Weka in your Java code","title":"See also"},{"location":"serialization/#links","text":"Java Serialization Specifications","title":"Links"},{"location":"snapshots/","text":"See How to get the latest bugfixes?","title":" Snapsnots"},{"location":"subversion/","text":"General The Weka Subversion repository is accessible and browseable via the following URL: https://svn.cms.waikato.ac.nz/svn/weka/ A Subversion repository has usually the following layout: root | +- trunk | +- tags | +- branches Where trunk contains the main trunk of the development, tags snapshots in time of the repository (e.g., when a new version got released) and branches development branches that forked off the main trunk at some stage (e.g., legacy versions that still get bugfixed). Source code The latest version of the Weka source code can be obtained with this URL: https://svn.cms.waikato.ac.nz/svn/weka/trunk/weka If you want to obtain the source code of the book version, use this URL: https://svn.cms.waikato.ac.nz/svn/weka/branches/stable-3-8/ Specific version Whenever a release of Weka is generated, the repository gets tagged : dev-X-Y-Z the tag for a release of the developer version, e.g., dev-3-9-2 for Weka 3.9.2 https://svn.cms.waikato.ac.nz/svn/weka/tags/dev-3-9-2 stable-X-Y-Z the tag for a release of a stable version. The book version is one of those stable versions, e.g., stable-3-8-2 for Weka 3.8.2. https://svn.cms.waikato.ac.nz/svn/weka/tags/stable-3-8-2 JUnit Weka's JUnit tests are no longer a separate module (as it was the case before the migration to Subversion). They are now located in the src/test directory of the Weka source code tree. Commandline Modern Linux distributions already come with Subversion either pre-installed or easily installed via the package manager of the distribution. If that shouldn't be case, or if you are using Windows, you have to download the appropriate client from [http://subversion.tigris.org/ Subversion's homepage]. A checkout of the current developer version of Weka looks like this: svn co https://svn.cms.waikato.ac.nz/svn/weka/trunk/weka You can also obtain the source code for a specific date. The -r option of the svn command-line client can also take dates (format: YYYYMMDD) instead of only revision numbers. In order to distinguish the dates from revision numbers, you have to enclose the date within curly brackets: svn co -r {20180616} https://svn.cms.waikato.ac.nz/svn/weka/trunk/weka Links Subversion on WikiPedia Subversion homepage JUnit homepage","title":" Subversion"},{"location":"subversion/#general","text":"The Weka Subversion repository is accessible and browseable via the following URL: https://svn.cms.waikato.ac.nz/svn/weka/ A Subversion repository has usually the following layout: root | +- trunk | +- tags | +- branches Where trunk contains the main trunk of the development, tags snapshots in time of the repository (e.g., when a new version got released) and branches development branches that forked off the main trunk at some stage (e.g., legacy versions that still get bugfixed).","title":"General"},{"location":"subversion/#source-code","text":"The latest version of the Weka source code can be obtained with this URL: https://svn.cms.waikato.ac.nz/svn/weka/trunk/weka If you want to obtain the source code of the book version, use this URL: https://svn.cms.waikato.ac.nz/svn/weka/branches/stable-3-8/","title":"Source code"},{"location":"subversion/#specific-version","text":"Whenever a release of Weka is generated, the repository gets tagged : dev-X-Y-Z the tag for a release of the developer version, e.g., dev-3-9-2 for Weka 3.9.2 https://svn.cms.waikato.ac.nz/svn/weka/tags/dev-3-9-2 stable-X-Y-Z the tag for a release of a stable version. The book version is one of those stable versions, e.g., stable-3-8-2 for Weka 3.8.2. https://svn.cms.waikato.ac.nz/svn/weka/tags/stable-3-8-2","title":"Specific version"},{"location":"subversion/#junit","text":"Weka's JUnit tests are no longer a separate module (as it was the case before the migration to Subversion). They are now located in the src/test directory of the Weka source code tree.","title":"JUnit"},{"location":"subversion/#commandline","text":"Modern Linux distributions already come with Subversion either pre-installed or easily installed via the package manager of the distribution. If that shouldn't be case, or if you are using Windows, you have to download the appropriate client from [http://subversion.tigris.org/ Subversion's homepage]. A checkout of the current developer version of Weka looks like this: svn co https://svn.cms.waikato.ac.nz/svn/weka/trunk/weka You can also obtain the source code for a specific date. The -r option of the svn command-line client can also take dates (format: YYYYMMDD) instead of only revision numbers. In order to distinguish the dates from revision numbers, you have to enclose the date within curly brackets: svn co -r {20180616} https://svn.cms.waikato.ac.nz/svn/weka/trunk/weka","title":"Commandline"},{"location":"subversion/#links","text":"Subversion on WikiPedia Subversion homepage JUnit homepage","title":"Links"},{"location":"troubleshooting/","text":"Click on one of the links for more information: Weka download problems OutOfMemoryException StackOverflowError just-in-time (JIT) compiler CSV file conversion ARFF file doesn't load Error message: nominal value not declared in header, read Token[X], line Y Spaces in labels of ARFF files Single quotes in labels of ARFF files CLASSPATH problems Instance ID Visualization Memory consumption and Garbage collector GUIChooser starts but not Experimenter or Explorer KnowledgeFlow toolbars are empty OSX Mountain Lion - Weka x-y-z is damaged and can't be installed. You should eject the disk image See also the Frequently Asked Questions .","title":"Troubleshooting"},{"location":"use_weka_in_your_java_code/","text":"The most common components you might want to use are Instances - your data Filter - for preprocessing the data Classifier/Clusterer - built on the processed data Evaluating - how good is the classifier/clusterer? Attribute selection* - removing irrelevant attributes from your data The following sections explain how to use them in your own code. A link to an example class can be found at the end of this page, under the Links section. The classifiers and filters always list their options in the Javadoc API ( stable , developer version) specification. A comprehensive source of information is the chapter Using the API of the Weka manual. Instances Datasets The DataSource class is not limited to ARFF files. It can also read CSV files and other formats (basically all file formats that Weka can import via its converters; it uses the file extension to determine the associated loader). import weka.core.converters.ConverterUtils.DataSource; ... DataSource source = new DataSource( /some/where/data.arff ); Instances data = source.getDataSet(); // setting class attribute if the data format does not provide this information // For example, the XRFF format saves the class attribute information as well if (data.classIndex() == -1) data.setClassIndex(data.numAttributes() - 1); Database Reading from Databases is slightly more complicated, but still very easy. First, you'll have to modify your DatabaseUtils.props file to reflect your database connection. Suppose you want to connect to a MySQL server that is running on the local machine on the default port 3306 . The MySQL JDBC driver is called Connector/J . (The driver class is org.gjt.mm.mysql.Driver .) The database where your target data resides is called some_database . Since you're only reading, you can use the default user nobody without a password. Your props file must contain the following lines: jdbcDriver=org.gjt.mm.mysql.Driver jdbcURL=jdbc:mysql://localhost:3306/some_database Secondly, your Java code needs to look like this to load the data from the database: import weka.core.Instances; import weka.experiment.InstanceQuery; ... InstanceQuery query = new InstanceQuery(); query.setUsername( nobody ); query.setPassword( ); query.setQuery( select * from whatsoever ); // You can declare that your data set is sparse // query.setSparseData(true); Instances data = query.retrieveInstances(); Notes: Don't forget to add the JDBC driver to your CLASSPATH . For MS Access, you must use the JDBC-ODBC-bridge that is part of a JDK. The [[Windows databases]] article explains how to do this. * InstanceQuery automatically converts VARCHAR database columns to NOMINAL attributes, and long TEXT database columns to STRING attributes. So if you use InstanceQuery to do text mining against text that appears in a VARCHAR column, Weka will regard such text as nominal values. Thus it will fail to tokenize and mine that text. Use the NominalToString or StringToNominal filter (package weka.filters.unsupervised.attribute ) to convert the attributes into the correct type. Option handling Weka schemes that implement the weka.core.OptionHandler interface, such as classifiers, clusterers, and filters, offer the following methods for setting and retrieving options: void setOptions(String[] options) String[] getOptions() There are several ways of setting the options: Manually creating a String array: String[] options = new String[2]; options[0] = -R ; options[1] = 1 ; Using a single command-line string and using the splitOptions method of the weka.core.Utils class to turn it into an array: String[] options = weka.core.Utils.splitOptions( -R 1 ); Using the OptionsToCode.java class to automatically turn a command line into code. Especially handy if the command line contains nested classes that have their own options, such as kernels for SMO: java OptionsToCode weka.classifiers.functions.SMO will generate output like this: // create new instance of scheme weka.classifiers.functions.SMO scheme = new weka.classifiers.functions.SMO(); // set options scheme.setOptions(weka.core.Utils.splitOptions( -C 1.0 -L 0.0010 -P 1.0E-12 -N 0 -V -1 -W 1 -K \\ weka.classifiers.functions.supportVector.PolyKernel -C 250007 -E 1.0\\ )); Also, the OptionTree.java tool allows you to view a nested options string, e.g., used at the command line, as a tree. This can help you spot nesting errors. Filter A filter has two different properties: supervised or unsupervised either takes the class attribute into account or not attribute - or instance -based e.g., removing a certain attribute or removing instances that meet a certain condition Most filters implement the OptionHandler interface, which means you can set the options via a String array, rather than setting them each manually via set -methods. For example, if you want to remove the first attribute of a dataset, you need this filter weka.filters.unsupervised.attribute.Remove with this option -R 1 If you have an Instances object, called data , you can create and apply the filter like this: import weka.core.Instances; import weka.filters.Filter; import weka.filters.unsupervised.attribute.Remove; ... String[] options = new String[2]; options[0] = -R ; // range options[1] = 1 ; // first attribute Remove remove = new Remove(); // new instance of filter remove.setOptions(options); // set options remove.setInputFormat(data); // inform filter about dataset **AFTER** setting options Instances newData = Filter.useFilter(data, remove); // apply filter Filtering on-the-fly The FilteredClassifer meta-classifier is an easy way of filtering data on the fly. It removes the necessity of filtering the data before the classifier can be trained. Also, the data need not be passed through the trained filter again at prediction time. The following is an example of using this meta-classifier with the Remove filter and J48 for getting rid of a numeric ID attribute in the data: import weka.classifiers.meta.FilteredClassifier; import weka.classifiers.trees.J48; import weka.filters.unsupervised.attribute.Remove; ... Instances train = ... // from somewhere Instances test = ... // from somewhere // filter Remove rm = new Remove(); rm.setAttributeIndices( 1 ); // remove 1st attribute // classifier J48 j48 = new J48(); j48.setUnpruned(true); // using an unpruned J48 // meta-classifier FilteredClassifier fc = new FilteredClassifier(); fc.setFilter(rm); fc.setClassifier(j48); // train and make predictions fc.buildClassifier(train); for (int i = 0; i test.numInstances(); i++) { double pred = fc.classifyInstance(test.instance(i)); System.out.print( ID: + test.instance(i).value(0)); System.out.print( , actual: + test.classAttribute().value((int) test.instance(i).classValue())); System.out.println( , predicted: + test.classAttribute().value((int) pred)); } Other handy meta-schemes in Weka: weka.clusterers.FilteredClusterer weka.assocations.FilteredAssociator Batch filtering On the command line, you can enable a second input/output pair (via -r and -s ) with the -b option, in order to process the second file with the same filter setup as the first one. Necessary, if you're using attribute selection or standardization - otherwise you end up with incompatible datasets. This is done fairly easy, since one initializes the filter only once with the setInputFormat(Instances) method, namely with the training set, and then applies the filter subsequently to the training set and the test set. The following example shows how to apply the Standardize filter to a train and a test set. Instances train = ... // from somewhere Instances test = ... // from somewhere Standardize filter = new Standardize(); filter.setInputFormat(train); // initializing the filter once with training set Instances newTrain = Filter.useFilter(train, filter); // configures the Filter based on train instances and returns filtered instances Instances newTest = Filter.useFilter(test, filter); // create new test set Calling conventions The setInputFormat(Instances) method always has to be the last call before the filter is applied, e.g., with Filter.useFilter(Instances,Filter) . Why? First, it is the convention for using filters and, secondly, lots of filters generate the header of the output format in the setInputFormat(Instances) method with the currently set options (setting otpions after this call doesn't have any effect any more). Classification The necessary classes can be found in this package: weka.classifiers Building a Classifier Batch A Weka classifier is rather simple to train on a given dataset. E.g., we can train an unpruned C4.5 tree algorithm on a given dataset data . The training is done via the buildClassifier(Instances) method. import weka.classifiers.trees.J48; ... String[] options = new String[1]; options[0] = -U ; // unpruned tree J48 tree = new J48(); // new instance of tree tree.setOptions(options); // set the options tree.buildClassifier(data); // build classifier Incremental Classifiers implementing the weka.classifiers.UpdateableClassifier interface can be trained incrementally. This conserves memory, since the data doesn't have to be loaded into memory all at once. See the Javadoc of this interface to see what classifiers are implementing it. The actual process of training an incremental classifier is fairly simple: Call buildClassifier(Instances) with the structure of the dataset (may or may not contain any actual data rows). Subsequently call the updateClassifier(Instance) method to feed the classifier new weka.core.Instance objects, one by one. Here is an example using data from a weka.core.converters.ArffLoader to train weka.classifiers.bayes.NaiveBayesUpdateable : // load data ArffLoader loader = new ArffLoader(); loader.setFile(new File( /some/where/data.arff )); Instances structure = loader.getStructure(); structure.setClassIndex(structure.numAttributes() - 1); // train NaiveBayes NaiveBayesUpdateable nb = new NaiveBayesUpdateable(); nb.buildClassifier(structure); Instance current; while ((current = loader.getNextInstance(structure)) != null) nb.updateClassifier(current); A working example is IncrementalClassifier.java . Evaluating Cross-validation If you only have a training set and no test you might want to evaluate the classifier by using 10 times 10-fold cross-validation. This can be easily done via the Evaluation class. Here we seed the random selection of our folds for the CV with 1 . Check out the Evaluation class for more information about the statistics it produces. import weka.classifiers.Evaluation; import java.util.Random; ... Evaluation eval = new Evaluation(newData); eval.crossValidateModel(tree, newData, 10, new Random(1)); Note: The classifier (in our example tree ) should not be trained when handed over to the crossValidateModel method. Why? If the classifier does not abide to the Weka convention that a classifier must be re-initialized every time the buildClassifier method is called (in other words: subsequent calls to the buildClassifier method always return the same results), you will get inconsistent and worthless results. The crossValidateModel takes care of training and evaluating the classifier. (It creates a copy of the original classifier that you hand over to the crossValidateModel for each run of the cross-validation.) Train/test set In case you have a dedicated test set, you can train the classifier and then evaluate it on this test set. In the following example, a J48 is instantiated, trained and then evaluated. Some statistics are printed to stdout : import weka.core.Instances; import weka.classifiers.Evaluation; import weka.classifiers.trees.J48; ... Instances train = ... // from somewhere Instances test = ... // from somewhere // train classifier Classifier cls = new J48(); cls.buildClassifier(train); // evaluate classifier and print some statistics Evaluation eval = new Evaluation(train); eval.evaluateModel(cls, test); System.out.println(eval.toSummaryString( \\nResults\\n======\\n , false)); Statistics Some methods for retrieving the results from the evaluation: nominal class correct() - number of correctly classified instances (see also incorrect() ) pctCorrect() - percentage of correctly classified instances (see also pctIncorrect() ) kappa() - Kappa statistics numeric class correlationCoefficient() - correlation coefficient general meanAbsoluteError() - the mean absolute error rootMeanSquaredError() - the root mean squared error unclassified() - number of unclassified instances pctUnclassified() - percentage of unclassified instances If you want to have the exact same behavior as from the command line, use this call: import weka.classifiers.trees.J48; import weka.classifiers.Evaluation; ... String[] options = new String[2]; options[0] = -t ; options[1] = /some/where/somefile.arff ; System.out.println(Evaluation.evaluateModel(new J48(), options)); ROC curves/AUC You can also generate ROC curves/AUC with the predictions Weka recorded during testing. You can access these predictions via the predictions() method of the Evaluation class. See the [[Generating ROC curve]] article for a full example of how to generate ROC curves. Classifying instances In case you have an unlabeled dataset that you want to classify with your newly trained classifier, you can use the following code snippet. It loads the file /some/where/unlabeled.arff , uses the previously built classifier tree to label the instances, and saves the labeled data as /some/where/labeled.arff . import java.io.BufferedReader; import java.io.BufferedWriter; import java.io.FileReader; import java.io.FileWriter; import weka.core.Instances; ... // load unlabeled data Instances unlabeled = new Instances( new BufferedReader( new FileReader( /some/where/unlabeled.arff ))); // set class attribute unlabeled.setClassIndex(unlabeled.numAttributes() - 1); // create copy Instances labeled = new Instances(unlabeled); // label instances for (int i = 0; i unlabeled.numInstances(); i++) { double clsLabel = tree.classifyInstance(unlabeled.instance(i)); labeled.instance(i).setClassValue(clsLabel); } // save labeled data BufferedWriter writer = new BufferedWriter( new FileWriter( /some/where/labeled.arff )); writer.write(labeled.toString()); writer.newLine(); writer.flush(); writer.close(); Note on nominal classes: If you're interested in the distribution over all the classes, use the method distributionForInstance(Instance) . This method returns a double array with the probability for each class. The returned double value from classifyInstance (or the index in the array returned by distributionForInstance ) is just the index for the string values in the attribute. That is, if you want the string representation for the class label returned above clsLabel , then you can print it like this: System.out.println(clsLabel + - + unlabeled.classAttribute().value((int) clsLabel)); Clustering Clustering is similar to classification. The necessary classes can be found in this package: weka.clusterers Building a Clusterer Batch A clusterer is built in much the same way as a classifier, but the buildClusterer(Instances) method instead of buildClassifier(Instances) . The following code snippet shows how to build an EM clusterer with a maximum of 100 iterations. import weka.clusterers.EM; ... String[] options = new String[2]; options[0] = -I ; // max. iterations options[1] = 100 ; EM clusterer = new EM(); // new instance of clusterer clusterer.setOptions(options); // set the options clusterer.buildClusterer(data); // build the clusterer Incremental Clusterers implementing the weka.clusterers.UpdateableClusterer interface can be trained incrementally. This conserves memory, since the data doesn't have to be loaded into memory all at once. See the Javadoc for this interface to see which clusterers implement it. The actual process of training an incremental clusterer is fairly simple: Call buildClusterer(Instances) with the structure of the dataset (may or may not contain any actual data rows). Subsequently call the updateClusterer(Instance) method to feed the clusterer new weka.core.Instance objects, one by one. Call updateFinished() after all Instance objects have been processed, for the clusterer to perform additional computations. Here is an example using data from a weka.core.converters.ArffLoader to train weka.clusterers.Cobweb : // load data ArffLoader loader = new ArffLoader(); loader.setFile(new File( /some/where/data.arff )); Instances structure = loader.getStructure(); // train Cobweb Cobweb cw = new Cobweb(); cw.buildClusterer(structure); Instance current; while ((current = loader.getNextInstance(structure)) != null) cw.updateClusterer(current); cw.updateFinished(); A working example is IncrementalClusterer.java . Evaluating For evaluating a clusterer, you can use the ClusterEvaluation class. In this example, the number of clusters found is written to output: import weka.clusterers.ClusterEvaluation; import weka.clusterers.Clusterer; ... ClusterEvaluation eval = new ClusterEvaluation(); Clusterer clusterer = new EM(); // new clusterer instance, default options clusterer.buildClusterer(data); // build clusterer eval.setClusterer(clusterer); // the cluster to evaluate eval.evaluateClusterer(newData); // data to evaluate the clusterer on System.out.println( # of clusters: + eval.getNumClusters()); // output # of clusters Or, in the case of DensityBasedClusterer , you can cross-validate the clusterer (Note: with MakeDensityBasedClusterer you can turn any clusterer into a density-based one): import weka.clusterers.ClusterEvaluation; import weka.clusterers.DensityBasedClusterer; import weka.core.Instances; import java.util.Random; ... Instances data = ... // from somewhere DensityBasedClusterer clusterer = new ... // the clusterer to evaluate double logLikelyhood = ClusterEvaluation.crossValidateModel( // cross-validate clusterer, data, 10, // with 10 folds new Random(1)); // and random number generator with seed 1 Or, if you want the same behavior/print-out from command line, use this call: import weka.clusterers.EM; import weka.clusterers.ClusterEvaluation; ... String[] options = new String[2]; options[0] = -t ; options[1] = /some/where/somefile.arff ; System.out.println(ClusterEvaluation.evaluateClusterer(new EM(), options)); Clustering instances The only difference with regard to classification is the method name. Instead of classifyInstance(Instance) , it is now clusterInstance(Instance) . The method for obtaining the distribution is still the same, i.e., distributionForInstance(Instance) . Classes to clusters evaluation If your data contains a class attribute and you want to check how well the generated clusters fit the classes, you can perform a so-called classes to clusters evaluation. The Weka Explorer offers this functionality, and it's quite easy to implement. These are the necessary steps (complete source code: ClassesToClusters.java ): load the data and set the class attribute Instances data = new Instances(new BufferedReader(new FileReader( /some/where/file.arff ))); data.setClassIndex(data.numAttributes() - 1); generate the class-less data to train the clusterer with weka.filters.unsupervised.attribute.Remove filter = new weka.filters.unsupervised.attribute.Remove(); filter.setAttributeIndices( + (data.classIndex() + 1)); filter.setInputFormat(data); Instances dataClusterer = Filter.useFilter(data, filter); train the clusterer, e.g., EM EM clusterer = new EM(); // set further options for EM, if necessary... clusterer.buildClusterer(dataClusterer); evaluate the clusterer with the data still containing the class attribute ClusterEvaluation eval = new ClusterEvaluation(); eval.setClusterer(clusterer); eval.evaluateClusterer(data); print the results of the evaluation to stdout System.out.println(eval.clusterResultsToString()); Attribute selection There is no real need to use the attribute selection classes directly in your own code, since there are already a meta-classifier and a filter available for applying attribute selection, but the low-level approach is still listed for the sake of completeness. The following examples all use CfsSubsetEval and GreedyStepwise (backwards). The code listed below is taken from the AttributeSelectionTest.java . Meta-Classifier The following meta-classifier performs a preprocessing step of attribute selection before the data gets presented to the base classifier (in the example here, this is J48 ). Instances data = ... // from somewhere AttributeSelectedClassifier classifier = new AttributeSelectedClassifier(); CfsSubsetEval eval = new CfsSubsetEval(); GreedyStepwise search = new GreedyStepwise(); search.setSearchBackwards(true); J48 base = new J48(); classifier.setClassifier(base); classifier.setEvaluator(eval); classifier.setSearch(search); // 10-fold cross-validation Evaluation evaluation = new Evaluation(data); evaluation.crossValidateModel(classifier, data, 10, new Random(1)); System.out.println(evaluation.toSummaryString()); Filter The filter approach is straightforward: after setting up the filter, one just filters the data through the filter and obtains the reduced dataset. Instances data = ... // from somewhere AttributeSelection filter = new AttributeSelection(); // package weka.filters.supervised.attribute! CfsSubsetEval eval = new CfsSubsetEval(); GreedyStepwise search = new GreedyStepwise(); search.setSearchBackwards(true); filter.setEvaluator(eval); filter.setSearch(search); filter.setInputFormat(data); // generate new data Instances newData = Filter.useFilter(data, filter); System.out.println(newData); Low-level If neither the meta-classifier nor filter approach is suitable for your purposes, you can use the attribute selection classes themselves. Instances data = ... // from somewhere AttributeSelection attsel = new AttributeSelection(); // package weka.attributeSelection! CfsSubsetEval eval = new CfsSubsetEval(); GreedyStepwise search = new GreedyStepwise(); search.setSearchBackwards(true); attsel.setEvaluator(eval); attsel.setSearch(search); attsel.SelectAttributes(data); // obtain the attribute indices that were selected int[] indices = attsel.selectedAttributes(); System.out.println(Utils.arrayToString(indices)); Note on randomization Most machine learning schemes, like classifiers and clusterers, are susceptible to the ordering of the data. Using a different seed for randomizing the data will most likely produce a different result. For example, the Explorer, or a classifier/clusterer run from the command line, uses only a seeded java.util.Random number generator, whereas the weka.core.Instances.getRandomNumberGenerator(int) (which the WekaDemo.java uses) also takes the data into account for seeding. Unless one runs 10-fold cross-validation 10 times and averages the results, one will most likely get different results. See also Weka Examples - pointer to collection of example classes Databases - for more information about using databases in Weka (includes ODBC, e.g., for MS Access) [[weka_experiment_DatabaseUtils.props|weka/experiment/DatabaseUtils.props]] - the database setup file Generating cross-validation folds (Java approach) - in case you want to run 10-fold cross-validation manually [[Generating classifier evaluation output manually]] - if you want to generate some of the evaluation statistics output manually Creating Instances on-the-fly - explains how to generate a weka.core.Instances object from scratch [[Save Instances to an ARFF File]] - shows how to output a dataset [[Using the Experiment API]] Examples The following are a few sample classes for using various parts of the Weka API: WekaDemo.java ( stable , developer ) - little demo class that loads data from a file, runs it through a filter and trains/evaluates a classifier ClusteringDemo.java ( stable , developer ) - a basic example for using the clusterer API ClassesToClusters.java ( stable , developer ) - performs a classes to clusters evaluation like in the Explorer AttributeSelectionTest.java ( stable , developer ) - example code for using the attribute selection API M5PExample.java ( stable , developer ) - example using M5P to obtain data from database, train model, serialize it to a file, and use this serialized model to make predictions again. OptionsToCode.java ( stable , developer ) - turns a Weka command line for a scheme with options into Java code, correctly escaping quotes and backslashes. OptionTree.java ( stable , developer ) - displays nested Weka options as tree. IncrementalClassifier.java ( stable , developer ) - Example class for how to train an incremental classifier (in this case, weka.classifiers.bayes.NaiveBayesUpdateable ). IncrementalClusterer.java ( stable , developer ) - Example class for how to train an incremental clusterer (in this case, weka.clusterers.Cobweb ). Links Weka API Stable version Developer version","title":" Use Weka in your Java code"},{"location":"use_weka_in_your_java_code/#instances","text":"","title":"Instances"},{"location":"use_weka_in_your_java_code/#datasets","text":"The DataSource class is not limited to ARFF files. It can also read CSV files and other formats (basically all file formats that Weka can import via its converters; it uses the file extension to determine the associated loader). import weka.core.converters.ConverterUtils.DataSource; ... DataSource source = new DataSource( /some/where/data.arff ); Instances data = source.getDataSet(); // setting class attribute if the data format does not provide this information // For example, the XRFF format saves the class attribute information as well if (data.classIndex() == -1) data.setClassIndex(data.numAttributes() - 1);","title":"Datasets"},{"location":"use_weka_in_your_java_code/#database","text":"Reading from Databases is slightly more complicated, but still very easy. First, you'll have to modify your DatabaseUtils.props file to reflect your database connection. Suppose you want to connect to a MySQL server that is running on the local machine on the default port 3306 . The MySQL JDBC driver is called Connector/J . (The driver class is org.gjt.mm.mysql.Driver .) The database where your target data resides is called some_database . Since you're only reading, you can use the default user nobody without a password. Your props file must contain the following lines: jdbcDriver=org.gjt.mm.mysql.Driver jdbcURL=jdbc:mysql://localhost:3306/some_database Secondly, your Java code needs to look like this to load the data from the database: import weka.core.Instances; import weka.experiment.InstanceQuery; ... InstanceQuery query = new InstanceQuery(); query.setUsername( nobody ); query.setPassword( ); query.setQuery( select * from whatsoever ); // You can declare that your data set is sparse // query.setSparseData(true); Instances data = query.retrieveInstances(); Notes: Don't forget to add the JDBC driver to your CLASSPATH . For MS Access, you must use the JDBC-ODBC-bridge that is part of a JDK. The [[Windows databases]] article explains how to do this. * InstanceQuery automatically converts VARCHAR database columns to NOMINAL attributes, and long TEXT database columns to STRING attributes. So if you use InstanceQuery to do text mining against text that appears in a VARCHAR column, Weka will regard such text as nominal values. Thus it will fail to tokenize and mine that text. Use the NominalToString or StringToNominal filter (package weka.filters.unsupervised.attribute ) to convert the attributes into the correct type.","title":"Database"},{"location":"use_weka_in_your_java_code/#option-handling","text":"Weka schemes that implement the weka.core.OptionHandler interface, such as classifiers, clusterers, and filters, offer the following methods for setting and retrieving options: void setOptions(String[] options) String[] getOptions() There are several ways of setting the options: Manually creating a String array: String[] options = new String[2]; options[0] = -R ; options[1] = 1 ; Using a single command-line string and using the splitOptions method of the weka.core.Utils class to turn it into an array: String[] options = weka.core.Utils.splitOptions( -R 1 ); Using the OptionsToCode.java class to automatically turn a command line into code. Especially handy if the command line contains nested classes that have their own options, such as kernels for SMO: java OptionsToCode weka.classifiers.functions.SMO will generate output like this: // create new instance of scheme weka.classifiers.functions.SMO scheme = new weka.classifiers.functions.SMO(); // set options scheme.setOptions(weka.core.Utils.splitOptions( -C 1.0 -L 0.0010 -P 1.0E-12 -N 0 -V -1 -W 1 -K \\ weka.classifiers.functions.supportVector.PolyKernel -C 250007 -E 1.0\\ )); Also, the OptionTree.java tool allows you to view a nested options string, e.g., used at the command line, as a tree. This can help you spot nesting errors.","title":"Option handling"},{"location":"use_weka_in_your_java_code/#filter","text":"A filter has two different properties: supervised or unsupervised either takes the class attribute into account or not attribute - or instance -based e.g., removing a certain attribute or removing instances that meet a certain condition Most filters implement the OptionHandler interface, which means you can set the options via a String array, rather than setting them each manually via set -methods. For example, if you want to remove the first attribute of a dataset, you need this filter weka.filters.unsupervised.attribute.Remove with this option -R 1 If you have an Instances object, called data , you can create and apply the filter like this: import weka.core.Instances; import weka.filters.Filter; import weka.filters.unsupervised.attribute.Remove; ... String[] options = new String[2]; options[0] = -R ; // range options[1] = 1 ; // first attribute Remove remove = new Remove(); // new instance of filter remove.setOptions(options); // set options remove.setInputFormat(data); // inform filter about dataset **AFTER** setting options Instances newData = Filter.useFilter(data, remove); // apply filter","title":"Filter"},{"location":"use_weka_in_your_java_code/#filtering-on-the-fly","text":"The FilteredClassifer meta-classifier is an easy way of filtering data on the fly. It removes the necessity of filtering the data before the classifier can be trained. Also, the data need not be passed through the trained filter again at prediction time. The following is an example of using this meta-classifier with the Remove filter and J48 for getting rid of a numeric ID attribute in the data: import weka.classifiers.meta.FilteredClassifier; import weka.classifiers.trees.J48; import weka.filters.unsupervised.attribute.Remove; ... Instances train = ... // from somewhere Instances test = ... // from somewhere // filter Remove rm = new Remove(); rm.setAttributeIndices( 1 ); // remove 1st attribute // classifier J48 j48 = new J48(); j48.setUnpruned(true); // using an unpruned J48 // meta-classifier FilteredClassifier fc = new FilteredClassifier(); fc.setFilter(rm); fc.setClassifier(j48); // train and make predictions fc.buildClassifier(train); for (int i = 0; i test.numInstances(); i++) { double pred = fc.classifyInstance(test.instance(i)); System.out.print( ID: + test.instance(i).value(0)); System.out.print( , actual: + test.classAttribute().value((int) test.instance(i).classValue())); System.out.println( , predicted: + test.classAttribute().value((int) pred)); } Other handy meta-schemes in Weka: weka.clusterers.FilteredClusterer weka.assocations.FilteredAssociator","title":"Filtering on-the-fly"},{"location":"use_weka_in_your_java_code/#batch-filtering","text":"On the command line, you can enable a second input/output pair (via -r and -s ) with the -b option, in order to process the second file with the same filter setup as the first one. Necessary, if you're using attribute selection or standardization - otherwise you end up with incompatible datasets. This is done fairly easy, since one initializes the filter only once with the setInputFormat(Instances) method, namely with the training set, and then applies the filter subsequently to the training set and the test set. The following example shows how to apply the Standardize filter to a train and a test set. Instances train = ... // from somewhere Instances test = ... // from somewhere Standardize filter = new Standardize(); filter.setInputFormat(train); // initializing the filter once with training set Instances newTrain = Filter.useFilter(train, filter); // configures the Filter based on train instances and returns filtered instances Instances newTest = Filter.useFilter(test, filter); // create new test set","title":"Batch filtering"},{"location":"use_weka_in_your_java_code/#calling-conventions","text":"The setInputFormat(Instances) method always has to be the last call before the filter is applied, e.g., with Filter.useFilter(Instances,Filter) . Why? First, it is the convention for using filters and, secondly, lots of filters generate the header of the output format in the setInputFormat(Instances) method with the currently set options (setting otpions after this call doesn't have any effect any more).","title":"Calling conventions"},{"location":"use_weka_in_your_java_code/#classification","text":"The necessary classes can be found in this package: weka.classifiers","title":"Classification"},{"location":"use_weka_in_your_java_code/#building-a-classifier","text":"","title":"Building a Classifier"},{"location":"use_weka_in_your_java_code/#batch","text":"A Weka classifier is rather simple to train on a given dataset. E.g., we can train an unpruned C4.5 tree algorithm on a given dataset data . The training is done via the buildClassifier(Instances) method. import weka.classifiers.trees.J48; ... String[] options = new String[1]; options[0] = -U ; // unpruned tree J48 tree = new J48(); // new instance of tree tree.setOptions(options); // set the options tree.buildClassifier(data); // build classifier","title":"Batch"},{"location":"use_weka_in_your_java_code/#incremental","text":"Classifiers implementing the weka.classifiers.UpdateableClassifier interface can be trained incrementally. This conserves memory, since the data doesn't have to be loaded into memory all at once. See the Javadoc of this interface to see what classifiers are implementing it. The actual process of training an incremental classifier is fairly simple: Call buildClassifier(Instances) with the structure of the dataset (may or may not contain any actual data rows). Subsequently call the updateClassifier(Instance) method to feed the classifier new weka.core.Instance objects, one by one. Here is an example using data from a weka.core.converters.ArffLoader to train weka.classifiers.bayes.NaiveBayesUpdateable : // load data ArffLoader loader = new ArffLoader(); loader.setFile(new File( /some/where/data.arff )); Instances structure = loader.getStructure(); structure.setClassIndex(structure.numAttributes() - 1); // train NaiveBayes NaiveBayesUpdateable nb = new NaiveBayesUpdateable(); nb.buildClassifier(structure); Instance current; while ((current = loader.getNextInstance(structure)) != null) nb.updateClassifier(current); A working example is IncrementalClassifier.java .","title":"Incremental"},{"location":"use_weka_in_your_java_code/#evaluating","text":"","title":"Evaluating"},{"location":"use_weka_in_your_java_code/#cross-validation","text":"If you only have a training set and no test you might want to evaluate the classifier by using 10 times 10-fold cross-validation. This can be easily done via the Evaluation class. Here we seed the random selection of our folds for the CV with 1 . Check out the Evaluation class for more information about the statistics it produces. import weka.classifiers.Evaluation; import java.util.Random; ... Evaluation eval = new Evaluation(newData); eval.crossValidateModel(tree, newData, 10, new Random(1)); Note: The classifier (in our example tree ) should not be trained when handed over to the crossValidateModel method. Why? If the classifier does not abide to the Weka convention that a classifier must be re-initialized every time the buildClassifier method is called (in other words: subsequent calls to the buildClassifier method always return the same results), you will get inconsistent and worthless results. The crossValidateModel takes care of training and evaluating the classifier. (It creates a copy of the original classifier that you hand over to the crossValidateModel for each run of the cross-validation.)","title":"Cross-validation"},{"location":"use_weka_in_your_java_code/#traintest-set","text":"In case you have a dedicated test set, you can train the classifier and then evaluate it on this test set. In the following example, a J48 is instantiated, trained and then evaluated. Some statistics are printed to stdout : import weka.core.Instances; import weka.classifiers.Evaluation; import weka.classifiers.trees.J48; ... Instances train = ... // from somewhere Instances test = ... // from somewhere // train classifier Classifier cls = new J48(); cls.buildClassifier(train); // evaluate classifier and print some statistics Evaluation eval = new Evaluation(train); eval.evaluateModel(cls, test); System.out.println(eval.toSummaryString( \\nResults\\n======\\n , false));","title":"Train/test set"},{"location":"use_weka_in_your_java_code/#statistics","text":"Some methods for retrieving the results from the evaluation: nominal class correct() - number of correctly classified instances (see also incorrect() ) pctCorrect() - percentage of correctly classified instances (see also pctIncorrect() ) kappa() - Kappa statistics numeric class correlationCoefficient() - correlation coefficient general meanAbsoluteError() - the mean absolute error rootMeanSquaredError() - the root mean squared error unclassified() - number of unclassified instances pctUnclassified() - percentage of unclassified instances If you want to have the exact same behavior as from the command line, use this call: import weka.classifiers.trees.J48; import weka.classifiers.Evaluation; ... String[] options = new String[2]; options[0] = -t ; options[1] = /some/where/somefile.arff ; System.out.println(Evaluation.evaluateModel(new J48(), options));","title":"Statistics"},{"location":"use_weka_in_your_java_code/#roc-curvesauc","text":"You can also generate ROC curves/AUC with the predictions Weka recorded during testing. You can access these predictions via the predictions() method of the Evaluation class. See the [[Generating ROC curve]] article for a full example of how to generate ROC curves.","title":"ROC curves/AUC"},{"location":"use_weka_in_your_java_code/#classifying-instances","text":"In case you have an unlabeled dataset that you want to classify with your newly trained classifier, you can use the following code snippet. It loads the file /some/where/unlabeled.arff , uses the previously built classifier tree to label the instances, and saves the labeled data as /some/where/labeled.arff . import java.io.BufferedReader; import java.io.BufferedWriter; import java.io.FileReader; import java.io.FileWriter; import weka.core.Instances; ... // load unlabeled data Instances unlabeled = new Instances( new BufferedReader( new FileReader( /some/where/unlabeled.arff ))); // set class attribute unlabeled.setClassIndex(unlabeled.numAttributes() - 1); // create copy Instances labeled = new Instances(unlabeled); // label instances for (int i = 0; i unlabeled.numInstances(); i++) { double clsLabel = tree.classifyInstance(unlabeled.instance(i)); labeled.instance(i).setClassValue(clsLabel); } // save labeled data BufferedWriter writer = new BufferedWriter( new FileWriter( /some/where/labeled.arff )); writer.write(labeled.toString()); writer.newLine(); writer.flush(); writer.close(); Note on nominal classes: If you're interested in the distribution over all the classes, use the method distributionForInstance(Instance) . This method returns a double array with the probability for each class. The returned double value from classifyInstance (or the index in the array returned by distributionForInstance ) is just the index for the string values in the attribute. That is, if you want the string representation for the class label returned above clsLabel , then you can print it like this: System.out.println(clsLabel + - + unlabeled.classAttribute().value((int) clsLabel));","title":"Classifying instances"},{"location":"use_weka_in_your_java_code/#clustering","text":"Clustering is similar to classification. The necessary classes can be found in this package: weka.clusterers","title":"Clustering"},{"location":"use_weka_in_your_java_code/#building-a-clusterer","text":"","title":"Building a Clusterer"},{"location":"use_weka_in_your_java_code/#batch_1","text":"A clusterer is built in much the same way as a classifier, but the buildClusterer(Instances) method instead of buildClassifier(Instances) . The following code snippet shows how to build an EM clusterer with a maximum of 100 iterations. import weka.clusterers.EM; ... String[] options = new String[2]; options[0] = -I ; // max. iterations options[1] = 100 ; EM clusterer = new EM(); // new instance of clusterer clusterer.setOptions(options); // set the options clusterer.buildClusterer(data); // build the clusterer","title":"Batch"},{"location":"use_weka_in_your_java_code/#incremental_1","text":"Clusterers implementing the weka.clusterers.UpdateableClusterer interface can be trained incrementally. This conserves memory, since the data doesn't have to be loaded into memory all at once. See the Javadoc for this interface to see which clusterers implement it. The actual process of training an incremental clusterer is fairly simple: Call buildClusterer(Instances) with the structure of the dataset (may or may not contain any actual data rows). Subsequently call the updateClusterer(Instance) method to feed the clusterer new weka.core.Instance objects, one by one. Call updateFinished() after all Instance objects have been processed, for the clusterer to perform additional computations. Here is an example using data from a weka.core.converters.ArffLoader to train weka.clusterers.Cobweb : // load data ArffLoader loader = new ArffLoader(); loader.setFile(new File( /some/where/data.arff )); Instances structure = loader.getStructure(); // train Cobweb Cobweb cw = new Cobweb(); cw.buildClusterer(structure); Instance current; while ((current = loader.getNextInstance(structure)) != null) cw.updateClusterer(current); cw.updateFinished(); A working example is IncrementalClusterer.java .","title":"Incremental"},{"location":"use_weka_in_your_java_code/#evaluating_1","text":"For evaluating a clusterer, you can use the ClusterEvaluation class. In this example, the number of clusters found is written to output: import weka.clusterers.ClusterEvaluation; import weka.clusterers.Clusterer; ... ClusterEvaluation eval = new ClusterEvaluation(); Clusterer clusterer = new EM(); // new clusterer instance, default options clusterer.buildClusterer(data); // build clusterer eval.setClusterer(clusterer); // the cluster to evaluate eval.evaluateClusterer(newData); // data to evaluate the clusterer on System.out.println( # of clusters: + eval.getNumClusters()); // output # of clusters Or, in the case of DensityBasedClusterer , you can cross-validate the clusterer (Note: with MakeDensityBasedClusterer you can turn any clusterer into a density-based one): import weka.clusterers.ClusterEvaluation; import weka.clusterers.DensityBasedClusterer; import weka.core.Instances; import java.util.Random; ... Instances data = ... // from somewhere DensityBasedClusterer clusterer = new ... // the clusterer to evaluate double logLikelyhood = ClusterEvaluation.crossValidateModel( // cross-validate clusterer, data, 10, // with 10 folds new Random(1)); // and random number generator with seed 1 Or, if you want the same behavior/print-out from command line, use this call: import weka.clusterers.EM; import weka.clusterers.ClusterEvaluation; ... String[] options = new String[2]; options[0] = -t ; options[1] = /some/where/somefile.arff ; System.out.println(ClusterEvaluation.evaluateClusterer(new EM(), options));","title":"Evaluating"},{"location":"use_weka_in_your_java_code/#clustering-instances","text":"The only difference with regard to classification is the method name. Instead of classifyInstance(Instance) , it is now clusterInstance(Instance) . The method for obtaining the distribution is still the same, i.e., distributionForInstance(Instance) .","title":"Clustering instances"},{"location":"use_weka_in_your_java_code/#classes-to-clusters-evaluation","text":"If your data contains a class attribute and you want to check how well the generated clusters fit the classes, you can perform a so-called classes to clusters evaluation. The Weka Explorer offers this functionality, and it's quite easy to implement. These are the necessary steps (complete source code: ClassesToClusters.java ): load the data and set the class attribute Instances data = new Instances(new BufferedReader(new FileReader( /some/where/file.arff ))); data.setClassIndex(data.numAttributes() - 1); generate the class-less data to train the clusterer with weka.filters.unsupervised.attribute.Remove filter = new weka.filters.unsupervised.attribute.Remove(); filter.setAttributeIndices( + (data.classIndex() + 1)); filter.setInputFormat(data); Instances dataClusterer = Filter.useFilter(data, filter); train the clusterer, e.g., EM EM clusterer = new EM(); // set further options for EM, if necessary... clusterer.buildClusterer(dataClusterer); evaluate the clusterer with the data still containing the class attribute ClusterEvaluation eval = new ClusterEvaluation(); eval.setClusterer(clusterer); eval.evaluateClusterer(data); print the results of the evaluation to stdout System.out.println(eval.clusterResultsToString());","title":"Classes to clusters evaluation"},{"location":"use_weka_in_your_java_code/#attribute-selection","text":"There is no real need to use the attribute selection classes directly in your own code, since there are already a meta-classifier and a filter available for applying attribute selection, but the low-level approach is still listed for the sake of completeness. The following examples all use CfsSubsetEval and GreedyStepwise (backwards). The code listed below is taken from the AttributeSelectionTest.java .","title":"Attribute selection"},{"location":"use_weka_in_your_java_code/#meta-classifier","text":"The following meta-classifier performs a preprocessing step of attribute selection before the data gets presented to the base classifier (in the example here, this is J48 ). Instances data = ... // from somewhere AttributeSelectedClassifier classifier = new AttributeSelectedClassifier(); CfsSubsetEval eval = new CfsSubsetEval(); GreedyStepwise search = new GreedyStepwise(); search.setSearchBackwards(true); J48 base = new J48(); classifier.setClassifier(base); classifier.setEvaluator(eval); classifier.setSearch(search); // 10-fold cross-validation Evaluation evaluation = new Evaluation(data); evaluation.crossValidateModel(classifier, data, 10, new Random(1)); System.out.println(evaluation.toSummaryString());","title":"Meta-Classifier"},{"location":"use_weka_in_your_java_code/#filter_1","text":"The filter approach is straightforward: after setting up the filter, one just filters the data through the filter and obtains the reduced dataset. Instances data = ... // from somewhere AttributeSelection filter = new AttributeSelection(); // package weka.filters.supervised.attribute! CfsSubsetEval eval = new CfsSubsetEval(); GreedyStepwise search = new GreedyStepwise(); search.setSearchBackwards(true); filter.setEvaluator(eval); filter.setSearch(search); filter.setInputFormat(data); // generate new data Instances newData = Filter.useFilter(data, filter); System.out.println(newData);","title":"Filter"},{"location":"use_weka_in_your_java_code/#low-level","text":"If neither the meta-classifier nor filter approach is suitable for your purposes, you can use the attribute selection classes themselves. Instances data = ... // from somewhere AttributeSelection attsel = new AttributeSelection(); // package weka.attributeSelection! CfsSubsetEval eval = new CfsSubsetEval(); GreedyStepwise search = new GreedyStepwise(); search.setSearchBackwards(true); attsel.setEvaluator(eval); attsel.setSearch(search); attsel.SelectAttributes(data); // obtain the attribute indices that were selected int[] indices = attsel.selectedAttributes(); System.out.println(Utils.arrayToString(indices));","title":"Low-level"},{"location":"use_weka_in_your_java_code/#note-on-randomization","text":"Most machine learning schemes, like classifiers and clusterers, are susceptible to the ordering of the data. Using a different seed for randomizing the data will most likely produce a different result. For example, the Explorer, or a classifier/clusterer run from the command line, uses only a seeded java.util.Random number generator, whereas the weka.core.Instances.getRandomNumberGenerator(int) (which the WekaDemo.java uses) also takes the data into account for seeding. Unless one runs 10-fold cross-validation 10 times and averages the results, one will most likely get different results.","title":"Note on randomization"},{"location":"use_weka_in_your_java_code/#see-also","text":"Weka Examples - pointer to collection of example classes Databases - for more information about using databases in Weka (includes ODBC, e.g., for MS Access) [[weka_experiment_DatabaseUtils.props|weka/experiment/DatabaseUtils.props]] - the database setup file Generating cross-validation folds (Java approach) - in case you want to run 10-fold cross-validation manually [[Generating classifier evaluation output manually]] - if you want to generate some of the evaluation statistics output manually Creating Instances on-the-fly - explains how to generate a weka.core.Instances object from scratch [[Save Instances to an ARFF File]] - shows how to output a dataset [[Using the Experiment API]]","title":"See also"},{"location":"use_weka_in_your_java_code/#examples","text":"The following are a few sample classes for using various parts of the Weka API: WekaDemo.java ( stable , developer ) - little demo class that loads data from a file, runs it through a filter and trains/evaluates a classifier ClusteringDemo.java ( stable , developer ) - a basic example for using the clusterer API ClassesToClusters.java ( stable , developer ) - performs a classes to clusters evaluation like in the Explorer AttributeSelectionTest.java ( stable , developer ) - example code for using the attribute selection API M5PExample.java ( stable , developer ) - example using M5P to obtain data from database, train model, serialize it to a file, and use this serialized model to make predictions again. OptionsToCode.java ( stable , developer ) - turns a Weka command line for a scheme with options into Java code, correctly escaping quotes and backslashes. OptionTree.java ( stable , developer ) - displays nested Weka options as tree. IncrementalClassifier.java ( stable , developer ) - Example class for how to train an incremental classifier (in this case, weka.classifiers.bayes.NaiveBayesUpdateable ). IncrementalClusterer.java ( stable , developer ) - Example class for how to train an incremental clusterer (in this case, weka.clusterers.Cobweb ).","title":"Examples"},{"location":"use_weka_in_your_java_code/#links","text":"Weka API Stable version Developer version","title":"Links"},{"location":"using_the_api/","text":"Several articles describe certain aspects of using the Weka API: Use Weka in your Java code Weka Examples Generating cross-validation folds Creating an ARFF file Binarize Attribute ARFF files from Text Collections Adding attributes to a dataset Save Instances to an ARFF File Generating ROC curve Visualizing ROC curve Serialization","title":"Using the API"},{"location":"visualizing_roc_curve/","text":"The following class lets you display a previously saved [[ROC curves|ROC curve]], which also displays the AUC . If you don't need the AUC , then you can also use this command to display the curve: java [CLASSPATH|-classpath your-classpath ] weka.gui.visualize.VisualizePanel file Source code: import java.awt.*; import java.io.*; import javax.swing.*; import weka.core.*; import weka.classifiers.evaluation.*; import weka.gui.visualize.*; /** * Visualizes a previously saved ROC curve. Code taken from the * code weka.gui.explorer.ClassifierPanel /code - involved methods: * ul * li visualize(String,int,int) /li * /li visualizeClassifierErrors(VisualizePanel) /li * /ul * * @author FracPete */ public class VisualizeROC { /** * takes one argument: previously saved ROC curve data (ARFF file) */ public static void main(String[] args) throws Exception { Instances result = new Instances( new BufferedReader( new FileReader(args[0]))); result.setClassIndex(result.numAttributes() - 1); ThresholdCurve tc = new ThresholdCurve(); // method visualize ThresholdVisualizePanel vmc = new ThresholdVisualizePanel(); vmc.setROCString( (Area under ROC = + Utils.doubleToString(tc.getROCArea(result), 4) + ) ); vmc.setName(result.relationName()); PlotData2D tempd = new PlotData2D(result); tempd.setPlotName(result.relationName()); tempd.addInstanceNumberAttribute(); // specify which points are connected boolean[] cp = new boolean[result.numInstances()]; for (int n = 1; n cp.length; n++) cp[n] = true; tempd.setConnectPoints(cp); // add plot vmc.addPlot(tempd); // method visualizeClassifierErrors String plotName = vmc.getName(); final javax.swing.JFrame jf = new javax.swing.JFrame( Weka Classifier Visualize: +plotName); jf.setSize(500,400); jf.getContentPane().setLayout(new BorderLayout()); jf.getContentPane().add(vmc, BorderLayout.CENTER); jf.addWindowListener(new java.awt.event.WindowAdapter() { public void windowClosing(java.awt.event.WindowEvent e) { jf.dispose(); } }); jf.setVisible(true); } } See also ROC curves Plotting multiple ROC curves - also contains a Java example of plotting multiple ROC curves in a single plot Downloads VisualizeROC.java ( stable , developer )","title":" Visualizing ROC curve"},{"location":"visualizing_roc_curve/#see-also","text":"ROC curves Plotting multiple ROC curves - also contains a Java example of plotting multiple ROC curves in a single plot","title":"See also"},{"location":"visualizing_roc_curve/#downloads","text":"VisualizeROC.java ( stable , developer )","title":"Downloads"},{"location":"weka_examples/","text":"The Weka Examples collection is a comprehensive collection of examples for the different versions of Weka in the form of an ANT project. You can access these examples as follows: Snapshots Through snapshots or releases (they contain a separate ZIP file with the ANT project) Subversion Through subversion stable-3.8 version (3.8.x): https://svn.cms.waikato.ac.nz/svn/weka/branches/stable-3-8/wekaexamples/ developer version (3.9.x): https://svn.cms.waikato.ac.nz/svn/weka/trunk/wekaexamples/","title":" Weka Examples"},{"location":"weka_examples/#snapshots","text":"Through snapshots or releases (they contain a separate ZIP file with the ANT project)","title":"Snapshots"},{"location":"weka_examples/#subversion","text":"Through subversion stable-3.8 version (3.8.x): https://svn.cms.waikato.ac.nz/svn/weka/branches/stable-3-8/wekaexamples/ developer version (3.9.x): https://svn.cms.waikato.ac.nz/svn/weka/trunk/wekaexamples/","title":"Subversion"},{"location":"writing_classifier/","text":"In case you have a flash idea for a new classifier and want to write one for Weka, this HOWTO will help you developing it. The Mindmap ( Build_classifier.pdf , produced with FreeMind ) helps you decide from which base classifier to start, what methods are to be implemented and general guidelines. The base classifiers are all located in the following package: weka.classifiers Note: This is also covered in chapter Extending WEKA of the WEKA manual. Packages A few comments about the different classifier sub-packages: bayes - contains bayesian classifiers, e.g. NaiveBayes evaluation - classes related to evaluation, e.g., cost matrix functions - e.g., Support Vector Machines, regression algorithms, neural nets lazy - no offline learning, that is done during runtime, e.g., k-NN meta - Meta classifiers that use a base classifier as input, e.g., boosting or bagging mi - classifiers that handle multi-instance data misc - various classifiers that don't fit in any another category rules - rule-based classifiers, e.g. ZeroR trees - tree classifiers, like decision trees Coding In the following you'll find notes about certain implementation parts listed in the Mindmap, which need a bit more explanation. Random number generators In order to get repeatable experiments, one is not allowed to use unseeded random number generators like Math.random() . Instead, one has to instantiate a java.util.Random object in the buildClassifier(Instances) method with a specific seed value. The seed value can be user supplied, of course, which all the Randomizable... abstract classifiers already implement. Capabilities In old versions of Weka (up to version 3.5.2), all classifiers could handle basically every kind of data by default, unless they were throwing an Exception (in the buildClassifier(Instances) method). Since this behavior makes it cumbersome to introduce new attribute types, for instance ( all classifiers have to be modified, which can't handle the new attribute type!), the general Capabilities were introduced. Base-classifier Normal classifiers only state what kind of attributes and what kind of classes they can handle. The getCapabilities() method of weka.classifiers.trees.RandomTree , for instance, looks like this: public Capabilities getCapabilities() { Capabilities result = super.getCapabilities(); // returns the object from weka.classifiers.Classifier // attributes result.enable(Capability.NOMINAL_ATTRIBUTES); result.enable(Capability.NUMERIC_ATTRIBUTES); result.enable(Capability.DATE_ATTRIBUTES); result.enable(Capability.MISSING_VALUES); // class result.enable(Capability.NOMINAL_CLASS); result.enable(Capability.MISSING_CLASS_VALUES); return result; } Special cases: incremental classifiers - By default, at least 1 instance has to be in the dataset, which does not apply for incremental classifiers. They have to lower the limit to 0 : result.setMinimumNumberInstances(0); multi-instance classifiers - The structure for multi-instance classifiers is always fixed to bagID,bag-data,class . To restrict the data to multi-instance data, add the following: result.enable(Capability.ONLY_MULTIINSTANCE); Multi-instance classifiers also implement the following interface, which returns the Capabilities for the bag-data, which is just a relational attribute (the reason why RELATIONAL_ATTRIBUTES has to be enabled): weka.core.MultiInstanceCapabilitiesHandler clusterer - Since clusterer don't need a class attribute like classifiers, the following Capability has to be specified to enable datasets without a class attribute (which is already done in the superclass weka.clusterers.Clusterer ): result.enable(Capability.NO_CLASS); Meta-classifier Meta-classifiers, by default, just return the capabilities of their base classifiers - in case of descendants of the weka.classifier.MultipleClassifiersCombiner , an AND over all the Capabilities of the base classifiers is returned. Due to this behavior, the Capabilities depend (normally) only on the currently configured base classifier(s). To soften filtering for certain behavior, meta-classifiers also define so-called Dependencies on a per-Capability basis. These dependencies tell the filter that even though a certain capability is not supported right now, it is possible that it will be supported with a different base classifier. By default, all Capabilities are initialized as Dependencies. weka.classifiers.meta.LogitBoost , e.g., is restricted to nominal classes. For that reason it disables the Dependencies for the class: result.disableAllClasses(); // disable all class types result.disableAllClassDependencies(); // no dependencies! result.enable(Capability.NOMINAL_CLASS); // only nominal classes allowed Relevant classes weka.core.Capabilities weka.core.CapabilitiesHandler weka.core.MultiInstanceCapabilitiesHandler (for multi-instance classifiers) Paper reference(s) In order to make it easy to generate a bibliography of all the algorithms in Weka, the [[paper references]] located so far in the Javadoc were extracted and placed in the code. Classes that are based on some technical paper should implement the TechnicalInformationHandler interface and return a customized TechnicalInformation instance. The format used is based on BibTeX and the TechnicalInformation class can either return a plain text string via the toString() method or a real BibTeX entry via the toBibTex() method. This two methods are then used to automatically update the Javadoc (see Javadoc further down) of a class. Relevant classes: weka.core.TechnicalInformation weka.core.TechnicalInformationHandler Javadoc Open-source software is only as good as its documentation. Hence, correct and up-to-date documentation is vital. So far most of the Javadoc was maintained manually, which made it hard to maintain, e.g., as soon as new options were added the Javadoc had to be changed accordingly, too. And that normally in several places: Class description setOptions(String[]) method Over the time the documentation got out of sync, which made it frustrating determining what options were really relevant and active. Since a lot of the documentation is already available in the code itself, the next logical step was to automate the Javadoc generation as much as possible. In the following you will see how to structure your Javadoc to reduce maintainance. For this purpose special comment tags are used, where the content in between will be replaced automatically by the classes listed below in the Relevant classes section. The indentation of the generated Javadoc depends on the indentation of the lt; of the starting comment tag. This general layout order should be used for all classes: class description Javadoc globalinfo bibtex - if available commandline options setOptions Javadoc commandline options General The general description for all classes displayed in the GenericObjectEditor was already in place, with the following method: globalInfo() The return value can be placed in the Javadoc, surrounded by the following comment tags: !-- globalinfo-start -- will be automatically replaced !-- globalinfo-end -- Paper reference(s) If available, the paper reference should also be listed in the Javadoc. Since the globalInfo() method should return a short version of the reference, it is sufficient to list the full BibTeX documentation: !-- technical-bibtex-start -- will be automatically replaced !-- technical-bibtex-end -- In case it is necessary to list the short, plain text version, too, one can use the following tags: !-- technical-plaintext-start -- will be automatically replaced !-- technical-plaintext-end -- Options To place the commandline options, use the following comment tags: !-- options-start -- will be automatically replaced !-- options-end -- Relevant classes weka.core.AllJavadoc - executes all Javadoc-producing classes weka.core.GlobalInfoJavadoc - updates the globalInfo tags weka.core.OptionHandlerJavadoc - updates the option tags weka.core.TechnicalInformationHandlerJavadoc - updates the technical tags (plain text and BibTeX ) Integration After finishing the coding stage, it's time to integrate your classifier in the Weka framework, i.e., to make it available in the Explorer, Experimenter, etc. The [[GenericObjectEditor]] article shows you how to tell Weka where to find your classifier and therefore displaying it in the GenericObjectEditor . Revisions Classifiers also implement the weka.core.RevisionHandler interface. This provides the functionality of obtaining the Subversion revision from within Java. Classifiers that are not part of the official Weka distribution will have to implement the method getRevision() as follows, which will return a dummy revision of 1.0 : /** * Returns the revision string. * * @return the revision */ public String getRevision() { return RevisionUtils.extract( $Revision: 1.0 $ ); } Testing Weka provides already a test framework to ensure the basic functionality of a classifier. It is essential for the classifier to pass these tests. Commandline test General Use the CheckClassifier class to test your classifier from commandline: weka.classifiers.CheckClassifier -W classname [-- additional parameters] Only the following tests may have \"no\" as result, the others must have a \"no (OK error message)\" or \"yes\": options updateable classifier weighted instances classifier multi-instance classifier Option handling Additionally, check the option handling of your classifier with the following tool from commandline: weka.core.CheckOptionHandler -W classname [-- additional parameters] All tests need to return yes . GenericObjectEditor The CheckGOE class checks whether all the properties available in the GUI have a tooltip accompanying them and whether the globalInfo() method is declared: weka.core.CheckGOE -W classname [-- additional parameters] All tests, once again, need to return yes . Source code Classifiers that implement the weka.classifiers.Sourcable interface can output Java code of their model. In order to check the generated code, one should not only compile the code, but also test it with the following test class: weka.classifiers.CheckSource This class takes the original Weka classifier, the generated code and the dataset used for generating the source code as parameters. It builds the Weka classifier on the dataset and compares the predictions, the ones from the Weka classifier and the ones from the generated source code, whether they are the same. Here's an example call for weka.classifiers.trees.Id3 and the generated class weka.classifiers.WekaWrapper (it wraps the actual generated code in a pseudo-classifier): java weka.classifiers.CheckSource \\ -W weka.classifiers.trees.Id3 \\ -S weka.classifiers.WekaWrapper \\ -t data.arff \\ -c last It needs to return Tests OK! . Unit tests In order to make sure that your classifier applies to the Weka criteria, you should add your classifier to the junit unit test framework, i.e., by creating a Test class derived from AbstractClassifierTest . This class uses the CheckClassifier , CheckOptionHandler and CheckGOE class to run a battery of tests. How to check out the unit test framework, you can find here . See also [[GenericObjectEditor|GenericObjectEditor/GenericPropertiesCreator]] [[Paper References|HOWTO extract paper references]] Links Build_classifier.pdf - MindMap for implementing a new classifier Weka API ( stable , developer ) Freemind junit","title":" Writing your own Classifier"},{"location":"writing_classifier/#packages","text":"A few comments about the different classifier sub-packages: bayes - contains bayesian classifiers, e.g. NaiveBayes evaluation - classes related to evaluation, e.g., cost matrix functions - e.g., Support Vector Machines, regression algorithms, neural nets lazy - no offline learning, that is done during runtime, e.g., k-NN meta - Meta classifiers that use a base classifier as input, e.g., boosting or bagging mi - classifiers that handle multi-instance data misc - various classifiers that don't fit in any another category rules - rule-based classifiers, e.g. ZeroR trees - tree classifiers, like decision trees","title":"Packages"},{"location":"writing_classifier/#coding","text":"In the following you'll find notes about certain implementation parts listed in the Mindmap, which need a bit more explanation.","title":"Coding"},{"location":"writing_classifier/#random-number-generators","text":"In order to get repeatable experiments, one is not allowed to use unseeded random number generators like Math.random() . Instead, one has to instantiate a java.util.Random object in the buildClassifier(Instances) method with a specific seed value. The seed value can be user supplied, of course, which all the Randomizable... abstract classifiers already implement.","title":"Random number generators"},{"location":"writing_classifier/#capabilities","text":"In old versions of Weka (up to version 3.5.2), all classifiers could handle basically every kind of data by default, unless they were throwing an Exception (in the buildClassifier(Instances) method). Since this behavior makes it cumbersome to introduce new attribute types, for instance ( all classifiers have to be modified, which can't handle the new attribute type!), the general Capabilities were introduced.","title":"Capabilities"},{"location":"writing_classifier/#base-classifier","text":"Normal classifiers only state what kind of attributes and what kind of classes they can handle. The getCapabilities() method of weka.classifiers.trees.RandomTree , for instance, looks like this: public Capabilities getCapabilities() { Capabilities result = super.getCapabilities(); // returns the object from weka.classifiers.Classifier // attributes result.enable(Capability.NOMINAL_ATTRIBUTES); result.enable(Capability.NUMERIC_ATTRIBUTES); result.enable(Capability.DATE_ATTRIBUTES); result.enable(Capability.MISSING_VALUES); // class result.enable(Capability.NOMINAL_CLASS); result.enable(Capability.MISSING_CLASS_VALUES); return result; } Special cases: incremental classifiers - By default, at least 1 instance has to be in the dataset, which does not apply for incremental classifiers. They have to lower the limit to 0 : result.setMinimumNumberInstances(0); multi-instance classifiers - The structure for multi-instance classifiers is always fixed to bagID,bag-data,class . To restrict the data to multi-instance data, add the following: result.enable(Capability.ONLY_MULTIINSTANCE); Multi-instance classifiers also implement the following interface, which returns the Capabilities for the bag-data, which is just a relational attribute (the reason why RELATIONAL_ATTRIBUTES has to be enabled): weka.core.MultiInstanceCapabilitiesHandler clusterer - Since clusterer don't need a class attribute like classifiers, the following Capability has to be specified to enable datasets without a class attribute (which is already done in the superclass weka.clusterers.Clusterer ): result.enable(Capability.NO_CLASS);","title":"Base-classifier"},{"location":"writing_classifier/#meta-classifier","text":"Meta-classifiers, by default, just return the capabilities of their base classifiers - in case of descendants of the weka.classifier.MultipleClassifiersCombiner , an AND over all the Capabilities of the base classifiers is returned. Due to this behavior, the Capabilities depend (normally) only on the currently configured base classifier(s). To soften filtering for certain behavior, meta-classifiers also define so-called Dependencies on a per-Capability basis. These dependencies tell the filter that even though a certain capability is not supported right now, it is possible that it will be supported with a different base classifier. By default, all Capabilities are initialized as Dependencies. weka.classifiers.meta.LogitBoost , e.g., is restricted to nominal classes. For that reason it disables the Dependencies for the class: result.disableAllClasses(); // disable all class types result.disableAllClassDependencies(); // no dependencies! result.enable(Capability.NOMINAL_CLASS); // only nominal classes allowed","title":"Meta-classifier"},{"location":"writing_classifier/#relevant-classes","text":"weka.core.Capabilities weka.core.CapabilitiesHandler weka.core.MultiInstanceCapabilitiesHandler (for multi-instance classifiers)","title":"Relevant classes"},{"location":"writing_classifier/#paper-references","text":"In order to make it easy to generate a bibliography of all the algorithms in Weka, the [[paper references]] located so far in the Javadoc were extracted and placed in the code. Classes that are based on some technical paper should implement the TechnicalInformationHandler interface and return a customized TechnicalInformation instance. The format used is based on BibTeX and the TechnicalInformation class can either return a plain text string via the toString() method or a real BibTeX entry via the toBibTex() method. This two methods are then used to automatically update the Javadoc (see Javadoc further down) of a class. Relevant classes: weka.core.TechnicalInformation weka.core.TechnicalInformationHandler","title":"Paper reference(s)"},{"location":"writing_classifier/#javadoc","text":"Open-source software is only as good as its documentation. Hence, correct and up-to-date documentation is vital. So far most of the Javadoc was maintained manually, which made it hard to maintain, e.g., as soon as new options were added the Javadoc had to be changed accordingly, too. And that normally in several places: Class description setOptions(String[]) method Over the time the documentation got out of sync, which made it frustrating determining what options were really relevant and active. Since a lot of the documentation is already available in the code itself, the next logical step was to automate the Javadoc generation as much as possible. In the following you will see how to structure your Javadoc to reduce maintainance. For this purpose special comment tags are used, where the content in between will be replaced automatically by the classes listed below in the Relevant classes section. The indentation of the generated Javadoc depends on the indentation of the lt; of the starting comment tag. This general layout order should be used for all classes: class description Javadoc globalinfo bibtex - if available commandline options setOptions Javadoc commandline options","title":"Javadoc"},{"location":"writing_classifier/#general","text":"The general description for all classes displayed in the GenericObjectEditor was already in place, with the following method: globalInfo() The return value can be placed in the Javadoc, surrounded by the following comment tags: !-- globalinfo-start -- will be automatically replaced !-- globalinfo-end --","title":"General"},{"location":"writing_classifier/#paper-references_1","text":"If available, the paper reference should also be listed in the Javadoc. Since the globalInfo() method should return a short version of the reference, it is sufficient to list the full BibTeX documentation: !-- technical-bibtex-start -- will be automatically replaced !-- technical-bibtex-end -- In case it is necessary to list the short, plain text version, too, one can use the following tags: !-- technical-plaintext-start -- will be automatically replaced !-- technical-plaintext-end --","title":"Paper reference(s)"},{"location":"writing_classifier/#options","text":"To place the commandline options, use the following comment tags: !-- options-start -- will be automatically replaced !-- options-end --","title":"Options"},{"location":"writing_classifier/#relevant-classes_1","text":"weka.core.AllJavadoc - executes all Javadoc-producing classes weka.core.GlobalInfoJavadoc - updates the globalInfo tags weka.core.OptionHandlerJavadoc - updates the option tags weka.core.TechnicalInformationHandlerJavadoc - updates the technical tags (plain text and BibTeX )","title":"Relevant classes"},{"location":"writing_classifier/#integration","text":"After finishing the coding stage, it's time to integrate your classifier in the Weka framework, i.e., to make it available in the Explorer, Experimenter, etc. The [[GenericObjectEditor]] article shows you how to tell Weka where to find your classifier and therefore displaying it in the GenericObjectEditor .","title":"Integration"},{"location":"writing_classifier/#revisions","text":"Classifiers also implement the weka.core.RevisionHandler interface. This provides the functionality of obtaining the Subversion revision from within Java. Classifiers that are not part of the official Weka distribution will have to implement the method getRevision() as follows, which will return a dummy revision of 1.0 : /** * Returns the revision string. * * @return the revision */ public String getRevision() { return RevisionUtils.extract( $Revision: 1.0 $ ); }","title":"Revisions"},{"location":"writing_classifier/#testing","text":"Weka provides already a test framework to ensure the basic functionality of a classifier. It is essential for the classifier to pass these tests.","title":"Testing"},{"location":"writing_classifier/#commandline-test","text":"","title":"Commandline test"},{"location":"writing_classifier/#general_1","text":"Use the CheckClassifier class to test your classifier from commandline: weka.classifiers.CheckClassifier -W classname [-- additional parameters] Only the following tests may have \"no\" as result, the others must have a \"no (OK error message)\" or \"yes\": options updateable classifier weighted instances classifier multi-instance classifier","title":"General"},{"location":"writing_classifier/#option-handling","text":"Additionally, check the option handling of your classifier with the following tool from commandline: weka.core.CheckOptionHandler -W classname [-- additional parameters] All tests need to return yes .","title":"Option handling"},{"location":"writing_classifier/#genericobjecteditor","text":"The CheckGOE class checks whether all the properties available in the GUI have a tooltip accompanying them and whether the globalInfo() method is declared: weka.core.CheckGOE -W classname [-- additional parameters] All tests, once again, need to return yes .","title":"GenericObjectEditor"},{"location":"writing_classifier/#source-code","text":"Classifiers that implement the weka.classifiers.Sourcable interface can output Java code of their model. In order to check the generated code, one should not only compile the code, but also test it with the following test class: weka.classifiers.CheckSource This class takes the original Weka classifier, the generated code and the dataset used for generating the source code as parameters. It builds the Weka classifier on the dataset and compares the predictions, the ones from the Weka classifier and the ones from the generated source code, whether they are the same. Here's an example call for weka.classifiers.trees.Id3 and the generated class weka.classifiers.WekaWrapper (it wraps the actual generated code in a pseudo-classifier): java weka.classifiers.CheckSource \\ -W weka.classifiers.trees.Id3 \\ -S weka.classifiers.WekaWrapper \\ -t data.arff \\ -c last It needs to return Tests OK! .","title":"Source code"},{"location":"writing_classifier/#unit-tests","text":"In order to make sure that your classifier applies to the Weka criteria, you should add your classifier to the junit unit test framework, i.e., by creating a Test class derived from AbstractClassifierTest . This class uses the CheckClassifier , CheckOptionHandler and CheckGOE class to run a battery of tests. How to check out the unit test framework, you can find here .","title":"Unit tests"},{"location":"writing_classifier/#see-also","text":"[[GenericObjectEditor|GenericObjectEditor/GenericPropertiesCreator]] [[Paper References|HOWTO extract paper references]]","title":"See also"},{"location":"writing_classifier/#links","text":"Build_classifier.pdf - MindMap for implementing a new classifier Weka API ( stable , developer ) Freemind junit","title":"Links"},{"location":"writing_filter/","text":"Note: This is also covered in chapter Extending WEKA of the WEKA manual. Packages A few comments about the different filter sub-packages: supervised - contains supervised filters, i.e., filters that take class distributions into account. Must implement the weka.filters.SupervisedFilter interface. attribute - filters that work column-wise. instance - filters that work row-wise. unsupervised - contains unsupervised filters, i.e., they work without taking any class distributions into account. Must implement the weka.filters.UnsupervisedFilter interface. attribute - filters that work column-wise. instance - filters that work row-wise. Choosing the superclass The base filters and interfaces are all located in the following package: weka.filters One can basically distinguish between two different kinds of filters: batch filters - they need to see the whole dataset before they can start processing it, which they do in one go stream filters - they can start producing output right away and the data just passes through while being modified All filters are derived from the abstract superclass weka.filters.Filter . To speed up development, there are also the following abstract filters, depending on the kind of classifier you want to implement: weka.filters.SimpleBatchFilter weka.filters.SimpleStreamFilter These filters simplify the rather general and complex framework introduced by the abstract superclass weka.filters.Filter . One only needs to implement a couple of abstract methods that will process the actual data and override, if necessary, a few existing methods for option handling. Filter Implementation The following methods are of importance for the implementation of a filter and explained in detail further down: getCapabilities() setInputFormat(Instances) getInputFormat() setOutputFormat(Instances) getOutputFormat() input(Instance) bufferInput(Instance) push(Instance) output() batchFinished() flushInput() getRevision() But only the following ones need normally be modified: getCapabilities() setInputFormat(Instances) input(Instance) batchFinished() getRevision() getCapabilities() Filters implement the weka.core.CapabilitiesHandler interface like the classifiers. This method returns what kind of data the filter is able to process. Needs to be adapted for each individual filter. setInputFormat(Instances) With this format, the user tells the filter what format, i.e., attributes, the input data has. This method also tests, whether the filter can actually process this data. All older Weka versions or book branch versions need to check the data manually and throw fitting exceptions, e.g., not being able to handle String attributes. If the output format of the filter, i.e., the new Instances header, can be determined based alone on this information, then the method should set the output format via setOutputFormat(Instances) and return true , otherwise it has to return false . getInputFormat() This method returns an Instances object containing all currently buffered Instance objects from the input queue. setOutputFormat(Instances) setOutputFormat(Instances) defines the new Instances header for the output data. For filters that work on a row-basis, there shouldn't be any changes between the input and output format. But filters that work on attributes, e.g., removing, adding, modifying, will affect this format. This method must be called with the appropriate Instances object as parameter, since all Instance objects being processed will rely on the output format. getOutputFormat() This method returns the currently set Instances object that defines the output format. In case setOutputFormat(Instances) hasn't been called yet, this method will return null . input(Instance) The input(Instance) method returns true if the given Instance can be processed straight away and can be collected immediately via the output() method (after adding it to the output queue via push(Instance) , of course). This is also the case if the first batch of data has been processed and the instance belongs to the second batch. Via isFirstBatchDone() one can query whether this instance is still part of the first batch or of the second. If the Instance cannot be processed immediately, e.g., the filter needs to collect all the data first before doing some calculations, then it needs to be buffered with bufferInput(Instance) until batchFinished() is called. bufferInput(Instance) In case an Instance cannot be processed immediately, one can use this method to buffer them in the input queue. All buffered Instance objects are available via the getInputFormat() method. push(Instance) push(Instance) adds the given Instance to the output queue. output() Returns the next Instance object from the output queue and removes it from there. In case there is no Instance available this method returns null . batchFinished() The batchFinished() method signifies the end of a dataset being pushed through the filter. In case of a filter that couldn't process the data of the first batch immediately, this is the place to determine what the output format will be (and set if via setOutputFormat(Instances) ) and process the actual data. The currently available data can be retrieved with the getInputFormat() method. After processing the data, one needs to call flushInput() to remove all the pending input data. flushInput() flushInput() removes all buffered Instance objects from the input queue. This method must be called after all the Instance objects have been processed in the batchFinished() method. Option handling If the filter should be able to handle commandline options, then the weka.core.OptionHandler interface needs to be implemented. In addition to that, the following code should be added at the end of the setOptions(String[]) method: if (getInputFormat() != null) setInputFormat(getInputFormat()); This will inform the filter about changes in the options and therefore reset it. Examples The following examples are to illustrate the filter framework. Note: unseeded random number generators like Math.random() should never be used since they will produce different results in each run and repeatable results are essential in machine learning. BatchFilter This simple batch filter adds a new attribute called //bla// at the end of the dataset. The rows of this attribute contain only the row's index in the data. Since the batch-filter need not see all the data before creating the output format, the setInputFormat(Instances) sets the output format and returns true (indicating that the output format can be queried immediately). The batchFinished() method performs the processing of all the data. import weka.core.*; import weka.core.Capabilities.*; public class BatchFilter extends Filter { public String globalInfo() { return A batch filter that adds an additional attribute 'bla' at the end + containing the index of the processed instance. The output format + can be collected immediately. ; } public Capabilities getCapabilities() { Capabilities result = super.getCapabilities(); result.enableAllAttributes(); result.enableAllClasses(); result.enable(Capability.NO_CLASS); // filter doesn't need class to be set return result; } public boolean setInputFormat(Instances instanceInfo) throws Exception { super.setInputFormat(instanceInfo); Instances outFormat = new Instances(instanceInfo, 0); outFormat.insertAttributeAt(new Attribute( bla ), outFormat.numAttributes()); setOutputFormat(outFormat); return true; // output format is immediately available } public boolean batchFinished() throws Exception { if (getInputFormat() = null) throw new NullPointerException( No input instance format defined ); Instances inst = getInputFormat(); Instances outFormat = getOutputFormat(); for (int i = 0; i inst.numInstances(); i++) { double[] newValues = new double[outFormat.numAttributes()]; double[] oldValues = inst.instance(i).toDoubleArray(); System.arraycopy(oldValues, 0, newValues, 0, oldValues.length); newValues[newValues.length - 1] = i; push(new Instance(1.0, newValues)); } flushInput(); m_NewBatch = true; m_FirstBatchDone = true; return (numPendingOutput() != 0); } public static void main(String[] args) { runFilter(new BatchFilter(), args); } } BatchFilter2 In contrast to the first batch filter, this one here cannot determine the output format immediately (the number of instances in the first batch is part of the attribute name now). This is done in the batchFinished() method. import weka.core.*; import weka.core.Capabilities.*; public class BatchFilter2 extends Filter { public String globalInfo() { return A batch filter that adds an additional attribute 'bla' at the end + containing the index of the processed instance. The output format + cannot be collected immediately. ; } public Capabilities getCapabilities() { Capabilities result = super.getCapabilities(); result.enableAllAttributes(); result.enableAllClasses(); result.enable(Capability.NO_CLASS); // filter doesn't need class to be set return result; } public boolean batchFinished() throws Exception { if (getInputFormat() = null) throw new NullPointerException( No input instance format defined ); // output format still needs to be set (depends on first batch of data) if (!isFirstBatchDone()) { Instances outFormat = new Instances(getInputFormat(), 0); outFormat.insertAttributeAt(new Attribute( bla- + getInputFormat().numInstances()), outFormat.numAttributes()); setOutputFormat(outFormat); } Instances inst = getInputFormat(); Instances outFormat = getOutputFormat(); for (int i = 0; i inst.numInstances(); i++) { double[] newValues = new double[outFormat.numAttributes()]; double[] oldValues = inst.instance(i).toDoubleArray(); System.arraycopy(oldValues, 0, newValues, 0, oldValues.length); newValues[newValues.length - 1] = i; push(new Instance(1.0, newValues)); } flushInput(); m_NewBatch = true; m_FirstBatchDone = true; return (numPendingOutput() != 0); } public static void main(String[] args) { runFilter(new BatchFilter2(), args); } } BatchFilter3 As soon as this batch filter's first batch is done, it can process Instance objects immediately in the input(Instance) method. It adds a new attribute which contains just a random number, but the random number generator being used is seeded with the number of instances from the first batch. import weka.core.*; import weka.core.Capabilities.*; import java.util.Random; public class BatchFilter3 extends Filter { protected int m_Seed; protected Random m_Random; public String globalInfo() { return A batch filter that adds an attribute 'bla' at the end + containing a random number. The output format cannot be collected + immediately. ; } public Capabilities getCapabilities() { Capabilities result = super.getCapabilities(); result.enableAllAttributes(); result.enableAllClasses(); result.enable(Capability.NO_CLASS); // filter doesn't need class to be set return result; } public boolean input(Instance instance) throws Exception { if (getInputFormat() = null) throw new NullPointerException( No input instance format defined ); if (isNewBatch()) { resetQueue(); m_NewBatch = false; } if (isFirstBatchDone()) convertInstance(instance); else bufferInput(instance); return isFirstBatchDone(); } public boolean batchFinished() throws Exception { if (getInputFormat() = null) throw new NullPointerException( No input instance format defined ); // output format still needs to be set (random number generator is seeded // with number of instances of first batch) if (!isFirstBatchDone()) { m_Seed = getInputFormat().numInstances(); Instances outFormat = new Instances(getInputFormat(), 0); outFormat.insertAttributeAt(new Attribute( bla- + getInputFormat().numInstances()), outFormat.numAttributes()); setOutputFormat(outFormat); } Instances inst = getInputFormat(); for (int i = 0; i inst.numInstances(); i++) { convertInstance(inst.instance(i)); } flushInput(); m_NewBatch = true; m_FirstBatchDone = true; m_Random = null; return (numPendingOutput() != 0); } protected void convertInstance(Instance instance) { if (m_Random = null) m_Random = new Random(m_Seed); double[] newValues = new double[instance.numAttributes() + 1]; double[] oldValues = instance.toDoubleArray(); newValues[newValues.length - 1] = m_Random.nextInt(); System.arraycopy(oldValues, 0, newValues, 0, oldValues.length); push(new Instance(1.0, newValues)); } public static void main(String[] args) { runFilter(new BatchFilter3(), args); } } StreamFilter This stream filter adds a random number at the end of each instance of the input data. Since this doesn't rely on having access to the full data of the first batch, the output format is accessible immediately after using setInputFormat(Instances) . All the Instance objects are immediately processed in input(Instance) via the convertInstance(Instance) method, which pushes them immediately to the output queue. import weka.core.*; import weka.core.Capabilities.*; import java.util.Random; public class StreamFilter extends Filter { protected Random m_Random; public String globalInfo() { return A stream filter that adds an attribute 'bla' at the end + containing a random number. The output format can be collected + immediately. ; } public Capabilities getCapabilities() { Capabilities result = super.getCapabilities(); result.enableAllAttributes(); result.enableAllClasses(); result.enable(Capability.NO_CLASS); // filter doesn't need class to be set return result; } public boolean setInputFormat(Instances instanceInfo) throws Exception { super.setInputFormat(instanceInfo); Instances outFormat = new Instances(instanceInfo, 0); outFormat.insertAttributeAt(new Attribute( bla ), outFormat.numAttributes()); setOutputFormat(outFormat); m_Random = new Random(1); return true; // output format is immediately available } public boolean input(Instance instance) throws Exception { if (getInputFormat() = null) throw new NullPointerException( No input instance format defined ); if (isNewBatch()) { resetQueue(); m_NewBatch = false; } convertInstance(instance); return true; // can be immediately collected via output() } protected void convertInstance(Instance instance) { double[] newValues = new double[instance.numAttributes() + 1]; double[] oldValues = instance.toDoubleArray(); newValues[newValues.length - 1] = m_Random.nextInt(); System.arraycopy(oldValues, 0, newValues, 0, oldValues.length); push(new Instance(1.0, newValues)); } public static void main(String[] args) { runFilter(new StreamFilter(), args); } } SimpleBatchFilter Only the following abstract methods need to be implemented: globalInfo() - returns a short description of what the filter does; will be displayed in the GUI determineOutputFormat(Instances) - generates the new format, based on the input data process(Instances) - processes the whole dataset in one go getRevision() - returns the Subversion revision information If you need access to the full input dataset in determineOutputFormat(Instances) , then you need to also override the method allowAccessToFullInputFormat() and make it return true. If more options are necessary, then the following methods need to be overridden: listOptions() - returns an enumeration of the available options; these are printed if one calls the filter with the -h option setOptions(String[]) - parses the given option array, that were passed from commandline getOptions() - returns an array of options, resembling the current setup of the filter In the following an example implementation that adds an additional attribute at the end, containing the index of the processed instance: import weka.core.*; import weka.core.Capabilities.*; import weka.filters.*; public class SimpleBatch extends SimpleBatchFilter { public String globalInfo() { return A simple batch filter that adds an additional attribute 'bla' at the end + containing the index of the processed instance. ; } public Capabilities getCapabilities() { Capabilities result = super.getCapabilities(); result.enableAllAttributes(); result.enableAllClasses(); result.enable(Capability.NO_CLASS); //// filter doesn't need class to be set// return result; } protected Instances determineOutputFormat(Instances inputFormat) { Instances result = new Instances(inputFormat, 0); result.insertAttributeAt(new Attribute( bla ), result.numAttributes()); return result; } protected Instances process(Instances inst) { Instances result = new Instances(determineOutputFormat(inst), 0); for (int i = 0; i inst.numInstances(); i++) { double[] values = new double[result.numAttributes()]; for (int n = 0; n inst.numAttributes(); n++) values[n] = inst.instance(i).value(n); values[values.length - 1] = i; result.add(new Instance(1, values)); } return result; } public static void main(String[] args) { runFilter(new SimpleBatch(), args); } } SimpleStreamFilter Only the following abstract methods need to be implemented: globalInfo() - returns a short description of what the filter does; will be displayed in the GUI determineOutputFormat(Instances) - generates the new format, based on the input data process(Instance) processes a single instance and turns it from the old format into the new one getRevision() - returns the Subversion revision information The reset() method is only used, since the random number generator needs to be re-initialized in order to obtain repeatable results. If more options are necessary, then the following methods need to be overridden: listOptions() - returns an enumeration of the available options; these are printed if one calls the filter with the -h option setOptions(String[]) - parses the given option array, that were passed from commandline getOptions() - returns an array of options, resembling the current setup of the filter In the following an example implementation of a stream filter that adds an extra attribute at the end, which is filled with random numbers: import weka.core.*; import weka.core.Capabilities.*; import weka.filters.*; import java.util.Random; public class SimpleStream extends SimpleStreamFilter { protected Random m_Random; public String globalInfo() { return A simple stream filter that adds an attribute 'bla' at the end + containing a random number. ; } public Capabilities getCapabilities() { Capabilities result = super.getCapabilities(); result.enableAllAttributes(); result.enableAllClasses(); result.enable(Capability.NO_CLASS); //// filter doesn't need class to be set// return result; } protected void reset() { super.reset(); m_Random = new Random(1); } protected Instances determineOutputFormat(Instances inputFormat) { Instances result = new Instances(inputFormat, 0); result.insertAttributeAt(new Attribute( bla ), result.numAttributes()); return result; } protected Instance process(Instance inst) { double[] values = new double[inst.numAttributes() + 1]; for (int n = 0; n inst.numAttributes(); n++) values[n] = inst.value(n); values[values.length - 1] = m_Random.nextInt(); Instance result = new Instance(1, values); return result; } public static void main(String[] args) { runFilter(new SimpleStream(), args); } } A real-world implementation of a stream filter is the MultiFilter (package weka.filters ), which passes the data through all the filters it contains. Depending on whether all the used filters are streamable or not, it acts either as a stream filter or as batch filter. Internals Some useful methods of the filter classes: isNewBatch() - returns true if an instance of the filter was just instantiated via new or a new batch was started via the batchFinished() method. isFirstBatchDone() - returns true as soon as the first batch was finished via the batchFinished() method. Useful for supervised filters, which should not be altered after being trained with the first batch of instances. Revisions Filters also implement the weka.core.RevisionHandler interface. This provides the functionality of obtaining the Subversion revision from within Java. Filters that are not part of the official Weka distribution will have to implement the method getRevision() as follows, which will return a dummy revision of 1.0 : /** * Returns the revision string. * * @return the revision */ public String getRevision() { return RevisionUtils.extract( $Revision: 1.0 $ ); } Integration After finishing the coding stage, it's time to integrate your filter in the Weka framework, i.e., to make it available in the Explorer, Experimenter, etc. The [[GenericObjectEditor]] article shows you how to tell Weka where to find your filter and therefore displaying it in the GenericObjectEditor (filters work in the same fashion as classifiers, regarding the discovery). Testing Weka provides already a test framework to ensure the basic functionality of a filter. It is essential for the filter to pass these tests. Option handling If your filter implements weka.core.OptionHandler , check the option handling of your filter with the following tool from commandline: weka.core.CheckOptionHandler -W classname [-- additional parameters] All tests need to return yes . GenericObjectEditor The CheckGOE class checks whether all the properties available in the GUI have a tooltip accompanying them and whether the globalInfo() method is declared: weka.core.CheckGOE -W classname [-- additional parameters] All tests, once again, need to return yes . Source code Filters that implement the weka.filters.Sourcable interface can output Java code of their internal representation. In order to check the generated code, one should not only compile the code, but also test it with the following test class: weka.filters.CheckSource This class takes the original Weka filter, the generated code and the dataset used for generating the source code (and an optional class index) as parameters. It builds the Weka filter on the dataset and compares the output, the one from the Weka filter and the one from the generated source code, whether they are the same. Here's an example call for weka.filters.unsupervised.attribute.ReplaceMissingValues and the generated class weka.filters.WekaWrapper (it wraps the actual generated code in a pseudo-filter): java weka.filters.CheckSource \\ -W weka.filters.unsupervised.attribute.ReplaceMissingValues \\ -S weka.filters.WekaWrapper \\ -t data.arff It needs to return Tests OK! . Unit tests In order to make sure that your filter applies to the Weka criteria, you should add your filter to the junit unit test framework, i.e., by creating a Test class. How to check out the unit test framework, you can find here . See also [[GenericObjectEditor|GenericObjectEditor/GenericPropertiesCreator]] Links junit","title":" Writing your own Filter"},{"location":"writing_filter/#packages","text":"A few comments about the different filter sub-packages: supervised - contains supervised filters, i.e., filters that take class distributions into account. Must implement the weka.filters.SupervisedFilter interface. attribute - filters that work column-wise. instance - filters that work row-wise. unsupervised - contains unsupervised filters, i.e., they work without taking any class distributions into account. Must implement the weka.filters.UnsupervisedFilter interface. attribute - filters that work column-wise. instance - filters that work row-wise.","title":"Packages"},{"location":"writing_filter/#choosing-the-superclass","text":"The base filters and interfaces are all located in the following package: weka.filters One can basically distinguish between two different kinds of filters: batch filters - they need to see the whole dataset before they can start processing it, which they do in one go stream filters - they can start producing output right away and the data just passes through while being modified All filters are derived from the abstract superclass weka.filters.Filter . To speed up development, there are also the following abstract filters, depending on the kind of classifier you want to implement: weka.filters.SimpleBatchFilter weka.filters.SimpleStreamFilter These filters simplify the rather general and complex framework introduced by the abstract superclass weka.filters.Filter . One only needs to implement a couple of abstract methods that will process the actual data and override, if necessary, a few existing methods for option handling.","title":"Choosing the superclass"},{"location":"writing_filter/#filter","text":"","title":"Filter"},{"location":"writing_filter/#implementation","text":"The following methods are of importance for the implementation of a filter and explained in detail further down: getCapabilities() setInputFormat(Instances) getInputFormat() setOutputFormat(Instances) getOutputFormat() input(Instance) bufferInput(Instance) push(Instance) output() batchFinished() flushInput() getRevision() But only the following ones need normally be modified: getCapabilities() setInputFormat(Instances) input(Instance) batchFinished() getRevision()","title":"Implementation"},{"location":"writing_filter/#getcapabilities","text":"Filters implement the weka.core.CapabilitiesHandler interface like the classifiers. This method returns what kind of data the filter is able to process. Needs to be adapted for each individual filter.","title":"getCapabilities()"},{"location":"writing_filter/#setinputformatinstances","text":"With this format, the user tells the filter what format, i.e., attributes, the input data has. This method also tests, whether the filter can actually process this data. All older Weka versions or book branch versions need to check the data manually and throw fitting exceptions, e.g., not being able to handle String attributes. If the output format of the filter, i.e., the new Instances header, can be determined based alone on this information, then the method should set the output format via setOutputFormat(Instances) and return true , otherwise it has to return false .","title":"setInputFormat(Instances)"},{"location":"writing_filter/#getinputformat","text":"This method returns an Instances object containing all currently buffered Instance objects from the input queue.","title":"getInputFormat()"},{"location":"writing_filter/#setoutputformatinstances","text":"setOutputFormat(Instances) defines the new Instances header for the output data. For filters that work on a row-basis, there shouldn't be any changes between the input and output format. But filters that work on attributes, e.g., removing, adding, modifying, will affect this format. This method must be called with the appropriate Instances object as parameter, since all Instance objects being processed will rely on the output format.","title":"setOutputFormat(Instances)"},{"location":"writing_filter/#getoutputformat","text":"This method returns the currently set Instances object that defines the output format. In case setOutputFormat(Instances) hasn't been called yet, this method will return null .","title":"getOutputFormat()"},{"location":"writing_filter/#inputinstance","text":"The input(Instance) method returns true if the given Instance can be processed straight away and can be collected immediately via the output() method (after adding it to the output queue via push(Instance) , of course). This is also the case if the first batch of data has been processed and the instance belongs to the second batch. Via isFirstBatchDone() one can query whether this instance is still part of the first batch or of the second. If the Instance cannot be processed immediately, e.g., the filter needs to collect all the data first before doing some calculations, then it needs to be buffered with bufferInput(Instance) until batchFinished() is called.","title":"input(Instance)"},{"location":"writing_filter/#bufferinputinstance","text":"In case an Instance cannot be processed immediately, one can use this method to buffer them in the input queue. All buffered Instance objects are available via the getInputFormat() method.","title":"bufferInput(Instance)"},{"location":"writing_filter/#pushinstance","text":"push(Instance) adds the given Instance to the output queue.","title":"push(Instance)"},{"location":"writing_filter/#output","text":"Returns the next Instance object from the output queue and removes it from there. In case there is no Instance available this method returns null .","title":"output()"},{"location":"writing_filter/#batchfinished","text":"The batchFinished() method signifies the end of a dataset being pushed through the filter. In case of a filter that couldn't process the data of the first batch immediately, this is the place to determine what the output format will be (and set if via setOutputFormat(Instances) ) and process the actual data. The currently available data can be retrieved with the getInputFormat() method. After processing the data, one needs to call flushInput() to remove all the pending input data.","title":"batchFinished()"},{"location":"writing_filter/#flushinput","text":"flushInput() removes all buffered Instance objects from the input queue. This method must be called after all the Instance objects have been processed in the batchFinished() method.","title":"flushInput()"},{"location":"writing_filter/#option-handling","text":"If the filter should be able to handle commandline options, then the weka.core.OptionHandler interface needs to be implemented. In addition to that, the following code should be added at the end of the setOptions(String[]) method: if (getInputFormat() != null) setInputFormat(getInputFormat()); This will inform the filter about changes in the options and therefore reset it.","title":"Option handling"},{"location":"writing_filter/#examples","text":"The following examples are to illustrate the filter framework. Note: unseeded random number generators like Math.random() should never be used since they will produce different results in each run and repeatable results are essential in machine learning.","title":"Examples"},{"location":"writing_filter/#batchfilter","text":"This simple batch filter adds a new attribute called //bla// at the end of the dataset. The rows of this attribute contain only the row's index in the data. Since the batch-filter need not see all the data before creating the output format, the setInputFormat(Instances) sets the output format and returns true (indicating that the output format can be queried immediately). The batchFinished() method performs the processing of all the data. import weka.core.*; import weka.core.Capabilities.*; public class BatchFilter extends Filter { public String globalInfo() { return A batch filter that adds an additional attribute 'bla' at the end + containing the index of the processed instance. The output format + can be collected immediately. ; } public Capabilities getCapabilities() { Capabilities result = super.getCapabilities(); result.enableAllAttributes(); result.enableAllClasses(); result.enable(Capability.NO_CLASS); // filter doesn't need class to be set return result; } public boolean setInputFormat(Instances instanceInfo) throws Exception { super.setInputFormat(instanceInfo); Instances outFormat = new Instances(instanceInfo, 0); outFormat.insertAttributeAt(new Attribute( bla ), outFormat.numAttributes()); setOutputFormat(outFormat); return true; // output format is immediately available } public boolean batchFinished() throws Exception { if (getInputFormat() = null) throw new NullPointerException( No input instance format defined ); Instances inst = getInputFormat(); Instances outFormat = getOutputFormat(); for (int i = 0; i inst.numInstances(); i++) { double[] newValues = new double[outFormat.numAttributes()]; double[] oldValues = inst.instance(i).toDoubleArray(); System.arraycopy(oldValues, 0, newValues, 0, oldValues.length); newValues[newValues.length - 1] = i; push(new Instance(1.0, newValues)); } flushInput(); m_NewBatch = true; m_FirstBatchDone = true; return (numPendingOutput() != 0); } public static void main(String[] args) { runFilter(new BatchFilter(), args); } }","title":"BatchFilter"},{"location":"writing_filter/#batchfilter2","text":"In contrast to the first batch filter, this one here cannot determine the output format immediately (the number of instances in the first batch is part of the attribute name now). This is done in the batchFinished() method. import weka.core.*; import weka.core.Capabilities.*; public class BatchFilter2 extends Filter { public String globalInfo() { return A batch filter that adds an additional attribute 'bla' at the end + containing the index of the processed instance. The output format + cannot be collected immediately. ; } public Capabilities getCapabilities() { Capabilities result = super.getCapabilities(); result.enableAllAttributes(); result.enableAllClasses(); result.enable(Capability.NO_CLASS); // filter doesn't need class to be set return result; } public boolean batchFinished() throws Exception { if (getInputFormat() = null) throw new NullPointerException( No input instance format defined ); // output format still needs to be set (depends on first batch of data) if (!isFirstBatchDone()) { Instances outFormat = new Instances(getInputFormat(), 0); outFormat.insertAttributeAt(new Attribute( bla- + getInputFormat().numInstances()), outFormat.numAttributes()); setOutputFormat(outFormat); } Instances inst = getInputFormat(); Instances outFormat = getOutputFormat(); for (int i = 0; i inst.numInstances(); i++) { double[] newValues = new double[outFormat.numAttributes()]; double[] oldValues = inst.instance(i).toDoubleArray(); System.arraycopy(oldValues, 0, newValues, 0, oldValues.length); newValues[newValues.length - 1] = i; push(new Instance(1.0, newValues)); } flushInput(); m_NewBatch = true; m_FirstBatchDone = true; return (numPendingOutput() != 0); } public static void main(String[] args) { runFilter(new BatchFilter2(), args); } }","title":"BatchFilter2"},{"location":"writing_filter/#batchfilter3","text":"As soon as this batch filter's first batch is done, it can process Instance objects immediately in the input(Instance) method. It adds a new attribute which contains just a random number, but the random number generator being used is seeded with the number of instances from the first batch. import weka.core.*; import weka.core.Capabilities.*; import java.util.Random; public class BatchFilter3 extends Filter { protected int m_Seed; protected Random m_Random; public String globalInfo() { return A batch filter that adds an attribute 'bla' at the end + containing a random number. The output format cannot be collected + immediately. ; } public Capabilities getCapabilities() { Capabilities result = super.getCapabilities(); result.enableAllAttributes(); result.enableAllClasses(); result.enable(Capability.NO_CLASS); // filter doesn't need class to be set return result; } public boolean input(Instance instance) throws Exception { if (getInputFormat() = null) throw new NullPointerException( No input instance format defined ); if (isNewBatch()) { resetQueue(); m_NewBatch = false; } if (isFirstBatchDone()) convertInstance(instance); else bufferInput(instance); return isFirstBatchDone(); } public boolean batchFinished() throws Exception { if (getInputFormat() = null) throw new NullPointerException( No input instance format defined ); // output format still needs to be set (random number generator is seeded // with number of instances of first batch) if (!isFirstBatchDone()) { m_Seed = getInputFormat().numInstances(); Instances outFormat = new Instances(getInputFormat(), 0); outFormat.insertAttributeAt(new Attribute( bla- + getInputFormat().numInstances()), outFormat.numAttributes()); setOutputFormat(outFormat); } Instances inst = getInputFormat(); for (int i = 0; i inst.numInstances(); i++) { convertInstance(inst.instance(i)); } flushInput(); m_NewBatch = true; m_FirstBatchDone = true; m_Random = null; return (numPendingOutput() != 0); } protected void convertInstance(Instance instance) { if (m_Random = null) m_Random = new Random(m_Seed); double[] newValues = new double[instance.numAttributes() + 1]; double[] oldValues = instance.toDoubleArray(); newValues[newValues.length - 1] = m_Random.nextInt(); System.arraycopy(oldValues, 0, newValues, 0, oldValues.length); push(new Instance(1.0, newValues)); } public static void main(String[] args) { runFilter(new BatchFilter3(), args); } }","title":"BatchFilter3"},{"location":"writing_filter/#streamfilter","text":"This stream filter adds a random number at the end of each instance of the input data. Since this doesn't rely on having access to the full data of the first batch, the output format is accessible immediately after using setInputFormat(Instances) . All the Instance objects are immediately processed in input(Instance) via the convertInstance(Instance) method, which pushes them immediately to the output queue. import weka.core.*; import weka.core.Capabilities.*; import java.util.Random; public class StreamFilter extends Filter { protected Random m_Random; public String globalInfo() { return A stream filter that adds an attribute 'bla' at the end + containing a random number. The output format can be collected + immediately. ; } public Capabilities getCapabilities() { Capabilities result = super.getCapabilities(); result.enableAllAttributes(); result.enableAllClasses(); result.enable(Capability.NO_CLASS); // filter doesn't need class to be set return result; } public boolean setInputFormat(Instances instanceInfo) throws Exception { super.setInputFormat(instanceInfo); Instances outFormat = new Instances(instanceInfo, 0); outFormat.insertAttributeAt(new Attribute( bla ), outFormat.numAttributes()); setOutputFormat(outFormat); m_Random = new Random(1); return true; // output format is immediately available } public boolean input(Instance instance) throws Exception { if (getInputFormat() = null) throw new NullPointerException( No input instance format defined ); if (isNewBatch()) { resetQueue(); m_NewBatch = false; } convertInstance(instance); return true; // can be immediately collected via output() } protected void convertInstance(Instance instance) { double[] newValues = new double[instance.numAttributes() + 1]; double[] oldValues = instance.toDoubleArray(); newValues[newValues.length - 1] = m_Random.nextInt(); System.arraycopy(oldValues, 0, newValues, 0, oldValues.length); push(new Instance(1.0, newValues)); } public static void main(String[] args) { runFilter(new StreamFilter(), args); } }","title":"StreamFilter"},{"location":"writing_filter/#simplebatchfilter","text":"Only the following abstract methods need to be implemented: globalInfo() - returns a short description of what the filter does; will be displayed in the GUI determineOutputFormat(Instances) - generates the new format, based on the input data process(Instances) - processes the whole dataset in one go getRevision() - returns the Subversion revision information If you need access to the full input dataset in determineOutputFormat(Instances) , then you need to also override the method allowAccessToFullInputFormat() and make it return true. If more options are necessary, then the following methods need to be overridden: listOptions() - returns an enumeration of the available options; these are printed if one calls the filter with the -h option setOptions(String[]) - parses the given option array, that were passed from commandline getOptions() - returns an array of options, resembling the current setup of the filter In the following an example implementation that adds an additional attribute at the end, containing the index of the processed instance: import weka.core.*; import weka.core.Capabilities.*; import weka.filters.*; public class SimpleBatch extends SimpleBatchFilter { public String globalInfo() { return A simple batch filter that adds an additional attribute 'bla' at the end + containing the index of the processed instance. ; } public Capabilities getCapabilities() { Capabilities result = super.getCapabilities(); result.enableAllAttributes(); result.enableAllClasses(); result.enable(Capability.NO_CLASS); //// filter doesn't need class to be set// return result; } protected Instances determineOutputFormat(Instances inputFormat) { Instances result = new Instances(inputFormat, 0); result.insertAttributeAt(new Attribute( bla ), result.numAttributes()); return result; } protected Instances process(Instances inst) { Instances result = new Instances(determineOutputFormat(inst), 0); for (int i = 0; i inst.numInstances(); i++) { double[] values = new double[result.numAttributes()]; for (int n = 0; n inst.numAttributes(); n++) values[n] = inst.instance(i).value(n); values[values.length - 1] = i; result.add(new Instance(1, values)); } return result; } public static void main(String[] args) { runFilter(new SimpleBatch(), args); } }","title":"SimpleBatchFilter"},{"location":"writing_filter/#simplestreamfilter","text":"Only the following abstract methods need to be implemented: globalInfo() - returns a short description of what the filter does; will be displayed in the GUI determineOutputFormat(Instances) - generates the new format, based on the input data process(Instance) processes a single instance and turns it from the old format into the new one getRevision() - returns the Subversion revision information The reset() method is only used, since the random number generator needs to be re-initialized in order to obtain repeatable results. If more options are necessary, then the following methods need to be overridden: listOptions() - returns an enumeration of the available options; these are printed if one calls the filter with the -h option setOptions(String[]) - parses the given option array, that were passed from commandline getOptions() - returns an array of options, resembling the current setup of the filter In the following an example implementation of a stream filter that adds an extra attribute at the end, which is filled with random numbers: import weka.core.*; import weka.core.Capabilities.*; import weka.filters.*; import java.util.Random; public class SimpleStream extends SimpleStreamFilter { protected Random m_Random; public String globalInfo() { return A simple stream filter that adds an attribute 'bla' at the end + containing a random number. ; } public Capabilities getCapabilities() { Capabilities result = super.getCapabilities(); result.enableAllAttributes(); result.enableAllClasses(); result.enable(Capability.NO_CLASS); //// filter doesn't need class to be set// return result; } protected void reset() { super.reset(); m_Random = new Random(1); } protected Instances determineOutputFormat(Instances inputFormat) { Instances result = new Instances(inputFormat, 0); result.insertAttributeAt(new Attribute( bla ), result.numAttributes()); return result; } protected Instance process(Instance inst) { double[] values = new double[inst.numAttributes() + 1]; for (int n = 0; n inst.numAttributes(); n++) values[n] = inst.value(n); values[values.length - 1] = m_Random.nextInt(); Instance result = new Instance(1, values); return result; } public static void main(String[] args) { runFilter(new SimpleStream(), args); } } A real-world implementation of a stream filter is the MultiFilter (package weka.filters ), which passes the data through all the filters it contains. Depending on whether all the used filters are streamable or not, it acts either as a stream filter or as batch filter.","title":"SimpleStreamFilter"},{"location":"writing_filter/#internals","text":"Some useful methods of the filter classes: isNewBatch() - returns true if an instance of the filter was just instantiated via new or a new batch was started via the batchFinished() method. isFirstBatchDone() - returns true as soon as the first batch was finished via the batchFinished() method. Useful for supervised filters, which should not be altered after being trained with the first batch of instances.","title":"Internals"},{"location":"writing_filter/#revisions","text":"Filters also implement the weka.core.RevisionHandler interface. This provides the functionality of obtaining the Subversion revision from within Java. Filters that are not part of the official Weka distribution will have to implement the method getRevision() as follows, which will return a dummy revision of 1.0 : /** * Returns the revision string. * * @return the revision */ public String getRevision() { return RevisionUtils.extract( $Revision: 1.0 $ ); }","title":"Revisions"},{"location":"writing_filter/#integration","text":"After finishing the coding stage, it's time to integrate your filter in the Weka framework, i.e., to make it available in the Explorer, Experimenter, etc. The [[GenericObjectEditor]] article shows you how to tell Weka where to find your filter and therefore displaying it in the GenericObjectEditor (filters work in the same fashion as classifiers, regarding the discovery).","title":"Integration"},{"location":"writing_filter/#testing","text":"Weka provides already a test framework to ensure the basic functionality of a filter. It is essential for the filter to pass these tests.","title":"Testing"},{"location":"writing_filter/#option-handling_1","text":"If your filter implements weka.core.OptionHandler , check the option handling of your filter with the following tool from commandline: weka.core.CheckOptionHandler -W classname [-- additional parameters] All tests need to return yes .","title":"Option handling"},{"location":"writing_filter/#genericobjecteditor","text":"The CheckGOE class checks whether all the properties available in the GUI have a tooltip accompanying them and whether the globalInfo() method is declared: weka.core.CheckGOE -W classname [-- additional parameters] All tests, once again, need to return yes .","title":"GenericObjectEditor"},{"location":"writing_filter/#source-code","text":"Filters that implement the weka.filters.Sourcable interface can output Java code of their internal representation. In order to check the generated code, one should not only compile the code, but also test it with the following test class: weka.filters.CheckSource This class takes the original Weka filter, the generated code and the dataset used for generating the source code (and an optional class index) as parameters. It builds the Weka filter on the dataset and compares the output, the one from the Weka filter and the one from the generated source code, whether they are the same. Here's an example call for weka.filters.unsupervised.attribute.ReplaceMissingValues and the generated class weka.filters.WekaWrapper (it wraps the actual generated code in a pseudo-filter): java weka.filters.CheckSource \\ -W weka.filters.unsupervised.attribute.ReplaceMissingValues \\ -S weka.filters.WekaWrapper \\ -t data.arff It needs to return Tests OK! .","title":"Source code"},{"location":"writing_filter/#unit-tests","text":"In order to make sure that your filter applies to the Weka criteria, you should add your filter to the junit unit test framework, i.e., by creating a Test class. How to check out the unit test framework, you can find here .","title":"Unit tests"},{"location":"writing_filter/#see-also","text":"[[GenericObjectEditor|GenericObjectEditor/GenericPropertiesCreator]]","title":"See also"},{"location":"writing_filter/#links","text":"junit","title":"Links"},{"location":"faqs/arff_does_not_load/","text":"One way to figure out why ARFF files are failing to load is to give them to the weka.core.Instances class. In the SimpleCLI or in the terminal, type the following: java weka.core.Instances filename.arff where you substitute filename for the actual name of your file. This should return an error if there is a problem reading the file, or show some statistics if the file is ok. The error message you get should give some indication of what is wrong. nominal value not declared in header, read Token[X], line Y If you get this error message than you seem to have declared a nominal attribute in the ARFF header section, but Weka came across a value ( \"X\" ) in the data (in line Y ) for this particular attribute that wasn't listed as possible value. All nominal values that appear in the data must be declared in the header.","title":" ARFF file does not load"},{"location":"faqs/arff_does_not_load/#nominal-value-not-declared-in-header-read-tokenx-line-y","text":"If you get this error message than you seem to have declared a nominal attribute in the ARFF header section, but Weka came across a value ( \"X\" ) in the data (in line Y ) for this particular attribute that wasn't listed as possible value. All nominal values that appear in the data must be declared in the header.","title":"nominal value not declared in header, read Token[X], line Y"},{"location":"faqs/commercial_applications/","text":"WEKA is licensed under the GNU General Public license ( GPL 2.0 for Weka 3.6 ) and ( GPL 3.0 for Weka 3.7.5 ). Any derivative work obtained under this license must be licensed under the GPL if this derivative work is distributed to a third party. For commercial projects that require the ability to distribute WEKA code as part of a program that cannot be distributed under the GPL, it may be possible to purchase an appropriate license from the copyright holders listed in the corresponding Java classes. The copyright for most WEKA code is owned by the University of Waikato. For information on licenses for this code please contact WaikatoLink , the commercialization unit of the University of Waikato , by sending email to weka-enquiries at waikatolink.co.nz.","title":" Can I use WEKA in commercial applications?"},{"location":"faqs/contribution/","text":"Information on how to contribute to WEKA can be found in the Contributing a package section of the How are packages structured for the package management system? article. The conditions for new classifiers (schemes in general) are that, firstly, they have to be published in the proceedings of a renowned conference (e.g., ICML) or as an article of respected journal (e.g., Machine Learning) and, secondly, that they outperform other standard schemes (e.g., J48/C4.5). But please bear in mind, that we don't have a lot of man power, i.e., being the WEKA maintainer is NOT a full-time position.","title":" How can I contribute to WEKA?"},{"location":"faqs/different_versions/","text":"Refer to History for a tabular overview of all Weka releases. Several branches are associated with the 1st, 2nd, 3rd, and 4th edition of the book Data Mining: Practical Machine Learning Tools and Techniques by Ian H. Witten and Eibe Frank , joined by Mark Hall for the 3rd edition and Chris Pal for the 4th edition. Once created, non-development branches receive bug fixes, but no new features (classifiers, filters, etc.). Version name Most recent base number Associated with book edition Book 1st ed. version 3.0.x 1st edition Old GUI version 3.2.x none Book 2nd ed. version 3.4.x 2nd edition Book 3rd ed. version 3.6.x 3rd edition Book 4th ed. version 3.8.x 4th edition Development version 3.9.x none For contributions , you should always develop against the developer version.","title":" What are the principal release branches of Weka?"},{"location":"faqs/latest_bugfixes/","text":"The article How to get the latest bugfixes explains it in detail (it's basically either obtaining the source code from Subversion and compiling it yourself or getting a snapshot from the download section of the WEKA homepage ).","title":" How do I get the latest bugfixes?"},{"location":"faqs/old_versions/","text":"If you need a specific version of WEKA, e.g., due to some third-party tools, go WEKA's project page on Sourceforge.net . In the Files section you have access to all the releases ever made.","title":" Where can I get old versions of WEKA?"},{"location":"faqs/package_manager_doesnt_start/","text":"The most likely reason for this is that your computer does not have direct access to the Internet and Java needs to be told to use a proxy server to access the web. The best way to achieve this is to configure an environment variable that provides the proxy details, e.g., _JAVA_OPTIONS which is read by Oracle Java virtual machines. There is more information on this variable here . Information on how to set environment variables in Windows is here . For Mac users, there is a nice program to set environment variables available here . Set the value of this variable to -Dhttp.proxyHost=some.proxy.somewhere.net -Dhttp.proxyPort=port where some.proxy.somewhere.net needs to be replaced by the name of your proxy server and port needs to be replaced by the appropriate port number on the proxy server. Your IT department should be able to give you these details. This should allow the package manager to connect to the website that hosts the package meta-information. However, if the package manager still cannot connect to the Internet, you can also force it to run in offline mode, by setting the above environment variable to -Dweka.packageManager.offline=true Then, you can download package .zip files manually via your web browser, by navigating to http://weka.sourceforge.net/packageMetaData/ clicking on the link for the package you want to install, then clicking on Latest , and finally clicking on the URL given next to PackageURL . Once you have downloaded the package .zip file, open the WEKA package manager, and click on the File/URL button in the top-right corner of the package manager window (in the Unofficial panel). Then navigate to your package .zip file and install it. If you are running Weka in offline mode, and the packages you are installing have some dependencies on one another, then there can still be some problems due to Weka not being able to verify the dependencies by checking against the central repository. This is usually a problem in the case where Weka has never been able to connect to the internet and thus has not downloaded and established a cache of the central package metadata repository. Fortunately there is a simple work-around to this, as long as you can access the internet via a web browser: Using your web browser, download http://weka.sourceforge.net/packageMetaData/repo.zip If it doesn't already exist, create the directory ~/wekafiles/repCache Copy the downloaded repo.zip into ~/wekafiles/repCache and unzip it there Start Weka (use the weka.packageManager.offline=true property to speed up the startup process; see [http://weka.wikispaces.com/How+do+I+use+the+package+manager%3F#Package%20manager%20property%20file] for info)","title":" Weka package manager does not start"},{"location":"not_so_faq/gsp/","text":"Class weka.associators.GeneralizedSequentialPatterns Publication Ramakrishnan Srikant, Rakesh Agrawal (1996). Mining Sequential Patterns: Generalizations and Performance Improvements. Downloads GeneralizedSequentialPattern_example.arff","title":" Generalized Sequential Patterns"},{"location":"not_so_faq/gsp/#class","text":"weka.associators.GeneralizedSequentialPatterns","title":"Class"},{"location":"not_so_faq/gsp/#publication","text":"Ramakrishnan Srikant, Rakesh Agrawal (1996). Mining Sequential Patterns: Generalizations and Performance Improvements.","title":"Publication"},{"location":"not_so_faq/gsp/#downloads","text":"GeneralizedSequentialPattern_example.arff","title":"Downloads"},{"location":"not_so_faq/j48_numbers/","text":"J48 pruned tree node-caps = yes | deg-malig = 1: recurrence-events (1.01/0.4) | deg-malig = 2: no-recurrence-events (26.2/8.0) | deg-malig = 3: recurrence-events (30.4/7.4) node-caps = no: no-recurrence-events (228.39/53.4) The first number is the total number of instances (weight of instances) reaching the leaf. The second number is the number (weight) of those instances that are misclassified. If your data has missing attribute values then you will end up with fractional instances at the leafs. When splitting on an attribute where some of the training instances have missing values, J48 will divide a training instance with a missing value for the split attribute up into fractional parts proportional to the frequencies of the observed non-missing values. This is discussed in the Witten Frank Data Mining book as well as Ross Quinlan's original publications on C4.5.","title":" What do those numbers mean in a J48 tree?"},{"location":"packages/","text":"Weka 3.7.2 introduced support for packages, making it easy to extend Weka without having to recompile or patch the underlying Weka installation. Here are some pointers for using and developing packages: How do I use the package manager? Unofficial packages How are packages structured for the package management system?","title":"Packages"},{"location":"packages/manager/","text":"Usually, the term \"package\" is used to refer to Java's concept of organizing classes. From version 3.7.2, Weka has the concept of a package as a bundle of additional functionality, separate from that supplied in the main weka.jar file. A package consists of various jar files, documentation, meta data, and possibly source code. Many learning algorithms and tools that were present in earlier versions of Weka have become separate packages from version 3.7.2. This simplifies the core Weka system and allows users to install just what they need or are interested in. It also provides a simple mechanism for people to use when contributing to Weka. There are a number of packages available for Weka that add learning schemes or extend the functionality of the core system in some fashion. Many are provided by the Weka team and others are from third parties. Weka includes a facility for the management of packages and a mechanism to load them dynamically at runtime. There is both a command-line and GUI package manager. If the package manager does not start when you try to run it, take a look at this page. Command line package management Assuming that the weka.jar file is in the classpath, the package manager can be accessed by typing: java weka.core.WekaPackageManager Supplying no options will print the usage information: Usage: weka.core.WekaPackageManager [option] Options: -list-packages all | installed | available -package-info repository | installed | archive packageName -install-package packageName | packageZip | URL [version] -uninstall-package packageName -refresh-cache Weka 3.7.8 and snapshot builds of the developer version of Weka after September 24 2012 now offer a completely \"offline\" mode that involves no attempts to connect to the internet. This mode can be used to install package zip files that the user already has on the file system, and to browse already installed packages. This mode can be accessed from the command line package manager by specifying the \"-offline\" option. Alternatively, the property weka.packageManager.offline=true can be provided to the Java virtual machine on the command line or in a properties file (see the section on properties below). Information (meta data) about packages is stored on a web server hosted on Sourceforge. The first time the package manager is run, for a new installation of Weka, there will be a short delay while the system downloads and stores a cache of the meta data from the server. Maintaining a cache speeds up the process of browsing the package information. From time to time you should update the local cache of package meta data in order to get the latest information on packages from the server. This can be achieved by supplying the -refresh-cache option. The -list-packages option will, as the name suggests, print information (version numbers and short descriptions) about various packages. The option must be followed by one of three keywords: all will print information on all packages that the system knows about installed will print information on all packages that are installed locally available will print information on all packages that are not installed The following shows an example of listing all packages installed locally: java weka.core.WekaPackageManager -list-packages installed Installed Repository Package ========= ========== ======= 1.0.0 1.0.0 DTNB: Class for building and using a decision table/naive bayes hybrid classifier. 1.0.0 1.0.0 massiveOnlineAnalysis: MOA (Massive On-line Analysis). 1.0.0 1.0.0 multiInstanceFilters: A collection of filters for manipulating multi-instance data. 1.0.0 1.0.0 naiveBayesTree: Class for generating a decision tree with naive Bayes classifiers at the leaves. 1.0.0 1.0.0 scatterPlot3D: A visualization component for displaying a 3D scatter plot of the data using Java 3D. The -package-info command lists information about a package given its name. The command is followed by one of three keywords and then the name of a package: repository will print info from the repository for the named package installed will print info on the installed version of the named package archive will print info for a package stored in a zip archive. In this case, the \u201carchive\u201d keyword must be followed by the path to an package zip archive file rather than just the name of a package The following shows an example of listing information for the \u201cisotonicRegression\u201d package from the server: java weka.core.WekaPackageManager -package-info repository isotonicRegression Description:Learns an isotonic regression model. Picks the attribute that results in the lowest squared error. Missing values are not allowed. Can only deal with numeric attributes. Considers the monotonically increasing case as well as the monotonically decreasing case. Version:1.0.0 PackageURL:http://prdownloads.sourceforge.net/weka/isotonicRegression1.0.0.zip?download Author:Eibe Frank PackageName:isotonicRegression Title:Learns an isotonic regression model. Date:2009-09-10 URL:http://weka.sourceforge.net/doc.dev/weka/classifiers/IsotonicRegression.html Category:Regression Depends:weka ( =3.7.1) License:GPL 2.0 Maintainer:Weka team wekalist@list.scms.waikato.ac.nz The -install-package command allows a package to be installed from one of three locations: specifying a name of a package will install the package using the information in the package description meta data stored on the server. If no version number is given, then the latest available version of the package is installed. providing a path to a zip file will attempt to unpack and install the archive as a Weka package providing a URL (beginning with http:// ) to a package zip file on the web will download and attempt to install the zip file as a Weka package The uninstall-package command will uninstall the named package. Of course, the named package has to be installed for this command to have any effect! Running installed learning algorithms Running learning algorithms that come with the main weka distribution (i.e. are contained in the weka.jar file) was covered earlier in the [[Primer]]. But what about algorithms from packages that you\u2019ve installed using the package manager? We don\u2019t want to have to add a ton of jar files to our classpath every time we wan\u2019t to run a particular algorithm. Fortunately, we don\u2019t have to. Weka has a mechanism to load installed packages dynamically at run time. This means that newly installed packages are available in Weka's GUIs immediately. What about running algorithms from packages on the command line I hear you ask? We can run a named algorithm by using the Run command: java weka.Run If no arguments are supplied, then Run outputs the following usage information: Usage: weka.Run [-no-scan] [-no-load] scheme name [scheme options] The Run command supports sub-string matching, so you can run a classifier (such as J48) like so: java weka.Run J48 When there are multiple matches on a supplied scheme name you will be presented with a list. For example: java weka.Run NaiveBayes Select a scheme to run, or return to exit: 1) weka.classifiers.bayes.ComplementNaiveBayes 2) weka.classifiers.bayes.NaiveBayes 3) weka.classifiers.bayes.NaiveBayesMultinomial 4) weka.classifiers.bayes.NaiveBayesMultinomialUpdateable 5) weka.classifiers.bayes.NaiveBayesSimple 6) weka.classifiers.bayes.NaiveBayesUpdateable Enter a number You can turn off the scanning of packages and sub-string matching by providing the -no-scan option. This is useful when using the Run command in a script. In this case, you need to specify the fully qualified name of the algorithm to use. E.g. java weka.Run -no-scan weka.classifiers.bayes.NaiveBayes To reduce startup time you can also turn off the dynamic loading of installed packages by specifying the -no-load option. In this case, you will need to explicitly include any packaged algorithms in your classpath if you plan to use them. E.g. java -classpath ./weka.jar:$HOME/wekafiles/packages/DTNB/DTNB.jar rweka.Run -no-load -no-scan weka.classifiers.rules.DTNB GUI package manager As well as a command line client, there is also a graphical interface to Weka\u2019s package management system. This is available from the Tools menu in the GUIChooser . All the functionality available in the command line client to the package management system is available in the GUI version, along with the ability to install and uninstall multiple packages in one hit. The package manager\u2019s window is split horizontally into two parts: at the top is a list of packages and at the bottom is a mini browser that can be used to display information on the currently selected package. The package list shows the name of a package, its category, the currently installed version (if installed), the latest version available via the repository and whether the package has been loaded or not. This list may be sorted by either package name or category by clicking on the appropriate column header. A second click on the same header reverses the sort order. Three radio buttons in the upper left of the window can be used to filter what is displayed in the list. All packages (default), all available packages (i.e. those not yet installed) or only installed packages can be displayed. If multiple versions of a package are available, they can be accessed by clicking on an entry in the \u201cRepository version\u201d column: Installing and removing packages At the very top of the window are three buttons. On the left-hand side is a button that can be used to refresh the cached copy of the package repository meta data. The first time that the package manager (GUI or command line) is used there will be a short delay as the initial cache is established. NOTE: Weka (3.7.2) will not automatically check for new information at the central repository, so it is a good idea to refresh the local cache regularly. From Weka 3.7.3 the package manager will notify you if there are new packages available at the central repository . The two buttons at the top right are used to install and remove packages repspectively. Multiple packages may be installed/removed by using a shift-left-click combination to select a range and/or by using a command-left-click combination to add to the selection. Underneath the install and uninstall but- tons is a checkbox that can be enabled to ignore any dependencies required by selected packages and any conflicts that may occur. Installing packages while this checkbox is selected will '''not''' install required dependencies. Some packages may have additional information on how to complete the installation or special instructions that gets displayed when the package is installed: Usually it is not necessary to restart Weka after packages have been installed\u2014the changes should be available immediately. An exception is when upgrading a package that is already installed. If in doubt, restart Weka. Unofficial packages The package list shows those packages that have their meta data stored in Weka\u2019s central meta data repository. These packages are \u201cofficial\u201d Weka packages and the Weka team as verified that they appear to provide what is advertised (and do not contain malicious code or malware). It is also possible to install an unofficial package that has not gone through the process of become official. Unofficial packages might be provided, for exam- ple, by researchers who want to make experimental algorithms quickly available to the community for feedback. Unofficial packages can be installed by clicking the \u201cFile/url\u201d button on the top-right of the package manager window. This will bring up an \u201cUnnoficial package install\u201d dialog where the user can browse their file system for a package zip file or directly enter an URL to the package zip file. Note that no dependency checking is done for unofficial packages. Using a HTTP proxy Both the GUI and command line package managers can operate via a http proxy. To do so, start Weka from the command line and supply property values for the proxy host and port: java -Dhttp.proxyHost=some.proxy.somewhere.net -Dhttp.proxyPort=port weka.gui.GUIChooser If your proxy requires authentication, then Weka will present a GUI dialog where you can enter your username and password. If you are running on a headless environment, then two more (non-standard) properties can be supplied: -Dhttp.proxyUser=some_user_name -Dhttp.proxyPassword=some_password Using an alternative central package meta data repository By default, both the command-line and GUI package managers use the central package meta data repository hosted on Sourceforge. In the unlikely event that this site is unavailable for one reason or another, it is possible to point the package management system at an alternative repository. This mechanism allows a temporary backup of the official repostory to be accessed, local mirrors to be established and alternative repositories to be set up for use etc. An alternative repository can be specified by setting a Java property: weka.core.wekaPackageRepositoryURL=http://some.mirror.somewhere This can either be set when starting Weka from the command line with the -D flag, or it can be placed into a file called \u201cPackageRepository.props\u201d in $WEKA_HOME/props . The default value of WEKA_HOME is user.home/wekafiles , where user.home is the user\u2019s home directory. More information on how and where Weka stores configuration information is given in the [[How are packages structured for the package management system?|how are package structured for the package management system?]] article. Package manager property file As mentioned in the previous section, an alternative package meta data repository can be specified by placing an entry in the PackageRepository.props file in $WEKA_HOME/props . From Weka 3.7.8 (and snapshot builds after 24 September 2012), the package manager also looks for properties placed in $WEKA_HOME/props/PackageManager.props . The current set of properties that can be set are: weka.core.wekaPackageRepositoryURL=http://some.mirror.somewhere weka.packageManager.offline=[true | false] weka.packageManager.loadPackages=[true | false] weka.pluginManager.disable=com.funky.FunkyExplorerPluginTab The default for offline mode (if unspecified) is false and for loadPackages is true . The weka.pluginManager.disable property can be used to specify a comma-separated list of fully qualified class names to \"disable\" in the GUI. This can be used to make problematic components unavailable in the GUI without having to prevent the entire package that contains them from being loaded. E.g. \"funkyPackage\" might provide several classifiers and a special Explorer plugin tab for visualization. Suppose, for example, that the plugin Explorer tab has issues with certain data sets and causes annoying exceptions to be generated (or perhaps in the worst cases crashes the Explorer!). In this case we might want to use the classifiers provided by the package and just disable the Explorer plugin. Listing the fully qualified name of the Explorer plugin as a member of the comma-separated list associated with the weka.pluginManager.disable property will achieve this.","title":" Package manager"},{"location":"packages/manager/#command-line-package-management","text":"Assuming that the weka.jar file is in the classpath, the package manager can be accessed by typing: java weka.core.WekaPackageManager Supplying no options will print the usage information: Usage: weka.core.WekaPackageManager [option] Options: -list-packages all | installed | available -package-info repository | installed | archive packageName -install-package packageName | packageZip | URL [version] -uninstall-package packageName -refresh-cache Weka 3.7.8 and snapshot builds of the developer version of Weka after September 24 2012 now offer a completely \"offline\" mode that involves no attempts to connect to the internet. This mode can be used to install package zip files that the user already has on the file system, and to browse already installed packages. This mode can be accessed from the command line package manager by specifying the \"-offline\" option. Alternatively, the property weka.packageManager.offline=true can be provided to the Java virtual machine on the command line or in a properties file (see the section on properties below). Information (meta data) about packages is stored on a web server hosted on Sourceforge. The first time the package manager is run, for a new installation of Weka, there will be a short delay while the system downloads and stores a cache of the meta data from the server. Maintaining a cache speeds up the process of browsing the package information. From time to time you should update the local cache of package meta data in order to get the latest information on packages from the server. This can be achieved by supplying the -refresh-cache option. The -list-packages option will, as the name suggests, print information (version numbers and short descriptions) about various packages. The option must be followed by one of three keywords: all will print information on all packages that the system knows about installed will print information on all packages that are installed locally available will print information on all packages that are not installed The following shows an example of listing all packages installed locally: java weka.core.WekaPackageManager -list-packages installed Installed Repository Package ========= ========== ======= 1.0.0 1.0.0 DTNB: Class for building and using a decision table/naive bayes hybrid classifier. 1.0.0 1.0.0 massiveOnlineAnalysis: MOA (Massive On-line Analysis). 1.0.0 1.0.0 multiInstanceFilters: A collection of filters for manipulating multi-instance data. 1.0.0 1.0.0 naiveBayesTree: Class for generating a decision tree with naive Bayes classifiers at the leaves. 1.0.0 1.0.0 scatterPlot3D: A visualization component for displaying a 3D scatter plot of the data using Java 3D. The -package-info command lists information about a package given its name. The command is followed by one of three keywords and then the name of a package: repository will print info from the repository for the named package installed will print info on the installed version of the named package archive will print info for a package stored in a zip archive. In this case, the \u201carchive\u201d keyword must be followed by the path to an package zip archive file rather than just the name of a package The following shows an example of listing information for the \u201cisotonicRegression\u201d package from the server: java weka.core.WekaPackageManager -package-info repository isotonicRegression Description:Learns an isotonic regression model. Picks the attribute that results in the lowest squared error. Missing values are not allowed. Can only deal with numeric attributes. Considers the monotonically increasing case as well as the monotonically decreasing case. Version:1.0.0 PackageURL:http://prdownloads.sourceforge.net/weka/isotonicRegression1.0.0.zip?download Author:Eibe Frank PackageName:isotonicRegression Title:Learns an isotonic regression model. Date:2009-09-10 URL:http://weka.sourceforge.net/doc.dev/weka/classifiers/IsotonicRegression.html Category:Regression Depends:weka ( =3.7.1) License:GPL 2.0 Maintainer:Weka team wekalist@list.scms.waikato.ac.nz The -install-package command allows a package to be installed from one of three locations: specifying a name of a package will install the package using the information in the package description meta data stored on the server. If no version number is given, then the latest available version of the package is installed. providing a path to a zip file will attempt to unpack and install the archive as a Weka package providing a URL (beginning with http:// ) to a package zip file on the web will download and attempt to install the zip file as a Weka package The uninstall-package command will uninstall the named package. Of course, the named package has to be installed for this command to have any effect!","title":"Command line package management"},{"location":"packages/manager/#running-installed-learning-algorithms","text":"Running learning algorithms that come with the main weka distribution (i.e. are contained in the weka.jar file) was covered earlier in the [[Primer]]. But what about algorithms from packages that you\u2019ve installed using the package manager? We don\u2019t want to have to add a ton of jar files to our classpath every time we wan\u2019t to run a particular algorithm. Fortunately, we don\u2019t have to. Weka has a mechanism to load installed packages dynamically at run time. This means that newly installed packages are available in Weka's GUIs immediately. What about running algorithms from packages on the command line I hear you ask? We can run a named algorithm by using the Run command: java weka.Run If no arguments are supplied, then Run outputs the following usage information: Usage: weka.Run [-no-scan] [-no-load] scheme name [scheme options] The Run command supports sub-string matching, so you can run a classifier (such as J48) like so: java weka.Run J48 When there are multiple matches on a supplied scheme name you will be presented with a list. For example: java weka.Run NaiveBayes Select a scheme to run, or return to exit: 1) weka.classifiers.bayes.ComplementNaiveBayes 2) weka.classifiers.bayes.NaiveBayes 3) weka.classifiers.bayes.NaiveBayesMultinomial 4) weka.classifiers.bayes.NaiveBayesMultinomialUpdateable 5) weka.classifiers.bayes.NaiveBayesSimple 6) weka.classifiers.bayes.NaiveBayesUpdateable Enter a number You can turn off the scanning of packages and sub-string matching by providing the -no-scan option. This is useful when using the Run command in a script. In this case, you need to specify the fully qualified name of the algorithm to use. E.g. java weka.Run -no-scan weka.classifiers.bayes.NaiveBayes To reduce startup time you can also turn off the dynamic loading of installed packages by specifying the -no-load option. In this case, you will need to explicitly include any packaged algorithms in your classpath if you plan to use them. E.g. java -classpath ./weka.jar:$HOME/wekafiles/packages/DTNB/DTNB.jar rweka.Run -no-load -no-scan weka.classifiers.rules.DTNB","title":"Running installed learning algorithms"},{"location":"packages/manager/#gui-package-manager","text":"As well as a command line client, there is also a graphical interface to Weka\u2019s package management system. This is available from the Tools menu in the GUIChooser . All the functionality available in the command line client to the package management system is available in the GUI version, along with the ability to install and uninstall multiple packages in one hit. The package manager\u2019s window is split horizontally into two parts: at the top is a list of packages and at the bottom is a mini browser that can be used to display information on the currently selected package. The package list shows the name of a package, its category, the currently installed version (if installed), the latest version available via the repository and whether the package has been loaded or not. This list may be sorted by either package name or category by clicking on the appropriate column header. A second click on the same header reverses the sort order. Three radio buttons in the upper left of the window can be used to filter what is displayed in the list. All packages (default), all available packages (i.e. those not yet installed) or only installed packages can be displayed. If multiple versions of a package are available, they can be accessed by clicking on an entry in the \u201cRepository version\u201d column:","title":"GUI package manager"},{"location":"packages/manager/#installing-and-removing-packages","text":"At the very top of the window are three buttons. On the left-hand side is a button that can be used to refresh the cached copy of the package repository meta data. The first time that the package manager (GUI or command line) is used there will be a short delay as the initial cache is established. NOTE: Weka (3.7.2) will not automatically check for new information at the central repository, so it is a good idea to refresh the local cache regularly. From Weka 3.7.3 the package manager will notify you if there are new packages available at the central repository . The two buttons at the top right are used to install and remove packages repspectively. Multiple packages may be installed/removed by using a shift-left-click combination to select a range and/or by using a command-left-click combination to add to the selection. Underneath the install and uninstall but- tons is a checkbox that can be enabled to ignore any dependencies required by selected packages and any conflicts that may occur. Installing packages while this checkbox is selected will '''not''' install required dependencies. Some packages may have additional information on how to complete the installation or special instructions that gets displayed when the package is installed: Usually it is not necessary to restart Weka after packages have been installed\u2014the changes should be available immediately. An exception is when upgrading a package that is already installed. If in doubt, restart Weka.","title":"Installing and removing packages"},{"location":"packages/manager/#unofficial-packages","text":"The package list shows those packages that have their meta data stored in Weka\u2019s central meta data repository. These packages are \u201cofficial\u201d Weka packages and the Weka team as verified that they appear to provide what is advertised (and do not contain malicious code or malware). It is also possible to install an unofficial package that has not gone through the process of become official. Unofficial packages might be provided, for exam- ple, by researchers who want to make experimental algorithms quickly available to the community for feedback. Unofficial packages can be installed by clicking the \u201cFile/url\u201d button on the top-right of the package manager window. This will bring up an \u201cUnnoficial package install\u201d dialog where the user can browse their file system for a package zip file or directly enter an URL to the package zip file. Note that no dependency checking is done for unofficial packages.","title":"Unofficial packages"},{"location":"packages/manager/#using-a-http-proxy","text":"Both the GUI and command line package managers can operate via a http proxy. To do so, start Weka from the command line and supply property values for the proxy host and port: java -Dhttp.proxyHost=some.proxy.somewhere.net -Dhttp.proxyPort=port weka.gui.GUIChooser If your proxy requires authentication, then Weka will present a GUI dialog where you can enter your username and password. If you are running on a headless environment, then two more (non-standard) properties can be supplied: -Dhttp.proxyUser=some_user_name -Dhttp.proxyPassword=some_password","title":"Using a HTTP proxy"},{"location":"packages/manager/#using-an-alternative-central-package-meta-data-repository","text":"By default, both the command-line and GUI package managers use the central package meta data repository hosted on Sourceforge. In the unlikely event that this site is unavailable for one reason or another, it is possible to point the package management system at an alternative repository. This mechanism allows a temporary backup of the official repostory to be accessed, local mirrors to be established and alternative repositories to be set up for use etc. An alternative repository can be specified by setting a Java property: weka.core.wekaPackageRepositoryURL=http://some.mirror.somewhere This can either be set when starting Weka from the command line with the -D flag, or it can be placed into a file called \u201cPackageRepository.props\u201d in $WEKA_HOME/props . The default value of WEKA_HOME is user.home/wekafiles , where user.home is the user\u2019s home directory. More information on how and where Weka stores configuration information is given in the [[How are packages structured for the package management system?|how are package structured for the package management system?]] article.","title":"Using an alternative central package meta data repository"},{"location":"packages/manager/#package-manager-property-file","text":"As mentioned in the previous section, an alternative package meta data repository can be specified by placing an entry in the PackageRepository.props file in $WEKA_HOME/props . From Weka 3.7.8 (and snapshot builds after 24 September 2012), the package manager also looks for properties placed in $WEKA_HOME/props/PackageManager.props . The current set of properties that can be set are: weka.core.wekaPackageRepositoryURL=http://some.mirror.somewhere weka.packageManager.offline=[true | false] weka.packageManager.loadPackages=[true | false] weka.pluginManager.disable=com.funky.FunkyExplorerPluginTab The default for offline mode (if unspecified) is false and for loadPackages is true . The weka.pluginManager.disable property can be used to specify a comma-separated list of fully qualified class names to \"disable\" in the GUI. This can be used to make problematic components unavailable in the GUI without having to prevent the entire package that contains them from being loaded. E.g. \"funkyPackage\" might provide several classifiers and a special Explorer plugin tab for visualization. Suppose, for example, that the plugin Explorer tab has issues with certain data sets and causes annoying exceptions to be generated (or perhaps in the worst cases crashes the Explorer!). In this case we might want to use the classifiers provided by the package and just disable the Explorer plugin. Listing the fully qualified name of the Explorer plugin as a member of the comma-separated list associated with the weka.pluginManager.disable property will achieve this.","title":"Package manager property file"},{"location":"packages/structure/","text":"Articles such as [[How do I use WEKA's classes in my own code?]] and [[How do I write a new classifier or filter?]] describe how to extend Weka to add your own learning algorithms and so forth. This article describes how such enhancements can be assembled into a package that can be accessed via Weka\u2019s package management system. Bundling your enhancements in a package makes it easy to share with other Weka users. In this article we refer to a package as an archive containing various resources such as compiled code, source code, javadocs, package description files (meta data), third-party libraries and configuration property files. Not all of the preceding may be in a given package, and there may be other resources included as well. This concept of a package is quite different to that of a Java packages, which simply define how classes are arranged hierarchically. Where does WEKA store packages and other configuration stuff? By default, Weka stores packages and other information in $WEKA_HOME . The default location for WEKA_HOME is user.home/wekafiles , where user.home is the user\u2019s home directory. You can change the default location for WEKA_HOME by setting this either as an evironment variable for your platform, or by specifying it as a Java property when starting Weka. E.g.: export WEKA_HOME=/home/somewhere/weka_bits_and_bobs will set the directory that Weka uses to /home/somewhere/weka_bits_and_bobs under the LINUX operating system. The same thing can be accomplished when starting Weka by specifying a Java property on the command line, E.g.: java -DWEKA_HOME=/home/somewhere/weka_bits_and_bobs -jar weka.jar Inside $WEKA_HOME you will find the main weka log file (weka.log) and a number of directories: packages holds installed packages. Each package is contained its own subdirectory. props holds various Java property files used by Weka. This directory replaces the user\u2019s home directory (used in earlier releases of Weka) as one of the locations checked by Weka for properties files (such as DatabaseUtils.props ). Weka first checks, in order, the current directory (i.e. the directory that Weka is launched from), then $WEKA_HOME/props and finally the weka.jar file for property files. repCache holds the cached copy of the meta data from the central package repository. If the contents of this directory get corrupted it can be safely deleted and Weka will recreate it on the next restart. systemDialogs holds marker files that are created when you check Don\u2019t show this again in various system popup dialogs. Removing this directory or its contents will cause Weka to display those prompts anew. Anatomy of a package A Weka package is a zip archive that must unpack to the current directory. For example, the DTNB package contains the decision table naive Bayes hybrid classifier and is delivered in a file called DTNB.zip . When unpacked this zip file creates the following directory structure: current directory +-DTNB.jar +-Description.props +-build_package.xml +-src | +-main | | +-java | | +-weka | | +-classifiers | | +-rules | | +-DTNB.java | +-test | +-java | +-weka | +-classifiers | +-rules | +-DTNBTest.java +-lib +-doc When installing, the package manager will use the value of the \"PackageName\" field in the Description.props file (see below) to create a directory in $WEKA_HOME/packages to hold the package contents. The contents of the doc directory have not been shown in the diagram above, but this directory contains javadoc for the DTNB class. A package must have a Description.props file and contain at least one jar file with compiled Java classes. The package manager will attempt to load all jar files that it finds in the root directory and the lib directory. Other files are optional, but if the package is open-source then it is nice to include the source code and an ant build file that can be used to compile the code. Template versions of the Description.props file and build_package.xml file are available from the Weka site and here. ==The description file== A valid package must contain a Description.props file that provides meta data on the package. Identical files are stored at the central package repository and the local cache maintained by the package manager. The package manager uses these files to compare what is installed to what is available and resolve dependencies. The Description.props contains basic information on the package in the following format: # Template Description file for a Weka package # Package name (required) PackageName=funkyPackage # Version (required) Version=1.0.0 #Date (year-month-day) Date=2010-01-01 # Title (required) Title=My cool algorithm # Category (recommended) Category=Classification # Author (required) Author=Joe Dev joe@somewhere.net ,Dev2 dev2@somewhereelse.net # Maintainer (required) Maintainer=Joe Dev joe@somewhere.net # License (required) License=GPL 2.0|Mozilla # Description (required) Description=This package contains the famous Funky Classifer that performs \\ truely funky prediction. # Changes and/or bug fixes in this package (optional) Changes=Fixed a serious bug that affected overall coolness of the Funky Classifier # Package URL for obtaining the package archive (required) PackageURL=http://somewhere.net/weka/funkyPackage.zip # URL for further information URL=http://somewhere.net/funkyResearchInfo.html # Enhances various other packages? Enhances=packageName1,packageName2,... # Related to other packages? Related=packageName1,packageName2,... # Dependencies (required; format: packageName (equality/inequality version_number) Depends=weka ( =3.7.1), packageName1 (=x.y.z), packageName2 ( u.v.w| =x.y.z),... The PackageName and Version give the name of the package and version number respectively. The name can consist of letters, numbers, and the dot character. It should not start with a dot and should not contain any spaces. The version number is a sequence of three non-negative integers separated by single . or - characters. The Title field should give a one sentence description of the package. The Description field can give a longer description of the package spaning multiple sentences. It may include technical references and can use HTML markup. The Category field is strongly recommended as this information is displayed on both the repository web site and in the GUI package manager client. In the latter, the user can sort the packages on the basis of the category field. It is recommended that an existing category be assigned if possible. Some examples include (Classification, Text classification, Ensemble learning, Regression, Clustering, Associations, Preprocessing, Visualization, Explorer, Experimenter, KnowledgeFlow). The Author field describes who wrote the package and may include multiple names (separated by commas). Email addresses may be given in angle brackets after each name. The field is intended for human readers and no email addresses are automatically extracted. The Maintainer field lists who maintains the package and should include a single email address, enclosed in angle brackets, for sending bug reports to. The License field lists the license(s) that apply to the package. This field may contain the short specification of a license (such as LGPL, GPL 2.0 etc.) or the string file LICENSE , where LICENSE exists as a file in the top-level directory of the package. The string Unlimited may be supplied to indicate that there are no restrictions on distribution or use aside from those imposed by relevant laws. The PackageURL field lists valid URL that points to the package zip file. This URL is used by the package manager to download and install the package. The optional Depends field gives a comma separated list of packages which this package depends on. The name of a package is optionally followed by a version number constraint enclosed in parenthesis. Valid operators for version number constraints include =, , , =, =. The keyword weka is reserved to refer to the base Weka system and can be used to indicate a dependency on a particular version of Weka. For example: Depends=weka ( =3.7.2), DTNB (=1.0.0) states that this package requires Weka 3.7.2 or higher and version 1.0.0 of the package DTNB. Depends=weka ( 3.7.1| 3.8.0) states that this package requires a version of Weka between 3.7.1 and 3.8.0. Depends=DTNB ( 1.5.0| =2.0.1) states that this package requires that a version of the DTNB package be installed that is either less than version 1.5.0 or greater than or equal to version 2.0.1. If there is no version number constraint following a package name, the package manager assumes that the latest version of the dependent package is suitable. The optional URL field gives a URL at which the user can find additional online information about the package or its constituent algorithms. The optional Enhances field can be used to indicate which other packages this package is based on (i.e. if it extends methods/algorithms from another package in some fashion). The optional Related field is similar to the Enhances field. It can be used to point the user to other packages that are related in some fashion to this one. The optional Changes field should be used to indicate what changes/bug fixes are included in the current release of the package. There are several other fields that can be used to provide information to assist the user with completing installation (if it can\u2019t be completely accomplished with the package zip file) or display error messages if necessary components are missing: MessageToDisplayOnInstallation=Funky package requires some extra\\n\\ stuff to be installed after installing this package. You will\\n\\ need to blah, blah, blah in order to blah, blah, blah... DoNotLoadIfFileNotPresent=lib/someLibrary.jar,otherStuff/important,... DoNotLoadIfFileNotPresentMessage=funkyPackage can't be loaded because some \\ funky libraries are missing. Please download funkyLibrary.jar from \\ http://www.funky.com and install in $WEKA_HOME/packages/funkyPackage/lib DoNotLoadIfClassNotPresent=com.some.class.from.some.Where,org.some.other.Class,... DoNotLoadIfClassNotPresentMessage=funkyPackage can't be loaded because \\ com.funky.FunkyClass can't be instantiated. Have you downloaded and run \\ the funky software installer for your platform? The optional MessageToDisplayOnInstallation field allows you to specify special instructions to the user in order to help them complete the intallation manually. This message gets displayed on the console, written to the log and appears in a pop-up information dialog if using the GUI package manager. It should include \\n in order to avoid long lines when displayed in a GUI pop-up dialog. The optional DoNotLoadIfFileNotPresent field can be used to prevent Weka from loading the package if the named ''files'' and/or ''directories'' are not present in the package\u2019s installation directory. An example is the massiveOnlineAnalysis package. This package is a connector only package and does not include the MOA library. Users of this package must download the moa.jar file separately and copy it to the package\u2019s lib directory manually. Multiple files and directories can be specified as a comma separated list. All paths are relative to the top-level directory of the package. IMPORTANT : use forward slashes as separator characters, as these are portable accross all platforms. The DoNotLoadIfFileNotPresentMessage field can be used to supply an optional message to display to the user if Weka detects that a file or directory is missing from the package. This message will be displayed on the console and in the log. The optional DoNotLoadIfClassNotPresent field can be used to prevent Weka from loading the package if the named ''class(es)'' can\u2019t be instantiated. This is useful for packages that rely on stuff that has to be installed manually by the user. For example, Java3D is a separate download on all platforms except for OSX, and installs itself into the system JRE/JDK. The DoNotLoadIfClassNotPresentMessage field can be used to supply an optional message to display to the user if Weka detects that a class can\u2019t be instantiated. Again, this will be displayed on the console and in the log. New in Weka 3.9.2 and 3.8.2 is the ability to constrain the OS and architecture that a package can be installed and loaded on. Two new fields are used for this: # Specify which OS's the package can operate with. Omitting this entry indicates no restrictions on OS. (optional) OSName=Windows,Mac,Linux # Specify which architecture the package can operate with. Omitting this entry indicates no restriction. (optional) OSArch=64 Entries in the OSName field are compared against the value of the Java property \"os.name\" using a String.toLowerCase().contains() operation. Any single match indicates a pass. If an OSName field exists in the Description.props, and it matches, then the optional OSArch field is examined. If present, values in the OSArch list are compared against the value of the Java property \"os.arch\". The special OSArch entries \"32\" and \"64\" are tested using a String().contains() operation; all other entries are compared using String.equalsIgnoreCase(). Additional configuration files Certain types of packages may require additional configuration files to be present as part of the package. The last chapter covered various ways in which Weka can be extended without having to alter the core Weka code. These plugin mechanisms have been subsumed by the package management system, so some of the configuration property files they require must be present in the package\u2019s top-level directory if the package in question contains such a plugin. Examples include additional tabs for the Explorer, mappings to custom property editors for Weka\u2019s GenericObjectEditor and Knowledge Flow plugins. Here are some examples: The scatterPlot3D package adds a new tab to the Explorer. In order to accomplish this a property has to be set in the Explorer.props file (which contains default values for the Explorer) in order to tell Weka to instantiate and display the new panel. The scatterPlot3D file includes an Explorer.props file in its top-level directory that has the following contents: # Explorer.props file. Adds the Explorer3DPanel to the Tabs key. Tabs=weka.gui.explorer.Explorer3DPanel TabsPolicy=append This property file is read by the package management system when the package is loaded and any key-value pairs are added to existing Explorer properties that have been loaded by the system at startup. If the key already exists in the Explorer properties, then the package has the option to either replace (i.e. overwrite) or append to the existing value. This can be specified with the \"TabsPolicy\" key. In this case, the value weka.gui.explorer.Explorer3DPanel is appended to any existing value associated with the \"Tabs\" key. Explorer3DPanel gets instantiated and added as a new tab when the Explorer starts. Another example is the kfGroovy package. This package adds a plugin component to Weka\u2019s Knowledge Flow that allows a Knowledge Flow step to be implemented and compiled dynamically at runtime as a Groovy script. In order for the Knowledge Flow to make the new step appear in its Plugins toolbar, there needs to be a Beans.props file in the package\u2019s top level directory. In the case of kfGroovy, this property file has the following contents: # Specifies that this component goes into the Plugins toolbar weka.gui.beans.KnowledgeFlow.Plugins=org.pentaho.dm.kf.GroovyComponent The new pluggable evaluation metrics for classification/regression (from Weka 3.7.8 and nightly developer snapshots from 15-11-2012) are managed by the PluginManager class. To tell PluginManager that your package provides a new evaluation metric you need to provide a \"PluginManager.props\" file in the package's top level directory. For example, a hypothetical bobsMetric package might declare a new \"Area under Bob curve\" metric like so: # Specify a new plugin Evaluation metric weka.classifiers.evaluation.AbstractEvaluationMetric=weka.classifiers.evaluation.BobsAUC Contributing a package If you have created a package for Weka then there are two options for making it available to the community. In both cases, hosting the package\u2019s zip archive is the responsibility of the contributer. The first, and official, route is to contact the current Weka maintainer (normally also the admin of the WEKA homepage) and supply your package\u2019s Description.props file. The Weka team will then test downloading and using your package to make sure that there are no obvious problems with what has been specified in the Description.props file and that the software runs and does not contain any malware/malicious code. If all is well, then the package will become an official Weka package and the central package repository meta data will be updated with the package\u2019s Description.props file . Responsibility for maintaining and supporting the package resides with the contributer . The second, and unofficial, route is to simply make the package\u2019s zip archive available on the web somewhere and advertise it yourself. Although users will not be able to browse it\u2019s description in the official package repository, they will be able to download and install it directly from your URL by using the command line version of the package manager. This route could be attractive for people who have published a new algorithm and want to quiclky make a beta version available for others to try without having to go through the official route. Creating a mirror of the package meta data repository In this section we discuss an easy approach to setting up and maintaining a mirror of the package meta data repository. Having a local mirror may provide faster access times than to that of the official repository on Sourceforge. Extending this approach to the creation of an alternative central repository (hosting packages not available at the official repository) should be straight forward. Just about everything necessary for creating a mirror exists in the local meta data cache created by Weka\u2019s package management system. This cache resides at $WEKA_HOME/repCache . The only thing missing (in Weka 3.7.2) for a complete mirror is the file images.txt , that lists all the image files used in the html index files. This file contains the following two lines: Title-Bird-Header.gif pentaho_logo_rgb_sm.png images.txt is downloaded automatically by the package management system in Weka 3.7.3 and higher. To create a mirror: 1. Copy the contents of $WEKA_HOME/repCache to a temporary directory. For the purposes of this example we\u2019ll call it tempRep 2. Change directory into tempRep and run java weka.core.RepositoryIndexGenerator . . Don't forget the \".\" after the command (this tells RepoistoryIndexGenerator to operate on the current directory) 3. Change directory to the parent of tempRep and synchronize its contents to wherever your web server is located (this is easy via rsync under Nix-like operating systems). RepositoryIndexGenerator automatically creates the main index.html file, all the package index.html files and html files correpsonding to all version prop files for each package. It will also create packageList.txt and numPackages.txt files. IMPORTANT : Make sure that all the files in tempRep are world readable. It is easy to make packages available that are not part of the official Weka repository. Assuming you want to add a package called funkyPackage (as specified by the PackageName field in the Description.props file): Create a directory called funkyPackage in tempRep Copy the Description.props file to tempRep/funkyPackage/Latest.props Copy the Description.props file to tempRep/funkyPackage/ version number .props , where version number is the version number specified in the Version field of Description.props Run RepositoryIndexGenerator as described previously and sync tempRep to your web server Adding a new version of an existing package is very similar to what has already been described. All that is required is that the new Description.props file corresponding to the new version is copied to Latest.props and to version numer .props in the package\u2019s folder. Running RepositoryIndexGenerator will ensure that all necessary html files are created and supporting text files are updated. Automating the mirroring process would simply involve using your OS\u2019s scheduler to execute a script that: Runs weka.core.WekaPackageManager -refresh-cache rsyncs $WEKA_HOME/repCache to tempRep Runs weka.core.RepoistoryIndexGenerator rsyncs tempRep to your web server","title":" How are packages structured for the package management system?"},{"location":"packages/structure/#where-does-weka-store-packages-and-other-configuration-stuff","text":"By default, Weka stores packages and other information in $WEKA_HOME . The default location for WEKA_HOME is user.home/wekafiles , where user.home is the user\u2019s home directory. You can change the default location for WEKA_HOME by setting this either as an evironment variable for your platform, or by specifying it as a Java property when starting Weka. E.g.: export WEKA_HOME=/home/somewhere/weka_bits_and_bobs will set the directory that Weka uses to /home/somewhere/weka_bits_and_bobs under the LINUX operating system. The same thing can be accomplished when starting Weka by specifying a Java property on the command line, E.g.: java -DWEKA_HOME=/home/somewhere/weka_bits_and_bobs -jar weka.jar Inside $WEKA_HOME you will find the main weka log file (weka.log) and a number of directories: packages holds installed packages. Each package is contained its own subdirectory. props holds various Java property files used by Weka. This directory replaces the user\u2019s home directory (used in earlier releases of Weka) as one of the locations checked by Weka for properties files (such as DatabaseUtils.props ). Weka first checks, in order, the current directory (i.e. the directory that Weka is launched from), then $WEKA_HOME/props and finally the weka.jar file for property files. repCache holds the cached copy of the meta data from the central package repository. If the contents of this directory get corrupted it can be safely deleted and Weka will recreate it on the next restart. systemDialogs holds marker files that are created when you check Don\u2019t show this again in various system popup dialogs. Removing this directory or its contents will cause Weka to display those prompts anew.","title":"Where does WEKA store packages and other configuration stuff?"},{"location":"packages/structure/#anatomy-of-a-package","text":"A Weka package is a zip archive that must unpack to the current directory. For example, the DTNB package contains the decision table naive Bayes hybrid classifier and is delivered in a file called DTNB.zip . When unpacked this zip file creates the following directory structure: current directory +-DTNB.jar +-Description.props +-build_package.xml +-src | +-main | | +-java | | +-weka | | +-classifiers | | +-rules | | +-DTNB.java | +-test | +-java | +-weka | +-classifiers | +-rules | +-DTNBTest.java +-lib +-doc When installing, the package manager will use the value of the \"PackageName\" field in the Description.props file (see below) to create a directory in $WEKA_HOME/packages to hold the package contents. The contents of the doc directory have not been shown in the diagram above, but this directory contains javadoc for the DTNB class. A package must have a Description.props file and contain at least one jar file with compiled Java classes. The package manager will attempt to load all jar files that it finds in the root directory and the lib directory. Other files are optional, but if the package is open-source then it is nice to include the source code and an ant build file that can be used to compile the code. Template versions of the Description.props file and build_package.xml file are available from the Weka site and here. ==The description file== A valid package must contain a Description.props file that provides meta data on the package. Identical files are stored at the central package repository and the local cache maintained by the package manager. The package manager uses these files to compare what is installed to what is available and resolve dependencies. The Description.props contains basic information on the package in the following format: # Template Description file for a Weka package # Package name (required) PackageName=funkyPackage # Version (required) Version=1.0.0 #Date (year-month-day) Date=2010-01-01 # Title (required) Title=My cool algorithm # Category (recommended) Category=Classification # Author (required) Author=Joe Dev joe@somewhere.net ,Dev2 dev2@somewhereelse.net # Maintainer (required) Maintainer=Joe Dev joe@somewhere.net # License (required) License=GPL 2.0|Mozilla # Description (required) Description=This package contains the famous Funky Classifer that performs \\ truely funky prediction. # Changes and/or bug fixes in this package (optional) Changes=Fixed a serious bug that affected overall coolness of the Funky Classifier # Package URL for obtaining the package archive (required) PackageURL=http://somewhere.net/weka/funkyPackage.zip # URL for further information URL=http://somewhere.net/funkyResearchInfo.html # Enhances various other packages? Enhances=packageName1,packageName2,... # Related to other packages? Related=packageName1,packageName2,... # Dependencies (required; format: packageName (equality/inequality version_number) Depends=weka ( =3.7.1), packageName1 (=x.y.z), packageName2 ( u.v.w| =x.y.z),... The PackageName and Version give the name of the package and version number respectively. The name can consist of letters, numbers, and the dot character. It should not start with a dot and should not contain any spaces. The version number is a sequence of three non-negative integers separated by single . or - characters. The Title field should give a one sentence description of the package. The Description field can give a longer description of the package spaning multiple sentences. It may include technical references and can use HTML markup. The Category field is strongly recommended as this information is displayed on both the repository web site and in the GUI package manager client. In the latter, the user can sort the packages on the basis of the category field. It is recommended that an existing category be assigned if possible. Some examples include (Classification, Text classification, Ensemble learning, Regression, Clustering, Associations, Preprocessing, Visualization, Explorer, Experimenter, KnowledgeFlow). The Author field describes who wrote the package and may include multiple names (separated by commas). Email addresses may be given in angle brackets after each name. The field is intended for human readers and no email addresses are automatically extracted. The Maintainer field lists who maintains the package and should include a single email address, enclosed in angle brackets, for sending bug reports to. The License field lists the license(s) that apply to the package. This field may contain the short specification of a license (such as LGPL, GPL 2.0 etc.) or the string file LICENSE , where LICENSE exists as a file in the top-level directory of the package. The string Unlimited may be supplied to indicate that there are no restrictions on distribution or use aside from those imposed by relevant laws. The PackageURL field lists valid URL that points to the package zip file. This URL is used by the package manager to download and install the package. The optional Depends field gives a comma separated list of packages which this package depends on. The name of a package is optionally followed by a version number constraint enclosed in parenthesis. Valid operators for version number constraints include =, , , =, =. The keyword weka is reserved to refer to the base Weka system and can be used to indicate a dependency on a particular version of Weka. For example: Depends=weka ( =3.7.2), DTNB (=1.0.0) states that this package requires Weka 3.7.2 or higher and version 1.0.0 of the package DTNB. Depends=weka ( 3.7.1| 3.8.0) states that this package requires a version of Weka between 3.7.1 and 3.8.0. Depends=DTNB ( 1.5.0| =2.0.1) states that this package requires that a version of the DTNB package be installed that is either less than version 1.5.0 or greater than or equal to version 2.0.1. If there is no version number constraint following a package name, the package manager assumes that the latest version of the dependent package is suitable. The optional URL field gives a URL at which the user can find additional online information about the package or its constituent algorithms. The optional Enhances field can be used to indicate which other packages this package is based on (i.e. if it extends methods/algorithms from another package in some fashion). The optional Related field is similar to the Enhances field. It can be used to point the user to other packages that are related in some fashion to this one. The optional Changes field should be used to indicate what changes/bug fixes are included in the current release of the package. There are several other fields that can be used to provide information to assist the user with completing installation (if it can\u2019t be completely accomplished with the package zip file) or display error messages if necessary components are missing: MessageToDisplayOnInstallation=Funky package requires some extra\\n\\ stuff to be installed after installing this package. You will\\n\\ need to blah, blah, blah in order to blah, blah, blah... DoNotLoadIfFileNotPresent=lib/someLibrary.jar,otherStuff/important,... DoNotLoadIfFileNotPresentMessage=funkyPackage can't be loaded because some \\ funky libraries are missing. Please download funkyLibrary.jar from \\ http://www.funky.com and install in $WEKA_HOME/packages/funkyPackage/lib DoNotLoadIfClassNotPresent=com.some.class.from.some.Where,org.some.other.Class,... DoNotLoadIfClassNotPresentMessage=funkyPackage can't be loaded because \\ com.funky.FunkyClass can't be instantiated. Have you downloaded and run \\ the funky software installer for your platform? The optional MessageToDisplayOnInstallation field allows you to specify special instructions to the user in order to help them complete the intallation manually. This message gets displayed on the console, written to the log and appears in a pop-up information dialog if using the GUI package manager. It should include \\n in order to avoid long lines when displayed in a GUI pop-up dialog. The optional DoNotLoadIfFileNotPresent field can be used to prevent Weka from loading the package if the named ''files'' and/or ''directories'' are not present in the package\u2019s installation directory. An example is the massiveOnlineAnalysis package. This package is a connector only package and does not include the MOA library. Users of this package must download the moa.jar file separately and copy it to the package\u2019s lib directory manually. Multiple files and directories can be specified as a comma separated list. All paths are relative to the top-level directory of the package. IMPORTANT : use forward slashes as separator characters, as these are portable accross all platforms. The DoNotLoadIfFileNotPresentMessage field can be used to supply an optional message to display to the user if Weka detects that a file or directory is missing from the package. This message will be displayed on the console and in the log. The optional DoNotLoadIfClassNotPresent field can be used to prevent Weka from loading the package if the named ''class(es)'' can\u2019t be instantiated. This is useful for packages that rely on stuff that has to be installed manually by the user. For example, Java3D is a separate download on all platforms except for OSX, and installs itself into the system JRE/JDK. The DoNotLoadIfClassNotPresentMessage field can be used to supply an optional message to display to the user if Weka detects that a class can\u2019t be instantiated. Again, this will be displayed on the console and in the log. New in Weka 3.9.2 and 3.8.2 is the ability to constrain the OS and architecture that a package can be installed and loaded on. Two new fields are used for this: # Specify which OS's the package can operate with. Omitting this entry indicates no restrictions on OS. (optional) OSName=Windows,Mac,Linux # Specify which architecture the package can operate with. Omitting this entry indicates no restriction. (optional) OSArch=64 Entries in the OSName field are compared against the value of the Java property \"os.name\" using a String.toLowerCase().contains() operation. Any single match indicates a pass. If an OSName field exists in the Description.props, and it matches, then the optional OSArch field is examined. If present, values in the OSArch list are compared against the value of the Java property \"os.arch\". The special OSArch entries \"32\" and \"64\" are tested using a String().contains() operation; all other entries are compared using String.equalsIgnoreCase().","title":"Anatomy of a package"},{"location":"packages/structure/#additional-configuration-files","text":"Certain types of packages may require additional configuration files to be present as part of the package. The last chapter covered various ways in which Weka can be extended without having to alter the core Weka code. These plugin mechanisms have been subsumed by the package management system, so some of the configuration property files they require must be present in the package\u2019s top-level directory if the package in question contains such a plugin. Examples include additional tabs for the Explorer, mappings to custom property editors for Weka\u2019s GenericObjectEditor and Knowledge Flow plugins. Here are some examples: The scatterPlot3D package adds a new tab to the Explorer. In order to accomplish this a property has to be set in the Explorer.props file (which contains default values for the Explorer) in order to tell Weka to instantiate and display the new panel. The scatterPlot3D file includes an Explorer.props file in its top-level directory that has the following contents: # Explorer.props file. Adds the Explorer3DPanel to the Tabs key. Tabs=weka.gui.explorer.Explorer3DPanel TabsPolicy=append This property file is read by the package management system when the package is loaded and any key-value pairs are added to existing Explorer properties that have been loaded by the system at startup. If the key already exists in the Explorer properties, then the package has the option to either replace (i.e. overwrite) or append to the existing value. This can be specified with the \"TabsPolicy\" key. In this case, the value weka.gui.explorer.Explorer3DPanel is appended to any existing value associated with the \"Tabs\" key. Explorer3DPanel gets instantiated and added as a new tab when the Explorer starts. Another example is the kfGroovy package. This package adds a plugin component to Weka\u2019s Knowledge Flow that allows a Knowledge Flow step to be implemented and compiled dynamically at runtime as a Groovy script. In order for the Knowledge Flow to make the new step appear in its Plugins toolbar, there needs to be a Beans.props file in the package\u2019s top level directory. In the case of kfGroovy, this property file has the following contents: # Specifies that this component goes into the Plugins toolbar weka.gui.beans.KnowledgeFlow.Plugins=org.pentaho.dm.kf.GroovyComponent The new pluggable evaluation metrics for classification/regression (from Weka 3.7.8 and nightly developer snapshots from 15-11-2012) are managed by the PluginManager class. To tell PluginManager that your package provides a new evaluation metric you need to provide a \"PluginManager.props\" file in the package's top level directory. For example, a hypothetical bobsMetric package might declare a new \"Area under Bob curve\" metric like so: # Specify a new plugin Evaluation metric weka.classifiers.evaluation.AbstractEvaluationMetric=weka.classifiers.evaluation.BobsAUC","title":"Additional configuration files"},{"location":"packages/structure/#contributing-a-package","text":"If you have created a package for Weka then there are two options for making it available to the community. In both cases, hosting the package\u2019s zip archive is the responsibility of the contributer. The first, and official, route is to contact the current Weka maintainer (normally also the admin of the WEKA homepage) and supply your package\u2019s Description.props file. The Weka team will then test downloading and using your package to make sure that there are no obvious problems with what has been specified in the Description.props file and that the software runs and does not contain any malware/malicious code. If all is well, then the package will become an official Weka package and the central package repository meta data will be updated with the package\u2019s Description.props file . Responsibility for maintaining and supporting the package resides with the contributer . The second, and unofficial, route is to simply make the package\u2019s zip archive available on the web somewhere and advertise it yourself. Although users will not be able to browse it\u2019s description in the official package repository, they will be able to download and install it directly from your URL by using the command line version of the package manager. This route could be attractive for people who have published a new algorithm and want to quiclky make a beta version available for others to try without having to go through the official route.","title":"Contributing a package"},{"location":"packages/structure/#creating-a-mirror-of-the-package-meta-data-repository","text":"In this section we discuss an easy approach to setting up and maintaining a mirror of the package meta data repository. Having a local mirror may provide faster access times than to that of the official repository on Sourceforge. Extending this approach to the creation of an alternative central repository (hosting packages not available at the official repository) should be straight forward. Just about everything necessary for creating a mirror exists in the local meta data cache created by Weka\u2019s package management system. This cache resides at $WEKA_HOME/repCache . The only thing missing (in Weka 3.7.2) for a complete mirror is the file images.txt , that lists all the image files used in the html index files. This file contains the following two lines: Title-Bird-Header.gif pentaho_logo_rgb_sm.png images.txt is downloaded automatically by the package management system in Weka 3.7.3 and higher. To create a mirror: 1. Copy the contents of $WEKA_HOME/repCache to a temporary directory. For the purposes of this example we\u2019ll call it tempRep 2. Change directory into tempRep and run java weka.core.RepositoryIndexGenerator . . Don't forget the \".\" after the command (this tells RepoistoryIndexGenerator to operate on the current directory) 3. Change directory to the parent of tempRep and synchronize its contents to wherever your web server is located (this is easy via rsync under Nix-like operating systems). RepositoryIndexGenerator automatically creates the main index.html file, all the package index.html files and html files correpsonding to all version prop files for each package. It will also create packageList.txt and numPackages.txt files. IMPORTANT : Make sure that all the files in tempRep are world readable. It is easy to make packages available that are not part of the official Weka repository. Assuming you want to add a package called funkyPackage (as specified by the PackageName field in the Description.props file): Create a directory called funkyPackage in tempRep Copy the Description.props file to tempRep/funkyPackage/Latest.props Copy the Description.props file to tempRep/funkyPackage/ version number .props , where version number is the version number specified in the Version field of Description.props Run RepositoryIndexGenerator as described previously and sync tempRep to your web server Adding a new version of an existing package is very similar to what has already been described. All that is required is that the new Description.props file corresponding to the new version is copied to Latest.props and to version numer .props in the package\u2019s folder. Running RepositoryIndexGenerator will ensure that all necessary html files are created and supporting text files are updated. Automating the mirroring process would simply involve using your OS\u2019s scheduler to execute a script that: Runs weka.core.WekaPackageManager -refresh-cache rsyncs $WEKA_HOME/repCache to tempRep Runs weka.core.RepoistoryIndexGenerator rsyncs tempRep to your web server","title":"Creating a mirror of the package meta data repository"},{"location":"packages/unofficial/","text":"There are a number of packages for WEKA 3.8 on the internet that are not listed in the \"official\" WEKA package repository. These packages can nevertheless be easily installed via the package manager in WEKA 3.8 (available via the Tools menu in WEKA's GUIChooser) by providing the URL for the package .zip file. Below is an (incomplete list) of packages that are available. Preprocessing dataset-weights -- filters for setting attribute and instance weights using various methods. missing-values-imputation -- various methods for imputing missing values using a filter. mxexpression -- filter for updating a target attribute using a mathematical expression. Classification Java neural network package -- Java (convolutional or fully-connected) neural network implementation with plugin for Weka . Uses dropout and rectified linear units. Implementation is multithreaded and uses MTJ matrix library with native libs for performance. HMMWeka -- This library makes Hidden Markov Model machine learning available in Weka. Collective classification -- Algorithms around semi-supervised learning and collective classification. Bagging ensemble selection -- Bagging Ensemble Selection - a new ensemble learning strategy. DataSqueezer -- Efficient rule builder that generates a set of production rules from labeled input data. It can handle missing data and has log-linear asymptotic complexity with the number of training examples. miDS -- mi-DS is a multiple-Instance learning supervised algorithm based on the DataSqueezer algorithm. LibD3C -- Ensemble classifiers with a clustering and dynamic selection strategy. ICRM -- An Interpretable Classification Rule Mining Algorithm. tclass -- TClass is a supervised learner for multivariate time series, originally developed by Waleed Kadous . wekaclassalgos -- collection of artificial neural network (ANN) algorithms and artificial immune system (AIS) algorithms, originally developed by Jason Brownlee . mxexpression -- classifier for making predictions using a mathematical expression. Clustering APCluster -- Affinity propagation algorithm for clustering, used especially in bioinformatics and computer vision. Fast Optics -- Fast Implementation of OPTICS algorithm using random projections for Euclidean distances. Similarity functions wekabiosimilarity -- implements several measures to compare binary feature vectors; and, additionally, extrapolates those measures to work with multi-value, string and numerical feature vectors. Discretization ur-CAIM -- Improved CAIM Discretization for Unbalanced and Balanced Data. CAIM -- Class-Attribute Interdependence Maximization algorithm: discretizes a continuous feature into a number of intervals. This is done by using class information, without requiring the user to provide this number. Feature selection RSARSubsetEval -- Rough set feature selection. Frequent pattern mining XApriori --Available case analysis modification of Apriori frequent pattern mining algorithm. Stemming Snowball stemmers -- Contains the actual snowball stemmer algorithms to make the Snowball stemmer wrapper in Weka work. PTStemmer -- Wrapper for Pedro Oliveira's stemmer library for Portuguese. Text mining nlp -- Contains components for natural language processing, eg part-of-speech tagging filter and Penn Tree Bank tokenizer. Makes use of the Stanford Parser (parser models need to be downloaded separately). Visualization graphviz-treevisualize -- Generating nice graphs in the Explorer from trees (eg J48) using the GraphViz executables. confusionmatrix -- Various visualizations of confusion matrices in the Explorer. serialized-model-viewer -- Adds a standalone tab to the Explorer that allows the user to load a serialized model and view its content as text (simply uses the objects' toString() method). Parameter optimization multisearch -- Meta-classifier similar to GridSearch, but for optimizing arbitrary number of parameters. Others screencast4j -- Allows you to record sound, webcam and screen feeds, storing them in separate files to be combined into a screencast using a video editor . This screencast you can then share on YouTube, for instance. command-to-code -- Turns command-lines (eg of classifiers or filters) into various Java code snippets. jshell-scripting -- Allows scripting in Java, using jshell","title":" Unofficial packages"},{"location":"packages/unofficial/#preprocessing","text":"dataset-weights -- filters for setting attribute and instance weights using various methods. missing-values-imputation -- various methods for imputing missing values using a filter. mxexpression -- filter for updating a target attribute using a mathematical expression.","title":"Preprocessing"},{"location":"packages/unofficial/#classification","text":"Java neural network package -- Java (convolutional or fully-connected) neural network implementation with plugin for Weka . Uses dropout and rectified linear units. Implementation is multithreaded and uses MTJ matrix library with native libs for performance. HMMWeka -- This library makes Hidden Markov Model machine learning available in Weka. Collective classification -- Algorithms around semi-supervised learning and collective classification. Bagging ensemble selection -- Bagging Ensemble Selection - a new ensemble learning strategy. DataSqueezer -- Efficient rule builder that generates a set of production rules from labeled input data. It can handle missing data and has log-linear asymptotic complexity with the number of training examples. miDS -- mi-DS is a multiple-Instance learning supervised algorithm based on the DataSqueezer algorithm. LibD3C -- Ensemble classifiers with a clustering and dynamic selection strategy. ICRM -- An Interpretable Classification Rule Mining Algorithm. tclass -- TClass is a supervised learner for multivariate time series, originally developed by Waleed Kadous . wekaclassalgos -- collection of artificial neural network (ANN) algorithms and artificial immune system (AIS) algorithms, originally developed by Jason Brownlee . mxexpression -- classifier for making predictions using a mathematical expression.","title":"Classification"},{"location":"packages/unofficial/#clustering","text":"APCluster -- Affinity propagation algorithm for clustering, used especially in bioinformatics and computer vision. Fast Optics -- Fast Implementation of OPTICS algorithm using random projections for Euclidean distances.","title":"Clustering"},{"location":"packages/unofficial/#similarity-functions","text":"wekabiosimilarity -- implements several measures to compare binary feature vectors; and, additionally, extrapolates those measures to work with multi-value, string and numerical feature vectors.","title":"Similarity functions"},{"location":"packages/unofficial/#discretization","text":"ur-CAIM -- Improved CAIM Discretization for Unbalanced and Balanced Data. CAIM -- Class-Attribute Interdependence Maximization algorithm: discretizes a continuous feature into a number of intervals. This is done by using class information, without requiring the user to provide this number.","title":"Discretization"},{"location":"packages/unofficial/#feature-selection","text":"RSARSubsetEval -- Rough set feature selection.","title":"Feature selection"},{"location":"packages/unofficial/#frequent-pattern-mining","text":"XApriori --Available case analysis modification of Apriori frequent pattern mining algorithm.","title":"Frequent pattern mining"},{"location":"packages/unofficial/#stemming","text":"Snowball stemmers -- Contains the actual snowball stemmer algorithms to make the Snowball stemmer wrapper in Weka work. PTStemmer -- Wrapper for Pedro Oliveira's stemmer library for Portuguese.","title":"Stemming"},{"location":"packages/unofficial/#text-mining","text":"nlp -- Contains components for natural language processing, eg part-of-speech tagging filter and Penn Tree Bank tokenizer. Makes use of the Stanford Parser (parser models need to be downloaded separately).","title":"Text mining"},{"location":"packages/unofficial/#visualization","text":"graphviz-treevisualize -- Generating nice graphs in the Explorer from trees (eg J48) using the GraphViz executables. confusionmatrix -- Various visualizations of confusion matrices in the Explorer. serialized-model-viewer -- Adds a standalone tab to the Explorer that allows the user to load a serialized model and view its content as text (simply uses the objects' toString() method).","title":"Visualization"},{"location":"packages/unofficial/#parameter-optimization","text":"multisearch -- Meta-classifier similar to GridSearch, but for optimizing arbitrary number of parameters.","title":"Parameter optimization"},{"location":"packages/unofficial/#others","text":"screencast4j -- Allows you to record sound, webcam and screen feeds, storing them in separate files to be combined into a screencast using a video editor . This screencast you can then share on YouTube, for instance. command-to-code -- Turns command-lines (eg of classifiers or filters) into various Java code snippets. jshell-scripting -- Allows scripting in Java, using jshell","title":"Others"}]}