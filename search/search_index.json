{
    "docs": [
        {
            "location": "/",
            "text": "Please note, the migration from the old wiki in this new format is still work in progress!\n\n\nNew to Weka?\n\n\nHave a look at the \nFrequently Asked Questions\n (= FAQ), the \nTroubleshooting\n article or search the \nmailing list archives\n.\nDon't forget to check out the documentation on the \nWeka homepage\n and the\n\nLearning Resources\n.\n\n\nYou have questions regarding Weka?\n\n\nYou can post questions to the \nWeka mailing list\n. Please keep\nin mind that you cannot expect an immediate answer to your question(s). The\nquestions are mainly answered by volunteers, Weka users just like you.\n\n\nYou are looking for packages?\n\n\nWith Weka 3.7.2 and later, you can easily install packages through Weka's\npackage manager interface, either official ones or unofficial ones.\nHave a look at the \nPackages\n article for more information on this topic.\n\n\nYou found a bug?\n\n\nPlease post the bug report to the \nWeka mailing list\n. The\nfollowing information will help tracking things down:\n\n\n\n\nversion of Weka (e.g., 3.9.2)\n\n\noperating system (e.g., Windows 10 or Ubuntu 16.04 64bit)\n\n\nJava version (e.g., 1.8.0_162 64bit)\n\n\n\n\nYou can also run the following command in the SimpleCLI and attach the generate output as text file to your post:\n\n\n    java weka.core.SystemInfo",
            "title": "Home"
        },
        {
            "location": "/#new-to-weka",
            "text": "Have a look at the  Frequently Asked Questions  (= FAQ), the  Troubleshooting  article or search the  mailing list archives .\nDon't forget to check out the documentation on the  Weka homepage  and the Learning Resources .",
            "title": "New to Weka?"
        },
        {
            "location": "/#you-have-questions-regarding-weka",
            "text": "You can post questions to the  Weka mailing list . Please keep\nin mind that you cannot expect an immediate answer to your question(s). The\nquestions are mainly answered by volunteers, Weka users just like you.",
            "title": "You have questions regarding Weka?"
        },
        {
            "location": "/#you-are-looking-for-packages",
            "text": "With Weka 3.7.2 and later, you can easily install packages through Weka's\npackage manager interface, either official ones or unofficial ones.\nHave a look at the  Packages  article for more information on this topic.",
            "title": "You are looking for packages?"
        },
        {
            "location": "/#you-found-a-bug",
            "text": "Please post the bug report to the  Weka mailing list . The\nfollowing information will help tracking things down:   version of Weka (e.g., 3.9.2)  operating system (e.g., Windows 10 or Ubuntu 16.04 64bit)  Java version (e.g., 1.8.0_162 64bit)   You can also run the following command in the SimpleCLI and attach the generate output as text file to your post:      java weka.core.SystemInfo",
            "title": "You found a bug?"
        },
        {
            "location": "/faq/",
            "text": "General\n\n\n\n\nWhat are the principal release branches of Weka?\n\n\nWhere can I get old versions of WEKA?\n\n\nHow do I get the latest bugfixes?\n\n\nCan I check my CLASSPATH from within WEKA?\n\n\nWhere is my home directory located?\n\n\nCan I check how much memory is available for WEKA?\n\n\nCan I use WEKA in commercial applications?\n\n\n\n\nBasic usage\n\n\n\n\nCan I use CSV files?\n\n\nHow do I perform CSV file conversion?\n\n\nHow do I divide a dataset into training and test set?\n\n\nHow do I generate compatible train and test sets that get processed with a filter?\n\n\nHow do I perform attribute selection?\n\n\nHow do I perform clustering?\n\n\nWhere do I find visualization of classifiers, etc.?\n\n\nHow do I perform text classification?\n\n\nHow can I perform multi-instance learning in WEKA?\n\n\nHow do I perform cost-sensitive classification?\n\n\nHow do I make predictions with a trained model?\n\n\nWhy am I missing certain nominal or string values from sparse instances?\n\n\nCan I use WEKA for time series analysis?\n\n\nDoes WEKA support multi-label classification?\n\n\nHow do I perform one-class classification?\n\n\nCan I make a screenshot of a plot or graph directly in WEKA?\n\n\nHow do I use the package manager?\n\n\nWhat do I do if the package manager does not start?\n\n\n\n\nAdvanced usage\n\n\n\n\nHow can I track instances in WEKA?\n\n\nHow do I use ID attributes?\n\n\nHow do I connect to a database?\n\n\nHow do I use WEKA from command line?\n\n\nCan I tune the parameters of a classifier?\n\n\nHow do I generate Learning curves?\n\n\nWhere can I find information regarding ROC curves?\n\n\nI have unbalanced data - now what?\n\n\nCan I run an experiment using clusterers in the Experimenter?\n\n\nHow can I use transactional data in Weka?\n\n\nHow can I use Weka with Matlab or Octave?\n\n\n\n\nCustomizing Weka\n\n\n\n\nCan I change the colors (background, axes, etc.) of the plots in WEKA?\n\n\nHow do I add a new classifier, filter, kernel, etc\n\n\n\n\nUsing third-party tools\n\n\n\n\nHow do I use libsvm in WEKA?\n\n\nThe snowball stemmers don't work, what am I doing wrong?\n\n\n\n\nDeveloping with WEKA\n\n\n\n\nWhere can I get WEKA's source code?\n\n\nHow do I compile WEKA?\n\n\nWhat is Subversion and what do I need to do to access it?\n\n\nHow do I use WEKA's classes in my own code?\n\n\nHow do I write a new classifier or filter?\n\n\nCan I compile WEKA into native code?\n\n\nCan I use WEKA from C#?\n\n\nCan I use WEKA from Python?\n\n\nCan I use WEKA from Groovy?\n\n\nSerialization is nice, but what about generating actual Java code from WEKA classes?\n\n\nHow are packages structured for the package management system?\n\n\nPluggable evaluation metrics for classification/regression\n\n\nHow can I contribute to WEKA?\n\n\n\n\nWindows\n\n\n\n\nHow do I modify the CLASSPATH?\n\n\nHow do I modify the RunWeka.bat file?\n\n\nCan I process UTF-8 datasets or files?\n\n\nHow do I run the Windows Weka installer in silent mode?\n\n\n\n\nTroubleshooting\n\n\n\n\nI have Weka download problems - what's going wrong?\n\n\nMy ARFF file doesn't load - why?\n\n\nWhat does nominal value not declared in header, read Token[X], line Y mean?\n\n\nHow do I get rid of this OutOfMemoryException?\n\n\nHow do I deal with a StackOverflowError?\n\n\nWhy do I get the error message 'training and test set are not compatible'?\n\n\nCouldn't read from database: unknown data type\n\n\nTrying to add JDBC driver: ... - Error, not in CLASSPATH?\n\n\nI cannot process large datasets - any ideas?\n\n\nSee \nTroubleshooting\n article for more troubleshooting.",
            "title": "FAQ"
        },
        {
            "location": "/faq/#general",
            "text": "What are the principal release branches of Weka?  Where can I get old versions of WEKA?  How do I get the latest bugfixes?  Can I check my CLASSPATH from within WEKA?  Where is my home directory located?  Can I check how much memory is available for WEKA?  Can I use WEKA in commercial applications?",
            "title": "General"
        },
        {
            "location": "/faq/#basic-usage",
            "text": "Can I use CSV files?  How do I perform CSV file conversion?  How do I divide a dataset into training and test set?  How do I generate compatible train and test sets that get processed with a filter?  How do I perform attribute selection?  How do I perform clustering?  Where do I find visualization of classifiers, etc.?  How do I perform text classification?  How can I perform multi-instance learning in WEKA?  How do I perform cost-sensitive classification?  How do I make predictions with a trained model?  Why am I missing certain nominal or string values from sparse instances?  Can I use WEKA for time series analysis?  Does WEKA support multi-label classification?  How do I perform one-class classification?  Can I make a screenshot of a plot or graph directly in WEKA?  How do I use the package manager?  What do I do if the package manager does not start?",
            "title": "Basic usage"
        },
        {
            "location": "/faq/#advanced-usage",
            "text": "How can I track instances in WEKA?  How do I use ID attributes?  How do I connect to a database?  How do I use WEKA from command line?  Can I tune the parameters of a classifier?  How do I generate Learning curves?  Where can I find information regarding ROC curves?  I have unbalanced data - now what?  Can I run an experiment using clusterers in the Experimenter?  How can I use transactional data in Weka?  How can I use Weka with Matlab or Octave?",
            "title": "Advanced usage"
        },
        {
            "location": "/faq/#customizing-weka",
            "text": "Can I change the colors (background, axes, etc.) of the plots in WEKA?  How do I add a new classifier, filter, kernel, etc",
            "title": "Customizing Weka"
        },
        {
            "location": "/faq/#using-third-party-tools",
            "text": "How do I use libsvm in WEKA?  The snowball stemmers don't work, what am I doing wrong?",
            "title": "Using third-party tools"
        },
        {
            "location": "/faq/#developing-with-weka",
            "text": "Where can I get WEKA's source code?  How do I compile WEKA?  What is Subversion and what do I need to do to access it?  How do I use WEKA's classes in my own code?  How do I write a new classifier or filter?  Can I compile WEKA into native code?  Can I use WEKA from C#?  Can I use WEKA from Python?  Can I use WEKA from Groovy?  Serialization is nice, but what about generating actual Java code from WEKA classes?  How are packages structured for the package management system?  Pluggable evaluation metrics for classification/regression  How can I contribute to WEKA?",
            "title": "Developing with WEKA"
        },
        {
            "location": "/faq/#windows",
            "text": "How do I modify the CLASSPATH?  How do I modify the RunWeka.bat file?  Can I process UTF-8 datasets or files?  How do I run the Windows Weka installer in silent mode?",
            "title": "Windows"
        },
        {
            "location": "/faq/#troubleshooting",
            "text": "I have Weka download problems - what's going wrong?  My ARFF file doesn't load - why?  What does nominal value not declared in header, read Token[X], line Y mean?  How do I get rid of this OutOfMemoryException?  How do I deal with a StackOverflowError?  Why do I get the error message 'training and test set are not compatible'?  Couldn't read from database: unknown data type  Trying to add JDBC driver: ... - Error, not in CLASSPATH?  I cannot process large datasets - any ideas?  See  Troubleshooting  article for more troubleshooting.",
            "title": "Troubleshooting"
        },
        {
            "location": "/not_so_faq/",
            "text": "Associators\n\n\n\n\nHow do I use the associator GeneralizedSequentialPatterns?\n\n\n\n\nClassifiers\n\n\n\n\nWhat do those numbers mean in a J48 tree?",
            "title": "Not so FAQ"
        },
        {
            "location": "/not_so_faq/#associators",
            "text": "How do I use the associator GeneralizedSequentialPatterns?",
            "title": "Associators"
        },
        {
            "location": "/not_so_faq/#classifiers",
            "text": "What do those numbers mean in a J48 tree?",
            "title": "Classifiers"
        },
        {
            "location": "/troubleshooting/",
            "text": "Click on one of the links for more information:\n\n\n\n\nWeka download problems\n\n\nOutOfMemoryException\n\n\nStackOverflowError\n\n\njust-in-time (JIT) compiler\n\n\nCSV file conversion\n\n\nARFF file doesn't load\n\n\nError message: nominal value not declared in header, read Token[X], line Y\n\n\nSpaces in labels of ARFF files\n\n\nSingle quotes in labels of ARFF files\n\n\nCLASSPATH problems\n\n\nInstance ID\n\n\nVisualization\n\n\nMemory consumption and Garbage collector\n\n\nGUIChooser starts but not Experimenter or Explorer\n\n\nKnowledgeFlow toolbars are empty\n\n\nOSX Mountain Lion - Weka x-y-z is damaged and can't be installed. You should eject the disk image\n\n\n\n\nSee also the \nFrequently Asked Questions\n.",
            "title": "Troubleshooting"
        },
        {
            "location": "/learning_resources/",
            "text": "Videos\n\n\n\n\nYoutube channel of Data Mining with Weka MOOCs\n\n\n\n\nTutorials\n\n\n\n\nLearn Data Science Online\n\n\n\n\nMOOCs\n\n\n\n\nData Mining with Weka\n\n\nMore Data Mining with Weka\n\n\nAdvanced Data Mining with Weka",
            "title": "Learning resources"
        },
        {
            "location": "/learning_resources/#videos",
            "text": "Youtube channel of Data Mining with Weka MOOCs",
            "title": "Videos"
        },
        {
            "location": "/learning_resources/#tutorials",
            "text": "Learn Data Science Online",
            "title": "Tutorials"
        },
        {
            "location": "/learning_resources/#moocs",
            "text": "Data Mining with Weka  More Data Mining with Weka  Advanced Data Mining with Weka",
            "title": "MOOCs"
        },
        {
            "location": "/using_the_api/",
            "text": "Several articles describe certain aspects of using the Weka API:\n\n\n\n\nUse Weka in your Java code\n\n\nWeka Examples",
            "title": "Using the API"
        },
        {
            "location": "/extending_weka/",
            "text": "The following articles describe how you can extend Weka:\n\n\n\n\nWriting a new Filter\n\n\nWriting a new Classifier",
            "title": "Extending Weka"
        },
        {
            "location": "/arff/",
            "text": "Data format\n\n\nA description of the ARFF format can be found in the following articles:\n\n\n\n\nARFF (stable version)\n\n\nARFF (developer version)\n\n\n\n\nCreating an ARFF file\n\n\nHow to create an ARFF file on the fly, i.e., inside Java, you can find here:\n\n\n\n\nCreating an ARFF file\n\n\n\n\nSee also\n\n\n\n\nARFF Syntax Highlighting\n for various editors\n\n\n\n\nLinks\n\n\n\n\nARFF2DB.py\n - a Python script for importing an ARFF file into a database (similar functionality to the \nweka.core.converters.DatabaseSaver\n class)",
            "title": "ARFF Format"
        },
        {
            "location": "/arff/#data-format",
            "text": "A description of the ARFF format can be found in the following articles:   ARFF (stable version)  ARFF (developer version)",
            "title": "Data format"
        },
        {
            "location": "/arff/#creating-an-arff-file",
            "text": "How to create an ARFF file on the fly, i.e., inside Java, you can find here:   Creating an ARFF file",
            "title": "Creating an ARFF file"
        },
        {
            "location": "/arff/#see-also",
            "text": "ARFF Syntax Highlighting  for various editors",
            "title": "See also"
        },
        {
            "location": "/arff/#links",
            "text": "ARFF2DB.py  - a Python script for importing an ARFF file into a database (similar functionality to the  weka.core.converters.DatabaseSaver  class)",
            "title": "Links"
        },
        {
            "location": "/packages/",
            "text": "Weka 3.7.2 introduced support for packages, making it easy to extend Weka\nwithout having to recompile or patch the underlying Weka installation.\n\n\nHere are some pointers for using and developing packages:\n\n\n\n\nHow do I use the package manager?\n\n\nUnofficial packages\n\n\nHow are packages structured for the package management system?",
            "title": "Packages"
        },
        {
            "location": "/mailing_list/",
            "text": "The WEKA Mailing list can be found here:\n\n\n\n\nList\n for subscribing/unsubscribing to the list\n\n\nArchives\n (\nMirror 1\n, \nMirror 2\n) for searching previous posted messages\n\n\n\n\nBefore posting, please read the \nMailing List Etiquette\n.",
            "title": "Mailing list"
        },
        {
            "location": "/subversion/",
            "text": "General\n\n\nThe Weka \nSubversion\n repository is accessible and browseable via the following URL:\n\n\n\n\nhttps://svn.cms.waikato.ac.nz/svn/weka/\n\n\n\n\nA Subversion repository has usually the following layout:\n\n\n root\n  |\n  +- trunk\n  |\n  +- tags\n  |\n  +- branches\n\n\n\n\nWhere \ntrunk\n contains the \nmain trunk\n of the development, \ntags\n snapshots in time of the repository (e.g., when a new version got released) and \nbranches\n development branches that forked off the main trunk at some stage (e.g., legacy versions that still get bugfixed).\n\n\nSource code\n\n\nThe latest version of the Weka source code can be obtained with this URL:\n\n\nhttps://svn.cms.waikato.ac.nz/svn/weka/trunk/weka\n\n\nIf you want to obtain the source code of the book version, use this URL:\n\n\nhttps://svn.cms.waikato.ac.nz/svn/weka/branches/stable-3-8/\n\n\nSpecific version\n\n\nWhenever a release of Weka is generated, the repository gets \ntagged\n:\n\n\ndev-X-Y-Z\n\n\nthe tag for a release of the developer version, e.g., \ndev-3-9-2\n for Weka 3.9.2\n\n\nhttps://svn.cms.waikato.ac.nz/svn/weka/tags/dev-3-9-2\n\n\nstable-X-Y-Z\n\n\nthe tag for a release of a stable version. The book version is one of those stable versions, e.g., \nstable-3-8-2\n for Weka 3.8.2.\n\n\nhttps://svn.cms.waikato.ac.nz/svn/weka/tags/stable-3-8-2\n\n\nJUnit\n\n\nWeka's JUnit tests are no longer a separate module (as it was the case before the migration to Subversion). They are now located in the \nsrc/test\n directory of the Weka source code tree.\n\n\nCommandline\n\n\nModern Linux distributions already come with Subversion either pre-installed or easily installed via the package manager of the distribution. If that shouldn't be case, or if you are using Windows, you have to download the appropriate client from [http://subversion.tigris.org/ Subversion's homepage].\n\n\nA checkout of the current developer version of Weka looks like this:\n\n\nsvn co https://svn.cms.waikato.ac.nz/svn/weka/trunk/weka\n\n\n\n\nYou can also obtain the source code for a specific date. The \n-r\n option of the \nsvn\n command-line client can also take dates (format: YYYYMMDD) instead of only revision numbers. In order to distinguish the dates from revision numbers, you have to enclose the date within curly brackets:\n\n\nsvn co -r {20180616} https://svn.cms.waikato.ac.nz/svn/weka/trunk/weka\n\n\n\n\nLinks\n\n\n\n\nSubversion on WikiPedia\n\n\nSubversion homepage\n\n\nJUnit homepage",
            "title": " Subversion"
        },
        {
            "location": "/subversion/#general",
            "text": "The Weka  Subversion  repository is accessible and browseable via the following URL:   https://svn.cms.waikato.ac.nz/svn/weka/   A Subversion repository has usually the following layout:   root\n  |\n  +- trunk\n  |\n  +- tags\n  |\n  +- branches  Where  trunk  contains the  main trunk  of the development,  tags  snapshots in time of the repository (e.g., when a new version got released) and  branches  development branches that forked off the main trunk at some stage (e.g., legacy versions that still get bugfixed).",
            "title": "General"
        },
        {
            "location": "/subversion/#source-code",
            "text": "The latest version of the Weka source code can be obtained with this URL:  https://svn.cms.waikato.ac.nz/svn/weka/trunk/weka  If you want to obtain the source code of the book version, use this URL:  https://svn.cms.waikato.ac.nz/svn/weka/branches/stable-3-8/",
            "title": "Source code"
        },
        {
            "location": "/subversion/#specific-version",
            "text": "Whenever a release of Weka is generated, the repository gets  tagged :  dev-X-Y-Z  the tag for a release of the developer version, e.g.,  dev-3-9-2  for Weka 3.9.2  https://svn.cms.waikato.ac.nz/svn/weka/tags/dev-3-9-2  stable-X-Y-Z  the tag for a release of a stable version. The book version is one of those stable versions, e.g.,  stable-3-8-2  for Weka 3.8.2.  https://svn.cms.waikato.ac.nz/svn/weka/tags/stable-3-8-2",
            "title": "Specific version"
        },
        {
            "location": "/subversion/#junit",
            "text": "Weka's JUnit tests are no longer a separate module (as it was the case before the migration to Subversion). They are now located in the  src/test  directory of the Weka source code tree.",
            "title": "JUnit"
        },
        {
            "location": "/subversion/#commandline",
            "text": "Modern Linux distributions already come with Subversion either pre-installed or easily installed via the package manager of the distribution. If that shouldn't be case, or if you are using Windows, you have to download the appropriate client from [http://subversion.tigris.org/ Subversion's homepage].  A checkout of the current developer version of Weka looks like this:  svn co https://svn.cms.waikato.ac.nz/svn/weka/trunk/weka  You can also obtain the source code for a specific date. The  -r  option of the  svn  command-line client can also take dates (format: YYYYMMDD) instead of only revision numbers. In order to distinguish the dates from revision numbers, you have to enclose the date within curly brackets:  svn co -r {20180616} https://svn.cms.waikato.ac.nz/svn/weka/trunk/weka",
            "title": "Commandline"
        },
        {
            "location": "/subversion/#links",
            "text": "Subversion on WikiPedia  Subversion homepage  JUnit homepage",
            "title": "Links"
        },
        {
            "location": "/get_latest_bugfixes/",
            "text": "Weka is actively developed, that means that bugs are fixed and new functionality is added (only to the developer version) all the time. Every now and then (about every 6-12 months), when there was a sufficiently large number of improvements or fixes, a release is made and uploaded to \nSourceforget.net\n.\n\n\nIf you don't want to wait that long, you have two options:\n\n\n\n\n\n\nGet the latest source code from \nSubversion\n and compile it yourself. See the following articles for more information\n\n\n\n\nobtaining the source code from Subversion\n, either book or developer version\n\n\ncompiling the source code\n\n\n\n\n\n\n\n\nDownload a \nsnapshot\n from the download section of the \nWeka homepage\n. Snapshots for book and developer version are generated automatically every night, based on the current Subversion code. The \nZIP files\n have the same content as a release, i.e., compiled classes (= weka.jar), source code (= weka-src.jar), Javadoc and other documentation.\n\n\n\n\n\n\nNote:\n compare the timestamp of the \nWeka Mailing List\n post that reports a bugfix with the one of the snapshot to make sure the bugfix is already included in the snapshot.",
            "title": " Get latest Bugfixes"
        },
        {
            "location": "/compiling_weka/",
            "text": "There are several ways of compiling the Weka source code:\n\n\n\n\nwith \nant\n\n\n\n\ntakes care of compiling all the necessary classes and easily generates jar archives\n\n\n\n\nwith \nmaven\n\n\n\n\nsimilar to ant\n\n\n\n\nwith an IDE, like IntelliJ IDEA, Eclipse or NetBeans\n\n\n\n\ncan be very helpful for debugging tricky bugs",
            "title": " Compiling Weka"
        },
        {
            "location": "/ant/",
            "text": "What is ANT? This is how the ANT \nhomepage\n defines its tool:\n\n\nApache Ant is a Java-based build tool. In theory, it is kind of like Make, but without Make's wrinkles.\n\n\nBasics\n\n\n\n\nthe ANT build file is based on \nXML\n\n\nthe usual name for the build file is \nbuild.xml\n\n\ninvocation - the usual build file needs not be specified explicitly, if it's in the current directory; if not target is specified, the default one is used\n\n\n\n\nant [-f <build-file>] [<target>]\n\n\n\n\ndisplaying all the available targets of a build file\n\n\n\n\nant [-f <build-file>] -projecthelp\n\n\nWeka and ANT\n\n\n\n\na build file for Weka is available from \nsubversion\n (it has been included in the \nweka-src.jar\n since version 3.4.8 and 3.5.3)\n\n\nit is located in the \nweka\n directory\n\n\n\n\nsome targets of interest\n\n\n\n\n\n\nclean\n - Removes the build, dist and reports directories; also any class files in the source tree\n\n\n\n\ncompile\n - Compile weka and deposit class files in \n${path_modifier}/build/classes\n\n\ndocs\n - Make javadocs into {${path_modifier}/doc}}\n\n\nexejar\n - Create an executable jar file in \n${path_modifier}/dist\n\n\n\n\nLinks\n\n\n\n\nANT homepage\n\n\nXML",
            "title": " Ant"
        },
        {
            "location": "/ant/#basics",
            "text": "the ANT build file is based on  XML  the usual name for the build file is  build.xml  invocation - the usual build file needs not be specified explicitly, if it's in the current directory; if not target is specified, the default one is used   ant [-f <build-file>] [<target>]   displaying all the available targets of a build file   ant [-f <build-file>] -projecthelp",
            "title": "Basics"
        },
        {
            "location": "/ant/#weka-and-ant",
            "text": "a build file for Weka is available from  subversion  (it has been included in the  weka-src.jar  since version 3.4.8 and 3.5.3)  it is located in the  weka  directory   some targets of interest    clean  - Removes the build, dist and reports directories; also any class files in the source tree   compile  - Compile weka and deposit class files in  ${path_modifier}/build/classes  docs  - Make javadocs into {${path_modifier}/doc}}  exejar  - Create an executable jar file in  ${path_modifier}/dist",
            "title": "Weka and ANT"
        },
        {
            "location": "/ant/#links",
            "text": "ANT homepage  XML",
            "title": "Links"
        },
        {
            "location": "/maven/",
            "text": "Maven\n is another build tool. But unlike \nAnt\n, it is a more high-level tool. Though its configuration file, \npom.xml\n is written in XML as well, Maven uses a different approach to the build process. In Ant, you tell it where to find Java classes for compilation, what libraries to compile against, where to put the compiled ones and then how to combine them into a jar. With Maven, you only specify dependent libraries, a compile and a jar plugin and maybe tweak the options a bit. For this to work, Maven enforces a strict \ndirectory structure\n (though you can tweak that, if you need to).\n\n\nSo why another build tool?\n\n\nWhereas Ant scripts quite often create a \nfat jar\n, i.e., a jar that contains not only the project's code, but also the contain of libraries the code was compiled against. Handy if you only want to have a single jar. However, this is a nightmare, if you need to update a single library, but all you have is a single, enormous jar. Maven handles \ndependencies automatically\n, relying on libraries (they call them artifacts) to be publicly available, e.g., on \nMaven Central\n. It allows you to use newer versions of libraries than defined by the dependent libraries (e.g., critical bug fixes), without having to modify any jars manually. Though Maven can also generate \nfar jar\n files, it is not considered good practice, as it defeats Maven's automatic version resolution.\n\n\nIn order to make Weka, and most of its packages, available to a wider audience (e.g., other software developers), we also publish on Maven Central.\n\n\nCompiling\n\n\nFor compiling Weka, you would issue a command like this (in the same directory as \npom.xml\n):\n\n\nmvn clean install\n\n\n\n\nIf you don't want the tests to run, use this:\n\n\nmvn clean install -DskipTests=true",
            "title": " Maven"
        },
        {
            "location": "/maven/#so-why-another-build-tool",
            "text": "Whereas Ant scripts quite often create a  fat jar , i.e., a jar that contains not only the project's code, but also the contain of libraries the code was compiled against. Handy if you only want to have a single jar. However, this is a nightmare, if you need to update a single library, but all you have is a single, enormous jar. Maven handles  dependencies automatically , relying on libraries (they call them artifacts) to be publicly available, e.g., on  Maven Central . It allows you to use newer versions of libraries than defined by the dependent libraries (e.g., critical bug fixes), without having to modify any jars manually. Though Maven can also generate  far jar  files, it is not considered good practice, as it defeats Maven's automatic version resolution.  In order to make Weka, and most of its packages, available to a wider audience (e.g., other software developers), we also publish on Maven Central.",
            "title": "So why another build tool?"
        },
        {
            "location": "/maven/#compiling",
            "text": "For compiling Weka, you would issue a command like this (in the same directory as  pom.xml ):  mvn clean install  If you don't want the tests to run, use this:  mvn clean install -DskipTests=true",
            "title": "Compiling"
        },
        {
            "location": "/snapshots/",
            "text": "See \nHow to get the latest bugfixes?",
            "title": " Snapsnots"
        },
        {
            "location": "/arff_stable/",
            "text": "An ARFF (Attribute-Relation File Format) file is an ASCII text file that describes a list of instances sharing a set of attributes. \n\n\nOverview\n\n\nARFF files have two distinct sections. The first section is the \nHeader\n information, which is followed the \nData\n information.\n\n\nThe \nHeader\n of the ARFF file contains the name of the relation, a list of the attributes (the columns in the data), and their types. An example header on the standard IRIS dataset looks like this:\n\n\n   % 1. Title: Iris Plants Database\n   % \n   % 2. Sources:\n   %      (a) Creator: R.A. Fisher\n   %      (b) Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n   %      (c) Date: July, 1988\n   % \n   @RELATION iris\n\n   @ATTRIBUTE sepallength  NUMERIC\n   @ATTRIBUTE sepalwidth   NUMERIC\n   @ATTRIBUTE petallength  NUMERIC\n   @ATTRIBUTE petalwidth   NUMERIC\n   @ATTRIBUTE class        {Iris-setosa,Iris-versicolor,Iris-virginica}\n\n\n\n\nThe \nData\n of the ARFF file looks like the following:\n\n\n   @DATA\n   5.1,3.5,1.4,0.2,Iris-setosa\n   4.9,3.0,1.4,0.2,Iris-setosa\n   4.7,3.2,1.3,0.2,Iris-setosa\n   4.6,3.1,1.5,0.2,Iris-setosa\n   5.0,3.6,1.4,0.2,Iris-setosa\n   5.4,3.9,1.7,0.4,Iris-setosa\n   4.6,3.4,1.4,0.3,Iris-setosa\n   5.0,3.4,1.5,0.2,Iris-setosa\n   4.4,2.9,1.4,0.2,Iris-setosa\n   4.9,3.1,1.5,0.1,Iris-setosa\n\n\n\n\nLines that begin with a \n%\n are comments. The \n@RELATION\n, \n@ATTRIBUTE\n and \n@DATA\n declarations are case insensitive.\n\n\nExamples\n\n\nSeveral well-known machine learning datasets are distributed with Weka in the \n$WEKAHOME/data\n directory as ARFF files.\n\n\nThe ARFF Header Section\n\n\nThe ARFF Header section of the file contains the relation declaration and attribute declarations.\n\n\nThe @relation Declaration\n\n\nThe relation name is defined as the first line in the ARFF file. The format is:\n\n\n    @relation [relation-name]\n\n\n\n\nwhere \n[relation-name]\n is a string. The string must be quoted if the name includes spaces. Furthermore, relation names or attribute names (see below) cannot begin with\n\n\n\n\na character below \\u0021\n\n\n'{', '}', ',', or '%' \n\n\n\n\nMoreover, it can only begin with a single or double quote if there is a corresponding quote at the end of the name.\n\n\nThe @attribute Declarations\n\n\nAttribute declarations take the form of an ordered sequence of \n@attribute\n statements. Each attribute in the data set has its own \n@attribute\n statement which uniquely defines the name of that attribute and its data type. The order the attributes are declared indicates the column position in the data section of the file. For example, if an attribute is the third one declared then Weka expects that all that attributes values will be found in the third comma delimited column.\n\n\nThe format for the \n@attribute\n statement is:\n\n\n    @attribute [attribute-name] [datatype]\n\n\n\n\nwhere the \n[attribute-name]\n must adhere to the constraints specified in the above section on the @relation declaration.\n\n\nThe [datatype] can be any of the four types supported by Weka:\n\n\n\n\nnumeric\n\n\ninteger\n is treated as \nnumeric\n\n\nreal\n is treated as \nnumeric\n\n\n[nominal-specification]\n\n\nstring\n\n\ndate\n [date-format]\n\n\nrelational\n for multi-instance data (for future use)\n\n\n\n\nwhere \n[nominal-specification]\n and \n[date-format]\n are defined below. The keywords \nnumeric\n, \nreal\n, \ninteger\n, \nstring\n and \ndate\n are case insensitive.\n\n\nNumeric attributes\n\n\nNumeric attributes can be real or integer numbers.\n\n\nNominal attributes\n\n\nNominal values are defined by providing an [nominal-specification] listing the possible values: {[nominal-name1], [nominal-name2], [nominal-name3], ...}\n\n\nFor example, the class value of the Iris dataset can be defined as follows:\n\n\n    @ATTRIBUTE class        {Iris-setosa,Iris-versicolor,Iris-virginica}\n\n\n\n\nValues that contain spaces must be quoted.\n\n\nString attributes\n\n\nString attributes allow us to create attributes containing arbitrary textual values. This is very useful in text-mining applications, as we can create datasets with string attributes, then write Weka Filters to manipulate strings (like \nStringToWordVectorFilter\n). String attributes are declared as follows:\n\n\n    @ATTRIBUTE LCC    string\n\n\n\n\nDate attributes\n\n\nDate attribute declarations take the form:\n\n\n    @attribute [name] date [[date-format]]\n\n\n\n\nwhere [name] is the name for the attribute and \n[date-format]\n is an optional string specifying how date values should be parsed and printed (this is the same format used by \nSimpleDateFormat\n). The default format string accepts the ISO-8601 combined date and time format: \nyyyy-MM-dd'T'HH:mm:ss\n. Check out the Javadoc of the \njava.text.SimpleDateFormat\n class for supported character patterns.\n\n\nDates must be specified in the data section as the corresponding string representations of the date/time (see example below).\n\n\nRelational attributes\n\n\nRelational attribute declarations take the form:\n\n\n    @attribute [name] relational\n      [further attribute definitions]\n    @end [name]\n\n\n\n\nFor the multi-instance dataset \nMUSK1\n the definition would look like this (\n\"...\"\n denotes an omission):\n\n\n @attribute molecule_name {MUSK-jf78,...,NON-MUSK-199}\n @attribute bag relational\n   @attribute f1 numeric\n   ...\n   @attribute f166 numeric\n @end bag\n @attribute class {0,1}\n ...\n\n\n\n\nThe ARFF Data Section\n\n\nThe ARFF Data section of the file contains the data declaration line and the actual instance lines.\n\n\nThe @data Declaration\n\n\nThe \n@data\n declaration is a single line denoting the start of the data segment in the file. The format is:\n\n\n    @data\n\n\n\n\nThe instance data\n\n\nEach instance is represented on a single line, with carriage returns denoting the end of the instance.  A percent sign (%) introduces a comment, which continues to the end of the line.\n\n\nAttribute values for each instance can be delimited by commas or tabs. A comma/tab may be followed by zero or more spaces. Attribute values must appear in the order in which they were declared in the header section (i.e., the data corresponding to the nth \n@attribute\n declaration is always the nth field of the attribute).\n\n\nA missing value is represented by a single question mark, as in:\n\n\n    @data\n    4.4,?,1.5,?,Iris-setosa\n\n\n\n\nValues of string and nominal attributes are case sensitive, and any that contain space or the comment-delimiter character % must be quoted.  (The code suggests that double-quotes are acceptable and that a backslash will escape individual characters.)  An example follows:\n\n\n    @relation LCCvsLCSH\n\n    @attribute LCC string\n    @attribute LCSH string\n\n    @data\n    AG5,   'Encyclopedias and dictionaries.;Twentieth century.'\n    AS262, 'Science -- Soviet Union -- History.'\n    AE5,   'Encyclopedias and dictionaries.'\n    AS281, 'Astronomy, Assyro-Babylonian.;Moon -- Phases.'\n    AS281, 'Astronomy, Assyro-Babylonian.;Moon -- Tables.'\n\n\n\n\nDates must be specified in the data section using the string representation specified in the attribute declaration. For example:\n\n\n    @RELATION Timestamps\n\n    @ATTRIBUTE timestamp DATE \"yyyy-MM-dd HH:mm:ss\"\n\n    @DATA \n    \"2001-04-03 12:12:12\"\n    \"2001-05-03 12:59:55\"\n\n\n\n\nRelational data must be enclosed within double quotes \n\"\n. For example an instance of the \nMUSK1\n dataset (\n\"...\"\n denotes an omission):\n\n\n MUSK-188,\"42,...,30\",1\n\n\n\n\nSparse ARFF files\n\n\nSparse ARFF files are very similar to ARFF files, but data with value 0 are not be explicitly represented.\n\n\nSparse ARFF files have the same header (i.e \n@relation\n and \n@attribute\n tags) but the data section is different. Instead of representing each value in order, like this:\n\n\n    @data\n    0, X, 0, Y, \"class A\"\n    0, 0, W, 0, \"class B\"\n\n\n\n\nthe non-zero attributes are explicitly identified by attribute number and their value stated, like this:\n\n\n    @data\n    {1 X, 3 Y, 4 \"class A\"}\n    {2 W, 4 \"class B\"}\n\n\n\n\nEach instance is surrounded by curly braces, and the format for each entry is: [index] [space] [value] where index is the attribute index (starting from 0).\n\n\nNote that the omitted values in a sparse instance are \n0\n, they are not \"missing\" values! If a value is unknown, you must explicitly represent it with a question mark (?).\n\n\nWarning:\n There is a known problem saving SparseInstance objects from datasets that have string attributes. In Weka, string and nominal data values are stored as numbers; these numbers act as indexes into an array of possible attribute values (this is very efficient). However, the first string value is assigned index 0: this means that, internally, this value is stored as a 0. When a SparseInstance is written, string instances with internal value 0 are not output, so their string value is lost (and when the arff file is read again, the default value 0 is the index of a different string value, so the attribute value appears to change). To get around this problem, add a dummy string value at index 0 that is never used whenever you declare string attributes that are likely to be used in SparseInstance objects and saved as Sparse ARFF files.\n\n\nInstance weights in ARFF files\n\n\nA weight can be associated with an instance in a standard ARFF file by appending it to the end of the line for that instance and enclosing the value in curly braces. E.g:\n\n\n    @data\n    0, X, 0, Y, \"class A\", {5}\n\n\n\n\nFor a sparse instance, this example would look like:\n\n\n    @data\n    {1 X, 3 Y, 4 \"class A\"}, {5}\n\n\n\n\nNote that any instance without a weight value specified is assumed to have a weight of 1 for backwards compatibility.\n\n\nSee also\n\n\n\n\n[[Add weights to dataset]]\n\n\nARFF Syntax Highlighting\n for various editors\n\n\n\n\nLinks\n\n\n\n\nISO-8601\n\n\nJavadoc of \njava.text.SimpleDateFormat\n (lists the supported character patterns)\n\n\nANTLR syntax by Staal A. Vinterbo \narff.g",
            "title": " ARFF (stable version)"
        },
        {
            "location": "/arff_stable/#overview",
            "text": "ARFF files have two distinct sections. The first section is the  Header  information, which is followed the  Data  information.  The  Header  of the ARFF file contains the name of the relation, a list of the attributes (the columns in the data), and their types. An example header on the standard IRIS dataset looks like this:     % 1. Title: Iris Plants Database\n   % \n   % 2. Sources:\n   %      (a) Creator: R.A. Fisher\n   %      (b) Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n   %      (c) Date: July, 1988\n   % \n   @RELATION iris\n\n   @ATTRIBUTE sepallength  NUMERIC\n   @ATTRIBUTE sepalwidth   NUMERIC\n   @ATTRIBUTE petallength  NUMERIC\n   @ATTRIBUTE petalwidth   NUMERIC\n   @ATTRIBUTE class        {Iris-setosa,Iris-versicolor,Iris-virginica}  The  Data  of the ARFF file looks like the following:     @DATA\n   5.1,3.5,1.4,0.2,Iris-setosa\n   4.9,3.0,1.4,0.2,Iris-setosa\n   4.7,3.2,1.3,0.2,Iris-setosa\n   4.6,3.1,1.5,0.2,Iris-setosa\n   5.0,3.6,1.4,0.2,Iris-setosa\n   5.4,3.9,1.7,0.4,Iris-setosa\n   4.6,3.4,1.4,0.3,Iris-setosa\n   5.0,3.4,1.5,0.2,Iris-setosa\n   4.4,2.9,1.4,0.2,Iris-setosa\n   4.9,3.1,1.5,0.1,Iris-setosa  Lines that begin with a  %  are comments. The  @RELATION ,  @ATTRIBUTE  and  @DATA  declarations are case insensitive.",
            "title": "Overview"
        },
        {
            "location": "/arff_stable/#examples",
            "text": "Several well-known machine learning datasets are distributed with Weka in the  $WEKAHOME/data  directory as ARFF files.",
            "title": "Examples"
        },
        {
            "location": "/arff_stable/#the-arff-header-section",
            "text": "The ARFF Header section of the file contains the relation declaration and attribute declarations.",
            "title": "The ARFF Header Section"
        },
        {
            "location": "/arff_stable/#the-relation-declaration",
            "text": "The relation name is defined as the first line in the ARFF file. The format is:      @relation [relation-name]  where  [relation-name]  is a string. The string must be quoted if the name includes spaces. Furthermore, relation names or attribute names (see below) cannot begin with   a character below \\u0021  '{', '}', ',', or '%'    Moreover, it can only begin with a single or double quote if there is a corresponding quote at the end of the name.",
            "title": "The @relation Declaration"
        },
        {
            "location": "/arff_stable/#the-attribute-declarations",
            "text": "Attribute declarations take the form of an ordered sequence of  @attribute  statements. Each attribute in the data set has its own  @attribute  statement which uniquely defines the name of that attribute and its data type. The order the attributes are declared indicates the column position in the data section of the file. For example, if an attribute is the third one declared then Weka expects that all that attributes values will be found in the third comma delimited column.  The format for the  @attribute  statement is:      @attribute [attribute-name] [datatype]  where the  [attribute-name]  must adhere to the constraints specified in the above section on the @relation declaration.  The [datatype] can be any of the four types supported by Weka:   numeric  integer  is treated as  numeric  real  is treated as  numeric  [nominal-specification]  string  date  [date-format]  relational  for multi-instance data (for future use)   where  [nominal-specification]  and  [date-format]  are defined below. The keywords  numeric ,  real ,  integer ,  string  and  date  are case insensitive.",
            "title": "The @attribute Declarations"
        },
        {
            "location": "/arff_stable/#numeric-attributes",
            "text": "Numeric attributes can be real or integer numbers.",
            "title": "Numeric attributes"
        },
        {
            "location": "/arff_stable/#nominal-attributes",
            "text": "Nominal values are defined by providing an [nominal-specification] listing the possible values: {[nominal-name1], [nominal-name2], [nominal-name3], ...}  For example, the class value of the Iris dataset can be defined as follows:      @ATTRIBUTE class        {Iris-setosa,Iris-versicolor,Iris-virginica}  Values that contain spaces must be quoted.",
            "title": "Nominal attributes"
        },
        {
            "location": "/arff_stable/#string-attributes",
            "text": "String attributes allow us to create attributes containing arbitrary textual values. This is very useful in text-mining applications, as we can create datasets with string attributes, then write Weka Filters to manipulate strings (like  StringToWordVectorFilter ). String attributes are declared as follows:      @ATTRIBUTE LCC    string",
            "title": "String attributes"
        },
        {
            "location": "/arff_stable/#date-attributes",
            "text": "Date attribute declarations take the form:      @attribute [name] date [[date-format]]  where [name] is the name for the attribute and  [date-format]  is an optional string specifying how date values should be parsed and printed (this is the same format used by  SimpleDateFormat ). The default format string accepts the ISO-8601 combined date and time format:  yyyy-MM-dd'T'HH:mm:ss . Check out the Javadoc of the  java.text.SimpleDateFormat  class for supported character patterns.  Dates must be specified in the data section as the corresponding string representations of the date/time (see example below).",
            "title": "Date attributes"
        },
        {
            "location": "/arff_stable/#relational-attributes",
            "text": "Relational attribute declarations take the form:      @attribute [name] relational\n      [further attribute definitions]\n    @end [name]  For the multi-instance dataset  MUSK1  the definition would look like this ( \"...\"  denotes an omission):   @attribute molecule_name {MUSK-jf78,...,NON-MUSK-199}\n @attribute bag relational\n   @attribute f1 numeric\n   ...\n   @attribute f166 numeric\n @end bag\n @attribute class {0,1}\n ...",
            "title": "Relational attributes"
        },
        {
            "location": "/arff_stable/#the-arff-data-section",
            "text": "The ARFF Data section of the file contains the data declaration line and the actual instance lines.",
            "title": "The ARFF Data Section"
        },
        {
            "location": "/arff_stable/#the-data-declaration",
            "text": "The  @data  declaration is a single line denoting the start of the data segment in the file. The format is:      @data",
            "title": "The @data Declaration"
        },
        {
            "location": "/arff_stable/#the-instance-data",
            "text": "Each instance is represented on a single line, with carriage returns denoting the end of the instance.  A percent sign (%) introduces a comment, which continues to the end of the line.  Attribute values for each instance can be delimited by commas or tabs. A comma/tab may be followed by zero or more spaces. Attribute values must appear in the order in which they were declared in the header section (i.e., the data corresponding to the nth  @attribute  declaration is always the nth field of the attribute).  A missing value is represented by a single question mark, as in:      @data\n    4.4,?,1.5,?,Iris-setosa  Values of string and nominal attributes are case sensitive, and any that contain space or the comment-delimiter character % must be quoted.  (The code suggests that double-quotes are acceptable and that a backslash will escape individual characters.)  An example follows:      @relation LCCvsLCSH\n\n    @attribute LCC string\n    @attribute LCSH string\n\n    @data\n    AG5,   'Encyclopedias and dictionaries.;Twentieth century.'\n    AS262, 'Science -- Soviet Union -- History.'\n    AE5,   'Encyclopedias and dictionaries.'\n    AS281, 'Astronomy, Assyro-Babylonian.;Moon -- Phases.'\n    AS281, 'Astronomy, Assyro-Babylonian.;Moon -- Tables.'  Dates must be specified in the data section using the string representation specified in the attribute declaration. For example:      @RELATION Timestamps\n\n    @ATTRIBUTE timestamp DATE \"yyyy-MM-dd HH:mm:ss\"\n\n    @DATA \n    \"2001-04-03 12:12:12\"\n    \"2001-05-03 12:59:55\"  Relational data must be enclosed within double quotes  \" . For example an instance of the  MUSK1  dataset ( \"...\"  denotes an omission):   MUSK-188,\"42,...,30\",1",
            "title": "The instance data"
        },
        {
            "location": "/arff_stable/#sparse-arff-files",
            "text": "Sparse ARFF files are very similar to ARFF files, but data with value 0 are not be explicitly represented.  Sparse ARFF files have the same header (i.e  @relation  and  @attribute  tags) but the data section is different. Instead of representing each value in order, like this:      @data\n    0, X, 0, Y, \"class A\"\n    0, 0, W, 0, \"class B\"  the non-zero attributes are explicitly identified by attribute number and their value stated, like this:      @data\n    {1 X, 3 Y, 4 \"class A\"}\n    {2 W, 4 \"class B\"}  Each instance is surrounded by curly braces, and the format for each entry is: [index] [space] [value] where index is the attribute index (starting from 0).  Note that the omitted values in a sparse instance are  0 , they are not \"missing\" values! If a value is unknown, you must explicitly represent it with a question mark (?).  Warning:  There is a known problem saving SparseInstance objects from datasets that have string attributes. In Weka, string and nominal data values are stored as numbers; these numbers act as indexes into an array of possible attribute values (this is very efficient). However, the first string value is assigned index 0: this means that, internally, this value is stored as a 0. When a SparseInstance is written, string instances with internal value 0 are not output, so their string value is lost (and when the arff file is read again, the default value 0 is the index of a different string value, so the attribute value appears to change). To get around this problem, add a dummy string value at index 0 that is never used whenever you declare string attributes that are likely to be used in SparseInstance objects and saved as Sparse ARFF files.",
            "title": "Sparse ARFF files"
        },
        {
            "location": "/arff_stable/#instance-weights-in-arff-files",
            "text": "A weight can be associated with an instance in a standard ARFF file by appending it to the end of the line for that instance and enclosing the value in curly braces. E.g:      @data\n    0, X, 0, Y, \"class A\", {5}  For a sparse instance, this example would look like:      @data\n    {1 X, 3 Y, 4 \"class A\"}, {5}  Note that any instance without a weight value specified is assumed to have a weight of 1 for backwards compatibility.",
            "title": "Instance weights in ARFF files"
        },
        {
            "location": "/arff_stable/#see-also",
            "text": "[[Add weights to dataset]]  ARFF Syntax Highlighting  for various editors",
            "title": "See also"
        },
        {
            "location": "/arff_stable/#links",
            "text": "ISO-8601  Javadoc of  java.text.SimpleDateFormat  (lists the supported character patterns)  ANTLR syntax by Staal A. Vinterbo  arff.g",
            "title": "Links"
        },
        {
            "location": "/arff_developer/",
            "text": "An ARFF (Attribute-Relation File Format) file is an ASCII text file that describes a list of instances sharing a set of attributes. \n\n\nOverview\n\n\nARFF files have two distinct sections. The first section is the \nHeader\n information, which is followed the \nData\n information.\n\n\nThe \nHeader\n of the ARFF file contains the name of the relation, a list of the attributes (the columns in the data), and their types. An example header on the standard IRIS dataset looks like this:\n\n\n   % 1. Title: Iris Plants Database\n   % \n   % 2. Sources:\n   %      (a) Creator: R.A. Fisher\n   %      (b) Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n   %      (c) Date: July, 1988\n   % \n   @RELATION iris\n\n   @ATTRIBUTE sepallength  NUMERIC\n   @ATTRIBUTE sepalwidth   NUMERIC\n   @ATTRIBUTE petallength  NUMERIC\n   @ATTRIBUTE petalwidth   NUMERIC\n   @ATTRIBUTE class        {Iris-setosa,Iris-versicolor,Iris-virginica}\n\n\n\n\nThe \nData\n of the ARFF file looks like the following:\n\n\n   @DATA\n   5.1,3.5,1.4,0.2,Iris-setosa\n   4.9,3.0,1.4,0.2,Iris-setosa\n   4.7,3.2,1.3,0.2,Iris-setosa\n   4.6,3.1,1.5,0.2,Iris-setosa\n   5.0,3.6,1.4,0.2,Iris-setosa\n   5.4,3.9,1.7,0.4,Iris-setosa\n   4.6,3.4,1.4,0.3,Iris-setosa\n   5.0,3.4,1.5,0.2,Iris-setosa\n   4.4,2.9,1.4,0.2,Iris-setosa\n   4.9,3.1,1.5,0.1,Iris-setosa\n\n\n\n\nLines that begin with a \n%\n are comments. The \n@RELATION\n, \n@ATTRIBUTE\n and \n@DATA\n declarations are case insensitive.\n\n\nExamples\n\n\nSeveral well-known machine learning datasets are distributed with Weka in the \n$WEKAHOME/data\n directory as ARFF files.\n\n\nThe ARFF Header Section\n\n\nThe ARFF Header section of the file contains the relation declaration and attribute declarations.\n\n\nThe @relation Declaration\n\n\nThe relation name is defined as the first line in the ARFF file. The format is:\n\n\n    @relation [relation-name]\n\n\n\n\nwhere \n[relation-name]\n is a string. The string must be quoted if the name includes spaces. Furthermore, relation names or attribute names (see below) cannot begin with\n\n\n\n\na character below \\u0021\n\n\n'{', '}', ',', or '%' \n\n\n\n\nMoreover, it can only begin with a single or double quote if there is a corresponding quote at the end of the name.\n\n\n== The @attribute Declarations ==\nAttribute declarations take the form of an ordered sequence of \n@attribute\n statements. Each attribute in the data set has its own \n@attribute\n statement which uniquely defines the name of that attribute and its data type. The order the attributes are declared indicates the column position in the data section of the file. For example, if an attribute is the third one declared then Weka expects that all that attributes values will be found in the third comma delimited column.\n\n\nThe format for the \n@attribute\n statement is:\n\n\n    @attribute [attribute-name] [datatype]\n\n\n\n\nwhere the \n[attribute-name]\n must adhere to the constraints specified in the above section on the @relation declaration.\n\n\nThe [datatype] can be any of the four types supported by Weka:\n\n\n\n\nnumeric\n\n\ninteger\n is treated as \nnumeric\n\n\nreal\n is treated as \nnumeric\n\n\n[nominal-specification]\n\n\nstring\n\n\ndate\n [date-format]\n\n\nrelational\n for multi-instance data (for future use)\n\n\n\n\nwhere \n[nominal-specification]\n and \n[date-format]\n are defined below. The keywords \nnumeric\n, \nreal\n, \ninteger\n, \nstring\n and \ndate\n are case insensitive.\n\n\nNumeric attributes\n\n\nNumeric attributes can be real or integer numbers.\n\n\nNominal attributes\n\n\nNominal values are defined by providing an [nominal-specification] listing the possible values: {[nominal-name1], [nominal-name2], [nominal-name3], ...}\n\n\nFor example, the class value of the Iris dataset can be defined as follows:\n\n\n    @ATTRIBUTE class        {Iris-setosa,Iris-versicolor,Iris-virginica}\n\n\n\n\nValues that contain spaces must be quoted.\n\n\nString attributes\n\n\nString attributes allow us to create attributes containing arbitrary textual values. This is very useful in text-mining applications, as we can create datasets with string attributes, then write Weka Filters to manipulate strings (like \nStringToWordVectorFilter\n). String attributes are declared as follows:\n\n\n    @ATTRIBUTE LCC    string\n\n\n\n\nDate attributes\n\n\nDate attribute declarations take the form:\n\n\n    @attribute [name] date [[date-format]]\n\n\n\n\nwhere [name] is the name for the attribute and \n[date-format]\n is an optional string specifying how date values should be parsed and printed (this is the same format used by \nSimpleDateFormat\n). The default format string accepts the ISO-8601 combined date and time format: \nyyyy-MM-dd'T'HH:mm:ss\n. Check out the Javadoc of the \njava.text.SimpleDateFormat\n class for supported character patterns.\n\n\nDates must be specified in the data section as the corresponding string representations of the date/time (see example below).\n\n\nRelational attributes\n\n\nRelational attribute declarations take the form:\n\n\n    @attribute [name] relational\n      [further attribute definitions]\n    @end [name]\n\n\n\n\nFor the multi-instance dataset \nMUSK1\n the definition would look like this (\n\"...\"\n denotes an omission):\n\n\n @attribute molecule_name {MUSK-jf78,...,NON-MUSK-199}\n @attribute bag relational\n   @attribute f1 numeric\n   ...\n   @attribute f166 numeric\n @end bag\n @attribute class {0,1}\n ...\n\n\n\n\nThe ARFF Data Section\n\n\nThe ARFF Data section of the file contains the data declaration line and the actual instance lines.\n\n\nThe @data Declaration\n\n\nThe \n@data\n declaration is a single line denoting the start of the data segment in the file. The format is:\n\n\n    @data\n\n\n\n\nThe instance data\n\n\nEach instance is represented on a single line, with carriage returns denoting the end of the instance.  A percent sign (%) introduces a comment, which continues to the end of the line.\n\n\nAttribute values for each instance can be delimited by commas or tabs. A comma/tab may be followed by zero or more spaces. Attribute values must appear in the order in which they were declared in the header section (i.e., the data corresponding to the nth \n@attribute\n declaration is always the nth field of the attribute).\n\n\nA missing value is represented by a single question mark, as in:\n\n\n    @data\n    4.4,?,1.5,?,Iris-setosa\n\n\n\n\nValues of string and nominal attributes are case sensitive, and any that contain space or the comment-delimiter character % must be quoted.  (The code suggests that double-quotes are acceptable and that a backslash will escape individual characters.)  An example follows:\n\n\n    @relation LCCvsLCSH\n\n    @attribute LCC string\n    @attribute LCSH string\n\n    @data\n    AG5,   'Encyclopedias and dictionaries.;Twentieth century.'\n    AS262, 'Science -- Soviet Union -- History.'\n    AE5,   'Encyclopedias and dictionaries.'\n    AS281, 'Astronomy, Assyro-Babylonian.;Moon -- Phases.'\n    AS281, 'Astronomy, Assyro-Babylonian.;Moon -- Tables.'\n\n\n\n\nDates must be specified in the data section using the string representation specified in the attribute declaration. For example:\n\n\n    @RELATION Timestamps\n\n    @ATTRIBUTE timestamp DATE \"yyyy-MM-dd HH:mm:ss\"\n\n    @DATA \n    \"2001-04-03 12:12:12\"\n    \"2001-05-03 12:59:55\"\n\n\n\n\nRelational data must be enclosed within double quotes \n\"\n. For example an instance of the \nMUSK1\n dataset (\n\"...\"\n denotes an omission):\n\n\nMUSK-188,\"42,...,30\",1\n\n\n\n\nSparse ARFF files\n\n\nSparse ARFF files are very similar to ARFF files, but data with value 0 are not be explicitly represented.\n\n\nSparse ARFF files have the same header (i.e \n@relation\n and \n@attribute\n tags) but the data section is different. Instead of representing each value in order, like this:\n\n\n    @data\n    0, X, 0, Y, \"class A\"\n    0, 0, W, 0, \"class B\"\n\n\n\n\nthe non-zero attributes are explicitly identified by attribute number and their value stated, like this:\n\n\n    @data\n    {1 X, 3 Y, 4 \"class A\"}\n    {2 W, 4 \"class B\"}\n\n\n\n\nEach instance is surrounded by curly braces, and the format for each entry is: [index] [space] [value] where index is the attribute index (starting from 0).\n\n\nNote that the omitted values in a sparse instance are \n0\n, they are not \"missing\" values! If a value is unknown, you must explicitly represent it with a question mark (?).\n\n\nWarning:\n There is a known problem saving SparseInstance objects from datasets that have string attributes. In Weka, string and nominal data values are stored as numbers; these numbers act as indexes into an array of possible attribute values (this is very efficient). However, the first string value is assigned index 0: this means that, internally, this value is stored as a 0. When a SparseInstance is written, string instances with internal value 0 are not output, so their string value is lost (and when the arff file is read again, the default value 0 is the index of a different string value, so the attribute value appears to change). To get around this problem, add a dummy string value at index 0 that is never used whenever you declare string attributes that are likely to be used in SparseInstance objects and saved as Sparse ARFF files.\n\n\nInstance weights in ARFF files\n\n\nA weight can be associated with an instance in a standard ARFF file by appending it to the end of the line for that instance and enclosing the value in curly braces. E.g:\n\n\n    @data\n    0, X, 0, Y, \"class A\", {5}\n\n\n\n\nFor a sparse instance, this example would look like:\n\n\n    @data\n    {1 X, 3 Y, 4 \"class A\"}, {5}\n\n\n\n\nNote that any instance without a weight value specified is assumed to have a weight of 1 for backwards compatibility.\n\n\nSee also\n\n\n\n\n[[Add weights to dataset]]\n\n\nARFF Syntax Highlighting\n for various editors\n\n\n\n\nLinks\n\n\n\n\nISO 8601\n\n\nJavadoc of \njava.text.SimpleDateFormat\n (lists the supported character patterns)\n\n\nANTLR syntax by Staal A. Vinterbo \narff.g",
            "title": " ARFF (developer version)"
        },
        {
            "location": "/arff_developer/#overview",
            "text": "ARFF files have two distinct sections. The first section is the  Header  information, which is followed the  Data  information.  The  Header  of the ARFF file contains the name of the relation, a list of the attributes (the columns in the data), and their types. An example header on the standard IRIS dataset looks like this:     % 1. Title: Iris Plants Database\n   % \n   % 2. Sources:\n   %      (a) Creator: R.A. Fisher\n   %      (b) Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\n   %      (c) Date: July, 1988\n   % \n   @RELATION iris\n\n   @ATTRIBUTE sepallength  NUMERIC\n   @ATTRIBUTE sepalwidth   NUMERIC\n   @ATTRIBUTE petallength  NUMERIC\n   @ATTRIBUTE petalwidth   NUMERIC\n   @ATTRIBUTE class        {Iris-setosa,Iris-versicolor,Iris-virginica}  The  Data  of the ARFF file looks like the following:     @DATA\n   5.1,3.5,1.4,0.2,Iris-setosa\n   4.9,3.0,1.4,0.2,Iris-setosa\n   4.7,3.2,1.3,0.2,Iris-setosa\n   4.6,3.1,1.5,0.2,Iris-setosa\n   5.0,3.6,1.4,0.2,Iris-setosa\n   5.4,3.9,1.7,0.4,Iris-setosa\n   4.6,3.4,1.4,0.3,Iris-setosa\n   5.0,3.4,1.5,0.2,Iris-setosa\n   4.4,2.9,1.4,0.2,Iris-setosa\n   4.9,3.1,1.5,0.1,Iris-setosa  Lines that begin with a  %  are comments. The  @RELATION ,  @ATTRIBUTE  and  @DATA  declarations are case insensitive.",
            "title": "Overview"
        },
        {
            "location": "/arff_developer/#examples",
            "text": "Several well-known machine learning datasets are distributed with Weka in the  $WEKAHOME/data  directory as ARFF files.",
            "title": "Examples"
        },
        {
            "location": "/arff_developer/#the-arff-header-section",
            "text": "The ARFF Header section of the file contains the relation declaration and attribute declarations.",
            "title": "The ARFF Header Section"
        },
        {
            "location": "/arff_developer/#the-relation-declaration",
            "text": "The relation name is defined as the first line in the ARFF file. The format is:      @relation [relation-name]  where  [relation-name]  is a string. The string must be quoted if the name includes spaces. Furthermore, relation names or attribute names (see below) cannot begin with   a character below \\u0021  '{', '}', ',', or '%'    Moreover, it can only begin with a single or double quote if there is a corresponding quote at the end of the name.  == The @attribute Declarations ==\nAttribute declarations take the form of an ordered sequence of  @attribute  statements. Each attribute in the data set has its own  @attribute  statement which uniquely defines the name of that attribute and its data type. The order the attributes are declared indicates the column position in the data section of the file. For example, if an attribute is the third one declared then Weka expects that all that attributes values will be found in the third comma delimited column.  The format for the  @attribute  statement is:      @attribute [attribute-name] [datatype]  where the  [attribute-name]  must adhere to the constraints specified in the above section on the @relation declaration.  The [datatype] can be any of the four types supported by Weka:   numeric  integer  is treated as  numeric  real  is treated as  numeric  [nominal-specification]  string  date  [date-format]  relational  for multi-instance data (for future use)   where  [nominal-specification]  and  [date-format]  are defined below. The keywords  numeric ,  real ,  integer ,  string  and  date  are case insensitive.",
            "title": "The @relation Declaration"
        },
        {
            "location": "/arff_developer/#numeric-attributes",
            "text": "Numeric attributes can be real or integer numbers.",
            "title": "Numeric attributes"
        },
        {
            "location": "/arff_developer/#nominal-attributes",
            "text": "Nominal values are defined by providing an [nominal-specification] listing the possible values: {[nominal-name1], [nominal-name2], [nominal-name3], ...}  For example, the class value of the Iris dataset can be defined as follows:      @ATTRIBUTE class        {Iris-setosa,Iris-versicolor,Iris-virginica}  Values that contain spaces must be quoted.",
            "title": "Nominal attributes"
        },
        {
            "location": "/arff_developer/#string-attributes",
            "text": "String attributes allow us to create attributes containing arbitrary textual values. This is very useful in text-mining applications, as we can create datasets with string attributes, then write Weka Filters to manipulate strings (like  StringToWordVectorFilter ). String attributes are declared as follows:      @ATTRIBUTE LCC    string",
            "title": "String attributes"
        },
        {
            "location": "/arff_developer/#date-attributes",
            "text": "Date attribute declarations take the form:      @attribute [name] date [[date-format]]  where [name] is the name for the attribute and  [date-format]  is an optional string specifying how date values should be parsed and printed (this is the same format used by  SimpleDateFormat ). The default format string accepts the ISO-8601 combined date and time format:  yyyy-MM-dd'T'HH:mm:ss . Check out the Javadoc of the  java.text.SimpleDateFormat  class for supported character patterns.  Dates must be specified in the data section as the corresponding string representations of the date/time (see example below).",
            "title": "Date attributes"
        },
        {
            "location": "/arff_developer/#relational-attributes",
            "text": "Relational attribute declarations take the form:      @attribute [name] relational\n      [further attribute definitions]\n    @end [name]  For the multi-instance dataset  MUSK1  the definition would look like this ( \"...\"  denotes an omission):   @attribute molecule_name {MUSK-jf78,...,NON-MUSK-199}\n @attribute bag relational\n   @attribute f1 numeric\n   ...\n   @attribute f166 numeric\n @end bag\n @attribute class {0,1}\n ...",
            "title": "Relational attributes"
        },
        {
            "location": "/arff_developer/#the-arff-data-section",
            "text": "The ARFF Data section of the file contains the data declaration line and the actual instance lines.",
            "title": "The ARFF Data Section"
        },
        {
            "location": "/arff_developer/#the-data-declaration",
            "text": "The  @data  declaration is a single line denoting the start of the data segment in the file. The format is:      @data",
            "title": "The @data Declaration"
        },
        {
            "location": "/arff_developer/#the-instance-data",
            "text": "Each instance is represented on a single line, with carriage returns denoting the end of the instance.  A percent sign (%) introduces a comment, which continues to the end of the line.  Attribute values for each instance can be delimited by commas or tabs. A comma/tab may be followed by zero or more spaces. Attribute values must appear in the order in which they were declared in the header section (i.e., the data corresponding to the nth  @attribute  declaration is always the nth field of the attribute).  A missing value is represented by a single question mark, as in:      @data\n    4.4,?,1.5,?,Iris-setosa  Values of string and nominal attributes are case sensitive, and any that contain space or the comment-delimiter character % must be quoted.  (The code suggests that double-quotes are acceptable and that a backslash will escape individual characters.)  An example follows:      @relation LCCvsLCSH\n\n    @attribute LCC string\n    @attribute LCSH string\n\n    @data\n    AG5,   'Encyclopedias and dictionaries.;Twentieth century.'\n    AS262, 'Science -- Soviet Union -- History.'\n    AE5,   'Encyclopedias and dictionaries.'\n    AS281, 'Astronomy, Assyro-Babylonian.;Moon -- Phases.'\n    AS281, 'Astronomy, Assyro-Babylonian.;Moon -- Tables.'  Dates must be specified in the data section using the string representation specified in the attribute declaration. For example:      @RELATION Timestamps\n\n    @ATTRIBUTE timestamp DATE \"yyyy-MM-dd HH:mm:ss\"\n\n    @DATA \n    \"2001-04-03 12:12:12\"\n    \"2001-05-03 12:59:55\"  Relational data must be enclosed within double quotes  \" . For example an instance of the  MUSK1  dataset ( \"...\"  denotes an omission):  MUSK-188,\"42,...,30\",1",
            "title": "The instance data"
        },
        {
            "location": "/arff_developer/#sparse-arff-files",
            "text": "Sparse ARFF files are very similar to ARFF files, but data with value 0 are not be explicitly represented.  Sparse ARFF files have the same header (i.e  @relation  and  @attribute  tags) but the data section is different. Instead of representing each value in order, like this:      @data\n    0, X, 0, Y, \"class A\"\n    0, 0, W, 0, \"class B\"  the non-zero attributes are explicitly identified by attribute number and their value stated, like this:      @data\n    {1 X, 3 Y, 4 \"class A\"}\n    {2 W, 4 \"class B\"}  Each instance is surrounded by curly braces, and the format for each entry is: [index] [space] [value] where index is the attribute index (starting from 0).  Note that the omitted values in a sparse instance are  0 , they are not \"missing\" values! If a value is unknown, you must explicitly represent it with a question mark (?).  Warning:  There is a known problem saving SparseInstance objects from datasets that have string attributes. In Weka, string and nominal data values are stored as numbers; these numbers act as indexes into an array of possible attribute values (this is very efficient). However, the first string value is assigned index 0: this means that, internally, this value is stored as a 0. When a SparseInstance is written, string instances with internal value 0 are not output, so their string value is lost (and when the arff file is read again, the default value 0 is the index of a different string value, so the attribute value appears to change). To get around this problem, add a dummy string value at index 0 that is never used whenever you declare string attributes that are likely to be used in SparseInstance objects and saved as Sparse ARFF files.",
            "title": "Sparse ARFF files"
        },
        {
            "location": "/arff_developer/#instance-weights-in-arff-files",
            "text": "A weight can be associated with an instance in a standard ARFF file by appending it to the end of the line for that instance and enclosing the value in curly braces. E.g:      @data\n    0, X, 0, Y, \"class A\", {5}  For a sparse instance, this example would look like:      @data\n    {1 X, 3 Y, 4 \"class A\"}, {5}  Note that any instance without a weight value specified is assumed to have a weight of 1 for backwards compatibility.",
            "title": "Instance weights in ARFF files"
        },
        {
            "location": "/arff_developer/#see-also",
            "text": "[[Add weights to dataset]]  ARFF Syntax Highlighting  for various editors",
            "title": "See also"
        },
        {
            "location": "/arff_developer/#links",
            "text": "ISO 8601  Javadoc of  java.text.SimpleDateFormat  (lists the supported character patterns)  ANTLR syntax by Staal A. Vinterbo  arff.g",
            "title": "Links"
        },
        {
            "location": "/arff_syntax/",
            "text": "Here you can find syntax highlightings for various editors:\n\n\nEmacs\n\n\nAdd the code from the \narff.emacs\n file into your startup file.\n\n\nNotepad++\n\n\n\n\nCopy the contents of tag of the \narff.notepadplus\n file into your \n%APPDATA%\\Notepad++\\userDefineLang.xml\n file.\n\n\n(Ensure that you maintain the XML structure). If \nuserDefineLang.xml\n does not exist, simply rename the \narff.notepadplus\n file to \nuserDefineLang.xml\n\n\n\n\nTextPad\n\n\n\n\nCopy the file \narff.syn\n into your \n<TEXTPAD-DIR>/system\n directory.\n\n\nThen run the wizard for adding a new document class (Configure -> New Document Class...).\n\n\n\n\nUltraedit\n\n\n\n\nJust copy/paste the content of the file \narff.ultraedit\n in your \n<ULTRAEDIT-DIR>/WORDFILE.TXT\n file.\n\n\nAdjust the \n/Lnn\n language number that it fits into the numbering of your current settings.\n\n\n\n\nvim/gvim\n\n\n\n\nSave the file \narff.vim\n in your \n$HOME/.vim/syntax\n directory.\n\n\nYou can enable the syntax with \n:set syntax=arff\n.\n\n\n\n\nLinks\n\n\n\n\nEmacs homepage\n\n\nNotepad++ homepage\n\n\nTextPad homepage\n\n\nUltraedit homepage\n\n\nvim homepage",
            "title": " ARFF Syntax Highlighting"
        },
        {
            "location": "/arff_syntax/#emacs",
            "text": "Add the code from the  arff.emacs  file into your startup file.",
            "title": "Emacs"
        },
        {
            "location": "/arff_syntax/#notepad",
            "text": "Copy the contents of tag of the  arff.notepadplus  file into your  %APPDATA%\\Notepad++\\userDefineLang.xml  file.  (Ensure that you maintain the XML structure). If  userDefineLang.xml  does not exist, simply rename the  arff.notepadplus  file to  userDefineLang.xml",
            "title": "Notepad++"
        },
        {
            "location": "/arff_syntax/#textpad",
            "text": "Copy the file  arff.syn  into your  <TEXTPAD-DIR>/system  directory.  Then run the wizard for adding a new document class (Configure -> New Document Class...).",
            "title": "TextPad"
        },
        {
            "location": "/arff_syntax/#ultraedit",
            "text": "Just copy/paste the content of the file  arff.ultraedit  in your  <ULTRAEDIT-DIR>/WORDFILE.TXT  file.  Adjust the  /Lnn  language number that it fits into the numbering of your current settings.",
            "title": "Ultraedit"
        },
        {
            "location": "/arff_syntax/#vimgvim",
            "text": "Save the file  arff.vim  in your  $HOME/.vim/syntax  directory.  You can enable the syntax with  :set syntax=arff .",
            "title": "vim/gvim"
        },
        {
            "location": "/arff_syntax/#links",
            "text": "Emacs homepage  Notepad++ homepage  TextPad homepage  Ultraedit homepage  vim homepage",
            "title": "Links"
        },
        {
            "location": "/creating_arff_file/",
            "text": "The following code generates an \nInstances\n object and outputs it to stdout as \nARFF\n file.\n\n\nIt generates the following types of attributes:\n\n\n\n\nnumeric\n\n\nnominal\n\n\nstring\n\n\ndate\n\n\nrelational\n\n\n\n\nExample class \nAttTest\n:\n\n\n import weka.core.Attribute;\n import weka.core.FastVector;\n import weka.core.Instance;\n import weka.core.Instances;\n\n /**\n  * Generates a little ARFF file with different attribute types.\n  *\n  * @author FracPete\n  */\n public class AttTest {\n   public static void main(String[] args) throws Exception {\n     FastVector      atts;\n     FastVector      attsRel;\n     FastVector      attVals;\n     FastVector      attValsRel;\n     Instances       data;\n     Instances       dataRel;\n     double[]        vals;\n     double[]        valsRel;\n     int             i;\n\n     // 1. set up attributes\n     atts = new FastVector();\n     // - numeric\n     atts.addElement(new Attribute(\"att1\"));\n     // - nominal\n     attVals = new FastVector();\n     for (i = 0; i < 5; i++)\n       attVals.addElement(\"val\" + (i+1));\n     atts.addElement(new Attribute(\"att2\", attVals));\n     // - string\n     atts.addElement(new Attribute(\"att3\", (FastVector) null));\n     // - date\n     atts.addElement(new Attribute(\"att4\", \"yyyy-MM-dd\"));\n     // - relational\n     attsRel = new FastVector();\n     // -- numeric\n     attsRel.addElement(new Attribute(\"att5.1\"));\n     // -- nominal\n     attValsRel = new FastVector();\n     for (i = 0; i < 5; i++)\n       attValsRel.addElement(\"val5.\" + (i+1));\n     attsRel.addElement(new Attribute(\"att5.2\", attValsRel));\n     dataRel = new Instances(\"att5\", attsRel, 0);\n     atts.addElement(new Attribute(\"att5\", dataRel, 0));\n\n     // 2. create Instances object\n     data = new Instances(\"MyRelation\", atts, 0);\n\n     // 3. fill with data\n     // first instance\n     vals = new double[data.numAttributes()];\n     // - numeric\n     vals[0] = Math.PI;\n     // - nominal\n     vals[1] = attVals.indexOf(\"val3\");\n     // - string\n     vals[2] = data.attribute(2).addStringValue(\"This is a string!\");\n     // - date\n     vals[3] = data.attribute(3).parseDate(\"2001-11-09\");\n     // - relational\n     dataRel = new Instances(data.attribute(4).relation(), 0);\n     // -- first instance\n     valsRel = new double[2];\n     valsRel[0] = Math.PI + 1;\n     valsRel[1] = attValsRel.indexOf(\"val5.3\");\n     dataRel.add(new Instance(1.0, valsRel));\n     // -- second instance\n     valsRel = new double[2];\n     valsRel[0] = Math.PI + 2;\n     valsRel[1] = attValsRel.indexOf(\"val5.2\");\n     dataRel.add(new Instance(1.0, valsRel));\n     vals[4] = data.attribute(4).addRelation(dataRel);\n     // add\n     data.add(new Instance(1.0, vals));\n\n     // second instance\n     vals = new double[data.numAttributes()];  // important: needs NEW array!\n     // - numeric\n     vals[0] = Math.E;\n     // - nominal\n     vals[1] = attVals.indexOf(\"val1\");\n     // - string\n     vals[2] = data.attribute(2).addStringValue(\"And another one!\");\n     // - date\n     vals[3] = data.attribute(3).parseDate(\"2000-12-01\");\n     // - relational\n     dataRel = new Instances(data.attribute(4).relation(), 0);\n     // -- first instance\n     valsRel = new double[2];\n     valsRel[0] = Math.E + 1;\n     valsRel[1] = attValsRel.indexOf(\"val5.4\");\n     dataRel.add(new Instance(1.0, valsRel));\n     // -- second instance\n     valsRel = new double[2];\n     valsRel[0] = Math.E + 2;\n     valsRel[1] = attValsRel.indexOf(\"val5.1\");\n     dataRel.add(new Instance(1.0, valsRel));\n     vals[4] = data.attribute(4).addRelation(dataRel);\n     // add\n     data.add(new Instance(1.0, vals));\n\n     // 4. output data\n     System.out.println(data);\n   }\n }\n\n\n\n\nMissing values\n\n\nBy default, a new double array will be initialized with 0s. In case you want to be a value missing at a certain position, you have to explicitly set the \nmissing value\n via the \nmissingValue()\n method of the \nweka.core.Instance\n class. In Weka > 3.7.1 Instance is an interface, so \nmissingValue()\n moved into \nweka.core.Utils\n. In case you already have an existing \nweka.core.Instance\n object, then you use its \nsetMissing(int)\n method, which sets a missing value at the given position. Here are examples, which set the \nthird\n attribute to missing:\n\n\n\n\n\n\ndouble array:\n\n\ndouble[] vals = ...  // from somewhere, e.g., from AttTest.java example\nvals[2] = Instance.missingValue(); // or ... = Utils.missingValue() for Weka > 3.7.1\n\n\n\n\n\n\nweka.core.Instance\n object:\n\n\ndouble[] vals = ... // from somewhere, e.g., from AttTest.java example\nInstance inst = new Instance(1.0, vals);\ninst.setMissing(2);\n\n\n\n\n\n\nDownloads\n\n\n\n\nAttTest.java\n (\nstable\n, \ndeveloper\n) - the above class\n\n\n\n\nSee also\n\n\n\n\n[[Save Instances to an ARFF File]] - if you want to save the data to a file instead of printing it to stdout\n\n\n[[Adding attributes to a dataset]] - shows how to add attributes to an existing dataset\n\n\nARFF\n format",
            "title": " Creating an ARFF file"
        },
        {
            "location": "/creating_arff_file/#missing-values",
            "text": "By default, a new double array will be initialized with 0s. In case you want to be a value missing at a certain position, you have to explicitly set the  missing value  via the  missingValue()  method of the  weka.core.Instance  class. In Weka > 3.7.1 Instance is an interface, so  missingValue()  moved into  weka.core.Utils . In case you already have an existing  weka.core.Instance  object, then you use its  setMissing(int)  method, which sets a missing value at the given position. Here are examples, which set the  third  attribute to missing:    double array:  double[] vals = ...  // from somewhere, e.g., from AttTest.java example\nvals[2] = Instance.missingValue(); // or ... = Utils.missingValue() for Weka > 3.7.1    weka.core.Instance  object:  double[] vals = ... // from somewhere, e.g., from AttTest.java example\nInstance inst = new Instance(1.0, vals);\ninst.setMissing(2);",
            "title": "Missing values"
        },
        {
            "location": "/creating_arff_file/#downloads",
            "text": "AttTest.java  ( stable ,  developer ) - the above class",
            "title": "Downloads"
        },
        {
            "location": "/creating_arff_file/#see-also",
            "text": "[[Save Instances to an ARFF File]] - if you want to save the data to a file instead of printing it to stdout  [[Adding attributes to a dataset]] - shows how to add attributes to an existing dataset  ARFF  format",
            "title": "See also"
        },
        {
            "location": "/creating_instances/",
            "text": "see \nCreating an ARFF file",
            "title": " Creating Instances on-the-fly"
        },
        {
            "location": "/packages/manager/",
            "text": "Usually, the term \"package\" is used to refer to Java's concept of organizing classes. From version 3.7.2, Weka has the concept of a package as a bundle of additional functionality, separate from that supplied in the main weka.jar file. A package consists of various jar files, documentation, meta data, and possibly source code. Many learning algorithms and tools that were present in earlier versions of Weka have become separate packages from version 3.7.2. This simplifies the core Weka system and allows users to install just what they need or are interested in. It also provides a simple mechanism for people to use when contributing to Weka. There are a number of packages available for Weka that add learning schemes or extend the functionality of the core system in some fashion. Many are provided by the Weka team and others are from third parties.\n\n\nWeka includes a facility for the management of packages and a mechanism to load them dynamically at runtime. There is both a command-line and GUI package manager. If the package manager does not start when you try to run it, take a look at \nthis\n page.\n\n\nCommand line package management\n\n\nAssuming that the weka.jar file is in the classpath, the package manager can be accessed by typing:\n\n\n java weka.core.WekaPackageManager\n\n\n\n\nSupplying no options will print the usage information:\n\n\nUsage: weka.core.WekaPackageManager [option]\nOptions:\n  -list-packages <all | installed | available>\n  -package-info <repository | installed | archive> packageName\n  -install-package <packageName | packageZip | URL> [version]\n  -uninstall-package <packageName>\n  -refresh-cache\n\n\n\n\n\n\nWeka 3.7.8 and snapshot builds of the developer version of Weka after September 24 2012 now offer a completely \"offline\" mode that involves no attempts to connect to the internet. This mode can be used to install package zip files that the user already has on the file system, and to browse already installed packages. This mode can be accessed from the command line package manager by specifying the \"-offline\" option. Alternatively, the property weka.packageManager.offline=true can be provided to the Java virtual machine on the command line or in a properties file (see the section on properties below).\n\n\n\n\nInformation (meta data) about packages is stored on a web server hosted on Sourceforge. The first time the package manager is run, for a new installation of Weka, there will be a short delay while the system downloads and stores a cache of the meta data from the server. Maintaining a cache speeds up the process of browsing the package information. From time to time you should update the local cache of package meta data in order to get the latest information on packages from the server. This can be achieved by supplying the \n-refresh-cache\n option.\n\n\nThe \n-list-packages\n option will, as the name suggests, print information (version numbers and short descriptions) about various packages. The option must be followed by one of three keywords:\n\n\n\n\nall\n will print information on all packages that the system knows about\n\n\ninstalled\n will print information on all packages that are installed locally\n\n\navailable\n will print information on all packages that are not installed\n\n\n\n\nThe following shows an example of listing all packages installed locally:\n\n\njava weka.core.WekaPackageManager -list-packages installed\n\nInstalled    Repository    Package\n=========    ==========    =======\n1.0.0        1.0.0         DTNB: Class for building and using a decision table/naive bayes hybrid classifier.\n1.0.0        1.0.0         massiveOnlineAnalysis: MOA (Massive On-line Analysis).\n1.0.0        1.0.0         multiInstanceFilters: A collection of filters for manipulating multi-instance data.\n1.0.0        1.0.0         naiveBayesTree: Class for generating a decision tree with naive Bayes classifiers at the leaves.\n1.0.0        1.0.0         scatterPlot3D: A visualization component for displaying a 3D scatter plot of the data using Java 3D.\n\n\n\n\nThe \n-package-info\n command lists information about a package given its name. The command is followed by one of three keywords and then the name of a package:\n\n\n\n\nrepository\n will print info from the repository for the named package\n\n\ninstalled\n will print info on the installed version of the named package\n\n\narchive\n will print info for a package stored in a zip archive. In this case, the \u201carchive\u201d keyword must be followed by the path to an package zip archive file rather than just the name of a package\n\n\n\n\nThe following shows an example of listing information for the \u201cisotonicRegression\u201d package from the server:\n\n\njava weka.core.WekaPackageManager -package-info repository isotonicRegression\nDescription:Learns an isotonic regression model. Picks the attribute that results in the lowest squared error. Missing values are not allowed. Can only deal with numeric attributes. Considers the monotonically increasing case as well as the monotonically decreasing case.\nVersion:1.0.0\nPackageURL:http://prdownloads.sourceforge.net/weka/isotonicRegression1.0.0.zip?download\nAuthor:Eibe Frank\nPackageName:isotonicRegression\nTitle:Learns an isotonic regression model.\nDate:2009-09-10\nURL:http://weka.sourceforge.net/doc.dev/weka/classifiers/IsotonicRegression.html\nCategory:Regression\nDepends:weka (>=3.7.1)\nLicense:GPL 2.0\nMaintainer:Weka team <wekalist@list.scms.waikato.ac.nz>\n\n\n\n\nThe \n-install-package\n command allows a package to be installed from one of three locations:\n\n\n\n\nspecifying a name of a package will install the package using the information in the package description meta data stored on the server. If no version number is given, then the latest available version of the package is installed.\n\n\nproviding a path to a zip file will attempt to unpack and install the archive as a Weka package\n\n\nproviding a URL (beginning with \nhttp://\n) to a package zip file on the web will download and attempt to install the zip file as a Weka package\n\n\n\n\nThe \nuninstall-package\n command will uninstall the named package. Of course, the named package has to be installed for this command to have any effect!\n\n\nRunning installed learning algorithms\n\n\nRunning learning algorithms that come with the main weka distribution (i.e. are contained in the weka.jar file) was covered earlier in the [[Primer]]. But what about algorithms from packages that you\u2019ve installed using the package manager? We don\u2019t want to have to add a ton of jar files to our classpath every time we wan\u2019t to run a particular algorithm. Fortunately, we don\u2019t have to. Weka has a mechanism to load installed packages dynamically at run time. This means that newly installed packages are available in Weka's GUIs immediately.\n\n\nWhat about running algorithms from packages on the command line I hear you ask? We can run a named algorithm by using the Run command:\n\n\n java weka.Run\n\n\n\n\nIf no arguments are supplied, then Run outputs the following usage information:\n\n\n Usage:\n     weka.Run [-no-scan] [-no-load] <scheme name [scheme options]>\n\n\n\n\nThe Run command supports sub-string matching, so you can run a classifier (such as J48) like so:\n\n\n java weka.Run J48\n\n\n\n\nWhen there are multiple matches on a supplied scheme name you will be presented with a list. For example:\n\n\n java weka.Run NaiveBayes\n\n Select a scheme to run, or <return> to exit:\n     1) weka.classifiers.bayes.ComplementNaiveBayes\n     2) weka.classifiers.bayes.NaiveBayes\n     3) weka.classifiers.bayes.NaiveBayesMultinomial\n     4) weka.classifiers.bayes.NaiveBayesMultinomialUpdateable\n     5) weka.classifiers.bayes.NaiveBayesSimple\n     6) weka.classifiers.bayes.NaiveBayesUpdateable\n\n Enter a number >\n\n\n\n\nYou can turn off the scanning of packages and sub-string matching by providing the \n-no-scan\n option. This is useful when using the \nRun\n command in a script. In this case, you need to specify the fully qualified name of the algorithm to use. E.g.\n\n\n java weka.Run -no-scan weka.classifiers.bayes.NaiveBayes\n\n\n\n\nTo reduce startup time you can also turn off the dynamic loading of installed packages by specifying the \n-no-load\n option. In this case, you will need to explicitly include any packaged algorithms in your classpath if you plan to use them. E.g.\n\n\njava -classpath ./weka.jar:$HOME/wekafiles/packages/DTNB/DTNB.jar rweka.Run -no-load -no-scan weka.classifiers.rules.DTNB\n\n\n\n\nGUI package manager\n\n\nAs well as a command line client, there is also a graphical interface to Weka\u2019s package management system. This is available from the \nTools\n menu in the \nGUIChooser\n. All the functionality available in the command line client to the package management system is available in the GUI version, along with the ability to install and uninstall multiple packages in one hit.\n\n\n\n\nThe package manager\u2019s window is split horizontally into two parts: at the top is a list of packages and at the bottom is a mini browser that can be used to display information on the currently selected package.\n\n\nThe package list shows the name of a package, its category, the currently installed version (if installed), the latest version available via the repository and whether the package has been loaded or not. This list may be sorted by either package name or category by clicking on the appropriate column header. A second click on the same header reverses the sort order. Three radio buttons in the upper left of the window can be used to filter what is displayed in the list. All packages (default), all available packages (i.e. those not yet installed) or only installed packages can be displayed.\n\n\nIf multiple versions of a package are available, they can be accessed by clicking on an entry in the \u201cRepository version\u201d column:\n\n\n\n\nInstalling and removing packages\n\n\nAt the very top of the window are three buttons. On the left-hand side is a button that can be used to refresh the cached copy of the package repository meta data. The first time that the package manager (GUI or command line) is used there will be a short delay as the initial cache is established. \n\n\nNOTE: Weka (3.7.2) will not automatically check for new information at the central repository, so it is a good idea to refresh the local cache regularly. From Weka 3.7.3 the package manager will notify you if there are new packages available at the central repository\n.\n\n\nThe two buttons at the top right are used to install and remove packages repspectively. Multiple packages may be installed/removed by using a shift-left-click combination to select a range and/or by using a command-left-click combination to add to the selection. Underneath the install and uninstall but- tons is a checkbox that can be enabled to ignore any dependencies required by selected packages and any conflicts that may occur. Installing packages while this checkbox is selected will '''not''' install required dependencies.\n\n\nSome packages may have additional information on how to complete the installation or special instructions that gets displayed when the package is installed:\n\n\n\n\nUsually it is not necessary to restart Weka after packages have been installed\u2014the changes should be available immediately. An exception is when upgrading a package that is already installed. If in doubt, restart Weka.\n\n\nUnofficial packages\n\n\nThe package list shows those packages that have their meta data stored in Weka\u2019s central meta data repository. These packages are \u201cofficial\u201d Weka packages and the Weka team as verified that they appear to provide what is advertised (and do not contain malicious code or malware).\n\n\nIt is also possible to install an \nunofficial\n package that has not gone through the process of become official. Unofficial packages might be provided, for exam- ple, by researchers who want to make experimental algorithms quickly available to the community for feedback. Unofficial packages can be installed by clicking the \u201cFile/url\u201d button on the top-right of the package manager window. This will bring up an \u201cUnnoficial package install\u201d dialog where the user can browse their file system for a package zip file or directly enter an URL to the package zip file. Note that no dependency checking is done for unofficial packages.\n\n\nUsing a HTTP proxy\n\n\nBoth the GUI and command line package managers can operate via a http proxy. To do so, start Weka from the command line and supply property values for the proxy host and port:\n\n\n java -Dhttp.proxyHost=some.proxy.somewhere.net -Dhttp.proxyPort=port weka.gui.GUIChooser\n\n\n\n\nIf your proxy requires authentication, then Weka will present a GUI dialog where you can enter your username and password. If you are running on a headless environment, then two more (non-standard) properties can be supplied:\n\n\n -Dhttp.proxyUser=some_user_name -Dhttp.proxyPassword=some_password\n\n\n\n\nUsing an alternative central package meta data repository\n\n\nBy default, both the command-line and GUI package managers use the central package meta data repository hosted on Sourceforge. In the unlikely event that this site is unavailable for one reason or another, it is possible to point the package management system at an alternative repository. This mechanism allows a temporary backup of the official repostory to be accessed, local mirrors to be established and alternative repositories to be set up for use etc.\n\n\nAn alternative repository can be specified by setting a Java property:\n\n\nweka.core.wekaPackageRepositoryURL=http://some.mirror.somewhere\n\n\n\n\nThis can either be set when starting Weka from the command line with the \n-D\n flag, or it can be placed into a file called \u201cPackageRepository.props\u201d in \n$WEKA_HOME/props\n. The default value of \nWEKA_HOME\n is \nuser.home/wekafiles\n, where \nuser.home\n is the user\u2019s home directory. More information on how and where Weka stores configuration information is given in the [[How are packages structured for the package management system?|how are package structured for the package management system?]] article.\n\n\nPackage manager property file\n\n\nAs mentioned in the previous section, an alternative package meta data repository can be specified by placing an entry in the PackageRepository.props file in \n$WEKA_HOME/props\n. From Weka 3.7.8 (and snapshot builds after 24 September 2012), the package manager also looks for properties placed in \n$WEKA_HOME/props/PackageManager.props\n. The current set of properties that can be set are:\n\n\nweka.core.wekaPackageRepositoryURL=http://some.mirror.somewhere\nweka.packageManager.offline=[true | false]\nweka.packageManager.loadPackages=[true | false]\nweka.pluginManager.disable=com.funky.FunkyExplorerPluginTab\n\n\n\n\nThe default for offline mode (if unspecified) is \nfalse\n and for loadPackages is \ntrue\n. The weka.pluginManager.disable property can be used to specify a comma-separated list of fully qualified class names to \"disable\" in the GUI. This can be used to make problematic components unavailable in the GUI without having to prevent the entire package that contains them from being loaded. E.g. \"funkyPackage\" might provide several classifiers and a special Explorer plugin tab for visualization. Suppose, for example, that the plugin Explorer tab has issues with certain data sets and causes annoying exceptions to be generated (or perhaps in the worst cases crashes the Explorer!). In this case we might want to use the classifiers provided by the package and just disable the Explorer plugin. Listing the fully qualified name of the Explorer plugin as a member of the comma-separated list associated with the weka.pluginManager.disable property will achieve this.",
            "title": " Package manager"
        },
        {
            "location": "/packages/manager/#command-line-package-management",
            "text": "Assuming that the weka.jar file is in the classpath, the package manager can be accessed by typing:   java weka.core.WekaPackageManager  Supplying no options will print the usage information:  Usage: weka.core.WekaPackageManager [option]\nOptions:\n  -list-packages <all | installed | available>\n  -package-info <repository | installed | archive> packageName\n  -install-package <packageName | packageZip | URL> [version]\n  -uninstall-package <packageName>\n  -refresh-cache   Weka 3.7.8 and snapshot builds of the developer version of Weka after September 24 2012 now offer a completely \"offline\" mode that involves no attempts to connect to the internet. This mode can be used to install package zip files that the user already has on the file system, and to browse already installed packages. This mode can be accessed from the command line package manager by specifying the \"-offline\" option. Alternatively, the property weka.packageManager.offline=true can be provided to the Java virtual machine on the command line or in a properties file (see the section on properties below).   Information (meta data) about packages is stored on a web server hosted on Sourceforge. The first time the package manager is run, for a new installation of Weka, there will be a short delay while the system downloads and stores a cache of the meta data from the server. Maintaining a cache speeds up the process of browsing the package information. From time to time you should update the local cache of package meta data in order to get the latest information on packages from the server. This can be achieved by supplying the  -refresh-cache  option.  The  -list-packages  option will, as the name suggests, print information (version numbers and short descriptions) about various packages. The option must be followed by one of three keywords:   all  will print information on all packages that the system knows about  installed  will print information on all packages that are installed locally  available  will print information on all packages that are not installed   The following shows an example of listing all packages installed locally:  java weka.core.WekaPackageManager -list-packages installed\n\nInstalled    Repository    Package\n=========    ==========    =======\n1.0.0        1.0.0         DTNB: Class for building and using a decision table/naive bayes hybrid classifier.\n1.0.0        1.0.0         massiveOnlineAnalysis: MOA (Massive On-line Analysis).\n1.0.0        1.0.0         multiInstanceFilters: A collection of filters for manipulating multi-instance data.\n1.0.0        1.0.0         naiveBayesTree: Class for generating a decision tree with naive Bayes classifiers at the leaves.\n1.0.0        1.0.0         scatterPlot3D: A visualization component for displaying a 3D scatter plot of the data using Java 3D.  The  -package-info  command lists information about a package given its name. The command is followed by one of three keywords and then the name of a package:   repository  will print info from the repository for the named package  installed  will print info on the installed version of the named package  archive  will print info for a package stored in a zip archive. In this case, the \u201carchive\u201d keyword must be followed by the path to an package zip archive file rather than just the name of a package   The following shows an example of listing information for the \u201cisotonicRegression\u201d package from the server:  java weka.core.WekaPackageManager -package-info repository isotonicRegression\nDescription:Learns an isotonic regression model. Picks the attribute that results in the lowest squared error. Missing values are not allowed. Can only deal with numeric attributes. Considers the monotonically increasing case as well as the monotonically decreasing case.\nVersion:1.0.0\nPackageURL:http://prdownloads.sourceforge.net/weka/isotonicRegression1.0.0.zip?download\nAuthor:Eibe Frank\nPackageName:isotonicRegression\nTitle:Learns an isotonic regression model.\nDate:2009-09-10\nURL:http://weka.sourceforge.net/doc.dev/weka/classifiers/IsotonicRegression.html\nCategory:Regression\nDepends:weka (>=3.7.1)\nLicense:GPL 2.0\nMaintainer:Weka team <wekalist@list.scms.waikato.ac.nz>  The  -install-package  command allows a package to be installed from one of three locations:   specifying a name of a package will install the package using the information in the package description meta data stored on the server. If no version number is given, then the latest available version of the package is installed.  providing a path to a zip file will attempt to unpack and install the archive as a Weka package  providing a URL (beginning with  http:// ) to a package zip file on the web will download and attempt to install the zip file as a Weka package   The  uninstall-package  command will uninstall the named package. Of course, the named package has to be installed for this command to have any effect!",
            "title": "Command line package management"
        },
        {
            "location": "/packages/manager/#running-installed-learning-algorithms",
            "text": "Running learning algorithms that come with the main weka distribution (i.e. are contained in the weka.jar file) was covered earlier in the [[Primer]]. But what about algorithms from packages that you\u2019ve installed using the package manager? We don\u2019t want to have to add a ton of jar files to our classpath every time we wan\u2019t to run a particular algorithm. Fortunately, we don\u2019t have to. Weka has a mechanism to load installed packages dynamically at run time. This means that newly installed packages are available in Weka's GUIs immediately.  What about running algorithms from packages on the command line I hear you ask? We can run a named algorithm by using the Run command:   java weka.Run  If no arguments are supplied, then Run outputs the following usage information:   Usage:\n     weka.Run [-no-scan] [-no-load] <scheme name [scheme options]>  The Run command supports sub-string matching, so you can run a classifier (such as J48) like so:   java weka.Run J48  When there are multiple matches on a supplied scheme name you will be presented with a list. For example:   java weka.Run NaiveBayes\n\n Select a scheme to run, or <return> to exit:\n     1) weka.classifiers.bayes.ComplementNaiveBayes\n     2) weka.classifiers.bayes.NaiveBayes\n     3) weka.classifiers.bayes.NaiveBayesMultinomial\n     4) weka.classifiers.bayes.NaiveBayesMultinomialUpdateable\n     5) weka.classifiers.bayes.NaiveBayesSimple\n     6) weka.classifiers.bayes.NaiveBayesUpdateable\n\n Enter a number >  You can turn off the scanning of packages and sub-string matching by providing the  -no-scan  option. This is useful when using the  Run  command in a script. In this case, you need to specify the fully qualified name of the algorithm to use. E.g.   java weka.Run -no-scan weka.classifiers.bayes.NaiveBayes  To reduce startup time you can also turn off the dynamic loading of installed packages by specifying the  -no-load  option. In this case, you will need to explicitly include any packaged algorithms in your classpath if you plan to use them. E.g.  java -classpath ./weka.jar:$HOME/wekafiles/packages/DTNB/DTNB.jar rweka.Run -no-load -no-scan weka.classifiers.rules.DTNB",
            "title": "Running installed learning algorithms"
        },
        {
            "location": "/packages/manager/#gui-package-manager",
            "text": "As well as a command line client, there is also a graphical interface to Weka\u2019s package management system. This is available from the  Tools  menu in the  GUIChooser . All the functionality available in the command line client to the package management system is available in the GUI version, along with the ability to install and uninstall multiple packages in one hit.   The package manager\u2019s window is split horizontally into two parts: at the top is a list of packages and at the bottom is a mini browser that can be used to display information on the currently selected package.  The package list shows the name of a package, its category, the currently installed version (if installed), the latest version available via the repository and whether the package has been loaded or not. This list may be sorted by either package name or category by clicking on the appropriate column header. A second click on the same header reverses the sort order. Three radio buttons in the upper left of the window can be used to filter what is displayed in the list. All packages (default), all available packages (i.e. those not yet installed) or only installed packages can be displayed.  If multiple versions of a package are available, they can be accessed by clicking on an entry in the \u201cRepository version\u201d column:",
            "title": "GUI package manager"
        },
        {
            "location": "/packages/manager/#installing-and-removing-packages",
            "text": "At the very top of the window are three buttons. On the left-hand side is a button that can be used to refresh the cached copy of the package repository meta data. The first time that the package manager (GUI or command line) is used there will be a short delay as the initial cache is established.   NOTE: Weka (3.7.2) will not automatically check for new information at the central repository, so it is a good idea to refresh the local cache regularly. From Weka 3.7.3 the package manager will notify you if there are new packages available at the central repository .  The two buttons at the top right are used to install and remove packages repspectively. Multiple packages may be installed/removed by using a shift-left-click combination to select a range and/or by using a command-left-click combination to add to the selection. Underneath the install and uninstall but- tons is a checkbox that can be enabled to ignore any dependencies required by selected packages and any conflicts that may occur. Installing packages while this checkbox is selected will '''not''' install required dependencies.  Some packages may have additional information on how to complete the installation or special instructions that gets displayed when the package is installed:   Usually it is not necessary to restart Weka after packages have been installed\u2014the changes should be available immediately. An exception is when upgrading a package that is already installed. If in doubt, restart Weka.",
            "title": "Installing and removing packages"
        },
        {
            "location": "/packages/manager/#unofficial-packages",
            "text": "The package list shows those packages that have their meta data stored in Weka\u2019s central meta data repository. These packages are \u201cofficial\u201d Weka packages and the Weka team as verified that they appear to provide what is advertised (and do not contain malicious code or malware).  It is also possible to install an  unofficial  package that has not gone through the process of become official. Unofficial packages might be provided, for exam- ple, by researchers who want to make experimental algorithms quickly available to the community for feedback. Unofficial packages can be installed by clicking the \u201cFile/url\u201d button on the top-right of the package manager window. This will bring up an \u201cUnnoficial package install\u201d dialog where the user can browse their file system for a package zip file or directly enter an URL to the package zip file. Note that no dependency checking is done for unofficial packages.",
            "title": "Unofficial packages"
        },
        {
            "location": "/packages/manager/#using-a-http-proxy",
            "text": "Both the GUI and command line package managers can operate via a http proxy. To do so, start Weka from the command line and supply property values for the proxy host and port:   java -Dhttp.proxyHost=some.proxy.somewhere.net -Dhttp.proxyPort=port weka.gui.GUIChooser  If your proxy requires authentication, then Weka will present a GUI dialog where you can enter your username and password. If you are running on a headless environment, then two more (non-standard) properties can be supplied:   -Dhttp.proxyUser=some_user_name -Dhttp.proxyPassword=some_password",
            "title": "Using a HTTP proxy"
        },
        {
            "location": "/packages/manager/#using-an-alternative-central-package-meta-data-repository",
            "text": "By default, both the command-line and GUI package managers use the central package meta data repository hosted on Sourceforge. In the unlikely event that this site is unavailable for one reason or another, it is possible to point the package management system at an alternative repository. This mechanism allows a temporary backup of the official repostory to be accessed, local mirrors to be established and alternative repositories to be set up for use etc.  An alternative repository can be specified by setting a Java property:  weka.core.wekaPackageRepositoryURL=http://some.mirror.somewhere  This can either be set when starting Weka from the command line with the  -D  flag, or it can be placed into a file called \u201cPackageRepository.props\u201d in  $WEKA_HOME/props . The default value of  WEKA_HOME  is  user.home/wekafiles , where  user.home  is the user\u2019s home directory. More information on how and where Weka stores configuration information is given in the [[How are packages structured for the package management system?|how are package structured for the package management system?]] article.",
            "title": "Using an alternative central package meta data repository"
        },
        {
            "location": "/packages/manager/#package-manager-property-file",
            "text": "As mentioned in the previous section, an alternative package meta data repository can be specified by placing an entry in the PackageRepository.props file in  $WEKA_HOME/props . From Weka 3.7.8 (and snapshot builds after 24 September 2012), the package manager also looks for properties placed in  $WEKA_HOME/props/PackageManager.props . The current set of properties that can be set are:  weka.core.wekaPackageRepositoryURL=http://some.mirror.somewhere\nweka.packageManager.offline=[true | false]\nweka.packageManager.loadPackages=[true | false]\nweka.pluginManager.disable=com.funky.FunkyExplorerPluginTab  The default for offline mode (if unspecified) is  false  and for loadPackages is  true . The weka.pluginManager.disable property can be used to specify a comma-separated list of fully qualified class names to \"disable\" in the GUI. This can be used to make problematic components unavailable in the GUI without having to prevent the entire package that contains them from being loaded. E.g. \"funkyPackage\" might provide several classifiers and a special Explorer plugin tab for visualization. Suppose, for example, that the plugin Explorer tab has issues with certain data sets and causes annoying exceptions to be generated (or perhaps in the worst cases crashes the Explorer!). In this case we might want to use the classifiers provided by the package and just disable the Explorer plugin. Listing the fully qualified name of the Explorer plugin as a member of the comma-separated list associated with the weka.pluginManager.disable property will achieve this.",
            "title": "Package manager property file"
        },
        {
            "location": "/packages/unofficial/",
            "text": "There are a number of packages for WEKA 3.8 on the internet that are not listed in the \"official\" WEKA package repository. These packages can nevertheless be easily installed via the package manager in WEKA 3.8 (available via the Tools menu in WEKA's GUIChooser) by providing the URL for the package .zip file.\n\n\nBelow is an (incomplete list) of packages that are available.\n\n\nPreprocessing\n\n\n\n\ndataset-weights\n -- filters for setting attribute and instance weights using various methods.\n\n\nmissing-values-imputation\n -- various methods for imputing missing values using a filter.\n\n\nmxexpression\n -- filter for updating a target attribute using a mathematical expression.\n\n\n\n\nClassification\n\n\n\n\nnetwork package\n -- Java (convolutional or fully-connected) neural network implementation with plugin for \nWeka\n. Uses dropout and rectified linear units. Implementation is multithreaded and uses \nMTJ\n matrix library with native libs for performance.\n\n\nHMMWeka\n -- This library makes Hidden Markov Model machine learning available in Weka.\n\n\nCollective classification\n -- Algorithms around semi-supervised learning and collective classification.\n\n\nBagging ensemble selection\n -- Bagging Ensemble Selection - a new ensemble learning strategy.\n\n\nDataSqueezer\n -- Efficient rule builder that generates a set of production rules from labeled input data. It can handle missing data and has log-linear asymptotic complexity with the number of training examples.\n\n\nmiDS\n -- mi-DS is a multiple-Instance learning supervised algorithm based on the DataSqueezer algorithm.\n\n\nLibD3C\n -- Ensemble classifiers with a clustering and dynamic selection strategy.\n\n\nICRM\n -- An Interpretable Classification Rule Mining Algorithm.\n\n\ntclass\n -- TClass is a supervised learner for multivariate time series, originally developed by \nWaleed Kadous\n.\n\n\nwekaclassalgos\n -- collection of artificial neural network (ANN) algorithms and artificial immune system (AIS) algorithms, originally developed by \nJason Brownlee\n.\n\n\nmxexpression\n -- classifier for making predictions using a mathematical expression.\n\n\n\n\nClustering\n\n\n\n\nAPCluster\n -- Affinity propagation algorithm for clustering, used especially in bioinformatics and computer vision.\n\n\nFast Optics\n -- Fast Implementation of OPTICS algorithm using random projections for Euclidean distances.\n\n\n\n\nSimilarity functions\n\n\n\n\nwekabiosimilarity\n -- implements several measures to compare binary feature vectors; and, additionally, extrapolates those measures to work with multi-value, string and numerical feature vectors.\n\n\n\n\nDiscretization\n\n\n\n\nur-CAIM\n -- Improved CAIM Discretization for Unbalanced and Balanced Data.\n\n\nCAIM\n -- Class-Attribute Interdependence Maximization algorithm: discretizes a continuous feature into a number of intervals. This is done by using class information, without requiring the user to provide this number.\n\n\n\n\nFeature selection\n\n\n\n\nRSARSubsetEval\n -- Rough set feature selection.\n\n\n\n\nFrequent pattern mining\n\n\n\n\nXApriori\n --Available case analysis modification of Apriori frequent pattern mining algorithm.\n\n\n\n\nStemming\n\n\n\n\nSnowball stemmers\n -- Contains the actual snowball stemmer algorithms to make the Snowball stemmer wrapper in Weka work.\n\n\nPTStemmer\n -- Wrapper for \nPedro Oliveira's stemmer library\n for Portuguese.\n\n\n\n\nText mining\n\n\n\n\nnlp\n -- Contains components for natural language processing, eg part-of-speech tagging filter and Penn Tree Bank tokenizer. Makes use of the \nStanford Parser\n (parser models need to be downloaded separately).\n\n\n\n\nVisualization\n\n\n\n\ngraphviz-treevisualize\n -- Generating nice graphs in the Explorer from trees (eg J48) using the GraphViz executables.\n\n\nconfusionmatrix\n -- Various visualizations of confusion matrices in the Explorer.\n\n\nserialized-model-viewer\n -- Adds a standalone tab to the Explorer that allows the user to load a serialized model and view its content as text (simply uses the objects' \ntoString()\n method).\n\n\n\n\nParameter optimization\n\n\n\n\nmultisearch\n -- Meta-classifier similar to GridSearch, but for optimizing arbitrary number of parameters.\n\n\n\n\nOthers\n\n\n\n\nscreencast4j\n -- Allows you to record sound, webcam and screen feeds, storing them in separate files to be combined into a screencast using a \nvideo editor\n. This screencast you can then share on YouTube, for instance.\n\n\ncommand-to-code\n -- Turns command-lines (eg of classifiers or filters) into various Java code snippets.",
            "title": " Unofficial packages"
        },
        {
            "location": "/packages/unofficial/#preprocessing",
            "text": "dataset-weights  -- filters for setting attribute and instance weights using various methods.  missing-values-imputation  -- various methods for imputing missing values using a filter.  mxexpression  -- filter for updating a target attribute using a mathematical expression.",
            "title": "Preprocessing"
        },
        {
            "location": "/packages/unofficial/#classification",
            "text": "network package  -- Java (convolutional or fully-connected) neural network implementation with plugin for  Weka . Uses dropout and rectified linear units. Implementation is multithreaded and uses  MTJ  matrix library with native libs for performance.  HMMWeka  -- This library makes Hidden Markov Model machine learning available in Weka.  Collective classification  -- Algorithms around semi-supervised learning and collective classification.  Bagging ensemble selection  -- Bagging Ensemble Selection - a new ensemble learning strategy.  DataSqueezer  -- Efficient rule builder that generates a set of production rules from labeled input data. It can handle missing data and has log-linear asymptotic complexity with the number of training examples.  miDS  -- mi-DS is a multiple-Instance learning supervised algorithm based on the DataSqueezer algorithm.  LibD3C  -- Ensemble classifiers with a clustering and dynamic selection strategy.  ICRM  -- An Interpretable Classification Rule Mining Algorithm.  tclass  -- TClass is a supervised learner for multivariate time series, originally developed by  Waleed Kadous .  wekaclassalgos  -- collection of artificial neural network (ANN) algorithms and artificial immune system (AIS) algorithms, originally developed by  Jason Brownlee .  mxexpression  -- classifier for making predictions using a mathematical expression.",
            "title": "Classification"
        },
        {
            "location": "/packages/unofficial/#clustering",
            "text": "APCluster  -- Affinity propagation algorithm for clustering, used especially in bioinformatics and computer vision.  Fast Optics  -- Fast Implementation of OPTICS algorithm using random projections for Euclidean distances.",
            "title": "Clustering"
        },
        {
            "location": "/packages/unofficial/#similarity-functions",
            "text": "wekabiosimilarity  -- implements several measures to compare binary feature vectors; and, additionally, extrapolates those measures to work with multi-value, string and numerical feature vectors.",
            "title": "Similarity functions"
        },
        {
            "location": "/packages/unofficial/#discretization",
            "text": "ur-CAIM  -- Improved CAIM Discretization for Unbalanced and Balanced Data.  CAIM  -- Class-Attribute Interdependence Maximization algorithm: discretizes a continuous feature into a number of intervals. This is done by using class information, without requiring the user to provide this number.",
            "title": "Discretization"
        },
        {
            "location": "/packages/unofficial/#feature-selection",
            "text": "RSARSubsetEval  -- Rough set feature selection.",
            "title": "Feature selection"
        },
        {
            "location": "/packages/unofficial/#frequent-pattern-mining",
            "text": "XApriori  --Available case analysis modification of Apriori frequent pattern mining algorithm.",
            "title": "Frequent pattern mining"
        },
        {
            "location": "/packages/unofficial/#stemming",
            "text": "Snowball stemmers  -- Contains the actual snowball stemmer algorithms to make the Snowball stemmer wrapper in Weka work.  PTStemmer  -- Wrapper for  Pedro Oliveira's stemmer library  for Portuguese.",
            "title": "Stemming"
        },
        {
            "location": "/packages/unofficial/#text-mining",
            "text": "nlp  -- Contains components for natural language processing, eg part-of-speech tagging filter and Penn Tree Bank tokenizer. Makes use of the  Stanford Parser  (parser models need to be downloaded separately).",
            "title": "Text mining"
        },
        {
            "location": "/packages/unofficial/#visualization",
            "text": "graphviz-treevisualize  -- Generating nice graphs in the Explorer from trees (eg J48) using the GraphViz executables.  confusionmatrix  -- Various visualizations of confusion matrices in the Explorer.  serialized-model-viewer  -- Adds a standalone tab to the Explorer that allows the user to load a serialized model and view its content as text (simply uses the objects'  toString()  method).",
            "title": "Visualization"
        },
        {
            "location": "/packages/unofficial/#parameter-optimization",
            "text": "multisearch  -- Meta-classifier similar to GridSearch, but for optimizing arbitrary number of parameters.",
            "title": "Parameter optimization"
        },
        {
            "location": "/packages/unofficial/#others",
            "text": "screencast4j  -- Allows you to record sound, webcam and screen feeds, storing them in separate files to be combined into a screencast using a  video editor . This screencast you can then share on YouTube, for instance.  command-to-code  -- Turns command-lines (eg of classifiers or filters) into various Java code snippets.",
            "title": "Others"
        },
        {
            "location": "/packages/structure/",
            "text": "Articles such as [[How do I use WEKA's classes in my own code?]] and [[How do I write a new classifier or filter?]] describe how to extend Weka to add your own learning algorithms and so forth. This article describes how such enhancements can be assembled into a \npackage\n that can be accessed via Weka\u2019s \npackage management\n system. Bundling your enhancements in a package makes it easy to share with other Weka users.\n\n\nIn this article we refer to a \npackage\n as an archive containing various resources such as compiled code, source code, javadocs, package description files (meta data), third-party libraries and configuration property files. Not all of the preceding may be in a given package, and there may be other resources included as well. This concept of a \npackage\n is quite different to that of a Java packages, which simply define how classes are arranged hierarchically.\n\n\nWhere does WEKA store packages and other configuration stuff?\n\n\nBy default, Weka stores packages and other information in \n$WEKA_HOME\n. The default location for \nWEKA_HOME\n is \nuser.home/wekafiles\n, where \nuser.home\n is the user\u2019s home directory. You can change the default location for \nWEKA_HOME\n by setting this either as an evironment variable for your platform, or by specifying it as a Java property when starting Weka. E.g.:\n\n\nexport WEKA_HOME=/home/somewhere/weka_bits_and_bobs\n\n\n\n\nwill set the directory that Weka uses to \n/home/somewhere/weka_bits_and_bobs\n under the LINUX operating system.\n\n\nThe same thing can be accomplished when starting Weka by specifying a Java property on the command line, E.g.:\n\n\njava -DWEKA_HOME=/home/somewhere/weka_bits_and_bobs -jar weka.jar\n\n\n\n\nInside \n$WEKA_HOME\n you will find the main weka log file (weka.log) and a number of directories:\n\n\n\n\npackages\n holds installed packages. Each package is contained its own subdirectory.\n\n\nprops\n holds various Java property files used by Weka. This directory replaces the user\u2019s home directory (used in earlier releases of Weka) as one of the locations checked by Weka for properties files (such as \nDatabaseUtils.props\n). Weka first checks, in order, the current directory (i.e. the directory that Weka is launched from), then \n$WEKA_HOME/props\n and finally the \nweka.jar\n file for property files.\n\n\nrepCache\n holds the cached copy of the meta data from the central package repository. If the contents of this directory get corrupted it can be safely deleted and Weka will recreate it on the next restart.\n\n\nsystemDialogs\n holds marker files that are created when you check \nDon\u2019t show this again\n in various system popup dialogs. Removing this directory or its contents will cause Weka to display those prompts anew.\n\n\n\n\nAnatomy of a package\n\n\nA Weka package is a zip archive that must unpack to the current directory. For example, the \nDTNB\n package contains the decision table naive Bayes hybrid classifier and is delivered in a file called \nDTNB.zip\n. When unpacked this zip file creates the following directory structure:\n\n\n   <current directory>\n     +-DTNB.jar\n     +-Description.props\n     +-build_package.xml\n     +-src\n     |   +-main\n     |   |   +-java\n     |   |       +-weka\n     |   |           +-classifiers\n     |   |               +-rules\n     |   |                   +-DTNB.java\n     |   +-test\n     |       +-java\n     |           +-weka\n     |               +-classifiers\n     |                   +-rules\n     |                       +-DTNBTest.java\n     +-lib\n     +-doc\n\n\n\n\nWhen installing, the package manager will use the value of the \"PackageName\" field in the Description.props file (see below) to create a directory in \n$WEKA_HOME/packages\n to hold the package contents. The contents of the \ndoc\n directory have not been shown in the diagram above, but this directory contains javadoc for the \nDTNB\n class. A package must have a \nDescription.props\n file and contain at least one jar file with compiled Java classes. The package manager will attempt to load all jar files that it finds in the root directory and the \nlib\n directory. Other files are optional, but if the package is open-source then it is nice to include the source code and an ant build file that can be used to compile the code. Template versions of the \nDescription.props\n file and\n\nbuild_package.xml\n file are available from the Weka site and here.\n\n\n==The description file==\nA valid package must contain a \nDescription.props\n file that provides meta data on the package. Identical files are stored at the central package repository and the local cache maintained by the package manager. The package manager uses these files to compare what is installed to what is available and resolve dependencies.\n\n\nThe \nDescription.props\n contains basic information on the package in the following format:\n\n\n# Template Description file for a Weka package\n\n# Package name (required)\nPackageName=funkyPackage\n\n# Version (required)\nVersion=1.0.0\n\n#Date (year-month-day)\nDate=2010-01-01\n\n# Title (required)\nTitle=My cool algorithm\n\n# Category (recommended)\nCategory=Classification\n\n# Author (required)\nAuthor=Joe Dev <joe@somewhere.net>,Dev2 <dev2@somewhereelse.net>\n\n# Maintainer (required)\nMaintainer=Joe Dev <joe@somewhere.net>\n\n# License (required)\nLicense=GPL 2.0|Mozilla\n\n# Description (required)\nDescription=This package contains the famous Funky Classifer that performs \\\n truely funky prediction.\n\n# Changes and/or bug fixes in this package (optional)\nChanges=Fixed a serious bug that affected overall coolness of the Funky Classifier\n\n# Package URL for obtaining the package archive (required)\nPackageURL=http://somewhere.net/weka/funkyPackage.zip\n\n# URL for further information\nURL=http://somewhere.net/funkyResearchInfo.html\n\n# Enhances various other packages?\nEnhances=packageName1,packageName2,...\n\n# Related to other packages?\nRelated=packageName1,packageName2,...\n\n# Dependencies (required; format: packageName (equality/inequality version_number)\nDepends=weka (>=3.7.1), packageName1 (=x.y.z), packageName2 (>u.v.w|<=x.y.z),...\n\n\n\n\nThe \nPackageName\n and \nVersion\n give the name of the package and version number respectively. The name can consist of letters, numbers, and the dot character. It should not start with a dot and should not contain any spaces. The version number is a sequence of three non-negative integers separated by single \n.\n or \n-\n characters.\n\n\nThe \nTitle\n field should give a one sentence description of the package. The \nDescription\n field can give a longer description of the package spaning multiple sentences. It may include technical references and can use HTML markup.\n\n\nThe \nCategory\n field is strongly recommended as this information is displayed on both the repository web site and in the GUI package manager client. In the latter, the user can sort the packages on the basis of the category field. It is recommended that an existing category be assigned if possible. Some examples include (Classification, Text classification, Ensemble learning, Regression, Clustering, Associations, Preprocessing, Visualization, Explorer, Experimenter, KnowledgeFlow).\n\n\nThe \nAuthor\n field describes who wrote the package and may include multiple names (separated by commas). Email addresses may be given in angle brackets after each name. The field is intended for human readers and no email addresses are automatically extracted.\n\n\nThe \nMaintainer\n field lists who maintains the package and should include a single email address, enclosed in angle brackets, for sending bug reports to.\n\n\nThe \nLicense\n field lists the license(s) that apply to the package. This field may contain the short specification of a license (such as LGPL, GPL 2.0 etc.) or the string \nfile LICENSE\n, where \nLICENSE\n exists as a file in the top-level directory of the package. The string \nUnlimited\n may be supplied to indicate that there are no restrictions on distribution or use aside from those imposed by relevant laws.\n\n\nThe \nPackageURL\n field lists valid URL that points to the package zip file. This URL is used by the package manager to download and install the package.\n\n\nThe optional \nDepends\n field gives a comma separated list of packages which this package depends on. The name of a package is optionally followed by a version number constraint enclosed in parenthesis. Valid operators for version number constraints include =, <, >, <=, >=. The keyword \nweka\n is reserved to refer to the base Weka system and can be used to indicate a dependency on\n  a particular version of Weka. For example:\n\n\nDepends=weka (>=3.7.2), DTNB (=1.0.0)\n\n\n\n\nstates that this package requires Weka 3.7.2 or higher and version 1.0.0 of the package DTNB.\n\n\nDepends=weka (>3.7.1|<3.8.0)\n\n\n\n\nstates that this package requires a version of Weka between 3.7.1 and 3.8.0.\n\n\nDepends=DTNB (<1.5.0|>=2.0.1)\n\n\n\n\nstates that this package requires that a version of the DTNB package be installed that is either less than version 1.5.0 or greater than or equal to version 2.0.1.\n\n\nIf there is no version number constraint following a package name, the package manager assumes that the latest version of the dependent package is suitable.\n\n\nThe optional \nURL\n field gives a URL at which the user can find additional online information about the package or its constituent algorithms.\n\n\nThe optional \nEnhances\n field can be used to indicate which other packages this package is based on (i.e. if it extends methods/algorithms from another\n  package in some fashion).\n\n\nThe optional \nRelated\n field is similar to the \nEnhances\n field. It can be used to point the user to other packages that are related in some fashion to this one.\n\n\nThe optional \nChanges\n field should be used to indicate what changes/bug fixes are included in the current release of the package.\n\n\nThere are several other fields that can be used to provide information to assist the user with completing installation (if it can\u2019t be completely accomplished with the package zip file) or display error messages if necessary components are missing:\n\n\nMessageToDisplayOnInstallation=Funky package requires some extra\\n\\\n stuff to be installed after installing this package. You will\\n\\\n need to blah, blah, blah in order to blah, blah, blah...\n\nDoNotLoadIfFileNotPresent=lib/someLibrary.jar,otherStuff/important,...\n\nDoNotLoadIfFileNotPresentMessage=funkyPackage can't be loaded because some \\\n funky libraries are missing. Please download funkyLibrary.jar from \\\n http://www.funky.com and install in $WEKA_HOME/packages/funkyPackage/lib\n\nDoNotLoadIfClassNotPresent=com.some.class.from.some.Where,org.some.other.Class,...\n\nDoNotLoadIfClassNotPresentMessage=funkyPackage can't be loaded because \\\n com.funky.FunkyClass can't be instantiated. Have you downloaded and run \\\n the funky software installer for your platform?\n\n\n\n\nThe optional \nMessageToDisplayOnInstallation\n field allows you to specify special instructions to the user in order to help them complete the intallation manually. This message gets displayed on the console, written to the log and appears in a pop-up information dialog if using the GUI package manager. It should include \n\\n\n in order to avoid long lines when displayed in a GUI pop-up dialog.\n\n\nThe optional \nDoNotLoadIfFileNotPresent\n field can be used to prevent Weka from loading the package if the named ''files'' and/or ''directories'' are not present in the package\u2019s installation directory. An example is the massiveOnlineAnalysis package. This package is a connector only package and does not include the MOA library. Users of this package must download the moa.jar file separately and copy it to the package\u2019s \nlib\n directory manually. Multiple files and directories can be specified as a comma separated list. All paths are relative to the top-level directory of the package. \nIMPORTANT\n: use forward slashes as separator characters, as these are portable accross all platforms. The \nDoNotLoadIfFileNotPresentMessage\n field can be used to supply an optional message to display to the user if Weka detects that a file or directory is missing from the package. This message will be displayed on the console and in the log.\n\n\nThe optional \nDoNotLoadIfClassNotPresent\n field can be used to prevent Weka from loading the package if the named ''class(es)'' can\u2019t be instantiated. This is useful for packages that rely on stuff that has to be installed manually by the user. For example, Java3D is a separate download on all platforms except for OSX, and installs itself into the system JRE/JDK. The \nDoNotLoadIfClassNotPresentMessage\n field can be used to supply an optional message to display to the user if Weka detects that a class can\u2019t be instantiated. Again, this will be displayed on the console and in the log.\n\n\nNew in Weka 3.9.2 and 3.8.2 is the ability to constrain the OS and architecture that a package can be installed and loaded on. Two new fields are used for this:\n\n\n# Specify which OS's the package can operate with. Omitting this entry indicates no restrictions on OS. (optional)\nOSName=Windows,Mac,Linux\n\n# Specify which architecture the package can operate with. Omitting this entry indicates no restriction. (optional)\nOSArch=64\n\n\n\n\nEntries in the OSName field are compared against the value of the Java property \"os.name\" using a String.toLowerCase().contains() operation. Any single match indicates a pass. If an OSName field exists in the Description.props, and it matches, then the optional OSArch field is examined. If present, values in the OSArch list are compared against the value of the Java property \"os.arch\". The special OSArch entries \"32\" and \"64\" are tested using a String().contains() operation; all other entries are compared using String.equalsIgnoreCase().\n\n\nAdditional configuration files\n\n\nCertain types of packages may require additional configuration files to be present as part of the package. The last chapter covered various ways in which Weka can be extended without having to alter the core Weka code. These plugin mechanisms have been subsumed by the package management system, so some of the configuration property files they require must be present in the package\u2019s top-level directory if the package in question contains such a plugin. Examples include additional tabs for the Explorer, mappings to custom property editors for Weka\u2019s GenericObjectEditor and Knowledge Flow plugins. Here are some examples:\n\n\nThe scatterPlot3D package adds a new tab to the Explorer. In order to accomplish this a property has to be set in the Explorer.props file (which contains default values for the Explorer) in order to tell Weka to instantiate and display the new panel. The scatterPlot3D file includes an \nExplorer.props\n file in its top-level directory that has the following contents:\n\n\n# Explorer.props file. Adds the Explorer3DPanel to the Tabs key.\nTabs=weka.gui.explorer.Explorer3DPanel\nTabsPolicy=append\n\n\n\n\nThis property file is read by the package management system when the package is loaded and any key-value pairs are added to existing Explorer properties that have been loaded by the system at startup. If the key already exists in the Explorer properties, then the package has the option to either replace (i.e. overwrite) or append to the existing value. This can be specified with the \"TabsPolicy\" key. In this case, the value \nweka.gui.explorer.Explorer3DPanel\n is appended to any existing value associated with the \"Tabs\" key. \nExplorer3DPanel\n gets instantiated and added as a new tab when the Explorer starts.\n\n\nAnother example is the kfGroovy package. This package adds a plugin component to Weka\u2019s Knowledge Flow that allows a Knowledge Flow step to be implemented and compiled dynamically at runtime as a Groovy script. In order for the Knowledge Flow to make the new step appear in its \nPlugins\n toolbar, there needs to be a \nBeans.props\n file in the package\u2019s top level directory. In the case of kfGroovy, this property file has the following contents:\n\n\n# Specifies that this component goes into the Plugins toolbar\nweka.gui.beans.KnowledgeFlow.Plugins=org.pentaho.dm.kf.GroovyComponent\n\n\n\n\nThe new pluggable evaluation metrics for classification/regression (from Weka 3.7.8 and nightly developer snapshots from 15-11-2012) are managed by the PluginManager class. To tell PluginManager that your package provides a new evaluation metric you need to provide a \"PluginManager.props\" file in the package's top level directory. For example, a hypothetical bobsMetric package might declare a new \"Area under Bob curve\" metric like so:\n\n\n# Specify a new plugin Evaluation metric\nweka.classifiers.evaluation.AbstractEvaluationMetric=weka.classifiers.evaluation.BobsAUC\n\n\n\n\nContributing a package\n\n\nIf you have created a package for Weka then there are two options for making it available to the community. In both cases, hosting the package\u2019s zip archive is the responsibility of the contributer.\n\n\nThe first, and official, route is to contact the current Weka maintainer (normally also the admin of the WEKA homepage) and supply your package\u2019s \nDescription.props\n file. The Weka team will then test downloading and using your package to make sure that there are no obvious problems with what has been specified in the \nDescription.props\n file and that the software runs and does not contain any malware/malicious code. If all is well, then the package will become an \nofficial\n Weka package and the central package repository meta data will be updated with the package\u2019s \nDescription.props file\n. \n\n\nResponsibility for maintaining and supporting the package resides with the contributer\n.\n\n\nThe second, and unofficial, route is to simply make the package\u2019s zip archive available on the web somewhere and advertise it yourself. Although users will not be able to browse it\u2019s description in the official package repository, they will be able to download and install it directly from your URL by using the command line version of the package manager. This route could be attractive for people who have published a new algorithm and want to quiclky make a beta version available for others to try without having to go through the official route.\n\n\nCreating a mirror of the package meta data repository\n\n\nIn this section we discuss an easy approach to setting up and maintaining a mirror of the package meta data repository. Having a local mirror may provide faster access times than to that of the official repository on Sourceforge. Extending this approach to the creation of an alternative central repository (hosting packages not available at the official repository) should be straight forward.\n\n\nJust about everything necessary for creating a mirror exists in the local meta data cache created by Weka\u2019s package management system. This cache resides at \n$WEKA_HOME/repCache\n. The only thing missing (in Weka 3.7.2) for a complete mirror is the file \nimages.txt\n, that lists all the image files used in the html index files. This file contains the following two lines:\n\n\nTitle-Bird-Header.gif\npentaho_logo_rgb_sm.png\n\n\n\n\nimages.txt\n is downloaded automatically by the package management system in Weka 3.7.3 and higher.\n\n\nTo create a mirror:\n1. Copy the contents of \n$WEKA_HOME/repCache\n to a temporary directory. For the purposes of this example we\u2019ll call it \ntempRep\n\n2. Change directory into \ntempRep\n and run \njava weka.core.RepositoryIndexGenerator .\n. Don't forget the \".\" after the command (this tells \nRepoistoryIndexGenerator\n to operate on the current directory)\n3. Change directory to the parent of \ntempRep\n and synchronize its contents to wherever your web server is located (this is easy via rsync under Nix-like operating systems).\n\n\nRepositoryIndexGenerator\n automatically creates the main index.html file, all the package index.html files and html files correpsonding to all version prop files for each package. It will also create \npackageList.txt\n and \nnumPackages.txt\n files.\n\n\nIMPORTANT\n: Make sure that all the files in \ntempRep\n are world readable. It is easy to make packages available that are not part of the official Weka repository. Assuming you want to add a package called \nfunkyPackage\n (as specified by the \nPackageName\n field in the \nDescription.props\n file):\n\n\n\n\nCreate a directory called \nfunkyPackage\n in \ntempRep\n\n\nCopy the \nDescription.props\n file to \ntempRep/funkyPackage/Latest.props\n\n\nCopy the \nDescription.props file\n to \ntempRep/funkyPackage/<version number>.props\n, where \nversion number\n is the version number specified in the \nVersion\n field of \nDescription.props\n\n\nRun \nRepositoryIndexGenerator\n as described previously and sync \ntempRep\n to your web server\n\n\n\n\nAdding a new version of an existing package is very similar to what has already been described. All that is required is that the new \nDescription.props\n file corresponding to the new version is copied to \nLatest.props\n and to \n<version numer>.props\n in the package\u2019s folder. Running \nRepositoryIndexGenerator\n will ensure that all necessary html files are created and supporting text files are updated.\n\n\nAutomating the mirroring process would simply involve using your OS\u2019s scheduler to execute a script that:\n\n\n\n\nRuns \nweka.core.WekaPackageManager -refresh-cache\n\n\nrsyncs \n$WEKA_HOME/repCache\n to \ntempRep\n\n\nRuns \nweka.core.RepoistoryIndexGenerator\n\n\nrsyncs \ntempRep\n to your web server",
            "title": " How are packages structured for the package management system?"
        },
        {
            "location": "/packages/structure/#where-does-weka-store-packages-and-other-configuration-stuff",
            "text": "By default, Weka stores packages and other information in  $WEKA_HOME . The default location for  WEKA_HOME  is  user.home/wekafiles , where  user.home  is the user\u2019s home directory. You can change the default location for  WEKA_HOME  by setting this either as an evironment variable for your platform, or by specifying it as a Java property when starting Weka. E.g.:  export WEKA_HOME=/home/somewhere/weka_bits_and_bobs  will set the directory that Weka uses to  /home/somewhere/weka_bits_and_bobs  under the LINUX operating system.  The same thing can be accomplished when starting Weka by specifying a Java property on the command line, E.g.:  java -DWEKA_HOME=/home/somewhere/weka_bits_and_bobs -jar weka.jar  Inside  $WEKA_HOME  you will find the main weka log file (weka.log) and a number of directories:   packages  holds installed packages. Each package is contained its own subdirectory.  props  holds various Java property files used by Weka. This directory replaces the user\u2019s home directory (used in earlier releases of Weka) as one of the locations checked by Weka for properties files (such as  DatabaseUtils.props ). Weka first checks, in order, the current directory (i.e. the directory that Weka is launched from), then  $WEKA_HOME/props  and finally the  weka.jar  file for property files.  repCache  holds the cached copy of the meta data from the central package repository. If the contents of this directory get corrupted it can be safely deleted and Weka will recreate it on the next restart.  systemDialogs  holds marker files that are created when you check  Don\u2019t show this again  in various system popup dialogs. Removing this directory or its contents will cause Weka to display those prompts anew.",
            "title": "Where does WEKA store packages and other configuration stuff?"
        },
        {
            "location": "/packages/structure/#anatomy-of-a-package",
            "text": "A Weka package is a zip archive that must unpack to the current directory. For example, the  DTNB  package contains the decision table naive Bayes hybrid classifier and is delivered in a file called  DTNB.zip . When unpacked this zip file creates the following directory structure:     <current directory>\n     +-DTNB.jar\n     +-Description.props\n     +-build_package.xml\n     +-src\n     |   +-main\n     |   |   +-java\n     |   |       +-weka\n     |   |           +-classifiers\n     |   |               +-rules\n     |   |                   +-DTNB.java\n     |   +-test\n     |       +-java\n     |           +-weka\n     |               +-classifiers\n     |                   +-rules\n     |                       +-DTNBTest.java\n     +-lib\n     +-doc  When installing, the package manager will use the value of the \"PackageName\" field in the Description.props file (see below) to create a directory in  $WEKA_HOME/packages  to hold the package contents. The contents of the  doc  directory have not been shown in the diagram above, but this directory contains javadoc for the  DTNB  class. A package must have a  Description.props  file and contain at least one jar file with compiled Java classes. The package manager will attempt to load all jar files that it finds in the root directory and the  lib  directory. Other files are optional, but if the package is open-source then it is nice to include the source code and an ant build file that can be used to compile the code. Template versions of the  Description.props  file and build_package.xml  file are available from the Weka site and here.  ==The description file==\nA valid package must contain a  Description.props  file that provides meta data on the package. Identical files are stored at the central package repository and the local cache maintained by the package manager. The package manager uses these files to compare what is installed to what is available and resolve dependencies.  The  Description.props  contains basic information on the package in the following format:  # Template Description file for a Weka package\n\n# Package name (required)\nPackageName=funkyPackage\n\n# Version (required)\nVersion=1.0.0\n\n#Date (year-month-day)\nDate=2010-01-01\n\n# Title (required)\nTitle=My cool algorithm\n\n# Category (recommended)\nCategory=Classification\n\n# Author (required)\nAuthor=Joe Dev <joe@somewhere.net>,Dev2 <dev2@somewhereelse.net>\n\n# Maintainer (required)\nMaintainer=Joe Dev <joe@somewhere.net>\n\n# License (required)\nLicense=GPL 2.0|Mozilla\n\n# Description (required)\nDescription=This package contains the famous Funky Classifer that performs \\\n truely funky prediction.\n\n# Changes and/or bug fixes in this package (optional)\nChanges=Fixed a serious bug that affected overall coolness of the Funky Classifier\n\n# Package URL for obtaining the package archive (required)\nPackageURL=http://somewhere.net/weka/funkyPackage.zip\n\n# URL for further information\nURL=http://somewhere.net/funkyResearchInfo.html\n\n# Enhances various other packages?\nEnhances=packageName1,packageName2,...\n\n# Related to other packages?\nRelated=packageName1,packageName2,...\n\n# Dependencies (required; format: packageName (equality/inequality version_number)\nDepends=weka (>=3.7.1), packageName1 (=x.y.z), packageName2 (>u.v.w|<=x.y.z),...  The  PackageName  and  Version  give the name of the package and version number respectively. The name can consist of letters, numbers, and the dot character. It should not start with a dot and should not contain any spaces. The version number is a sequence of three non-negative integers separated by single  .  or  -  characters.  The  Title  field should give a one sentence description of the package. The  Description  field can give a longer description of the package spaning multiple sentences. It may include technical references and can use HTML markup.  The  Category  field is strongly recommended as this information is displayed on both the repository web site and in the GUI package manager client. In the latter, the user can sort the packages on the basis of the category field. It is recommended that an existing category be assigned if possible. Some examples include (Classification, Text classification, Ensemble learning, Regression, Clustering, Associations, Preprocessing, Visualization, Explorer, Experimenter, KnowledgeFlow).  The  Author  field describes who wrote the package and may include multiple names (separated by commas). Email addresses may be given in angle brackets after each name. The field is intended for human readers and no email addresses are automatically extracted.  The  Maintainer  field lists who maintains the package and should include a single email address, enclosed in angle brackets, for sending bug reports to.  The  License  field lists the license(s) that apply to the package. This field may contain the short specification of a license (such as LGPL, GPL 2.0 etc.) or the string  file LICENSE , where  LICENSE  exists as a file in the top-level directory of the package. The string  Unlimited  may be supplied to indicate that there are no restrictions on distribution or use aside from those imposed by relevant laws.  The  PackageURL  field lists valid URL that points to the package zip file. This URL is used by the package manager to download and install the package.  The optional  Depends  field gives a comma separated list of packages which this package depends on. The name of a package is optionally followed by a version number constraint enclosed in parenthesis. Valid operators for version number constraints include =, <, >, <=, >=. The keyword  weka  is reserved to refer to the base Weka system and can be used to indicate a dependency on   a particular version of Weka. For example:  Depends=weka (>=3.7.2), DTNB (=1.0.0)  states that this package requires Weka 3.7.2 or higher and version 1.0.0 of the package DTNB.  Depends=weka (>3.7.1|<3.8.0)  states that this package requires a version of Weka between 3.7.1 and 3.8.0.  Depends=DTNB (<1.5.0|>=2.0.1)  states that this package requires that a version of the DTNB package be installed that is either less than version 1.5.0 or greater than or equal to version 2.0.1.  If there is no version number constraint following a package name, the package manager assumes that the latest version of the dependent package is suitable.  The optional  URL  field gives a URL at which the user can find additional online information about the package or its constituent algorithms.  The optional  Enhances  field can be used to indicate which other packages this package is based on (i.e. if it extends methods/algorithms from another   package in some fashion).  The optional  Related  field is similar to the  Enhances  field. It can be used to point the user to other packages that are related in some fashion to this one.  The optional  Changes  field should be used to indicate what changes/bug fixes are included in the current release of the package.  There are several other fields that can be used to provide information to assist the user with completing installation (if it can\u2019t be completely accomplished with the package zip file) or display error messages if necessary components are missing:  MessageToDisplayOnInstallation=Funky package requires some extra\\n\\\n stuff to be installed after installing this package. You will\\n\\\n need to blah, blah, blah in order to blah, blah, blah...\n\nDoNotLoadIfFileNotPresent=lib/someLibrary.jar,otherStuff/important,...\n\nDoNotLoadIfFileNotPresentMessage=funkyPackage can't be loaded because some \\\n funky libraries are missing. Please download funkyLibrary.jar from \\\n http://www.funky.com and install in $WEKA_HOME/packages/funkyPackage/lib\n\nDoNotLoadIfClassNotPresent=com.some.class.from.some.Where,org.some.other.Class,...\n\nDoNotLoadIfClassNotPresentMessage=funkyPackage can't be loaded because \\\n com.funky.FunkyClass can't be instantiated. Have you downloaded and run \\\n the funky software installer for your platform?  The optional  MessageToDisplayOnInstallation  field allows you to specify special instructions to the user in order to help them complete the intallation manually. This message gets displayed on the console, written to the log and appears in a pop-up information dialog if using the GUI package manager. It should include  \\n  in order to avoid long lines when displayed in a GUI pop-up dialog.  The optional  DoNotLoadIfFileNotPresent  field can be used to prevent Weka from loading the package if the named ''files'' and/or ''directories'' are not present in the package\u2019s installation directory. An example is the massiveOnlineAnalysis package. This package is a connector only package and does not include the MOA library. Users of this package must download the moa.jar file separately and copy it to the package\u2019s  lib  directory manually. Multiple files and directories can be specified as a comma separated list. All paths are relative to the top-level directory of the package.  IMPORTANT : use forward slashes as separator characters, as these are portable accross all platforms. The  DoNotLoadIfFileNotPresentMessage  field can be used to supply an optional message to display to the user if Weka detects that a file or directory is missing from the package. This message will be displayed on the console and in the log.  The optional  DoNotLoadIfClassNotPresent  field can be used to prevent Weka from loading the package if the named ''class(es)'' can\u2019t be instantiated. This is useful for packages that rely on stuff that has to be installed manually by the user. For example, Java3D is a separate download on all platforms except for OSX, and installs itself into the system JRE/JDK. The  DoNotLoadIfClassNotPresentMessage  field can be used to supply an optional message to display to the user if Weka detects that a class can\u2019t be instantiated. Again, this will be displayed on the console and in the log.  New in Weka 3.9.2 and 3.8.2 is the ability to constrain the OS and architecture that a package can be installed and loaded on. Two new fields are used for this:  # Specify which OS's the package can operate with. Omitting this entry indicates no restrictions on OS. (optional)\nOSName=Windows,Mac,Linux\n\n# Specify which architecture the package can operate with. Omitting this entry indicates no restriction. (optional)\nOSArch=64  Entries in the OSName field are compared against the value of the Java property \"os.name\" using a String.toLowerCase().contains() operation. Any single match indicates a pass. If an OSName field exists in the Description.props, and it matches, then the optional OSArch field is examined. If present, values in the OSArch list are compared against the value of the Java property \"os.arch\". The special OSArch entries \"32\" and \"64\" are tested using a String().contains() operation; all other entries are compared using String.equalsIgnoreCase().",
            "title": "Anatomy of a package"
        },
        {
            "location": "/packages/structure/#additional-configuration-files",
            "text": "Certain types of packages may require additional configuration files to be present as part of the package. The last chapter covered various ways in which Weka can be extended without having to alter the core Weka code. These plugin mechanisms have been subsumed by the package management system, so some of the configuration property files they require must be present in the package\u2019s top-level directory if the package in question contains such a plugin. Examples include additional tabs for the Explorer, mappings to custom property editors for Weka\u2019s GenericObjectEditor and Knowledge Flow plugins. Here are some examples:  The scatterPlot3D package adds a new tab to the Explorer. In order to accomplish this a property has to be set in the Explorer.props file (which contains default values for the Explorer) in order to tell Weka to instantiate and display the new panel. The scatterPlot3D file includes an  Explorer.props  file in its top-level directory that has the following contents:  # Explorer.props file. Adds the Explorer3DPanel to the Tabs key.\nTabs=weka.gui.explorer.Explorer3DPanel\nTabsPolicy=append  This property file is read by the package management system when the package is loaded and any key-value pairs are added to existing Explorer properties that have been loaded by the system at startup. If the key already exists in the Explorer properties, then the package has the option to either replace (i.e. overwrite) or append to the existing value. This can be specified with the \"TabsPolicy\" key. In this case, the value  weka.gui.explorer.Explorer3DPanel  is appended to any existing value associated with the \"Tabs\" key.  Explorer3DPanel  gets instantiated and added as a new tab when the Explorer starts.  Another example is the kfGroovy package. This package adds a plugin component to Weka\u2019s Knowledge Flow that allows a Knowledge Flow step to be implemented and compiled dynamically at runtime as a Groovy script. In order for the Knowledge Flow to make the new step appear in its  Plugins  toolbar, there needs to be a  Beans.props  file in the package\u2019s top level directory. In the case of kfGroovy, this property file has the following contents:  # Specifies that this component goes into the Plugins toolbar\nweka.gui.beans.KnowledgeFlow.Plugins=org.pentaho.dm.kf.GroovyComponent  The new pluggable evaluation metrics for classification/regression (from Weka 3.7.8 and nightly developer snapshots from 15-11-2012) are managed by the PluginManager class. To tell PluginManager that your package provides a new evaluation metric you need to provide a \"PluginManager.props\" file in the package's top level directory. For example, a hypothetical bobsMetric package might declare a new \"Area under Bob curve\" metric like so:  # Specify a new plugin Evaluation metric\nweka.classifiers.evaluation.AbstractEvaluationMetric=weka.classifiers.evaluation.BobsAUC",
            "title": "Additional configuration files"
        },
        {
            "location": "/packages/structure/#contributing-a-package",
            "text": "If you have created a package for Weka then there are two options for making it available to the community. In both cases, hosting the package\u2019s zip archive is the responsibility of the contributer.  The first, and official, route is to contact the current Weka maintainer (normally also the admin of the WEKA homepage) and supply your package\u2019s  Description.props  file. The Weka team will then test downloading and using your package to make sure that there are no obvious problems with what has been specified in the  Description.props  file and that the software runs and does not contain any malware/malicious code. If all is well, then the package will become an  official  Weka package and the central package repository meta data will be updated with the package\u2019s  Description.props file .   Responsibility for maintaining and supporting the package resides with the contributer .  The second, and unofficial, route is to simply make the package\u2019s zip archive available on the web somewhere and advertise it yourself. Although users will not be able to browse it\u2019s description in the official package repository, they will be able to download and install it directly from your URL by using the command line version of the package manager. This route could be attractive for people who have published a new algorithm and want to quiclky make a beta version available for others to try without having to go through the official route.",
            "title": "Contributing a package"
        },
        {
            "location": "/packages/structure/#creating-a-mirror-of-the-package-meta-data-repository",
            "text": "In this section we discuss an easy approach to setting up and maintaining a mirror of the package meta data repository. Having a local mirror may provide faster access times than to that of the official repository on Sourceforge. Extending this approach to the creation of an alternative central repository (hosting packages not available at the official repository) should be straight forward.  Just about everything necessary for creating a mirror exists in the local meta data cache created by Weka\u2019s package management system. This cache resides at  $WEKA_HOME/repCache . The only thing missing (in Weka 3.7.2) for a complete mirror is the file  images.txt , that lists all the image files used in the html index files. This file contains the following two lines:  Title-Bird-Header.gif\npentaho_logo_rgb_sm.png  images.txt  is downloaded automatically by the package management system in Weka 3.7.3 and higher.  To create a mirror:\n1. Copy the contents of  $WEKA_HOME/repCache  to a temporary directory. For the purposes of this example we\u2019ll call it  tempRep \n2. Change directory into  tempRep  and run  java weka.core.RepositoryIndexGenerator . . Don't forget the \".\" after the command (this tells  RepoistoryIndexGenerator  to operate on the current directory)\n3. Change directory to the parent of  tempRep  and synchronize its contents to wherever your web server is located (this is easy via rsync under Nix-like operating systems).  RepositoryIndexGenerator  automatically creates the main index.html file, all the package index.html files and html files correpsonding to all version prop files for each package. It will also create  packageList.txt  and  numPackages.txt  files.  IMPORTANT : Make sure that all the files in  tempRep  are world readable. It is easy to make packages available that are not part of the official Weka repository. Assuming you want to add a package called  funkyPackage  (as specified by the  PackageName  field in the  Description.props  file):   Create a directory called  funkyPackage  in  tempRep  Copy the  Description.props  file to  tempRep/funkyPackage/Latest.props  Copy the  Description.props file  to  tempRep/funkyPackage/<version number>.props , where  version number  is the version number specified in the  Version  field of  Description.props  Run  RepositoryIndexGenerator  as described previously and sync  tempRep  to your web server   Adding a new version of an existing package is very similar to what has already been described. All that is required is that the new  Description.props  file corresponding to the new version is copied to  Latest.props  and to  <version numer>.props  in the package\u2019s folder. Running  RepositoryIndexGenerator  will ensure that all necessary html files are created and supporting text files are updated.  Automating the mirroring process would simply involve using your OS\u2019s scheduler to execute a script that:   Runs  weka.core.WekaPackageManager -refresh-cache  rsyncs  $WEKA_HOME/repCache  to  tempRep  Runs  weka.core.RepoistoryIndexGenerator  rsyncs  tempRep  to your web server",
            "title": "Creating a mirror of the package meta data repository"
        },
        {
            "location": "/not_so_faq/gsp/",
            "text": "Class\n\n\nweka.associators.GeneralizedSequentialPatterns\n\n\nPublication\n\n\nRamakrishnan Srikant, Rakesh Agrawal (1996). Mining Sequential Patterns: Generalizations and Performance Improvements.\n\n\nDownloads\n\n\n\n\nGeneralizedSequentialPattern_example.arff",
            "title": " Generalized Sequential Patterns"
        },
        {
            "location": "/not_so_faq/gsp/#class",
            "text": "weka.associators.GeneralizedSequentialPatterns",
            "title": "Class"
        },
        {
            "location": "/not_so_faq/gsp/#publication",
            "text": "Ramakrishnan Srikant, Rakesh Agrawal (1996). Mining Sequential Patterns: Generalizations and Performance Improvements.",
            "title": "Publication"
        },
        {
            "location": "/not_so_faq/gsp/#downloads",
            "text": "GeneralizedSequentialPattern_example.arff",
            "title": "Downloads"
        },
        {
            "location": "/not_so_faq/j48_numbers/",
            "text": "J48 pruned tree\nnode-caps = yes\n| deg-malig = 1: recurrence-events (1.01/0.4)\n| deg-malig = 2: no-recurrence-events (26.2/8.0)\n| deg-malig = 3: recurrence-events (30.4/7.4)\nnode-caps = no: no-recurrence-events (228.39/53.4)\n\n\n\n\nThe \nfirst\n number is the total number of instances (weight of instances) reaching the leaf. The \nsecond\n number is the number (weight) of those instances that are misclassified.\n\n\nIf your data has \nmissing\n attribute values then you will end up with \nfractional\n instances at the leafs. When splitting on an attribute where some of the training instances have missing values, J48 will divide a training instance with a missing value for the split attribute up into fractional parts proportional to the frequencies of the observed non-missing values. This is discussed in the Witten & Frank Data Mining book as well as Ross Quinlan's original publications on C4.5.",
            "title": " What do those numbers mean in a J48 tree?"
        },
        {
            "location": "/faqs/different_versions/",
            "text": "Refer to \nHistory\n for a tabular overview of all Weka releases.\n\n\nSeveral branches are associated with the 1st, 2nd, 3rd, and \n4th\n edition of the book \nData Mining: Practical Machine Learning Tools and Techniques\n by \nIan H. Witten\n and \nEibe Frank\n, joined by \nMark Hall\n for the 3rd edition and \nChris Pal\n for the 4th edition.\n\n\nOnce created, non-development branches receive bug fixes, but no new features (classifiers, filters, etc.).\n\n\n\n\n\n\n\n\nVersion name\n\n\nMost recent base number\n\n\nAssociated with book edition\n\n\n\n\n\n\n\n\n\n\nBook 1st ed. version\n\n\n3.0.x\n\n\n1st edition\n\n\n\n\n\n\nOld GUI version\n\n\n3.2.x\n\n\nnone\n\n\n\n\n\n\nBook 2nd ed. version\n\n\n3.4.x\n\n\n2nd edition\n\n\n\n\n\n\nBook 3rd ed. version\n\n\n3.6.x\n\n\n3rd edition\n\n\n\n\n\n\nBook 4th ed. version\n\n\n3.8.x\n\n\n4th edition\n\n\n\n\n\n\nDevelopment version\n\n\n3.9.x\n\n\nnone\n\n\n\n\n\n\n\n\nFor \ncontributions\n, you should always develop against the developer version.",
            "title": " What are the principal release branches of Weka?"
        },
        {
            "location": "/faqs/old_versions/",
            "text": "If you need a specific version of WEKA, e.g., due to some third-party tools, go \nWEKA's project page\n on \nSourceforge.net\n. In the \nFiles section\n you have access to all the releases ever made.",
            "title": " Where can I get old versions of WEKA?"
        },
        {
            "location": "/faqs/latest_bugfixes/",
            "text": "The article \nHow to get the latest bugfixes\n explains it in detail (it's basically either obtaining the source code from \nSubversion\n and compiling it yourself or getting a snapshot from the download section of the \nWEKA homepage\n).",
            "title": " How do I get the latest bugfixes?"
        },
        {
            "location": "/faqs/contribution/",
            "text": "Information on how to contribute to WEKA can be found in the \nContributing a package\n section of the \nHow are packages structured for the package management system?\n article. The conditions for new classifiers (schemes in general) are that, firstly, they have to be published in the proceedings of a renowned conference (e.g., ICML) or as an article of respected journal (e.g., Machine Learning) and, secondly, that they outperform other standard schemes (e.g., J48/C4.5).\n\n\nBut please bear in mind, that we don't have a lot of man power, i.e., being the WEKA maintainer is \nNOT\n a full-time position.",
            "title": " How can I contribute to WEKA?"
        },
        {
            "location": "/faqs/package_manager_doesnt_start/",
            "text": "The most likely reason for this is that your computer does not have direct access to the Internet and Java needs to be told to use a proxy server to access the web. The best way to achieve this is to configure an environment variable that provides the proxy details, e.g.,\n\n\n_JAVA_OPTIONS\n\n\n\n\nwhich is read by Oracle Java virtual machines. There is more information on this variable \nhere\n. Information on how to set environment variables in Windows is \nhere\n. For Mac users, there is a nice program to set environment variables available \nhere\n.\n\n\nSet the value of this variable to\n\n\n-Dhttp.proxyHost=some.proxy.somewhere.net -Dhttp.proxyPort=port\n\n\n\n\nwhere \nsome.proxy.somewhere.net\n needs to be replaced by the name of your proxy server and \nport\n needs to be replaced by the appropriate port number on the proxy server. Your IT department should be able to give you these details.\n\n\nThis should allow the package manager to connect to the website that hosts the package meta-information. However, if the package manager still cannot connect to the Internet, you can also force it to run in offline mode, by setting the above environment variable to\n\n\n-Dweka.packageManager.offline=true\n\n\n\n\nThen, you can download package .zip files manually via your web browser, by navigating to\n\n\nhttp://weka.sourceforge.net/packageMetaData/\n\n\nclicking on the link for the package you want to install, then clicking on \nLatest\n, and finally clicking on the URL given next to \nPackageURL\n.\n\n\nOnce you have downloaded the package .zip file, open the WEKA package manager, and click on the \nFile/URL\n button in the top-right corner of the package manager window (in the \nUnofficial\n panel). Then navigate to your package .zip file and install it.\n\n\nIf you are running Weka in \noffline\n mode, and the packages you are installing have some dependencies on one another, then there can still be some problems due to Weka not being able to verify the dependencies by checking against the central repository. This is usually a problem in the case where Weka has never been able to connect to the internet and thus has not downloaded and established a cache of the central package metadata repository. Fortunately there is a simple work-around to this, as long as you can access the internet via a web browser:\n\n\n\n\nUsing your web browser, download \nhttp://weka.sourceforge.net/packageMetaData/repo.zip\n\n\nIf it doesn't already exist, create the directory \n~/wekafiles/repCache\n\n\nCopy the downloaded \nrepo.zip\n into \n~/wekafiles/repCache\n and unzip it there\n\n\nStart Weka (use the \nweka.packageManager.offline=true\n property to speed up the startup process; see [http://weka.wikispaces.com/How+do+I+use+the+package+manager%3F#Package%20manager%20property%20file] for info)",
            "title": " Weka package manager does not start"
        },
        {
            "location": "/faqs/arff_does_not_load/",
            "text": "One way to figure out why \nARFF\n files are failing to load is to give them to the \nweka.core.Instances\n class. In the SimpleCLI or in the terminal, type the following:\n\n\n java weka.core.Instances filename.arff\n\n\n\n\nwhere you substitute \nfilename\n for the actual name of your file. This should return an error if there is a problem reading the file, or show some statistics if the file is ok. The error message you get should give some indication of what is wrong.\n\n\nnominal value not declared in header, read Token[X], line Y\n\n\nIf you get this error message than you seem to have declared a nominal attribute in the ARFF header section, but Weka came across a value (\n\"X\"\n) in the data (in line \nY\n) for this particular attribute that wasn't listed as possible value.\n\n\nAll\n nominal values that appear in the data must be declared in the header.",
            "title": " ARFF file does not load"
        },
        {
            "location": "/faqs/arff_does_not_load/#nominal-value-not-declared-in-header-read-tokenx-line-y",
            "text": "If you get this error message than you seem to have declared a nominal attribute in the ARFF header section, but Weka came across a value ( \"X\" ) in the data (in line  Y ) for this particular attribute that wasn't listed as possible value.  All  nominal values that appear in the data must be declared in the header.",
            "title": "nominal value not declared in header, read Token[X], line Y"
        },
        {
            "location": "/use_weka_in_your_java_code/",
            "text": "The most common components you might want to use are\n\n \nInstances\n - your data\n\n \nFilter\n - for preprocessing the data\n\n \nClassifier/Clusterer\n - built on the processed data\n\n \nEvaluating\n - how good is the classifier/clusterer?\n\n \nAttribute selection* - removing irrelevant attributes from your data\n\n\nThe following sections explain how to use them in your own code. A link to an \nexample class\n can be found at the end of this page, under the \nLinks\n section. The classifiers and filters always list their options in the Javadoc API (\nstable\n, \ndeveloper\n version) specification.\n\n\nA comprehensive source of information is the chapter \nUsing the API\n of the Weka manual.\n\n\nInstances\n\n\nDatasets\n\n\nThe \nDataSource\n class is not limited to ARFF files. It can also read CSV files and other formats (basically all file formats that Weka can import via its converters; it uses the file extension to determine the associated loader).\n\n\n import weka.core.converters.ConverterUtils.DataSource;\n ...\n DataSource source = new DataSource(\"/some/where/data.arff\");\n Instances data = source.getDataSet();\n // setting class attribute if the data format does not provide this information\n // For example, the XRFF format saves the class attribute information as well\n if (data.classIndex() == -1)\n   data.setClassIndex(data.numAttributes() - 1);\n\n\n\n\nDatabase\n\n\nReading from \nDatabases\n is slightly more complicated, but still very easy. First, you'll have to modify your \nDatabaseUtils.props\n file to reflect your database connection. Suppose you want to connect to a \nMySQL\n server that is running on the local machine on the default port \n3306\n. The MySQL JDBC driver is called \nConnector/J\n. (The driver class is \norg.gjt.mm.mysql.Driver\n.) The database where your target data resides is called \nsome_database\n. Since you're only reading, you can use the default user \nnobody\n without a password. Your props file must contain the following lines:\n\n\n jdbcDriver=org.gjt.mm.mysql.Driver\n jdbcURL=jdbc:mysql://localhost:3306/some_database\n\n\n\n\nSecondly, your Java code needs to look like this to load the data from the database:\n\n\n import weka.core.Instances;\n import weka.experiment.InstanceQuery;\n ...\n InstanceQuery query = new InstanceQuery();\n query.setUsername(\"nobody\");\n query.setPassword(\"\");\n query.setQuery(\"select * from whatsoever\");\n // You can declare that your data set is sparse\n // query.setSparseData(true);\n Instances data = query.retrieveInstances();\n\n\n\n\nNotes:\n\n\n Don't forget to add the JDBC driver to your \nCLASSPATH\n.\n\n For MS Access, you must use the JDBC-ODBC-bridge that is part of a JDK. The [[Windows databases]] article explains how to do this.\n* InstanceQuery automatically converts VARCHAR database columns to NOMINAL attributes, and long TEXT database columns to STRING attributes. So if you use InstanceQuery to do text mining against text that appears in a VARCHAR column, Weka will regard such text as nominal values. Thus it will fail to tokenize and mine that text. Use the \nNominalToString\n or \nStringToNominal\n filter (package \nweka.filters.unsupervised.attribute\n) to convert the attributes into the correct type.\n\n\nOption handling\n\n\nWeka schemes that implement the \nweka.core.OptionHandler\n interface, such as classifiers, clusterers, and filters, offer the following methods for setting and retrieving options:\n\n\n\n\nvoid setOptions(String[] options)\n\n\nString[] getOptions()\n\n\n\n\nThere are several ways of setting the options:\n\n\n\n\nManually creating a String array:\n\n\n\n\n   String[] options = new String[2];\n   options[0] = \"-R\";\n   options[1] = \"1\";\n\n\n\n\n\n\nUsing a single command-line string and using the \nsplitOptions\n method of the \nweka.core.Utils\n class to turn it into an array:\n\n\n\n\n    String[] options = weka.core.Utils.splitOptions(\"-R 1\");\n\n\n\n\n\n\nUsing the \nOptionsToCode.java\n class to automatically turn a command line into code. Especially handy if the command line contains nested classes that have their own options, such as kernels for SMO:\n\n\n\n\n  java OptionsToCode weka.classifiers.functions.SMO\n\n\n\n\nwill generate output like this:\n\n\n // create new instance of scheme\n weka.classifiers.functions.SMO scheme = new weka.classifiers.functions.SMO();\n // set options\n scheme.setOptions(weka.core.Utils.splitOptions(\"-C 1.0 -L 0.0010 -P 1.0E-12 -N 0 -V -1 -W 1 -K \\\"weka.classifiers.functions.supportVector.PolyKernel -C 250007 -E 1.0\\\"\"));\n\n\n\n\nAlso, the \nOptionTree.java\n tool allows you to view a nested options string, e.g., used at the command line, as a tree. This can help you spot nesting errors.\n\n\nFilter\n\n\nA filter has two different properties:\n\n\n\n\n\n\nsupervised\n or \nunsupervised\n\n  either takes the class attribute into account or not\n\n\n\n\n\n\nattribute\n- or \ninstance\n-based\n  e.g., removing a certain attribute or removing instances that meet a certain condition\n\n\n\n\n\n\nMost filters implement the \nOptionHandler\n interface, which means you can set the options via a String array, rather than setting them each manually via \nset\n-methods.\nFor example, if you want to remove the \nfirst\n attribute of a dataset, you need this filter\n\n\n weka.filters.unsupervised.attribute.Remove\n\n\n\n\nwith this option\n\n\n -R 1\n\n\n\n\nIf you have an \nInstances\n object, called \ndata\n, you can create and apply the filter like this:\n\n\n import weka.core.Instances;\n import weka.filters.Filter;\n import weka.filters.unsupervised.attribute.Remove;\n ...\n String[] options = new String[2];\n options[0] = \"-R\";                                    // \"range\"\n options[1] = \"1\";                                     // first attribute\n Remove remove = new Remove();                         // new instance of filter\n remove.setOptions(options);                           // set options\n remove.setInputFormat(data);                          // inform filter about dataset **AFTER** setting options\n Instances newData = Filter.useFilter(data, remove);   // apply filter\n\n\n\n\nFiltering on-the-fly\n\n\nThe \nFilteredClassifer\n meta-classifier is an easy way of filtering data on the fly. It removes the necessity of filtering the data before the classifier can be trained. Also, the data need not be passed through the trained filter again at prediction time. The following is an example of using this meta-classifier with the \nRemove\n filter and \nJ48\n for getting rid of a numeric ID attribute in the data:\n\n\n import weka.classifiers.meta.FilteredClassifier;\n import weka.classifiers.trees.J48;\n import weka.filters.unsupervised.attribute.Remove;\n ...\n Instances train = ...         // from somewhere\n Instances test = ...          // from somewhere\n // filter\n Remove rm = new Remove();\n rm.setAttributeIndices(\"1\");  // remove 1st attribute\n // classifier\n J48 j48 = new J48();\n j48.setUnpruned(true);        // using an unpruned J48\n // meta-classifier\n FilteredClassifier fc = new FilteredClassifier();\n fc.setFilter(rm);\n fc.setClassifier(j48);\n // train and make predictions\n fc.buildClassifier(train);\n for (int i = 0; i < test.numInstances(); i++) {\n   double pred = fc.classifyInstance(test.instance(i));\n   System.out.print(\"ID: \" + test.instance(i).value(0));\n   System.out.print(\", actual: \" + test.classAttribute().value((int) test.instance(i).classValue()));\n   System.out.println(\", predicted: \" + test.classAttribute().value((int) pred));\n }\n\n\n\n\nOther handy meta-schemes in Weka:\n\n\n\n\nweka.clusterers.FilteredClusterer\n\n\nweka.assocations.FilteredAssociator\n\n\n\n\nBatch filtering\n\n\nOn the command line, you can enable a second input/output pair (via \n-r\n and \n-s\n) with the \n-b\n option, in order to process the second file with the same filter setup as the first one. Necessary, if you're using attribute selection or standardization - otherwise you end up with incompatible datasets. This is done fairly easy, since one initializes the filter only once with the \nsetInputFormat(Instances)\n method, namely with the training set, and then applies the filter subsequently to the training set \nand\n the test set. The following example shows how to apply the \nStandardize\n filter to a train and a test set.\n\n\n Instances train = ...   // from somewhere\n Instances test = ...    // from somewhere\n Standardize filter = new Standardize();\n filter.setInputFormat(train);  // initializing the filter once with training set\n Instances newTrain = Filter.useFilter(train, filter);  // configures the Filter based on train instances and returns filtered instances\n Instances newTest = Filter.useFilter(test, filter);    // create new test set\n\n\n\n\nCalling conventions\n\n\nThe \nsetInputFormat(Instances)\n method \nalways\n has to be the last call before the filter is applied, e.g., with \nFilter.useFilter(Instances,Filter)\n. \nWhy?\n First, it is the convention for using filters and, secondly, lots of filters generate the header of the output format in the \nsetInputFormat(Instances)\n method with the currently set options (setting otpions \nafter\n this call doesn't have any effect any more).\n\n\nClassification\n\n\nThe necessary classes can be found in this package:\n\n\n weka.classifiers\n\n\n\n\nBuilding a Classifier\n\n\nBatch\n\n\nA Weka classifier is rather simple to train on a given dataset. E.g., we can train an unpruned C4.5 tree algorithm on a given dataset \ndata\n. The training is done via the \nbuildClassifier(Instances)\n method.\n\n\n import weka.classifiers.trees.J48;\n ...\n String[] options = new String[1];\n options[0] = \"-U\";            // unpruned tree\n J48 tree = new J48();         // new instance of tree\n tree.setOptions(options);     // set the options\n tree.buildClassifier(data);   // build classifier\n\n\n\n\nIncremental\n\n\nClassifiers implementing the \nweka.classifiers.UpdateableClassifier\n interface can be trained incrementally. This conserves memory, since the data doesn't have to be loaded into memory all at once. See the Javadoc of this interface to see what classifiers are implementing it.\n\n\nThe actual process of training an incremental classifier is fairly simple:\n\n\n\n\nCall \nbuildClassifier(Instances)\n with the structure of the dataset (may or may not contain any actual data rows).\n\n\nSubsequently call the \nupdateClassifier(Instance)\n method to feed the classifier new \nweka.core.Instance\n objects, one by one.\n\n\n\n\nHere is an example using data from a \nweka.core.converters.ArffLoader\n to train \nweka.classifiers.bayes.NaiveBayesUpdateable\n:\n\n\n // load data\n ArffLoader loader = new ArffLoader();\n loader.setFile(new File(\"/some/where/data.arff\"));\n Instances structure = loader.getStructure();\n structure.setClassIndex(structure.numAttributes() - 1);\n\n // train NaiveBayes\n NaiveBayesUpdateable nb = new NaiveBayesUpdateable();\n nb.buildClassifier(structure);\n Instance current;\n while ((current = loader.getNextInstance(structure)) != null)\n   nb.updateClassifier(current);\n\n\n\n\nA working example is \nIncrementalClassifier.java\n.\n\n\nEvaluating\n\n\nCross-validation\n\n\nIf you only have a training set and no test you might want to evaluate the classifier by using 10 times 10-fold cross-validation. This can be easily done via the \nEvaluation\n class. Here we \nseed\n the random selection of our folds for the CV with \n1\n. Check out the \nEvaluation\n class for more information about the statistics it produces.\n\n\n import weka.classifiers.Evaluation;\n import java.util.Random;\n ...\n Evaluation eval = new Evaluation(newData);\n eval.crossValidateModel(tree, newData, 10, new Random(1));\n\n\n\n\nNote:\n The classifier (in our example \ntree\n) should not be trained when handed over to the \ncrossValidateModel\n method. \nWhy?\n If the classifier does not abide to the Weka convention that a classifier must be re-initialized every time the \nbuildClassifier\n method is called (in other words: subsequent calls to the \nbuildClassifier\n method always return the same results), you will get inconsistent and worthless results. The \ncrossValidateModel\n takes care of training and evaluating the classifier. (It creates a copy of the original classifier that you hand over to the \ncrossValidateModel\n for each run of the cross-validation.)\n\n\nTrain/test set\n\n\nIn case you have a dedicated test set, you can train the classifier and then evaluate it on this test set. In the following example, a J48 is instantiated, trained and then evaluated. Some statistics are printed to \nstdout\n:\n\n\n import weka.core.Instances;\n import weka.classifiers.Evaluation;\n import weka.classifiers.trees.J48;\n ...\n Instances train = ...   // from somewhere\n Instances test = ...    // from somewhere\n // train classifier\n Classifier cls = new J48();\n cls.buildClassifier(train);\n // evaluate classifier and print some statistics\n Evaluation eval = new Evaluation(train);\n eval.evaluateModel(cls, test);\n System.out.println(eval.toSummaryString(\"\\nResults\\n======\\n\", false));\n\n\n\n\nStatistics\n\n\nSome methods for retrieving the results from the evaluation:\n\n\n\n\n\n\nnominal class\n\n\n\n\ncorrect()\n - number of correctly classified instances (see also \nincorrect()\n)\n\n\npctCorrect()\n - percentage of correctly classified instances (see also \npctIncorrect()\n)\n\n\nkappa()\n - Kappa statistics\n\n\n\n\n\n\n\n\nnumeric class\n\n\n\n\ncorrelationCoefficient()\n - correlation coefficient\n\n\n\n\n\n\n\n\ngeneral\n\n\n\n\nmeanAbsoluteError()\n - the mean absolute error\n\n\nrootMeanSquaredError()\n - the root mean squared error\n\n\nunclassified()\n - number of unclassified instances\n\n\npctUnclassified()\n - percentage of unclassified instances\n\n\n\n\n\n\n\n\nIf you want to have the exact same behavior as from the command line, use this call:\n\n\n import weka.classifiers.trees.J48;\n import weka.classifiers.Evaluation;\n ...\n String[] options = new String[2];\n options[0] = \"-t\";\n options[1] = \"/some/where/somefile.arff\";\n System.out.println(Evaluation.evaluateModel(new J48(), options));\n\n\n\n\nROC curves/AUC\n\n\nYou can also generate ROC curves/AUC with the predictions Weka recorded during testing. You can access these predictions via the \npredictions()\n method of the \nEvaluation\n class. See the [[Generating ROC curve]] article for a full example of how to generate ROC curves.\n\n\nClassifying instances\n\n\nIn case you have an unlabeled dataset that you want to classify with your newly trained classifier, you can use the following code snippet. It loads the file \n/some/where/unlabeled.arff\n, uses the previously built classifier \ntree\n to label the instances, and saves the labeled data as \n/some/where/labeled.arff\n.\n\n\n import java.io.BufferedReader;\n import java.io.BufferedWriter;\n import java.io.FileReader;\n import java.io.FileWriter;\n import weka.core.Instances;\n ...\n // load unlabeled data\n Instances unlabeled = new Instances(\n                         new BufferedReader(\n                           new FileReader(\"/some/where/unlabeled.arff\")));\n\n // set class attribute\n unlabeled.setClassIndex(unlabeled.numAttributes() - 1);\n\n // create copy\n Instances labeled = new Instances(unlabeled);\n\n // label instances\n for (int i = 0; i < unlabeled.numInstances(); i++) {\n   double clsLabel = tree.classifyInstance(unlabeled.instance(i));\n   labeled.instance(i).setClassValue(clsLabel);\n }\n // save labeled data\n BufferedWriter writer = new BufferedWriter(\n                           new FileWriter(\"/some/where/labeled.arff\"));\n writer.write(labeled.toString());\n writer.newLine();\n writer.flush();\n writer.close();\n\n\n\n\nNote on nominal classes:\n\n\n\n\nIf you're interested in the distribution over all the classes, use the method \ndistributionForInstance(Instance)\n. This method returns a double array with the probability for each class.\n\n\nThe returned double value from \nclassifyInstance\n (or the index in the array returned by \ndistributionForInstance\n) is just the index for the string values in the attribute. That is, if you want the string representation for the class label returned above \nclsLabel\n, then you can print it like this:\n\n\n\n\nSystem.out.println(clsLabel + \" -> \" + unlabeled.classAttribute().value((int) clsLabel));\n\n\n\n\nClustering\n\n\nClustering is similar to classification. The necessary classes can be found in this package:\n\n\n weka.clusterers\n\n\n\n\nBuilding a Clusterer\n\n\nBatch\n\n\nA clusterer is built in much the same way as a classifier, but the \nbuildClusterer(Instances)\n method instead of \nbuildClassifier(Instances)\n. The following code snippet shows how to build an \nEM\n clusterer with a maximum of \n100\n iterations.\n\n\n import weka.clusterers.EM;\n ...\n String[] options = new String[2];\n options[0] = \"-I\";                 // max. iterations\n options[1] = \"100\";\n EM clusterer = new EM();   // new instance of clusterer\n clusterer.setOptions(options);     // set the options\n clusterer.buildClusterer(data);    // build the clusterer\n\n\n\n\nIncremental\n\n\nClusterers implementing the \nweka.clusterers.UpdateableClusterer\n interface can be trained incrementally. This conserves memory, since the data doesn't have to be loaded into memory all at once. See the Javadoc for this interface to see which clusterers implement it.\n\n\nThe actual process of training an incremental clusterer is fairly simple:\n\n\n\n\nCall \nbuildClusterer(Instances)\n with the structure of the dataset (may or may not contain any actual data rows).\n\n\nSubsequently call the \nupdateClusterer(Instance)\n method to feed the clusterer new \nweka.core.Instance\n objects, one by one.\n\n\nCall \nupdateFinished()\n after all Instance objects have been processed, for the clusterer to perform additional computations.\n\n\n\n\nHere is an example using data from a \nweka.core.converters.ArffLoader\n to train \nweka.clusterers.Cobweb\n:\n\n\n // load data\n ArffLoader loader = new ArffLoader();\n loader.setFile(new File(\"/some/where/data.arff\"));\n Instances structure = loader.getStructure();\n\n // train Cobweb\n Cobweb cw = new Cobweb();\n cw.buildClusterer(structure);\n Instance current;\n while ((current = loader.getNextInstance(structure)) != null)\n   cw.updateClusterer(current);\n cw.updateFinished();\n\n\n\n\nA working example is \nIncrementalClusterer.java\n.\n\n\nEvaluating\n\n\nFor evaluating a clusterer, you can use the \nClusterEvaluation\n class. In this example, the number of clusters found is written to output:\n\n\n import weka.clusterers.ClusterEvaluation;\n import weka.clusterers.Clusterer;\n ...\n ClusterEvaluation eval = new ClusterEvaluation();\n Clusterer clusterer = new EM();                                 // new clusterer instance, default options\n clusterer.buildClusterer(data);                                 // build clusterer\n eval.setClusterer(clusterer);                                   // the cluster to evaluate\n eval.evaluateClusterer(newData);                                // data to evaluate the clusterer on\n System.out.println(\"# of clusters: \" + eval.getNumClusters());  // output # of clusters\n\n\n\n\nOr, in the case of \nDensityBasedClusterer\n, you can cross-validate the clusterer (Note: with \nMakeDensityBasedClusterer\n you can turn any clusterer into a density-based one):\n\n\n import weka.clusterers.ClusterEvaluation;\n import weka.clusterers.DensityBasedClusterer;\n import weka.core.Instances;\n import java.util.Random;\n ...\n Instances data = ...                                     // from somewhere\n DensityBasedClusterer clusterer = new ...                // the clusterer to evaluate\n double logLikelyhood =\n    ClusterEvaluation.crossValidateModel(                 // cross-validate\n    clusterer, data, 10,                                  // with 10 folds\n    new Random(1));                                       // and random number generator with seed 1\n\n\n\n\nOr, if you want the same behavior/print-out from command line, use this call:\n\n\n import weka.clusterers.EM;\n import weka.clusterers.ClusterEvaluation;\n ...\n String[] options = new String[2];\n options[0] = \"-t\";\n options[1] = \"/some/where/somefile.arff\";\n System.out.println(ClusterEvaluation.evaluateClusterer(new EM(), options));\n\n\n\n\nClustering instances\n\n\nThe only difference with regard to classification is the method name. Instead of \nclassifyInstance(Instance)\n, it is now \nclusterInstance(Instance)\n. The method for obtaining the distribution is still the same, i.e., \ndistributionForInstance(Instance)\n.\n\n\nClasses to clusters evaluation\n\n\nIf your data contains a class attribute and you want to check how well the generated clusters fit the classes, you can perform a so-called \nclasses to clusters\n evaluation. The Weka Explorer offers this functionality, and it's quite easy to implement. These are the necessary steps (complete source code: \nClassesToClusters.java\n):\n\n\n\n\nload the data and set the class attribute\n\n\n\n\n Instances data = new Instances(new BufferedReader(new FileReader(\"/some/where/file.arff\")));\n data.setClassIndex(data.numAttributes() - 1);\n\n\n\n\n\n\ngenerate the \nclass-less\n data to train the clusterer with\n\n\n\n\n weka.filters.unsupervised.attribute.Remove filter = new weka.filters.unsupervised.attribute.Remove();\n filter.setAttributeIndices(\"\" + (data.classIndex() + 1));\n filter.setInputFormat(data);\n Instances dataClusterer = Filter.useFilter(data, filter);\n\n\n\n\n\n\ntrain the clusterer, e.g., \nEM\n\n\n\n\n EM clusterer = new EM();\n // set further options for EM, if necessary...\n clusterer.buildClusterer(dataClusterer);\n\n\n\n\n\n\nevaluate the clusterer with the data still containing the class attribute\n\n\n\n\n ClusterEvaluation eval = new ClusterEvaluation();\n eval.setClusterer(clusterer);\n eval.evaluateClusterer(data);\n\n\n\n\n\n\nprint the results of the evaluation to \nstdout\n\n\n\n\n System.out.println(eval.clusterResultsToString());\n\n\n\n\nAttribute selection\n\n\nThere is no real need to use the attribute selection classes directly in your own code, since there are already a meta-classifier and a filter available for applying attribute selection, but the low-level approach is still listed for the sake of completeness. The following examples all use \nCfsSubsetEval\n and \nGreedyStepwise\n (backwards). The code listed below is taken from the \nAttributeSelectionTest.java\n.\n\n\nMeta-Classifier\n\n\nThe following meta-classifier performs a preprocessing step of attribute selection before the data gets presented to the base classifier (in the example here, this is \nJ48\n).\n\n\n  Instances data = ...  // from somewhere\n  AttributeSelectedClassifier classifier = new AttributeSelectedClassifier();\n  CfsSubsetEval eval = new CfsSubsetEval();\n  GreedyStepwise search = new GreedyStepwise();\n  search.setSearchBackwards(true);\n  J48 base = new J48();\n  classifier.setClassifier(base);\n  classifier.setEvaluator(eval);\n  classifier.setSearch(search);\n  // 10-fold cross-validation\n  Evaluation evaluation = new Evaluation(data);\n  evaluation.crossValidateModel(classifier, data, 10, new Random(1));\n  System.out.println(evaluation.toSummaryString());\n\n\n\n\nFilter\n\n\nThe filter approach is straightforward: after setting up the filter, one just filters the data through the filter and obtains the reduced dataset.\n\n\n  Instances data = ...  // from somewhere\n  AttributeSelection filter = new AttributeSelection();  // package weka.filters.supervised.attribute!\n  CfsSubsetEval eval = new CfsSubsetEval();\n  GreedyStepwise search = new GreedyStepwise();\n  search.setSearchBackwards(true);\n  filter.setEvaluator(eval);\n  filter.setSearch(search);\n  filter.setInputFormat(data);\n  // generate new data\n  Instances newData = Filter.useFilter(data, filter);\n  System.out.println(newData);\n\n\n\n\nLow-level\n\n\nIf neither the meta-classifier nor filter approach is suitable for your purposes, you can use the attribute selection classes themselves.\n\n\n  Instances data = ...  // from somewhere\n  AttributeSelection attsel = new AttributeSelection();  // package weka.attributeSelection!\n  CfsSubsetEval eval = new CfsSubsetEval();\n  GreedyStepwise search = new GreedyStepwise();\n  search.setSearchBackwards(true);\n  attsel.setEvaluator(eval);\n  attsel.setSearch(search);\n  attsel.SelectAttributes(data);\n  // obtain the attribute indices that were selected\n  int[] indices = attsel.selectedAttributes();\n  System.out.println(Utils.arrayToString(indices));\n\n\n\n\nNote on randomization\n\n\nMost machine learning schemes, like classifiers and clusterers, are susceptible to the ordering of the data. Using a different seed for randomizing the data will most likely produce a different result. For example, the Explorer, or a classifier/clusterer run from the command line, uses only a seeded \njava.util.Random\n number generator, whereas the \nweka.core.Instances.getRandomNumberGenerator(int)\n (which the \nWekaDemo.java\n uses) also takes the data into account for seeding. Unless one runs 10-fold cross-validation 10 times and averages the results, one will most likely get different results.\n\n\nSee also\n\n\n\n\nWeka Examples\n - pointer to collection of example classes\n\n\nDatabases\n - for more information about using databases in Weka (includes ODBC, e.g., for MS Access)\n\n\n[[weka_experiment_DatabaseUtils.props|weka/experiment/DatabaseUtils.props]] - the database setup file\n\n\n[[Generating cross-validation folds (Java approach)]] - in case you want to run 10-fold cross-validation manually\n\n\n[[Generating classifier evaluation output manually]] - if you want to generate some of the evaluation statistics output manually\n\n\n[[Creating Instances on-the-fly]] - explains how to generate a \nweka.core.Instances\n object from scratch\n\n\n[[Save Instances to an ARFF File]] - shows how to output a dataset\n\n\n[[Using the Experiment API]]\n\n\n\n\nExamples\n\n\nThe following are a few sample classes for using various parts of the Weka API:\n\n\n\n\n\n\nWekaDemo.java\n (\nstable\n, \ndeveloper\n) - little demo class that loads data from a file, runs it through a filter and trains/evaluates a classifier\n\n\n\n\n\n\nClusteringDemo.java\n (\nstable\n, \ndeveloper\n) - a basic example for using the clusterer API\n\n\n\n\n\n\nClassesToClusters.java\n (\nstable\n, \ndeveloper\n) - performs a \nclasses to clusters\n evaluation like in the Explorer\n\n\n\n\n\n\nAttributeSelectionTest.java\n (\nstable\n, \ndeveloper\n) - example code for using the attribute selection API\n\n\n\n\n\n\nM5PExample.java\n (\nstable\n, \ndeveloper\n) - example using M5P to obtain data from database, train model, serialize it to a file, and use this serialized model to make predictions again.\n\n\n\n\n\n\nOptionsToCode.java\n (\nstable\n, \ndeveloper\n) - turns a Weka command line for a scheme with options into Java code, correctly escaping quotes and backslashes.\n\n\n\n\n\n\nOptionTree.java\n (\nstable\n, \ndeveloper\n) - displays nested Weka options as tree.\n\n\n\n\n\n\nIncrementalClassifier.java\n (\nstable\n, \ndeveloper\n) - Example class for how to train an incremental classifier (in this case, \nweka.classifiers.bayes.NaiveBayesUpdateable\n).\n\n\n\n\n\n\nIncrementalClusterer.java\n (\nstable\n, \ndeveloper\n) - Example class for how to train an incremental clusterer (in this case, \nweka.clusterers.Cobweb\n).\n\n\n\n\n\n\nLinks\n\n\n\n\nWeka API\n\n\nStable version\n\n\nDeveloper version",
            "title": " Use Weka in your Java code"
        },
        {
            "location": "/use_weka_in_your_java_code/#instances",
            "text": "",
            "title": "Instances"
        },
        {
            "location": "/use_weka_in_your_java_code/#datasets",
            "text": "The  DataSource  class is not limited to ARFF files. It can also read CSV files and other formats (basically all file formats that Weka can import via its converters; it uses the file extension to determine the associated loader).   import weka.core.converters.ConverterUtils.DataSource;\n ...\n DataSource source = new DataSource(\"/some/where/data.arff\");\n Instances data = source.getDataSet();\n // setting class attribute if the data format does not provide this information\n // For example, the XRFF format saves the class attribute information as well\n if (data.classIndex() == -1)\n   data.setClassIndex(data.numAttributes() - 1);",
            "title": "Datasets"
        },
        {
            "location": "/use_weka_in_your_java_code/#database",
            "text": "Reading from  Databases  is slightly more complicated, but still very easy. First, you'll have to modify your  DatabaseUtils.props  file to reflect your database connection. Suppose you want to connect to a  MySQL  server that is running on the local machine on the default port  3306 . The MySQL JDBC driver is called  Connector/J . (The driver class is  org.gjt.mm.mysql.Driver .) The database where your target data resides is called  some_database . Since you're only reading, you can use the default user  nobody  without a password. Your props file must contain the following lines:   jdbcDriver=org.gjt.mm.mysql.Driver\n jdbcURL=jdbc:mysql://localhost:3306/some_database  Secondly, your Java code needs to look like this to load the data from the database:   import weka.core.Instances;\n import weka.experiment.InstanceQuery;\n ...\n InstanceQuery query = new InstanceQuery();\n query.setUsername(\"nobody\");\n query.setPassword(\"\");\n query.setQuery(\"select * from whatsoever\");\n // You can declare that your data set is sparse\n // query.setSparseData(true);\n Instances data = query.retrieveInstances();  Notes:   Don't forget to add the JDBC driver to your  CLASSPATH .  For MS Access, you must use the JDBC-ODBC-bridge that is part of a JDK. The [[Windows databases]] article explains how to do this.\n* InstanceQuery automatically converts VARCHAR database columns to NOMINAL attributes, and long TEXT database columns to STRING attributes. So if you use InstanceQuery to do text mining against text that appears in a VARCHAR column, Weka will regard such text as nominal values. Thus it will fail to tokenize and mine that text. Use the  NominalToString  or  StringToNominal  filter (package  weka.filters.unsupervised.attribute ) to convert the attributes into the correct type.",
            "title": "Database"
        },
        {
            "location": "/use_weka_in_your_java_code/#option-handling",
            "text": "Weka schemes that implement the  weka.core.OptionHandler  interface, such as classifiers, clusterers, and filters, offer the following methods for setting and retrieving options:   void setOptions(String[] options)  String[] getOptions()   There are several ways of setting the options:   Manually creating a String array:      String[] options = new String[2];\n   options[0] = \"-R\";\n   options[1] = \"1\";   Using a single command-line string and using the  splitOptions  method of the  weka.core.Utils  class to turn it into an array:       String[] options = weka.core.Utils.splitOptions(\"-R 1\");   Using the  OptionsToCode.java  class to automatically turn a command line into code. Especially handy if the command line contains nested classes that have their own options, such as kernels for SMO:     java OptionsToCode weka.classifiers.functions.SMO  will generate output like this:   // create new instance of scheme\n weka.classifiers.functions.SMO scheme = new weka.classifiers.functions.SMO();\n // set options\n scheme.setOptions(weka.core.Utils.splitOptions(\"-C 1.0 -L 0.0010 -P 1.0E-12 -N 0 -V -1 -W 1 -K \\\"weka.classifiers.functions.supportVector.PolyKernel -C 250007 -E 1.0\\\"\"));  Also, the  OptionTree.java  tool allows you to view a nested options string, e.g., used at the command line, as a tree. This can help you spot nesting errors.",
            "title": "Option handling"
        },
        {
            "location": "/use_weka_in_your_java_code/#filter",
            "text": "A filter has two different properties:    supervised  or  unsupervised \n  either takes the class attribute into account or not    attribute - or  instance -based\n  e.g., removing a certain attribute or removing instances that meet a certain condition    Most filters implement the  OptionHandler  interface, which means you can set the options via a String array, rather than setting them each manually via  set -methods.\nFor example, if you want to remove the  first  attribute of a dataset, you need this filter   weka.filters.unsupervised.attribute.Remove  with this option   -R 1  If you have an  Instances  object, called  data , you can create and apply the filter like this:   import weka.core.Instances;\n import weka.filters.Filter;\n import weka.filters.unsupervised.attribute.Remove;\n ...\n String[] options = new String[2];\n options[0] = \"-R\";                                    // \"range\"\n options[1] = \"1\";                                     // first attribute\n Remove remove = new Remove();                         // new instance of filter\n remove.setOptions(options);                           // set options\n remove.setInputFormat(data);                          // inform filter about dataset **AFTER** setting options\n Instances newData = Filter.useFilter(data, remove);   // apply filter",
            "title": "Filter"
        },
        {
            "location": "/use_weka_in_your_java_code/#filtering-on-the-fly",
            "text": "The  FilteredClassifer  meta-classifier is an easy way of filtering data on the fly. It removes the necessity of filtering the data before the classifier can be trained. Also, the data need not be passed through the trained filter again at prediction time. The following is an example of using this meta-classifier with the  Remove  filter and  J48  for getting rid of a numeric ID attribute in the data:   import weka.classifiers.meta.FilteredClassifier;\n import weka.classifiers.trees.J48;\n import weka.filters.unsupervised.attribute.Remove;\n ...\n Instances train = ...         // from somewhere\n Instances test = ...          // from somewhere\n // filter\n Remove rm = new Remove();\n rm.setAttributeIndices(\"1\");  // remove 1st attribute\n // classifier\n J48 j48 = new J48();\n j48.setUnpruned(true);        // using an unpruned J48\n // meta-classifier\n FilteredClassifier fc = new FilteredClassifier();\n fc.setFilter(rm);\n fc.setClassifier(j48);\n // train and make predictions\n fc.buildClassifier(train);\n for (int i = 0; i < test.numInstances(); i++) {\n   double pred = fc.classifyInstance(test.instance(i));\n   System.out.print(\"ID: \" + test.instance(i).value(0));\n   System.out.print(\", actual: \" + test.classAttribute().value((int) test.instance(i).classValue()));\n   System.out.println(\", predicted: \" + test.classAttribute().value((int) pred));\n }  Other handy meta-schemes in Weka:   weka.clusterers.FilteredClusterer  weka.assocations.FilteredAssociator",
            "title": "Filtering on-the-fly"
        },
        {
            "location": "/use_weka_in_your_java_code/#batch-filtering",
            "text": "On the command line, you can enable a second input/output pair (via  -r  and  -s ) with the  -b  option, in order to process the second file with the same filter setup as the first one. Necessary, if you're using attribute selection or standardization - otherwise you end up with incompatible datasets. This is done fairly easy, since one initializes the filter only once with the  setInputFormat(Instances)  method, namely with the training set, and then applies the filter subsequently to the training set  and  the test set. The following example shows how to apply the  Standardize  filter to a train and a test set.   Instances train = ...   // from somewhere\n Instances test = ...    // from somewhere\n Standardize filter = new Standardize();\n filter.setInputFormat(train);  // initializing the filter once with training set\n Instances newTrain = Filter.useFilter(train, filter);  // configures the Filter based on train instances and returns filtered instances\n Instances newTest = Filter.useFilter(test, filter);    // create new test set",
            "title": "Batch filtering"
        },
        {
            "location": "/use_weka_in_your_java_code/#calling-conventions",
            "text": "The  setInputFormat(Instances)  method  always  has to be the last call before the filter is applied, e.g., with  Filter.useFilter(Instances,Filter) .  Why?  First, it is the convention for using filters and, secondly, lots of filters generate the header of the output format in the  setInputFormat(Instances)  method with the currently set options (setting otpions  after  this call doesn't have any effect any more).",
            "title": "Calling conventions"
        },
        {
            "location": "/use_weka_in_your_java_code/#classification",
            "text": "The necessary classes can be found in this package:   weka.classifiers",
            "title": "Classification"
        },
        {
            "location": "/use_weka_in_your_java_code/#building-a-classifier",
            "text": "",
            "title": "Building a Classifier"
        },
        {
            "location": "/use_weka_in_your_java_code/#batch",
            "text": "A Weka classifier is rather simple to train on a given dataset. E.g., we can train an unpruned C4.5 tree algorithm on a given dataset  data . The training is done via the  buildClassifier(Instances)  method.   import weka.classifiers.trees.J48;\n ...\n String[] options = new String[1];\n options[0] = \"-U\";            // unpruned tree\n J48 tree = new J48();         // new instance of tree\n tree.setOptions(options);     // set the options\n tree.buildClassifier(data);   // build classifier",
            "title": "Batch"
        },
        {
            "location": "/use_weka_in_your_java_code/#incremental",
            "text": "Classifiers implementing the  weka.classifiers.UpdateableClassifier  interface can be trained incrementally. This conserves memory, since the data doesn't have to be loaded into memory all at once. See the Javadoc of this interface to see what classifiers are implementing it.  The actual process of training an incremental classifier is fairly simple:   Call  buildClassifier(Instances)  with the structure of the dataset (may or may not contain any actual data rows).  Subsequently call the  updateClassifier(Instance)  method to feed the classifier new  weka.core.Instance  objects, one by one.   Here is an example using data from a  weka.core.converters.ArffLoader  to train  weka.classifiers.bayes.NaiveBayesUpdateable :   // load data\n ArffLoader loader = new ArffLoader();\n loader.setFile(new File(\"/some/where/data.arff\"));\n Instances structure = loader.getStructure();\n structure.setClassIndex(structure.numAttributes() - 1);\n\n // train NaiveBayes\n NaiveBayesUpdateable nb = new NaiveBayesUpdateable();\n nb.buildClassifier(structure);\n Instance current;\n while ((current = loader.getNextInstance(structure)) != null)\n   nb.updateClassifier(current);  A working example is  IncrementalClassifier.java .",
            "title": "Incremental"
        },
        {
            "location": "/use_weka_in_your_java_code/#evaluating",
            "text": "",
            "title": "Evaluating"
        },
        {
            "location": "/use_weka_in_your_java_code/#cross-validation",
            "text": "If you only have a training set and no test you might want to evaluate the classifier by using 10 times 10-fold cross-validation. This can be easily done via the  Evaluation  class. Here we  seed  the random selection of our folds for the CV with  1 . Check out the  Evaluation  class for more information about the statistics it produces.   import weka.classifiers.Evaluation;\n import java.util.Random;\n ...\n Evaluation eval = new Evaluation(newData);\n eval.crossValidateModel(tree, newData, 10, new Random(1));  Note:  The classifier (in our example  tree ) should not be trained when handed over to the  crossValidateModel  method.  Why?  If the classifier does not abide to the Weka convention that a classifier must be re-initialized every time the  buildClassifier  method is called (in other words: subsequent calls to the  buildClassifier  method always return the same results), you will get inconsistent and worthless results. The  crossValidateModel  takes care of training and evaluating the classifier. (It creates a copy of the original classifier that you hand over to the  crossValidateModel  for each run of the cross-validation.)",
            "title": "Cross-validation"
        },
        {
            "location": "/use_weka_in_your_java_code/#traintest-set",
            "text": "In case you have a dedicated test set, you can train the classifier and then evaluate it on this test set. In the following example, a J48 is instantiated, trained and then evaluated. Some statistics are printed to  stdout :   import weka.core.Instances;\n import weka.classifiers.Evaluation;\n import weka.classifiers.trees.J48;\n ...\n Instances train = ...   // from somewhere\n Instances test = ...    // from somewhere\n // train classifier\n Classifier cls = new J48();\n cls.buildClassifier(train);\n // evaluate classifier and print some statistics\n Evaluation eval = new Evaluation(train);\n eval.evaluateModel(cls, test);\n System.out.println(eval.toSummaryString(\"\\nResults\\n======\\n\", false));",
            "title": "Train/test set"
        },
        {
            "location": "/use_weka_in_your_java_code/#statistics",
            "text": "Some methods for retrieving the results from the evaluation:    nominal class   correct()  - number of correctly classified instances (see also  incorrect() )  pctCorrect()  - percentage of correctly classified instances (see also  pctIncorrect() )  kappa()  - Kappa statistics     numeric class   correlationCoefficient()  - correlation coefficient     general   meanAbsoluteError()  - the mean absolute error  rootMeanSquaredError()  - the root mean squared error  unclassified()  - number of unclassified instances  pctUnclassified()  - percentage of unclassified instances     If you want to have the exact same behavior as from the command line, use this call:   import weka.classifiers.trees.J48;\n import weka.classifiers.Evaluation;\n ...\n String[] options = new String[2];\n options[0] = \"-t\";\n options[1] = \"/some/where/somefile.arff\";\n System.out.println(Evaluation.evaluateModel(new J48(), options));",
            "title": "Statistics"
        },
        {
            "location": "/use_weka_in_your_java_code/#roc-curvesauc",
            "text": "You can also generate ROC curves/AUC with the predictions Weka recorded during testing. You can access these predictions via the  predictions()  method of the  Evaluation  class. See the [[Generating ROC curve]] article for a full example of how to generate ROC curves.",
            "title": "ROC curves/AUC"
        },
        {
            "location": "/use_weka_in_your_java_code/#classifying-instances",
            "text": "In case you have an unlabeled dataset that you want to classify with your newly trained classifier, you can use the following code snippet. It loads the file  /some/where/unlabeled.arff , uses the previously built classifier  tree  to label the instances, and saves the labeled data as  /some/where/labeled.arff .   import java.io.BufferedReader;\n import java.io.BufferedWriter;\n import java.io.FileReader;\n import java.io.FileWriter;\n import weka.core.Instances;\n ...\n // load unlabeled data\n Instances unlabeled = new Instances(\n                         new BufferedReader(\n                           new FileReader(\"/some/where/unlabeled.arff\")));\n\n // set class attribute\n unlabeled.setClassIndex(unlabeled.numAttributes() - 1);\n\n // create copy\n Instances labeled = new Instances(unlabeled);\n\n // label instances\n for (int i = 0; i < unlabeled.numInstances(); i++) {\n   double clsLabel = tree.classifyInstance(unlabeled.instance(i));\n   labeled.instance(i).setClassValue(clsLabel);\n }\n // save labeled data\n BufferedWriter writer = new BufferedWriter(\n                           new FileWriter(\"/some/where/labeled.arff\"));\n writer.write(labeled.toString());\n writer.newLine();\n writer.flush();\n writer.close();  Note on nominal classes:   If you're interested in the distribution over all the classes, use the method  distributionForInstance(Instance) . This method returns a double array with the probability for each class.  The returned double value from  classifyInstance  (or the index in the array returned by  distributionForInstance ) is just the index for the string values in the attribute. That is, if you want the string representation for the class label returned above  clsLabel , then you can print it like this:   System.out.println(clsLabel + \" -> \" + unlabeled.classAttribute().value((int) clsLabel));",
            "title": "Classifying instances"
        },
        {
            "location": "/use_weka_in_your_java_code/#clustering",
            "text": "Clustering is similar to classification. The necessary classes can be found in this package:   weka.clusterers",
            "title": "Clustering"
        },
        {
            "location": "/use_weka_in_your_java_code/#building-a-clusterer",
            "text": "",
            "title": "Building a Clusterer"
        },
        {
            "location": "/use_weka_in_your_java_code/#batch_1",
            "text": "A clusterer is built in much the same way as a classifier, but the  buildClusterer(Instances)  method instead of  buildClassifier(Instances) . The following code snippet shows how to build an  EM  clusterer with a maximum of  100  iterations.   import weka.clusterers.EM;\n ...\n String[] options = new String[2];\n options[0] = \"-I\";                 // max. iterations\n options[1] = \"100\";\n EM clusterer = new EM();   // new instance of clusterer\n clusterer.setOptions(options);     // set the options\n clusterer.buildClusterer(data);    // build the clusterer",
            "title": "Batch"
        },
        {
            "location": "/use_weka_in_your_java_code/#incremental_1",
            "text": "Clusterers implementing the  weka.clusterers.UpdateableClusterer  interface can be trained incrementally. This conserves memory, since the data doesn't have to be loaded into memory all at once. See the Javadoc for this interface to see which clusterers implement it.  The actual process of training an incremental clusterer is fairly simple:   Call  buildClusterer(Instances)  with the structure of the dataset (may or may not contain any actual data rows).  Subsequently call the  updateClusterer(Instance)  method to feed the clusterer new  weka.core.Instance  objects, one by one.  Call  updateFinished()  after all Instance objects have been processed, for the clusterer to perform additional computations.   Here is an example using data from a  weka.core.converters.ArffLoader  to train  weka.clusterers.Cobweb :   // load data\n ArffLoader loader = new ArffLoader();\n loader.setFile(new File(\"/some/where/data.arff\"));\n Instances structure = loader.getStructure();\n\n // train Cobweb\n Cobweb cw = new Cobweb();\n cw.buildClusterer(structure);\n Instance current;\n while ((current = loader.getNextInstance(structure)) != null)\n   cw.updateClusterer(current);\n cw.updateFinished();  A working example is  IncrementalClusterer.java .",
            "title": "Incremental"
        },
        {
            "location": "/use_weka_in_your_java_code/#evaluating_1",
            "text": "For evaluating a clusterer, you can use the  ClusterEvaluation  class. In this example, the number of clusters found is written to output:   import weka.clusterers.ClusterEvaluation;\n import weka.clusterers.Clusterer;\n ...\n ClusterEvaluation eval = new ClusterEvaluation();\n Clusterer clusterer = new EM();                                 // new clusterer instance, default options\n clusterer.buildClusterer(data);                                 // build clusterer\n eval.setClusterer(clusterer);                                   // the cluster to evaluate\n eval.evaluateClusterer(newData);                                // data to evaluate the clusterer on\n System.out.println(\"# of clusters: \" + eval.getNumClusters());  // output # of clusters  Or, in the case of  DensityBasedClusterer , you can cross-validate the clusterer (Note: with  MakeDensityBasedClusterer  you can turn any clusterer into a density-based one):   import weka.clusterers.ClusterEvaluation;\n import weka.clusterers.DensityBasedClusterer;\n import weka.core.Instances;\n import java.util.Random;\n ...\n Instances data = ...                                     // from somewhere\n DensityBasedClusterer clusterer = new ...                // the clusterer to evaluate\n double logLikelyhood =\n    ClusterEvaluation.crossValidateModel(                 // cross-validate\n    clusterer, data, 10,                                  // with 10 folds\n    new Random(1));                                       // and random number generator with seed 1  Or, if you want the same behavior/print-out from command line, use this call:   import weka.clusterers.EM;\n import weka.clusterers.ClusterEvaluation;\n ...\n String[] options = new String[2];\n options[0] = \"-t\";\n options[1] = \"/some/where/somefile.arff\";\n System.out.println(ClusterEvaluation.evaluateClusterer(new EM(), options));",
            "title": "Evaluating"
        },
        {
            "location": "/use_weka_in_your_java_code/#clustering-instances",
            "text": "The only difference with regard to classification is the method name. Instead of  classifyInstance(Instance) , it is now  clusterInstance(Instance) . The method for obtaining the distribution is still the same, i.e.,  distributionForInstance(Instance) .",
            "title": "Clustering instances"
        },
        {
            "location": "/use_weka_in_your_java_code/#classes-to-clusters-evaluation",
            "text": "If your data contains a class attribute and you want to check how well the generated clusters fit the classes, you can perform a so-called  classes to clusters  evaluation. The Weka Explorer offers this functionality, and it's quite easy to implement. These are the necessary steps (complete source code:  ClassesToClusters.java ):   load the data and set the class attribute    Instances data = new Instances(new BufferedReader(new FileReader(\"/some/where/file.arff\")));\n data.setClassIndex(data.numAttributes() - 1);   generate the  class-less  data to train the clusterer with    weka.filters.unsupervised.attribute.Remove filter = new weka.filters.unsupervised.attribute.Remove();\n filter.setAttributeIndices(\"\" + (data.classIndex() + 1));\n filter.setInputFormat(data);\n Instances dataClusterer = Filter.useFilter(data, filter);   train the clusterer, e.g.,  EM    EM clusterer = new EM();\n // set further options for EM, if necessary...\n clusterer.buildClusterer(dataClusterer);   evaluate the clusterer with the data still containing the class attribute    ClusterEvaluation eval = new ClusterEvaluation();\n eval.setClusterer(clusterer);\n eval.evaluateClusterer(data);   print the results of the evaluation to  stdout    System.out.println(eval.clusterResultsToString());",
            "title": "Classes to clusters evaluation"
        },
        {
            "location": "/use_weka_in_your_java_code/#attribute-selection",
            "text": "There is no real need to use the attribute selection classes directly in your own code, since there are already a meta-classifier and a filter available for applying attribute selection, but the low-level approach is still listed for the sake of completeness. The following examples all use  CfsSubsetEval  and  GreedyStepwise  (backwards). The code listed below is taken from the  AttributeSelectionTest.java .",
            "title": "Attribute selection"
        },
        {
            "location": "/use_weka_in_your_java_code/#meta-classifier",
            "text": "The following meta-classifier performs a preprocessing step of attribute selection before the data gets presented to the base classifier (in the example here, this is  J48 ).    Instances data = ...  // from somewhere\n  AttributeSelectedClassifier classifier = new AttributeSelectedClassifier();\n  CfsSubsetEval eval = new CfsSubsetEval();\n  GreedyStepwise search = new GreedyStepwise();\n  search.setSearchBackwards(true);\n  J48 base = new J48();\n  classifier.setClassifier(base);\n  classifier.setEvaluator(eval);\n  classifier.setSearch(search);\n  // 10-fold cross-validation\n  Evaluation evaluation = new Evaluation(data);\n  evaluation.crossValidateModel(classifier, data, 10, new Random(1));\n  System.out.println(evaluation.toSummaryString());",
            "title": "Meta-Classifier"
        },
        {
            "location": "/use_weka_in_your_java_code/#filter_1",
            "text": "The filter approach is straightforward: after setting up the filter, one just filters the data through the filter and obtains the reduced dataset.    Instances data = ...  // from somewhere\n  AttributeSelection filter = new AttributeSelection();  // package weka.filters.supervised.attribute!\n  CfsSubsetEval eval = new CfsSubsetEval();\n  GreedyStepwise search = new GreedyStepwise();\n  search.setSearchBackwards(true);\n  filter.setEvaluator(eval);\n  filter.setSearch(search);\n  filter.setInputFormat(data);\n  // generate new data\n  Instances newData = Filter.useFilter(data, filter);\n  System.out.println(newData);",
            "title": "Filter"
        },
        {
            "location": "/use_weka_in_your_java_code/#low-level",
            "text": "If neither the meta-classifier nor filter approach is suitable for your purposes, you can use the attribute selection classes themselves.    Instances data = ...  // from somewhere\n  AttributeSelection attsel = new AttributeSelection();  // package weka.attributeSelection!\n  CfsSubsetEval eval = new CfsSubsetEval();\n  GreedyStepwise search = new GreedyStepwise();\n  search.setSearchBackwards(true);\n  attsel.setEvaluator(eval);\n  attsel.setSearch(search);\n  attsel.SelectAttributes(data);\n  // obtain the attribute indices that were selected\n  int[] indices = attsel.selectedAttributes();\n  System.out.println(Utils.arrayToString(indices));",
            "title": "Low-level"
        },
        {
            "location": "/use_weka_in_your_java_code/#note-on-randomization",
            "text": "Most machine learning schemes, like classifiers and clusterers, are susceptible to the ordering of the data. Using a different seed for randomizing the data will most likely produce a different result. For example, the Explorer, or a classifier/clusterer run from the command line, uses only a seeded  java.util.Random  number generator, whereas the  weka.core.Instances.getRandomNumberGenerator(int)  (which the  WekaDemo.java  uses) also takes the data into account for seeding. Unless one runs 10-fold cross-validation 10 times and averages the results, one will most likely get different results.",
            "title": "Note on randomization"
        },
        {
            "location": "/use_weka_in_your_java_code/#see-also",
            "text": "Weka Examples  - pointer to collection of example classes  Databases  - for more information about using databases in Weka (includes ODBC, e.g., for MS Access)  [[weka_experiment_DatabaseUtils.props|weka/experiment/DatabaseUtils.props]] - the database setup file  [[Generating cross-validation folds (Java approach)]] - in case you want to run 10-fold cross-validation manually  [[Generating classifier evaluation output manually]] - if you want to generate some of the evaluation statistics output manually  [[Creating Instances on-the-fly]] - explains how to generate a  weka.core.Instances  object from scratch  [[Save Instances to an ARFF File]] - shows how to output a dataset  [[Using the Experiment API]]",
            "title": "See also"
        },
        {
            "location": "/use_weka_in_your_java_code/#examples",
            "text": "The following are a few sample classes for using various parts of the Weka API:    WekaDemo.java  ( stable ,  developer ) - little demo class that loads data from a file, runs it through a filter and trains/evaluates a classifier    ClusteringDemo.java  ( stable ,  developer ) - a basic example for using the clusterer API    ClassesToClusters.java  ( stable ,  developer ) - performs a  classes to clusters  evaluation like in the Explorer    AttributeSelectionTest.java  ( stable ,  developer ) - example code for using the attribute selection API    M5PExample.java  ( stable ,  developer ) - example using M5P to obtain data from database, train model, serialize it to a file, and use this serialized model to make predictions again.    OptionsToCode.java  ( stable ,  developer ) - turns a Weka command line for a scheme with options into Java code, correctly escaping quotes and backslashes.    OptionTree.java  ( stable ,  developer ) - displays nested Weka options as tree.    IncrementalClassifier.java  ( stable ,  developer ) - Example class for how to train an incremental classifier (in this case,  weka.classifiers.bayes.NaiveBayesUpdateable ).    IncrementalClusterer.java  ( stable ,  developer ) - Example class for how to train an incremental clusterer (in this case,  weka.clusterers.Cobweb ).",
            "title": "Examples"
        },
        {
            "location": "/use_weka_in_your_java_code/#links",
            "text": "Weka API  Stable version  Developer version",
            "title": "Links"
        },
        {
            "location": "/weka_examples/",
            "text": "The \nWeka Examples\n collection is a comprehensive collection of examples for the different versions of Weka in the form of an \nANT\n project. You can access these examples as follows:\n\n\n\n\nthrough \nsnapshots\n or releases (they contain a separate ZIP file with the \nANT\n project)\n\n\n\n\nsubversion\n\n\n\n\n\n\nstable-3.8 version (3.8.x): \n\n\nhttps://svn.cms.waikato.ac.nz/svn/weka/branches/stable-3-8/wekaexamples/\n\n\n\n\n\n\ndeveloper version (3.9.x): \n\n\nhttps://svn.cms.waikato.ac.nz/svn/weka/trunk/wekaexamples/",
            "title": " Weka Examples"
        },
        {
            "location": "/generating_cv_folds/",
            "text": "You have two choices of generating cross-validation folds:\n\n\n\n\nFilter approach\n - uses a \nbash\n script to generate the train/test pairs beforehand\n\n\nJava approach\n - to be used from within your own Java code, creates train/test pairs on the fly",
            "title": " Generating cross-validation folds"
        },
        {
            "location": "/generating_cv_folds_filter/",
            "text": "The filter \nRemoveFolds\n (package \nweka.filters.unsupervised.instance\n) can be used to generate the train/test splits used in cross-validation (for stratified folds, use \nweka.filters.supervised.instance.StratifiedRemoveFolds\n). The filter has to be used twice for each train/test split, first to generate the train set and then to obtain the test set.\n\n\nSince this is rather cumbersome by hand, one can also put this into a \nbash\n script:\n\n\n #!/bin/bash\n #\n # expects the weka.jar as first parameter and the datasets to work on as \n # second parameter.\n #\n # FracPete, 2007-04-10\n\n if [ ! $# -eq 2 ]\n then\n   echo\n   echo \"usage: folds.sh <weka.jar> <dataset>\"\n   echo\n   exit 1\n fi\n\n JAR=$1\n DATASET=$2\n FOLDS=10\n FILTER=weka.filters.unsupervised.instance.RemoveFolds\n SEED=1\n\n for ((i = 1; i <= $FOLDS; i++))\n do\n   echo \"Generating pair $i/$FOLDS...\"\n\n   OUTFILE=`echo $DATASET | sed s/\"\\.arff\"//g`\n   # train set\n   java -cp $JAR $FILTER -V -N $FOLDS -F $i -S $SEED -i $DATASET -o \"$OUTFILE-train-$i-of-$FOLDS.arff\"\n   # test set\n   java -cp $JAR $FILTER    -N $FOLDS -F $i -S $SEED -i $DATASET -o \"$OUTFILE-test-$i-of-$FOLDS.arff\"\n done\n\n\n\n\nThe script expects two parameters:\n\n\n\n\nthe \nweka.jar\n (or the path to the Weka classes)\n\n\nthe dataset to generate the train/test pairs from \n\n\n\n\nExample:\n\n\n ./folds.sh /some/where/weka.jar /some/where/else/dataset.arff\n\n\n\n\nThis example will create the train/test splits for a 10-fold cross-validation at the same location as the original dataset, i.e., in the directory \n/some/where/else/\n.\n\n\nDownloads\n\n\n\n\nfolds.sh",
            "title": " Generating cross-validation folds (Filter approach)"
        },
        {
            "location": "/generating_cv_folds_filter/#downloads",
            "text": "folds.sh",
            "title": "Downloads"
        },
        {
            "location": "/generating_cv_folds_java/",
            "text": "This article describes how to generate train/test splits for \ncross-validation\n using the Weka API directly. \n\n\nThe following variables are given:\n\n\n Instances data =  ...;   // contains the full dataset we wann create train/test sets from\n int seed = ...;          // the seed for randomizing the data\n int folds = ...;         // the number of folds to generate, >=2\n\n\n\n\nRandomize the data\n\n\nFirst, randomize your data:\n\n\n Random rand = new Random(seed);   // create seeded number generator\n randData = new Instances(data);   // create copy of original data\n randData.randomize(rand);         // randomize data with number generator\n\n\n\n\nIn case your data has a nominal class and you wanna perform stratified cross-validation:\n\n\n randData.stratify(folds);\n\n\n\n\nGenerate the folds\n\n\nSingle run\n\n\nNext thing that we have to do is creating the train and the test set:\n\n\n for (int n = 0; n < folds; n++) {\n   Instances train = randData.trainCV(folds, n, rand);\n   Instances test = randData.testCV(folds, n);\n\n   // further processing, classification, etc.\n   ...\n }\n\n\n\n\nNote:\n\n\n\n\nthe above code is used by the \nweka.filters.supervised.instance.StratifiedRemoveFolds\n filter\n\n\nthe \nweka.classifiers.Evaluation\n class and the Explorer/Experimenter would use this method for obtaining the train set:\n[[code format=\"java\"]]\n Instances train = randData.trainCV(folds, n, rand);\n[[code]]\n\n\n\n\nMultiple runs\n\n\nThe example above only performs one run of a cross-validation. In case you want to run 10 runs of 10-fold cross-validation, use the following loop:\n\n\n Instances data = ...;  // our dataset again, obtained from somewhere\n int runs = 10;\n for (int i = 0; i < runs; i++) {\n   seed = i+1;  // every run gets a new, but defined seed value\n\n   // see: randomize the data\n   ...\n\n   // see: generate the folds\n   ...\n }\n\n\n\n\nSee also\n\n\n\n\nUse Weka in your Java code\n - for general use of the Weka API\n\n\n\n\nDownloads\n\n\n\n\nCrossValidationSingleRun.java\n (\nstable\n, \ndeveloper\n) - simulates a single run of 10-fold cross-validation\n\n\nCrossValidationSingleRunVariant.java\n (\nstable\n, \ndeveloper\n) - simulates a single run of 10-fold cross-validation, but outputs the confusion matrices for each single train/test pair as well.\n\n\nCrossValidationMultipleRuns.java\n (\nstable\n, \ndeveloper\n) - simulates 10 runs of 10-fold cross-validation\n\n\nCrossValidationAddPrediction.java\n (\nstable\n, \ndeveloper\n) - simulates a single run of 10-fold cross-validation, but also adds the classification/distribution/error flag to the test data (uses the \nAddClassification\n filter)",
            "title": " Generating cross-validation folds (Java approach)"
        },
        {
            "location": "/generating_cv_folds_java/#randomize-the-data",
            "text": "First, randomize your data:   Random rand = new Random(seed);   // create seeded number generator\n randData = new Instances(data);   // create copy of original data\n randData.randomize(rand);         // randomize data with number generator  In case your data has a nominal class and you wanna perform stratified cross-validation:   randData.stratify(folds);",
            "title": "Randomize the data"
        },
        {
            "location": "/generating_cv_folds_java/#generate-the-folds",
            "text": "",
            "title": "Generate the folds"
        },
        {
            "location": "/generating_cv_folds_java/#single-run",
            "text": "Next thing that we have to do is creating the train and the test set:   for (int n = 0; n < folds; n++) {\n   Instances train = randData.trainCV(folds, n, rand);\n   Instances test = randData.testCV(folds, n);\n\n   // further processing, classification, etc.\n   ...\n }  Note:   the above code is used by the  weka.filters.supervised.instance.StratifiedRemoveFolds  filter  the  weka.classifiers.Evaluation  class and the Explorer/Experimenter would use this method for obtaining the train set:\n[[code format=\"java\"]]\n Instances train = randData.trainCV(folds, n, rand);\n[[code]]",
            "title": "Single run"
        },
        {
            "location": "/generating_cv_folds_java/#multiple-runs",
            "text": "The example above only performs one run of a cross-validation. In case you want to run 10 runs of 10-fold cross-validation, use the following loop:   Instances data = ...;  // our dataset again, obtained from somewhere\n int runs = 10;\n for (int i = 0; i < runs; i++) {\n   seed = i+1;  // every run gets a new, but defined seed value\n\n   // see: randomize the data\n   ...\n\n   // see: generate the folds\n   ...\n }",
            "title": "Multiple runs"
        },
        {
            "location": "/generating_cv_folds_java/#see-also",
            "text": "Use Weka in your Java code  - for general use of the Weka API",
            "title": "See also"
        },
        {
            "location": "/generating_cv_folds_java/#downloads",
            "text": "CrossValidationSingleRun.java  ( stable ,  developer ) - simulates a single run of 10-fold cross-validation  CrossValidationSingleRunVariant.java  ( stable ,  developer ) - simulates a single run of 10-fold cross-validation, but outputs the confusion matrices for each single train/test pair as well.  CrossValidationMultipleRuns.java  ( stable ,  developer ) - simulates 10 runs of 10-fold cross-validation  CrossValidationAddPrediction.java  ( stable ,  developer ) - simulates a single run of 10-fold cross-validation, but also adds the classification/distribution/error flag to the test data (uses the  AddClassification  filter)",
            "title": "Downloads"
        },
        {
            "location": "/generating_cv_folds/",
            "text": "You have two choices of generating cross-validation folds:\n\n\n\n\nFilter approach\n - uses a \nbash\n script to generate the train/test pairs beforehand\n\n\nJava approach\n - to be used from within your own Java code, creates train/test pairs on the fly",
            "title": " Generating cross-validation folds"
        }
    ]
}